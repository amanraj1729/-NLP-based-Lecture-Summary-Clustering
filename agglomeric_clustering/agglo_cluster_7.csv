SerialNo,Session_Summary,BERT_Feature_0,BERT_Feature_1,BERT_Feature_2,BERT_Feature_3,BERT_Feature_4,BERT_Feature_5,BERT_Feature_6,BERT_Feature_7,BERT_Feature_8,BERT_Feature_9,BERT_Feature_10,BERT_Feature_11,BERT_Feature_12,BERT_Feature_13,BERT_Feature_14,BERT_Feature_15,BERT_Feature_16,BERT_Feature_17,BERT_Feature_18,BERT_Feature_19,BERT_Feature_20,BERT_Feature_21,BERT_Feature_22,BERT_Feature_23,BERT_Feature_24,BERT_Feature_25,BERT_Feature_26,BERT_Feature_27,BERT_Feature_28,BERT_Feature_29,BERT_Feature_30,BERT_Feature_31,BERT_Feature_32,BERT_Feature_33,BERT_Feature_34,BERT_Feature_35,BERT_Feature_36,BERT_Feature_37,BERT_Feature_38,BERT_Feature_39,BERT_Feature_40,BERT_Feature_41,BERT_Feature_42,BERT_Feature_43,BERT_Feature_44,BERT_Feature_45,BERT_Feature_46,BERT_Feature_47,BERT_Feature_48,BERT_Feature_49,BERT_Feature_50,BERT_Feature_51,BERT_Feature_52,BERT_Feature_53,BERT_Feature_54,BERT_Feature_55,BERT_Feature_56,BERT_Feature_57,BERT_Feature_58,BERT_Feature_59,BERT_Feature_60,BERT_Feature_61,BERT_Feature_62,BERT_Feature_63,BERT_Feature_64,BERT_Feature_65,BERT_Feature_66,BERT_Feature_67,BERT_Feature_68,BERT_Feature_69,BERT_Feature_70,BERT_Feature_71,BERT_Feature_72,BERT_Feature_73,BERT_Feature_74,BERT_Feature_75,BERT_Feature_76,BERT_Feature_77,BERT_Feature_78,BERT_Feature_79,BERT_Feature_80,BERT_Feature_81,BERT_Feature_82,BERT_Feature_83,BERT_Feature_84,BERT_Feature_85,BERT_Feature_86,BERT_Feature_87,BERT_Feature_88,BERT_Feature_89,BERT_Feature_90,BERT_Feature_91,BERT_Feature_92,BERT_Feature_93,BERT_Feature_94,BERT_Feature_95,BERT_Feature_96,BERT_Feature_97,BERT_Feature_98,BERT_Feature_99,BERT_Feature_100,BERT_Feature_101,BERT_Feature_102,BERT_Feature_103,BERT_Feature_104,BERT_Feature_105,BERT_Feature_106,BERT_Feature_107,BERT_Feature_108,BERT_Feature_109,BERT_Feature_110,BERT_Feature_111,BERT_Feature_112,BERT_Feature_113,BERT_Feature_114,BERT_Feature_115,BERT_Feature_116,BERT_Feature_117,BERT_Feature_118,BERT_Feature_119,BERT_Feature_120,BERT_Feature_121,BERT_Feature_122,BERT_Feature_123,BERT_Feature_124,BERT_Feature_125,BERT_Feature_126,BERT_Feature_127,BERT_Feature_128,BERT_Feature_129,BERT_Feature_130,BERT_Feature_131,BERT_Feature_132,BERT_Feature_133,BERT_Feature_134,BERT_Feature_135,BERT_Feature_136,BERT_Feature_137,BERT_Feature_138,BERT_Feature_139,BERT_Feature_140,BERT_Feature_141,BERT_Feature_142,BERT_Feature_143,BERT_Feature_144,BERT_Feature_145,BERT_Feature_146,BERT_Feature_147,BERT_Feature_148,BERT_Feature_149,BERT_Feature_150,BERT_Feature_151,BERT_Feature_152,BERT_Feature_153,BERT_Feature_154,BERT_Feature_155,BERT_Feature_156,BERT_Feature_157,BERT_Feature_158,BERT_Feature_159,BERT_Feature_160,BERT_Feature_161,BERT_Feature_162,BERT_Feature_163,BERT_Feature_164,BERT_Feature_165,BERT_Feature_166,BERT_Feature_167,BERT_Feature_168,BERT_Feature_169,BERT_Feature_170,BERT_Feature_171,BERT_Feature_172,BERT_Feature_173,BERT_Feature_174,BERT_Feature_175,BERT_Feature_176,BERT_Feature_177,BERT_Feature_178,BERT_Feature_179,BERT_Feature_180,BERT_Feature_181,BERT_Feature_182,BERT_Feature_183,BERT_Feature_184,BERT_Feature_185,BERT_Feature_186,BERT_Feature_187,BERT_Feature_188,BERT_Feature_189,BERT_Feature_190,BERT_Feature_191,BERT_Feature_192,BERT_Feature_193,BERT_Feature_194,BERT_Feature_195,BERT_Feature_196,BERT_Feature_197,BERT_Feature_198,BERT_Feature_199,BERT_Feature_200,BERT_Feature_201,BERT_Feature_202,BERT_Feature_203,BERT_Feature_204,BERT_Feature_205,BERT_Feature_206,BERT_Feature_207,BERT_Feature_208,BERT_Feature_209,BERT_Feature_210,BERT_Feature_211,BERT_Feature_212,BERT_Feature_213,BERT_Feature_214,BERT_Feature_215,BERT_Feature_216,BERT_Feature_217,BERT_Feature_218,BERT_Feature_219,BERT_Feature_220,BERT_Feature_221,BERT_Feature_222,BERT_Feature_223,BERT_Feature_224,BERT_Feature_225,BERT_Feature_226,BERT_Feature_227,BERT_Feature_228,BERT_Feature_229,BERT_Feature_230,BERT_Feature_231,BERT_Feature_232,BERT_Feature_233,BERT_Feature_234,BERT_Feature_235,BERT_Feature_236,BERT_Feature_237,BERT_Feature_238,BERT_Feature_239,BERT_Feature_240,BERT_Feature_241,BERT_Feature_242,BERT_Feature_243,BERT_Feature_244,BERT_Feature_245,BERT_Feature_246,BERT_Feature_247,BERT_Feature_248,BERT_Feature_249,BERT_Feature_250,BERT_Feature_251,BERT_Feature_252,BERT_Feature_253,BERT_Feature_254,BERT_Feature_255,BERT_Feature_256,BERT_Feature_257,BERT_Feature_258,BERT_Feature_259,BERT_Feature_260,BERT_Feature_261,BERT_Feature_262,BERT_Feature_263,BERT_Feature_264,BERT_Feature_265,BERT_Feature_266,BERT_Feature_267,BERT_Feature_268,BERT_Feature_269,BERT_Feature_270,BERT_Feature_271,BERT_Feature_272,BERT_Feature_273,BERT_Feature_274,BERT_Feature_275,BERT_Feature_276,BERT_Feature_277,BERT_Feature_278,BERT_Feature_279,BERT_Feature_280,BERT_Feature_281,BERT_Feature_282,BERT_Feature_283,BERT_Feature_284,BERT_Feature_285,BERT_Feature_286,BERT_Feature_287,BERT_Feature_288,BERT_Feature_289,BERT_Feature_290,BERT_Feature_291,BERT_Feature_292,BERT_Feature_293,BERT_Feature_294,BERT_Feature_295,BERT_Feature_296,BERT_Feature_297,BERT_Feature_298,BERT_Feature_299,BERT_Feature_300,BERT_Feature_301,BERT_Feature_302,BERT_Feature_303,BERT_Feature_304,BERT_Feature_305,BERT_Feature_306,BERT_Feature_307,BERT_Feature_308,BERT_Feature_309,BERT_Feature_310,BERT_Feature_311,BERT_Feature_312,BERT_Feature_313,BERT_Feature_314,BERT_Feature_315,BERT_Feature_316,BERT_Feature_317,BERT_Feature_318,BERT_Feature_319,BERT_Feature_320,BERT_Feature_321,BERT_Feature_322,BERT_Feature_323,BERT_Feature_324,BERT_Feature_325,BERT_Feature_326,BERT_Feature_327,BERT_Feature_328,BERT_Feature_329,BERT_Feature_330,BERT_Feature_331,BERT_Feature_332,BERT_Feature_333,BERT_Feature_334,BERT_Feature_335,BERT_Feature_336,BERT_Feature_337,BERT_Feature_338,BERT_Feature_339,BERT_Feature_340,BERT_Feature_341,BERT_Feature_342,BERT_Feature_343,BERT_Feature_344,BERT_Feature_345,BERT_Feature_346,BERT_Feature_347,BERT_Feature_348,BERT_Feature_349,BERT_Feature_350,BERT_Feature_351,BERT_Feature_352,BERT_Feature_353,BERT_Feature_354,BERT_Feature_355,BERT_Feature_356,BERT_Feature_357,BERT_Feature_358,BERT_Feature_359,BERT_Feature_360,BERT_Feature_361,BERT_Feature_362,BERT_Feature_363,BERT_Feature_364,BERT_Feature_365,BERT_Feature_366,BERT_Feature_367,BERT_Feature_368,BERT_Feature_369,BERT_Feature_370,BERT_Feature_371,BERT_Feature_372,BERT_Feature_373,BERT_Feature_374,BERT_Feature_375,BERT_Feature_376,BERT_Feature_377,BERT_Feature_378,BERT_Feature_379,BERT_Feature_380,BERT_Feature_381,BERT_Feature_382,BERT_Feature_383,kmeans_cluster,TSNE_1,TSNE_2,agglo_cluster
1,"we started our lecture with a recap of previous lecture particularly about the difference between statistically similar values and statistically significant values. we then started a new topic called as multiple linear regression. as the name suggests, it is a linear regression but dependent upon multiple features. we looked how things like sales depend on multiple factors like age, earning, family size etc. the things on which our y is dependent are known as features. our goal is to represent y as a linear combination of x1, x2, x3, ..., xn. suppose y = b0 + b1x1 + b2x2 + ...... our objective is to find the values of b0, b1, b2 which we do by a method called as gradient descent which is a numerical method very similar to newton raphson method, but used for n dimensions. prof then showed an illustration on excel where the y was dependent on 5 features. we performed mlr and had a look at the p-values of the calculated coefficients. every coefficient had a p-value greater than 0.05 except the intercept which shows that the probability of them being correct is very low. we then started dropping the features one by one which had the highest p-value. there also is a metric known as f value where f = msr/mse. higher f corresponds to better model. as we dropped the features, f value started to increase.",-0.010497359,-0.012882799,0.0032749344,-0.008045133,0.07049808,0.0767078,-0.031465363,0.021003146,0.00786094,-0.037153695,0.015840644,0.017098939,0.0051201913,0.0596227,0.028075352,0.091148816,-0.04537752,0.029607145,-0.06871285,-0.004795666,0.02111154,-0.07867903,-0.0598282,0.031853393,0.050979894,0.02181167,-0.023147583,-0.023076827,-0.043219384,0.04144184,0.083353944,0.045174506,0.013266705,-0.019290082,-0.03028135,-0.014789352,-0.07325286,0.06732146,-0.0477181,-0.009329221,0.0048053907,-0.074123226,0.043248907,-0.005912901,0.11248611,-0.002581669,-0.06607204,-0.028777042,0.015734468,-0.023035677,-0.006623613,-0.013753931,-0.027879095,0.010168706,0.024192266,-0.11421545,-0.0012004111,0.010214649,0.02728281,-0.00893013,0.043963116,0.0068750586,-0.038632806,-0.0011899997,0.058900896,0.070546135,-0.054630753,-0.0783011,-0.101928644,0.0584229,-0.03552339,0.0130103845,-0.02914513,0.0020901638,-0.00043535433,-0.01817787,0.036160804,0.009896552,-0.069672555,0.0024542636,0.049844496,0.07723883,-0.024803504,-0.09146666,0.0032032442,-0.03746551,-0.00017960646,-0.12566733,-0.034310795,0.05161629,0.01825592,-0.034977898,-0.029728452,0.023112418,-0.017743321,0.016085198,-0.011022606,-0.10371407,0.09297354,0.033319723,-0.038319714,0.0052084494,0.05149605,-0.008420343,0.030686567,0.026609546,0.0111544235,-0.019394843,0.075562924,-0.041754358,-0.003150835,-0.03435946,-0.08710955,0.066340916,0.053860173,-0.077106625,-0.036919232,0.017915053,-0.028194198,0.03975686,0.021652415,-0.029781844,0.0782169,0.012625688,-0.0065584145,0.026893243,-0.12152054,6.6980425e-33,-0.018056067,-0.07238229,-0.08028489,0.0073395823,0.00709018,-0.044923313,-0.013157855,0.015335795,0.042612765,0.0731784,-0.042167593,-0.062031455,0.037817433,0.05659159,0.048963536,0.06614731,0.03565239,-0.010315498,0.04236608,0.025545862,-0.03133056,-0.051912446,0.002508654,-0.061875142,-0.05514477,-0.020815125,-0.0066610794,0.045040622,-0.1093572,-0.009140093,0.008597575,0.04164342,-0.04323611,-0.0102712205,-0.027737336,-0.054292038,0.03491815,-0.049135335,0.06817334,0.09312636,-0.058480147,-0.03217291,-0.0271548,-0.00395372,0.009400544,0.06764912,-0.044001237,-0.013154571,-0.07315079,-0.028397385,-0.055783074,0.010764862,-0.030159935,0.00838055,-0.032765556,0.045160398,-0.17441422,-0.05008672,-0.043427456,0.020822724,-0.032274276,-0.011534026,-0.005105192,-0.10218638,-0.0022242023,0.041604303,0.08239273,0.056067582,-0.0035016837,0.034603387,-0.011223203,0.0074317423,0.030905534,-0.14030977,0.12385246,0.035117686,0.11023135,-0.022425722,0.055307787,-0.025256388,-0.01196453,0.10917868,0.05823242,-0.0036402273,-0.036826733,0.06732671,0.013175303,-0.09277116,-0.009248633,-0.007930529,-0.07968283,0.06540364,-0.03214555,-0.006589182,0.052696005,-7.425439e-33,-0.035327185,0.06097799,0.05403264,-0.046256974,-0.0065880883,0.020029655,-0.011547205,-0.1074597,-0.04309645,-0.030703805,-0.0018086018,-0.026418837,-0.013476396,-0.036092527,0.07218339,0.075657465,-0.04258295,-0.00028885115,0.0206562,0.032997474,0.018892583,0.08466402,-0.049976975,-0.020600405,-0.08394997,-0.0076309075,-0.09556295,0.008455617,0.005221608,-0.044454373,-0.02366631,0.034981772,0.006902415,-0.0128999045,0.0154413115,0.07647076,-0.08154017,-0.0635788,0.03085812,0.03751265,0.08429836,0.017584592,-0.006799732,-0.09242315,0.018827949,-0.019526279,0.009512675,0.027285041,0.1451333,0.02134496,0.013813104,0.045785304,-0.06954793,0.059922483,-0.028410383,0.020085564,0.0035994027,0.009423607,0.058510244,0.01675406,-0.053755797,-0.053533036,0.027913155,0.09393272,0.06327533,0.059218757,0.036250014,-0.025374549,-0.01197876,0.009560328,-0.06500887,-0.049019862,0.102680676,-0.029990926,-0.04654801,-0.06994258,-0.08605845,-0.08180578,-0.08538371,0.05521827,0.067845866,-0.006612929,-0.007399213,-0.0012532481,-0.03218545,-0.03203359,0.0061621484,0.053094897,0.08023513,-0.056400258,0.020026289,0.0878914,-0.048021823,-0.026538633,0.09831551,-7.850219e-08,-0.0116517935,0.001553384,-0.056916177,-0.0062926523,0.013911747,-0.0069615976,0.011768611,0.05580302,-0.01386571,-0.015649438,0.009141587,0.02308574,-0.05572429,0.057575505,-0.0059147268,0.0030183767,-0.072933964,-0.018514588,-0.002262926,0.025563227,0.050129753,-0.05531705,0.022693729,-0.01919045,0.008630137,-0.10105979,0.048082557,0.11207511,0.014889346,0.067914136,0.0048229094,0.025037035,-0.023881262,-0.019685442,0.14611699,0.084756695,0.07929587,-0.026298674,0.07363305,0.01733427,-0.04741708,-0.02068654,-0.04126583,0.06867724,0.11401161,-0.018377533,0.034377635,-0.056117166,0.002683327,-0.01350369,0.037964884,0.0491547,0.017636994,0.10088516,0.040270656,-0.006463524,-0.10108398,0.019097177,0.022294551,-0.017700905,-0.010480618,-0.07957122,0.015796166,-0.11914989,5,-8.308279,-6.1448483,7
10,"the lecture began with a recap of confidence intervals. using an example, we analyzed points a and b within the confidence interval range to determine whether they were truly distinct or if their values appeared by chance. another point, d, which was outside the confidence interval, was identified as statistically significant. since the probability of obtaining d from the sample was very low, we concluded that it was statistically different from a and b.  

next, we introduced embedding vectors as a way to process data. we converted data into vectors that included various features, allowing for feature engineering.  

following this, we discussed multiple linear regression (mlr). the objective of mlr is to express a dependent variable (y) as a linear combination of independent features (xâ‚, xâ‚‚, xâ‚ƒ, ...). like simple linear regression, mlr aims to minimize the sum of squared errors to determine the best-fit coefficients for the features in the model.  

the lecture then covered the matrices used in the derivation of mlr and the cost function which helps quantify the error. the gradient descent process was then introduced to optimize the model parameters.  

we also discussed the f-value, calculated as the mean square regression (msr) divided by the mean square error (mse). a higher f-value indicates a better model fit.  

next, we explored the role of the p-value in multiple linear regression. if a feature's p-value is greater than 0.05, it suggests that zero falls within the confidence interval, meaning that the feature is not statistically significant. based on this, we learned that features with high p-values can be dropped from the model, as their presence does not significantly impact the predictions. features with the highest p-values are the least significant and should be removed to improve the model's efficiency.  ",-0.06123843,-0.027368894,0.025860038,0.020446789,0.08004599,0.07292836,-0.021684468,0.043286208,0.026567947,-0.08206949,0.0023150102,0.024677021,0.0832886,-0.044936467,-0.0028762126,0.06804086,0.02212136,0.03869156,-0.09869618,-0.028822638,0.019586638,0.0019650732,-0.06341875,-0.028020188,0.10007702,-0.024251724,-0.009579953,-0.002210524,-0.022183858,0.04407713,0.06762934,0.007569328,0.020763947,-0.031142151,-0.056315824,-0.03674995,-0.030870153,0.053400293,-0.052982323,0.0008580126,-0.06437555,-0.06070022,0.06233733,0.048752055,0.068228364,0.03210298,0.009387311,-0.05738714,-0.052782096,0.055723775,-0.028249662,-0.03139984,-0.10080835,-0.021804368,-0.055367187,-0.09348459,-0.03731413,3.0517056e-05,0.055834576,0.063415796,-0.0016151957,-0.061178934,0.0333807,0.05243014,0.013191598,-0.03647045,-0.060097195,0.011386231,-0.007117312,0.024319394,-0.098688066,0.039948873,-0.10003788,-0.021078909,-0.03127401,0.015784927,0.039803542,0.030401206,0.029087733,0.029823802,0.007160917,0.0988952,0.006954706,-0.04703956,0.07502171,-0.03619674,0.041521873,-0.059011165,-0.069633,0.041917104,0.04008415,-0.035202183,-0.057416037,0.055311333,0.007411186,-0.044746745,-0.016814506,-0.100308865,0.08100692,0.049952988,-0.011284073,0.06631872,0.03894517,-0.06448227,-0.04519196,-0.065083556,0.015885297,-0.013076826,0.079321764,-0.024325842,-0.016717171,-0.01115551,-0.059649657,0.03375312,0.08982956,-0.075603455,-0.04634429,-0.00672624,0.08434561,0.04536919,0.018172015,0.047078043,0.10721271,-0.0035632765,0.022723258,-0.021746637,-0.114369676,3.598734e-33,-0.01339519,-0.0047856634,-0.028283194,0.044717215,0.04499956,-0.0019450663,-0.039111637,0.029817542,0.045292187,0.05975363,-0.02861323,-0.08261101,0.0055131097,0.060812652,0.082548805,0.055255212,0.038657445,-0.0025750462,0.032701574,-0.0033834865,0.026938163,-0.07607491,0.043372735,-0.03518431,-0.031180277,0.008258357,0.082932085,0.008034249,-0.051124226,-0.0034357496,-0.07334245,0.0004594878,-0.032842346,0.004130301,0.016681472,-0.02617172,0.028917333,-0.055709716,0.02055105,0.054358337,0.023461383,-0.004035863,0.03888327,-0.02011903,0.037793897,0.051500406,-0.047350403,-0.010026306,-0.04406605,-0.05597072,-0.09339776,0.010710729,-0.04881832,-0.033287223,-0.018235354,0.031892072,-0.16308282,-0.0226869,-0.076780014,0.019900221,-0.11361047,0.076732635,-0.0045566508,-0.07786645,-0.004018837,0.012349248,0.06258045,0.067949675,0.00019845688,0.047274146,0.0061170193,-0.04205707,0.018913595,-0.12655504,0.01842904,-0.010782726,0.02715938,-0.052314922,0.063872375,-0.003226423,-0.021128202,0.060437232,-0.042743634,-0.06407716,-0.05388044,-0.016926412,0.026800493,-0.081391886,0.015015596,-0.0021190017,-0.06609653,0.053292185,-0.04846798,0.00046324878,0.022655074,-4.655403e-33,0.0004932842,0.04498879,0.06647951,0.03557411,-0.0014475806,-0.005752658,-0.00738275,-0.026618231,0.033050273,-0.018309303,-0.079557255,-0.094220445,0.030035168,-0.06415508,0.0016863323,0.07692857,-0.012319802,-0.051718522,-0.039064176,0.05504011,0.03636631,0.062923506,0.010711818,0.022057941,-0.075399406,0.01662477,-0.004617425,0.05095415,-0.045232892,-0.082855396,-0.020223936,-0.043181654,-0.074758105,-0.0054675066,-0.03862596,0.029587472,-0.040896755,-0.062077068,0.02496452,0.061479617,0.10140253,0.042882852,-0.005357985,-0.08908278,0.007223844,-0.01684369,0.021330463,-0.0044180853,0.09418311,-0.05510689,-0.025116855,0.029294929,-0.08656226,0.055160187,-0.0023753883,-0.037693176,-0.026819294,-0.038602438,0.07238746,0.06729798,-0.014425706,-0.026154555,-0.0020638853,0.029782297,0.036925387,0.04899618,0.026202744,0.04613126,-0.030725596,0.027528558,-0.013747607,-0.030178254,0.070684604,0.003522703,-0.04177871,-0.05334609,-0.017288186,-0.12339166,-0.10772211,0.009006105,0.02861213,-0.05353126,0.021257766,0.024698142,0.0036226036,0.03082994,0.014275707,0.08518214,0.09559674,-0.058421902,-0.02105248,0.075599566,-0.048322503,0.035127856,0.06714628,-6.580681e-08,-0.06178817,0.051505987,0.001726614,-0.035554897,-0.03629942,-0.031969406,0.0102334265,0.07099727,-0.067271255,-0.026537042,0.056630142,-0.011805282,-0.0926908,0.024836443,0.0569258,0.048514742,0.010151087,0.0469765,0.0317807,-0.028996281,0.045348443,-0.039412927,0.027180806,-0.045865837,0.0033077276,-0.09197134,0.022875091,0.149937,0.06768828,0.029905928,-0.06963189,0.050007373,0.034903396,-0.041124254,0.05771371,0.04424502,0.12531084,-0.0756186,0.024661299,-0.0040761866,-0.033936728,0.064948484,-0.01604465,0.038861085,0.116434455,0.016539976,0.053462382,-0.044625387,-0.016323406,-0.012908481,0.083544455,0.026737012,0.03195388,0.16548459,0.057287518,0.06575825,-0.05950147,0.0005595191,-0.043606963,-0.02154448,-0.016661445,-0.019538587,0.0011911334,-0.07147529,5,-11.561274,-5.6769667,7
134,"in confidence intervals, if î²â‚â€™s ci is around zero, it may still be statistically similar to zero, meaning it doesnâ€™t significantly impact y. mlr involves multiple independent variables (xâ‚, xâ‚‚, â€¦ xâ‚–) and requires feature engineering (e.g., embedding vectors or dxâ‚/dt). since thereâ€™s no closed-form solution, we use gradient descent over an n-dimensional hypersurface. model evaluation isnâ€™t just about râ²; we check error metrics (mse, rmse) and p-values to determine statistical significance and guide feature selection.",0.005292262,-0.05607278,0.03247715,0.0058251834,0.031861477,-0.045433816,-0.09371701,0.057214268,0.032072302,-0.031473123,0.017444875,-0.032633595,0.027715977,0.004322378,0.0040940666,0.032574303,0.067182764,-0.012950292,-0.06524602,-0.01989834,0.022017492,0.04421039,-0.07565449,-0.0133154355,-0.001156121,-0.08889176,0.031365804,0.03897606,0.04338858,0.03934012,0.04367491,0.01735662,-0.047423545,-0.032503683,-0.06789818,-0.069981486,0.08075035,-0.016079232,0.031489853,0.035422795,0.030762475,-0.036021333,0.043771513,-0.03511808,0.10559454,-0.007028969,-0.023933602,-0.05360774,-0.043212015,0.032637477,-0.009365827,-0.021385174,-0.048180472,-0.068906255,-0.04538716,-0.0071213157,-0.09119679,-0.0063885264,0.021446286,0.12613079,0.019531531,-0.05320601,0.035593744,0.01410153,0.017620087,-0.029066654,-0.0029775072,-0.082045354,0.016885148,-0.010396245,-0.08068894,0.027380647,-0.10667469,0.03839126,0.004272655,0.055890415,0.008557063,0.010403363,0.016261034,0.04012094,0.027601283,0.067619994,0.010345746,-0.050973732,-0.007219406,-0.043885943,0.035349842,-0.012478629,0.024993604,0.057126902,0.038986117,-0.01703496,-0.10571895,-0.002225843,-0.041685443,0.010759056,0.019188862,-0.069838986,-0.02163965,0.024927782,0.005256122,-0.03414458,-0.06041543,-0.057249818,-0.06483587,-0.020132016,0.13206413,-0.006214501,0.038704038,0.08257951,0.036466274,0.01973226,-0.039556876,0.030728022,0.039466586,0.0019348981,-0.019285047,0.035361134,0.08218432,-0.036046434,-0.054492656,0.031235216,0.09398816,-0.037862565,-0.023845263,-0.025182122,-0.13015552,3.9569877e-33,-0.07367685,0.058269866,0.04060753,0.051474117,-0.03537518,-0.02977499,-0.013018495,-0.032617785,0.03722323,0.044068366,-0.04789707,0.00058379647,-0.027944308,0.044454582,0.04524018,0.075181186,0.09182594,-0.012609145,-0.00952608,0.03186456,0.10552113,-0.08622102,0.03466335,0.018946202,0.011039173,0.032770168,-0.019472066,-0.054731175,-0.052320592,0.015597542,-0.060402226,0.0067529557,-0.045972425,-0.05510051,0.053789657,0.048332695,0.016654458,0.0017224252,0.030057589,0.04660394,-0.055106275,0.060601052,0.14430337,-0.063114926,0.05188363,-0.014753632,-0.018330544,0.017248446,-0.034555983,-0.00637725,-0.06303962,0.033119287,-0.10812862,0.015581252,0.0027262652,0.06280106,-0.062759764,0.008904717,-0.022838105,-0.0045257336,-0.05690393,-0.01286564,-0.044250134,-0.053288586,0.026554925,0.04734306,-0.014518491,-0.029632878,-0.0498226,0.033300065,0.037112977,-0.033981513,0.012377458,-0.09816284,0.054308955,-0.09829764,0.04980121,-0.0350807,0.12750418,0.0352205,-0.049080413,0.115585014,-0.03945433,-0.12476748,0.010986269,-0.08225388,0.019326635,-0.021824675,-0.009115719,-0.094783485,-0.010181805,0.05275426,-0.0059588933,-0.015303213,-0.038569458,-6.761294e-33,-0.019465646,0.004696126,0.11488602,0.019880222,-0.06545845,-0.004113004,0.034421757,0.0077919294,0.021821747,-0.07124669,0.04802536,0.03843767,-0.0058681364,0.0011433188,0.1087526,0.08784671,-0.028230118,-0.05152704,-0.044200253,0.036266647,0.032873776,-0.00393571,0.019081831,0.015660968,-0.08646638,-0.0454999,0.007067442,0.016233232,-0.030701503,-0.033455275,-0.010014061,-0.007174319,-0.05509815,-0.046608407,0.006787686,-0.0064013,-0.037327893,-0.08919599,-0.018003892,0.042375498,0.06857182,0.07242544,-0.03946667,-0.0037687682,-0.048838075,-0.051766764,0.013312978,-0.0029460383,0.06771871,-0.008398683,-0.042080685,-0.0148757715,-0.012590213,0.05808531,0.0083748875,-0.02844378,-0.025139537,-0.05096078,0.00032347682,0.07252143,-0.031356223,0.03307279,-0.04733743,0.06494624,-0.035578515,0.034545626,0.04781823,0.04358051,0.0030691435,0.018037124,0.08460463,-0.0381797,-0.018427454,-0.041573577,-0.063004255,-0.103882656,0.005709553,0.022609293,-0.05882297,-0.057520162,0.017078644,0.03007298,0.040152054,-0.091323465,0.14550528,0.0078096334,0.0305461,0.03516006,-0.011978503,-0.033026017,0.027864505,0.0359623,0.031726215,-0.018690802,-0.02342133,-6.066211e-08,-0.0758457,0.01975042,0.025134012,-0.046569277,-0.04177711,-7.300891e-06,-0.11834115,0.055114605,-0.058815736,0.04390216,0.04515899,-0.060002938,-0.07782044,-0.07218681,0.08510407,0.13648771,0.09477188,0.057180814,0.0021099732,-0.03778257,-0.022425726,-0.048950493,-0.03882694,-0.049351085,0.0010818674,0.016987886,0.005007379,0.08412382,0.028372888,0.006506643,-0.058966078,0.02451538,0.037328344,0.031698354,0.01706041,-0.015857078,0.113876164,-0.05782829,-0.013084457,-0.034838695,-0.046886828,0.024760306,-0.025228173,-0.028560799,0.074307814,0.07439578,0.034073923,-0.055776678,0.011871136,-0.0035748098,-0.020837296,0.047348935,-0.032601103,0.15905751,0.031859525,0.087767474,-0.05351617,0.06162985,-0.06742942,-0.045553133,0.08799504,-0.04406148,-0.09370674,-0.08397701,8,-12.971424,-5.183589,7
195,"in today's lecture, we learnt what is meant by saying that a sample is statistically significant  and statistically equal to zero (when the sample/observation and zero both lie in sane c.i.), also learnt the distinction between statistically significant and 'by chance'. then we shifted to mlr, it deals with more than one independent variable known as features. to analyse texts as data, texts can be embedded in a form of vector (these vector essentially contains features), process used here is known as feature engineering. later on we saw how the mlr dataset looks like and how to operate it on excel. dataset can be written in form of equations which then can be converted into matrices. the objective function for the mlr also remains the same i.e, to minimize the sse. after that, we discussed mlr gradient descent process, like how this method actually works,etc. solvers take an observation and try to minimize the objective function. then we saw a mlr dataset with 1 dependent and 5 independent variables. we removed 3 independent variables because of their high p-values(variable with highest p value was removed first) one by one in order to fit the dataset at its best. we finally look at what f-value is, it is = msr/mse, we cannot inference anything for a particular model from it's own f-value. but we can compare f-values of 2 models and the model with high f-value fits the data better than the other. also, râ² increases with the increase in number of independent variables and vice-versa.",-0.046927772,-0.06176904,-0.02667597,0.015189329,0.060352173,0.0021378389,-0.05211697,0.040405836,0.064307734,-0.017227221,0.007948637,0.046111383,0.030129246,-0.028698865,-0.007285305,0.015552824,0.00549518,-0.0061355345,-0.10033968,-0.05348758,-0.012653928,0.007960421,-0.07769045,0.037027977,0.039163068,0.00036529812,0.028332375,-0.009906021,0.009193393,0.044672336,0.10287953,0.06541677,0.023040658,0.011521873,-0.09096904,0.034181796,-0.03454104,0.027242666,0.0029723311,-0.0014918594,-0.0111495955,-0.10317036,-0.015990958,-0.07052819,0.0694905,0.048811536,-0.034481023,-0.08791778,-0.047695454,0.003846025,-0.124389,-0.0048281075,-0.078325726,-0.03950776,-0.015433609,-0.06255182,0.017348481,-0.00055901235,0.062856264,0.016537044,-0.027166601,-0.06950732,0.0013628615,0.036968224,0.061183084,-0.05910074,-0.0011132518,-0.019049857,0.04000708,-0.04825117,-0.080214195,0.027588787,-0.08566783,0.041867506,-0.038531072,-0.015901677,0.020412168,-0.017681528,0.029286262,0.0185927,0.04874464,0.06607716,0.01226281,0.07940962,0.0023144914,-0.09932109,0.039121356,-0.010510342,0.02608023,0.06458938,-0.018607201,-0.048993256,-0.07162983,0.023291608,0.0074526295,-0.002747702,-0.0016981054,-0.036577243,0.10909017,0.008950595,0.010336702,0.0639269,0.06831522,-0.0139232315,-0.06083542,-0.10883587,0.081741616,-0.023731306,0.0989547,-0.043428738,0.042996783,0.0048575276,-0.046409715,-0.06258863,0.01451654,-0.13653962,-0.05403906,0.014090899,-0.029243499,0.077797815,-0.07269787,0.011880836,0.08142625,-0.034872934,0.05683302,0.056157745,-0.07739963,3.2176844e-33,-0.047623314,-0.029526623,-0.06849265,0.011977076,-0.004653667,-0.028239338,-0.021870345,-0.0010159252,0.08277225,0.043544073,-0.018961351,-0.014938897,-0.006049352,0.07607281,0.08029275,-0.03274664,-0.04282454,0.014695353,-0.019805225,-0.026970688,0.061084192,-0.026432961,0.08054122,-0.048944462,0.018105073,-0.0010087737,0.044691652,-0.07406197,-0.09129491,-0.028898468,-0.032010023,0.045740772,-0.025663843,0.07694722,0.038695786,-0.02767612,0.009175689,-0.08721211,0.08697775,0.07187253,-0.018516615,-0.0021857533,0.064636946,-0.021418994,-0.05117748,0.012609311,-0.051411673,0.014719032,0.0070604715,-0.080544606,-0.042815305,-0.026693225,-0.008451502,0.005383796,0.0003693857,0.097965956,-0.121381156,-0.04233223,-0.05332135,0.05394064,-0.07228904,0.043056495,0.020370262,-0.013924054,0.043258335,-0.009820789,0.054089766,0.040954437,0.023633247,-0.03337115,0.009343543,0.024747718,0.019426368,-0.108232476,0.04444136,0.029767264,0.019487102,0.018323598,0.05474977,0.03391695,-0.012196619,0.06822743,0.017486537,-0.07495776,-0.0691782,0.0540393,0.01498493,-0.12785864,0.0016144033,-0.07443485,-0.044719093,-0.030383747,-0.035947107,-0.017424969,0.012999207,-6.1550923e-33,-0.0600483,0.010504039,0.03426453,0.03156975,-0.008181839,-0.00084332,-0.017460523,-0.044556748,0.026030425,-0.03418568,-0.030269207,-0.048370127,0.032358415,-0.033272382,-0.007033214,0.030306414,-0.03851807,0.0027808559,-0.1024936,0.032213356,-0.00407988,0.10266427,-0.08688862,-0.015791176,0.0028019478,-0.011272218,-0.051530983,0.041987184,0.004684663,0.0025296658,0.0077732736,-0.0019365089,-0.02280003,-0.03910592,-0.025746236,0.022378018,0.02388189,-0.08525525,0.09795436,0.026918197,0.10023814,0.08665862,-0.027052445,-0.034302674,0.044192184,0.018058682,-0.01963412,0.002317027,0.096853964,-0.06652534,0.011547715,0.034385905,-0.04170243,0.050153747,-0.00980427,-0.06263543,0.0055165007,-0.07827621,-0.0030874913,0.032143265,-0.0057101743,-0.025112504,0.046148863,0.065563016,-0.043605935,0.056301057,0.031556502,-0.03421192,-0.036183003,-0.032540064,0.027042868,-0.06330658,0.069585375,-0.000483548,0.0064373906,-0.032540854,-0.048280064,-0.07711928,-0.1266602,-0.04431958,0.07377215,-0.040897463,-0.00016540324,-0.03229449,0.05343056,-0.0049929153,-0.0032787076,-0.06466462,0.008705351,-0.061849892,-0.056777,0.02025005,0.0034969011,0.08584039,-0.037962418,-6.959026e-08,-0.027351586,-0.019831592,-0.008212121,-0.017021257,0.036981933,-0.010894718,-0.02998618,0.067544326,-0.027524536,0.014288438,0.05302596,-0.05798703,-0.12741616,-0.032945756,0.12490783,0.0864923,0.023890698,0.030935721,-0.0024493781,-0.053490676,0.070413,-0.04850845,0.005549875,-0.012070214,0.07416067,-0.059819266,0.011220419,0.08816561,0.057347067,-0.054289512,-0.049476944,0.050759725,0.076049075,0.05082374,0.12395529,0.083099216,0.14323112,-0.046389632,-0.0066899066,-0.008267481,-0.086327925,0.048978146,-0.08863935,0.03461148,0.060025383,0.050154846,-0.027545294,-0.018514832,0.007421836,0.026352255,0.052364346,0.033445302,0.056464333,0.102944896,0.0577759,0.06355033,-0.020178584,0.037049484,-0.0006024645,-0.010173932,0.0029713043,0.030091507,-0.0084037185,-0.12231607,1,-9.57328,-7.696017,7
207,"in today's class, we began with summarizing the learnings from the previous class. for y = b0 + b1x; we saw that b1 is more important than b0 because with b0 being zero, we at least have b1 which gives a slope for the regression. we discussed what statistically same and statistically significant means. we studied what happens when zero is the interval depending on which we can say if two values a and b are distinct or obtained by chance. if zero lies in the interval, then the values obtained are just by chance. as the models are similar to one another in the interval; the values obtained for a and b could have been zero. later, sir showed the class his interesting plot from class data. he used nlp and heatmaps to find the similarity in student's submissions. later, the class proceeded with multiple linear regression. in multiple linear regression, we have more than one independent variable. to extract information from text, we need to create feature out of it. this technique that deals with features is feature engineering. we need to create additional features from the data, because sometimes we may be given data about 'x', but we need information of its derivative. so, we create a feature of its derivative. 
we then studied the mlr gradient descent method and how it correlates the beta values, y-values, x-values and errors. from the data analysis toolpack, we studied the impact of different independent variables, on the f-value, p-value and r-sqaure terms. we notice that some independent variables do not contribute significantly to the regression process and hence we can omit them.",-0.04721254,0.053768914,0.008892823,0.018920176,0.07722617,0.043825123,0.00582924,0.0054289466,0.048531257,-0.036514726,0.039246954,-0.0049705897,0.033126634,0.039467182,0.031482916,0.07958832,0.03005279,-0.0075217513,-0.15696265,-0.03515212,0.097272575,-0.022292092,-0.037069757,0.053938214,0.039139207,0.044027276,-0.021882217,0.0030593355,0.011128147,0.028773472,-0.013248784,0.08390725,-0.046634234,0.018814882,-0.027188031,-0.008229619,0.03636827,0.114766374,-0.023572478,-0.02664412,-0.063403785,-0.071980216,0.07320972,0.042216107,0.10807901,0.012150429,-0.0780048,-0.05887539,0.042568356,-0.019018605,-0.039953876,0.0001836153,-0.08420443,0.009785259,0.011138791,-0.005684778,-0.0014004528,0.02267322,0.008552556,0.002451133,-0.046904463,-0.032280847,-0.000185928,0.0011997636,0.0523299,-0.029153109,-0.07549471,-0.005270699,-0.0073857144,0.10927502,-0.043079603,0.01088145,-0.046499543,0.03515136,0.021429982,-0.0073034447,-0.01589375,0.043043535,-0.017459713,0.025874743,-0.0348464,0.002308758,0.036567423,-0.0039103096,-0.008201048,-0.02876176,0.047615968,-0.037966706,-0.046632502,0.010048906,-0.047062956,-0.023927193,-0.040412467,-0.0075624776,0.005013051,0.010918976,-0.039925262,-0.08413788,0.08428306,0.059886135,-0.032931965,-0.013950143,-0.017391132,-0.016670046,-0.010570522,-0.019010592,0.031803917,-0.046689853,0.07232703,-0.07185519,0.04088636,-0.017278176,-0.11011013,0.042067003,0.056306414,-0.088136,-0.008751167,-0.013759219,-0.063157246,0.012773559,0.02155846,-0.017462034,0.07998132,0.022971816,-0.019897666,0.021838514,-0.039789688,5.9256674e-33,0.061906006,0.0087908795,-0.08223729,0.08996209,0.031341594,-0.031380642,-0.07914543,0.05137689,0.036054935,0.028777871,0.0031691848,0.0046578804,0.074308254,0.045059983,0.029376082,0.031459972,-0.0010877006,-0.016484212,0.07438739,0.024832858,0.017965069,-0.026645403,0.08494019,-0.052069116,-0.021168036,-0.012085576,-0.01711202,-0.026712166,-0.10974139,0.0033505117,-0.01608863,0.077373676,-0.024482433,0.011259842,-0.015542357,-0.006081444,-0.008434494,-0.08516026,0.07058888,0.08020816,-0.029599914,-0.030524552,-0.014735065,-0.07188386,0.023441538,0.08619292,-0.031266306,0.036928657,-0.0575771,-0.08502612,-0.071549214,0.021226326,-0.025482064,-0.054018658,-0.066132165,0.03472058,-0.05842687,-0.071958505,-0.06821283,0.038968373,-0.071584485,0.049084194,-0.034554105,-0.05749858,-0.032600854,0.026929984,0.033932917,0.04035922,0.07578134,-0.060884546,-0.02008302,-0.028574564,-0.04354578,-0.12240456,0.06393748,-0.011649232,0.07221287,-3.746912e-05,0.03949672,0.02140738,0.011671372,-0.07980396,0.059683587,-0.09634211,-0.07653961,0.03260281,0.04950594,-0.094814315,0.011720405,-0.043028165,-0.07347278,-0.0038859607,-0.06054768,0.029316671,0.040419348,-8.692794e-33,-0.028870525,0.08262842,-0.052641176,-0.01376589,-0.002921238,0.027737759,0.008912188,-0.087749094,-0.014236338,-0.035181515,-0.004201103,0.0026843564,-0.014031461,-0.054573096,0.00039807844,0.004274703,-0.0014904309,-0.030896127,-0.05767094,0.07526382,0.041563977,0.09164391,-0.12062223,-0.018071722,-0.051241063,-0.001172059,-0.085300125,0.01752462,-0.08183362,-0.018136598,0.0034201182,-0.019907877,-0.037221268,-0.1209832,-0.052336574,0.06528949,-0.035452046,-0.03990894,0.025794908,0.09120688,0.07124545,0.027195618,-0.04175976,-0.06649855,0.016748322,-0.015265795,0.00082415505,0.06437012,0.05866178,-0.026237108,0.05520174,0.039947964,-0.027754309,0.0048801047,-0.03096511,-0.06526676,0.00052961207,-0.05398583,-0.0138981035,0.09286232,-0.031806998,-0.015345187,0.026810626,0.09651801,0.021509223,0.00141743,0.0075245225,-0.032659706,-0.029218933,-0.0065172655,0.02927422,-0.01277081,8.386026e-07,-0.052528717,-0.04286016,-0.012513238,-0.07378851,-0.07264362,-0.12655771,0.047918726,0.015622344,-0.0042439857,0.025239803,0.04837353,0.009138137,0.014701097,0.03820714,0.08383133,0.124619186,-0.050445877,-0.04543527,0.098603874,-0.1263991,0.028370073,0.06768089,-7.513532e-08,-0.061980836,-0.04151895,-0.021364264,0.017320596,-0.016126134,-0.032309383,-0.031366583,0.05409858,-0.022952477,0.0036842367,-0.0045964434,0.05705478,-0.08782209,0.0026555753,0.053905617,0.034707874,-0.0010154559,-0.01519535,-0.01474651,-0.04197503,0.07167776,-0.13044006,0.012982125,-0.0042617405,0.053225365,-0.07332361,0.027547583,0.07381783,0.07351818,0.07536435,-0.03401714,0.008808682,-0.0060651414,-0.0057405857,0.16361557,0.047467805,0.126702,-0.0044771247,0.046993297,-0.0016916567,-0.040295176,0.007230136,-0.029339269,0.08377561,0.13002653,-0.016460748,-0.017898519,-0.08933085,0.012878804,0.021886302,0.04333342,0.015418439,-0.047107544,0.08700697,-0.012416242,0.0008490899,-0.082036495,0.013925886,-0.014433099,-0.021032682,0.026695738,0.051684413,0.029649185,-0.104656495,1,-9.902792,-5.1149025,7
224,"in today's class first to discuss some topics from the previous class like the confidence interval and what happens if you lies in the confidence interval .then in today's class we started with multiple linear language regression ,mlr which deals with more than one independent variable. then we learn about embedding vector like we do the example of a single photo and then that photo can be divided into many different kinds of features which together are called emitting vector. then we learned that feature engineering is taking basic data and ensuring that these features are represented on the final set we also learned about how these changes with time and the term give me to jack was liberation whose unit is hertz. what mlr is basically is that use the values of all x and corresponding values of their y to create beta(s). then we learned about mlr gradient descent which is when we have k dimensional hyper surface and when we move along the hyper surface we try to find out the minima and after continuously moving we will come to that minima and resonate about that minimum point which will be the minima.
in the later half of the lecture we worked upon some data values in which we had 5 different features - five values of x. we tried various analysis of that like we dropped the feature with highest p value. ideally the p value should be less than 0.05.the f value should be as iarge as possible because it is msr / mse and msr should be as large as possible and mse should be very small. then we dropped other features as well according to their p value and analysed the data.",-0.07826774,-0.09116335,0.0463435,-0.012680114,0.061248038,0.03824657,0.026946392,0.034752827,0.015365588,-0.056887936,0.025148317,0.07011988,0.0206307,-0.018032229,-0.060717486,0.037993465,-0.0048198886,-0.0016715361,-0.16563569,-0.032394506,0.022530157,-0.008831057,-0.08253884,0.041096434,0.06130785,0.025514001,0.013498117,-0.0005566694,0.087231494,-0.045831986,0.069619894,0.06757411,0.05508489,0.020677362,-0.09475553,0.04342198,0.07748595,-0.013463762,-0.021699911,0.04325723,-0.052628674,-0.11519329,0.028255265,-0.013721883,0.05430305,0.023746414,-0.009447196,-0.13231291,-0.06823275,-0.033249248,-0.0063769934,-0.020550068,-0.09656255,-0.04069366,-0.025489101,-0.027792778,0.009057934,0.03482675,0.03316994,-0.008244119,-0.019842742,-0.037830688,-0.00040097226,0.04206613,0.027252434,-0.072314374,0.013210583,0.005157687,0.011050501,-0.010655477,-0.0823682,-0.042398777,-0.07910687,-0.0036021185,0.037160367,-0.046756197,0.10406638,0.03315956,0.044173203,0.019246913,0.041769132,0.07230951,-0.01638132,0.015267709,-0.06268427,-0.027357442,-0.032029428,-0.010962932,0.023064837,-0.015773725,0.0019809562,-0.08755314,-0.1725421,0.046328254,0.05748949,-0.022173788,-0.01660043,-0.048170533,0.13010243,0.03808359,0.014721255,-0.007881971,0.035526212,-0.009164098,-0.054286014,-0.13488658,0.071471795,0.020955253,0.05658883,0.008408272,0.027707122,-0.02080704,-0.036074698,0.026744273,0.0635056,-0.09397573,-0.056172136,0.016547427,0.018671134,-0.0064886543,-0.01431858,0.012434285,0.06743047,0.00096007047,0.017822178,-0.022596244,-0.08785197,4.9026626e-33,-0.032352906,0.02001687,-0.05210348,0.053926248,-0.001971374,-0.0057159862,-0.0036266688,0.04312287,0.09203467,0.03228246,-0.006037982,0.004604237,0.014374772,0.049457032,0.07563563,-0.010288425,-0.028794145,-0.015545469,0.0010152168,-0.051416755,0.09345218,0.029922852,0.05781898,-0.065797366,-0.020400908,0.0019815892,0.02674435,-0.053139012,-0.055779487,0.02513399,-0.06920867,0.059403356,-0.05419609,0.022600068,0.032741047,0.045959532,0.06373168,-0.07284833,-0.018062433,-0.021933675,0.020669255,0.01835121,-0.031114219,-0.031513546,0.008730399,0.0145138195,0.029402627,-0.010817098,-0.045197792,-0.13581899,-0.06783401,-0.044089936,-0.052396055,-0.06736175,0.06863928,0.13768525,-0.09191537,0.026302086,-0.051466692,0.051503923,-0.09595032,0.08517206,0.06300629,-0.033359833,0.03617084,-0.050701585,0.028915517,0.017344365,0.028877333,-0.01063683,-0.003308658,0.03522891,-0.050910678,-0.14612778,0.01567083,0.003303207,0.0665372,0.008735011,0.053974614,0.026178047,-0.033873565,0.027353998,-0.024211304,-0.041446112,-0.087439395,0.020298084,0.09254572,-0.053734917,0.010423534,-0.018711152,-0.03532532,-0.046963654,-0.0042285467,-0.045589592,-0.0068842797,-8.502896e-33,-0.02648191,0.029129568,-0.02354107,0.07073024,0.03112481,-0.01852837,-0.017812513,0.063797556,-0.05087888,-0.016023153,-0.035198584,-0.041364435,-0.0022573848,-0.009679924,-0.021260561,-0.012543119,-0.04757454,-0.065518394,-0.068213485,0.06899382,0.021717282,0.08074901,-0.08113594,-0.009005897,-0.08020673,0.009454026,-0.06665616,0.070746884,-0.022343168,-0.003252052,0.058302738,-0.067630276,-0.0034200158,-0.035319336,-0.032913767,0.023986448,0.010969262,-0.015800586,0.01352902,-0.014908123,0.04391595,-0.006838599,0.009956807,-0.089473106,0.049858026,-0.05558527,-0.011750376,0.016680377,0.06403976,-0.049365956,0.05816312,-0.014470229,-0.044403512,0.045932434,-0.019950893,-0.030439056,0.027457848,-0.074622914,0.04991656,0.08657845,-0.021509804,-0.07544583,-0.013884109,0.069378704,-0.02676038,-0.0009696948,0.016796961,-0.0115453685,-0.028032003,-0.014113873,-0.013512835,-0.02393706,0.06525213,0.010967438,0.014864987,-0.05931092,-0.031483013,-0.069886036,-0.04396704,-0.052343942,-0.021540925,-0.075896025,0.014482626,0.012361679,0.043028563,-0.032074895,0.054584924,0.028034324,0.054953072,-0.11247821,-0.002215483,-0.0022046668,-0.07121556,-0.008565465,0.017399542,-7.23134e-08,-0.08247997,-0.021728277,0.04374986,-0.025248036,0.05631963,0.01505422,0.0020591256,0.027007956,-0.014352637,0.08122659,-0.0041487375,-0.0375614,-0.05913562,0.01687858,0.10055325,0.060081925,0.05043712,0.03513885,0.0031932408,-0.017662285,0.05190171,-0.036193296,0.06456206,-0.08664424,0.029796688,-0.069202796,-0.003665893,0.11884893,0.055732165,-0.05030847,-0.067362495,0.13395323,0.013235765,0.002914229,0.01850902,0.06446835,0.105634846,-0.087664224,-0.0017425744,0.05806225,-0.055531714,0.016777953,-0.052397996,0.0007294198,0.0717365,0.07765678,-0.0037441181,-0.0320088,-0.046908285,0.08205762,0.031013666,0.048317682,0.024918387,0.0842321,0.052646525,0.037160255,-0.017626636,-0.023592949,-0.05889938,0.002818644,-0.014396507,0.053057637,-0.040193472,-0.07656839,1,-9.605956,-7.453098,7
247,"in this class first we did the recap for the p-value and the statistics and later we continued where we left off. the mlr. we focused on how various features get selected. like for the output value depends on how many independent variable. selection of appropriate feature from the data is called feature engineering. suppose for eg we are given a data of vibration wrt time. and we are interested in rate of change of freq instead of vibration. feature engineering deals with ensuring the data contains the rate of change also.

mlr continuation:
  here also like slr our objective is to minimize the square of errors. we use matrix terms to simplify the equations and then take the derivative of the cost function. but since mlr may contain different type of errors. mlr is not considered as a closed form. so we cant fix a soln instead we take a random point (random coefficient  values i mean wrt the cost function graph) and use gradient descent to approach the minima after a lot of iterations. same as newton raphson method.

then we moved onto the analysis. f-stats. the greater the value the better the model performed. it is msr/mse  (variance explained by regression/variance not explained roughly). mse,sse are useful in optimization whereas rmse, mae are relatable and helpful in interpretation, explanation. although the p value is not 0 . coefficient is very much different from 0. but if 0 lies in the 95% confidence interval we are unsure. there is not stability and the coefficient analysis is not statistically significant. 

i still didnt understand the last part of the lecture about dropping the feature with maximum p-value. and how it gave a better results and why did we want to bring one of the feature coeff. p-value close to 0.05.",-0.09090441,-0.008798981,0.027838266,0.018707486,0.031973578,0.0035421455,-0.052300002,0.0908454,0.046236277,0.035500053,-0.004995727,0.057086352,0.022850456,0.0063037216,-0.0053012576,0.010420852,-0.010408311,0.030738804,-0.07497172,-0.043667562,0.011663108,-0.003445649,-0.055322435,0.06373776,-0.033741545,-0.018290346,0.0038313968,0.06302544,0.06682006,0.0020024537,0.08128174,0.03943893,-0.011151412,-0.021725096,-0.075918905,0.037483737,-0.066964485,0.035002377,0.027492093,0.01544672,-0.02532448,-0.08841826,0.0004949012,-0.0369845,0.078661576,-0.025591673,-0.014705647,-0.090764984,-0.02795963,0.026411872,-0.03185163,0.028932381,-0.11125547,-0.106547326,0.026614418,-0.033618994,0.09957447,-0.073759094,0.0123428125,-0.036310736,-0.020564854,-0.09994866,-0.04140772,0.018411843,0.03257344,-0.05539392,0.07433785,-0.046568457,0.014024917,0.0127540575,-0.07008254,0.015644204,-0.06149495,-0.023442121,0.0033286961,-0.016423954,0.06313554,0.06775407,0.013404756,0.026883382,0.0887095,0.032886323,0.007549474,-0.046529755,0.06527097,-0.06033869,0.014957043,-0.053727496,0.052859616,0.0051656766,0.0340501,-0.066668026,-0.15050833,0.047534194,-0.0077098603,0.015832406,-0.019118913,-0.042172123,0.09995131,0.040017087,-0.034897897,0.014213557,0.017223895,-0.07782904,-0.029307455,-0.06934985,0.029185431,0.007827563,0.06254174,0.01645268,0.00526184,0.027500322,-0.073238075,0.02108088,0.06262291,-0.04645558,-0.014319911,0.0030453308,0.016181223,0.06742495,-0.012906473,-0.05142331,0.035083115,0.027298704,0.07134447,0.021727184,-0.12722176,2.694054e-33,-0.07259588,-0.022155872,0.0011426886,-0.06559013,0.0025608765,0.012773765,-0.00066116947,0.016034508,0.084867395,-0.014069901,-0.017464342,-0.042789847,-0.01248829,-0.03317095,0.11326205,-0.05532255,-0.030528381,0.0122441435,-0.04202428,-0.08655596,0.053452183,-0.082293116,0.10494862,-0.054732714,0.009206498,0.0034944997,0.033535734,-0.0016809285,-0.028226824,-0.00035393517,0.001395826,0.012157603,-0.007647951,0.06547628,0.00040764056,-0.027024359,0.0457624,-0.08149979,-0.010992942,-0.04478537,-0.0626815,0.016867349,0.03741517,0.01818489,-0.012799274,-0.0017153863,0.01967035,-0.01828376,-0.023548549,-0.0358827,-0.043961324,-0.02951939,-0.08439571,0.006327834,-0.0029433174,0.018233957,-0.06597915,-0.07030216,-0.07203433,0.08618236,-0.028081357,0.0059437593,0.07792249,-0.10195524,0.079101995,0.021782534,0.05320635,0.0537928,0.007885527,-0.012433536,0.013073084,0.04510157,0.021446984,-0.04559572,0.034170575,0.020774681,0.062499635,0.0011458349,0.027979579,-0.0036615233,-0.05123908,0.111339994,-0.0723786,-0.02967574,-0.06251645,-0.043886583,0.032992516,-0.04291259,0.009680745,-0.05391915,-0.10023992,0.032893874,-0.02253677,0.039701834,0.01395526,-5.6941975e-33,-0.050110523,0.038387377,0.06933562,0.051041894,0.018389512,-0.051837053,-0.033437643,-0.050142128,0.049871217,-0.025863172,-0.088634565,-0.052591152,-0.040190596,-0.046621248,0.011340864,0.045633134,-0.05590693,-0.05634948,-0.032032322,0.06915413,0.032870527,0.12918985,-0.039426174,-0.03432522,-0.076490805,0.012462247,-0.06176259,0.055421077,0.052760717,-0.01917992,-0.03223582,0.011462039,-0.038662538,-0.04574607,-0.024553832,0.06682621,0.063194446,-0.024941633,0.07187223,0.10093126,0.12890793,0.055206597,0.023958521,-0.030925501,0.10646744,0.009896113,-0.02767155,-0.05840501,0.061992995,-0.049984746,0.06755412,-0.02455894,-0.06595587,0.06551492,-0.08521393,-0.009387287,-0.020411544,-0.07621214,-0.059866324,0.079869546,-0.022996886,-0.04394961,-0.060129806,0.07944785,-0.036706056,0.08713687,0.06923668,-0.072680995,0.0003748953,0.04052554,-0.07109558,-0.020696897,0.065608904,0.055434294,0.025587121,-0.07226914,-0.056382664,-0.036369976,-0.025642486,-0.0027492926,0.015992755,-0.04239123,-0.02274606,-0.04280767,-0.04200405,-0.06370233,0.049539387,-0.014284364,0.01897068,-0.10827459,-0.044414867,0.047909837,0.09126515,-0.0060023637,0.0010860108,-6.7966546e-08,-0.06522867,-0.05554575,0.049138237,-0.0024173476,0.09948029,-0.0032763644,-0.058449753,-0.0038604203,-0.049193192,0.031641737,0.031432115,0.00387457,-0.049876623,0.024682848,0.016859654,0.061591107,0.016057154,0.059211135,0.015832577,-0.057226308,0.019158952,-0.026221042,0.005097356,-0.048195895,0.040837474,-0.08670729,0.018976359,0.060103744,0.05639885,-0.011694201,-0.03716154,0.061696343,0.094526984,0.07784143,0.06513038,0.03877324,0.104130074,-0.05566362,-0.027944883,0.12787767,-0.03951245,0.07586574,-0.0616362,-0.004800733,0.088440076,0.073714145,-0.028061997,-0.061426353,-0.01488315,-0.016280219,0.08770974,0.032280177,-0.0048332494,0.043649245,0.011444723,0.045573957,-0.005998942,-0.027502544,-0.024510862,-0.028224707,-0.021451477,-0.027904544,-0.0054798494,-0.04673734,5,-8.579492,-8.145664,7
253,"sir started the classes by discussing about confidence intervals and revisited â€˜area under the curveâ€™. then he talked about statistically significant and statistically similar values.
then he presented the session summary analysis and he was showing us the various methods how he analyses the summaries, one of them was â€˜common bag of words. he then told us that there were 9 suspicious submissions and 2 of them were almost alike.
then he talked a bit about the e1 submissions and then told us that those who are not comfortable with python should make efforts. for this he has uploaded a few things on moodle.
then he started talking about multiple linear regression. he said that before being able to process images or body of text through machine learning first of all, we need to convert them into a vector ïƒ  [x1, x2â€¦, xk].
for example, sales would be a function of the features such as age, earning, location, family size, etc.
then he talked about feature engineering. feature engineering means improving data by choosing or creating useful information to help a machine learning model work better.
then he said that features can be measured in hertz.

 
 
",-0.07990799,-0.0024110414,-0.022320753,0.038085762,0.019670585,-0.030482981,0.023739412,0.05253517,-0.07927608,-0.028068405,-0.007248667,0.11080748,0.075598545,-0.006156055,0.025517989,0.034437787,-0.04657349,-0.024142554,-0.09699649,-0.04219856,0.0904983,-0.0042510396,0.006580523,0.020685924,0.0447964,0.029025579,-0.045423888,-0.0029757877,0.007565434,-0.00594183,0.0011180363,0.083822004,0.03690273,0.04916485,-0.04064069,0.038651247,0.074034266,0.05819639,-0.05998447,-0.039521508,-0.02341531,-0.10134454,0.022240244,0.0347298,0.11915038,-0.023126837,-0.013475168,-0.105817385,0.008557034,-0.05851638,-0.09654556,0.006184063,-0.049492892,-0.0034583933,-0.048446026,0.0136604225,0.03046151,0.054475777,-0.031497523,-0.0300429,-0.061739977,-0.064567626,-0.03011982,0.03499405,0.025764229,0.0090078,-0.07740432,-0.009710593,-0.00752568,0.015002755,-0.06499663,-0.013974782,0.003053183,0.07941225,0.08592309,-0.0633428,0.046271656,0.02120016,0.028908702,-0.0022076357,-0.0518887,-0.025164587,0.037070114,-0.0011184573,-0.025443302,-0.03942246,-0.0016839424,-0.038104046,-0.047309745,-0.006188105,0.038612902,-0.084529765,0.018465722,0.040083345,-0.0031142978,0.041547865,0.020828847,-0.052033108,0.05822547,0.06312981,-0.01679766,-0.041869473,0.010252043,0.056412145,-0.03742034,-0.018068934,0.061928246,-0.041786466,0.0735914,-0.087878495,-0.072845064,-0.050820056,-0.121843144,0.0065273726,0.07307369,-0.036593746,-0.0543458,0.07823881,0.03947081,-0.00096141803,0.03032105,0.070068106,0.047348104,0.04050067,0.002932822,-0.018913584,-0.07830802,1.1525847e-32,-0.0032924686,0.04015682,-0.017882042,0.08857571,-0.016083205,0.00011606787,-0.04611568,0.023505824,0.08395161,-0.002729816,-0.013285827,0.07362219,0.06871212,0.098682195,0.05285305,0.04134237,-0.02213763,0.034539316,0.04347723,-0.014899475,0.083525956,-0.062580705,0.082583,-0.017310996,-0.059706755,0.007025054,0.005408683,-0.015167399,-0.053094223,0.04104257,-0.09183731,0.012822726,-0.019731864,0.011338602,0.017192973,0.005805212,0.021904334,-0.0879335,0.08470312,0.03043602,-0.04440763,0.016291687,0.03795216,-0.059472177,-0.076759644,0.10041779,-0.00241719,0.0067547653,-0.03916879,-0.003628298,-0.059840295,-0.017601741,0.01867796,-0.031817406,0.022420183,0.08516168,-0.002535219,-0.024052233,0.004328001,-0.0039207297,-0.0008519428,0.08369362,0.02823896,-0.058245912,-0.032747485,0.030594913,0.047784857,0.069217995,0.037796646,-0.022537775,-0.07297189,0.027254753,-0.12837522,-0.16461277,0.09071742,-0.018148003,0.054603495,0.015226853,-0.021984465,0.05327981,-0.0030588491,-0.0022444464,0.0005510554,-0.088396534,-0.107939735,0.034044165,0.06250238,-0.06566844,-0.0021005664,0.06142811,-0.056755472,-0.009982191,0.029398018,0.03362269,-0.014183713,-1.2793788e-32,-0.093512796,0.03640942,-0.06985628,0.06072476,-0.013114047,0.017825665,-0.0028504294,-0.02355871,0.047432553,-0.054566853,0.0131223025,-0.006723169,0.0017639644,-0.057971943,-0.038070228,0.024846556,-0.050672036,-0.06886122,-0.02543599,5.0952865e-05,-0.0036746203,0.069253236,-0.07863909,-0.046377353,-0.075884774,-0.03206648,-0.049309447,-0.0065132757,-0.04279869,0.0005027032,-0.0035887486,-0.0025131733,-0.05946461,-0.035515204,0.010457005,0.018561494,-0.038558215,-0.10584934,0.047014464,0.060068753,0.150352,0.02186292,-0.058855522,-0.07962271,-0.042775225,-0.02758508,-0.010165057,0.035973087,0.041731276,-0.0030420022,0.050037634,0.094125606,-0.06290186,-0.05381697,-0.074574195,-0.061499078,0.022163441,-0.016535103,-0.01232391,0.04715115,-0.0802062,-0.016607711,0.0027754728,0.071911305,-0.0127717,-0.076825544,-0.028759405,-0.006113294,-0.042510495,0.0023177501,0.0064867293,-0.03691416,0.030487621,0.020326303,-0.053436503,-0.007093181,-0.043163396,-0.03946102,-0.12564793,-0.0073291515,0.040884137,-0.0024223193,0.040524937,0.0652813,0.012827488,0.016408952,0.06785694,0.0593737,0.057702098,-0.044665784,-0.022198556,0.041848082,-0.072057426,0.081415586,0.061596952,-7.969107e-08,-0.100078665,-0.017927635,0.05869506,0.027788155,-0.015312963,0.009521341,-0.057551417,0.10265745,-0.04566059,0.04536772,0.024326133,0.025384616,-0.07449476,-0.019120311,0.034353323,0.09094638,0.050805487,0.04471974,-0.036608562,-0.040837996,0.14869119,-0.12233243,-0.00975722,-0.03367884,0.029161287,-0.08988701,0.00994648,0.057224438,-0.012536655,-0.031738378,-0.034549374,0.040794607,-0.0023108458,-0.010726446,0.1108541,0.012430444,0.08115627,-0.05635229,0.012455149,0.024286207,-0.08378395,0.0037527732,-0.011961329,0.044921663,0.09510043,0.030881122,0.008896487,-0.074349314,-0.034564205,-0.00548493,0.054384537,0.010612072,-0.0023716765,0.042357583,0.054690283,0.005580205,-0.0021603191,-0.0342343,-0.055472117,0.016322102,-0.01566765,0.030568589,0.001470309,-0.06989967,1,-9.4077215,-4.5887427,7
276,"today in class we continued discussing multiple linear regression (mlr) which is a statistical method used to model the relationship between one dependent variable (y) and multiple independent variables (x1,x2,...,xn). the general equation is y=beta0â€‹+beta1â€‹x1â€‹+beta2â€‹x2â€‹+...+betanâ€‹xnâ€‹+epsilon. the goal is to find the best-fit line (or hyperplane) that minimizes the difference between the predicted and actual values. we showed how it can be derived using simple calculus and how it yields an answer identical to gradient descent. gradient descent is an iterative optimization algorithm used to minimize the cost function, typically mean squared error (mse). j(beta)= sum(beta(xiâ€‹)âˆ’yiâ€‹)**2). several metrics evaluate the performance of a multiple linear regression model such as the r squared, adjusted r squared, condition number and f statistic. we also touched upon backward and forward feature engineering. ",-0.07697506,-0.05063066,-0.0058163665,-0.00063343125,0.035480555,0.032912914,-0.033220198,0.034411043,-0.00060602557,0.0030045619,-0.05217717,0.039740898,0.068601966,0.04638898,-0.047985207,0.04438344,-0.0187916,0.08439999,-0.07499927,-0.047087528,0.029238341,0.009571964,-0.087370254,0.033485424,0.027733894,0.009336043,-0.03838375,0.014592911,-0.0626152,0.05289376,0.050737415,-0.036589816,0.00046653117,0.059084,-0.13109963,-0.009720946,-0.04015206,0.039094,-0.014837132,0.029518621,-0.06215413,-0.04374986,0.025381258,0.0011980416,0.094477065,-0.008612719,-0.0103205945,-0.10304284,0.0019633588,-0.024336897,-0.03081353,-0.035767827,-0.07108352,-0.05994857,0.045125823,0.0004417689,0.025206704,0.035762705,0.011814372,0.061716873,0.014369902,-0.13686462,-0.01639612,-0.014080892,-0.005039238,-0.04886898,0.018537078,-0.02127342,-0.03162267,0.037628908,-0.08499096,0.060423963,-0.08615994,-0.04556127,-0.015883354,-0.0008515851,0.18303649,0.10894489,-0.0045136474,0.019912846,0.01200025,0.09629274,0.006662225,-0.00461506,0.0317411,-0.020351477,-0.006050955,-0.057255924,0.08903802,0.03342901,0.010586722,0.0035348318,-0.12479415,0.044928346,0.01930358,-0.03334363,-0.0011459221,-0.06791778,0.042643927,0.022129348,-0.02845731,0.06349195,0.058227055,-0.029524507,0.06518348,-0.057494994,0.12324734,0.010650256,0.04722992,-0.044611856,-0.017776832,-0.027780345,-0.011857882,0.059023775,0.023525698,-0.07338287,0.025006823,0.024661606,0.029718976,0.06929582,-0.084825195,-0.017499242,0.075707756,0.011931951,0.04096976,-0.019472038,-0.07492918,6.305756e-33,-0.052575827,0.038463503,0.029011883,0.013808417,0.016339967,-0.042167004,-0.07099981,-0.008779549,0.07941061,0.03536118,-0.01149108,-0.06821419,-0.027520712,0.067491345,0.058566682,0.01098455,-0.051793177,-0.02391881,0.024510538,0.023288734,-0.022288868,-0.048768487,0.010912463,-0.08031133,-0.03660894,0.015320989,0.081995465,0.014833522,-0.09758424,0.008889781,0.017314117,-0.01177216,-0.03907158,0.01432013,0.0084196525,-0.008520905,0.010520022,-0.045620933,0.06319532,0.042656872,-0.03672211,0.06615055,0.033545814,-0.028623585,-0.019003935,0.039151262,-0.052856095,0.049346358,-0.011483404,-0.048792087,-0.11551749,0.026951177,-0.0576916,-0.04095463,-0.011555833,0.042087343,-0.11765858,-0.0097092865,-0.04282881,0.057959806,-0.048996143,0.041239522,0.013659598,0.013741195,0.015518202,-0.0077767572,0.054531768,0.03933295,0.0040143663,-0.0026113158,0.009291664,-0.018783981,0.08480344,-0.10274555,0.09975023,-0.03642208,0.07260876,-0.086440966,0.061921846,-0.08488599,-0.09690234,0.13611221,-0.026281765,-0.04592368,-0.029109215,0.028511323,0.02482814,-0.08003556,-0.0041839494,-0.044078857,-0.15232448,0.019866578,-0.025374936,0.033570405,0.0019561737,-7.0142365e-33,-0.0430636,0.013150893,0.046426915,-0.0062733786,-0.0028350416,0.007173715,-0.03473134,-0.036853023,0.036780275,0.024221601,-0.028491765,-0.0056348825,-0.02393793,0.019586705,0.016408123,0.06965299,0.0019874244,-0.03229194,-0.012949959,-0.0458481,0.060792625,0.053161927,-0.041170284,0.005548558,-0.06850049,-0.061153345,-0.009220559,0.11736099,-0.07375119,-0.058038935,-0.0037614948,0.0030501375,-0.014791743,-0.036115605,-0.04521447,0.06767233,-0.054735843,0.001251499,0.047875203,0.09039196,0.05857929,-0.005683655,0.06450273,-0.11844658,0.02753536,0.051040307,0.0029435058,-0.0211043,0.04591809,-0.00856381,0.026115716,-0.016308773,-0.04266625,0.05916283,-0.05295546,-0.030865014,-0.025480848,0.0014876545,0.036880262,-0.0022781126,0.01912078,-0.05479884,-0.03300929,0.12942284,-0.041746765,0.056652386,0.042982843,-0.022094702,0.047891226,0.050543156,0.0009782272,-0.014801154,0.031903792,0.06021537,-0.0038719084,-0.07778839,-0.0020158193,-0.100236185,-0.0626164,-0.012773358,0.10363112,0.021791825,0.036734235,-0.046356563,0.021616306,-0.004351714,0.03758203,0.112464964,-0.0071332688,-0.07442271,-0.044370446,0.047046907,-0.0009332417,-0.08126566,-0.043417394,-6.7957124e-08,-0.071773104,-0.039950922,0.047245488,-0.013317421,-0.015359273,0.04106449,0.021618018,0.06947581,-0.021893376,-0.0482763,0.019476747,0.029352961,-0.05109401,0.046840742,0.025569359,0.05477747,-0.030724805,0.0075472062,0.012928365,0.037020247,-0.027439194,0.018643182,0.0260097,-0.0641739,0.0781369,-0.07940297,0.038641687,0.10111677,0.051324688,-0.019899113,-0.014912878,0.061067563,0.019960573,0.006941259,-0.003083478,0.05458981,0.0850193,-0.03440879,0.05356264,0.048516203,-0.03392586,0.059688054,-0.0059442185,0.015863148,0.106542684,0.024302058,0.032592382,-0.037811227,-0.07002091,-0.077142105,0.12180022,0.06108243,0.031494915,0.05003897,-0.0037705624,-0.04735826,-0.09301043,-0.012826597,-0.024053084,0.06510734,-0.020707322,0.0055833035,0.007016307,-0.067369275,5,-9.392505,-9.223531,7
300,"recap:
emphasis on:
all the values in the ci are not really distinct values

what is the probability of getting d(a value far from the .95 ci)- really small
this is statistically significant

why are we worried about zero
-if beta1 is statistically similar to 0 then we are in trouble -> model not appropriate

if i am getting a non zero value in the ci arund 0 still not good

if i calculate a value if beta1 and it lies in the ci around 0-> still similar to 0
---------
mlr:
so far dealt with one independent variable
mlr- more than 1

relevant examples:
photos- pixels- x1, x2, x3, x4â€¦.
body of text need to be converted to a vector before being processed
â€œembedding vectorâ€
if i want to deal with sales processes- (age, earning, location, family size)- x1,x2, x3â€¦. x_k- features

techniques to select/create features- feature engineering

let us say you want to detect vibrations (x1) which is measured in terms of hertz

we measure the frequency, but it is the rate of change of frequency that we are more interested in

dx1/dt - doesnâ€™t exist as measured value- but we have to calculate it-  this is called feature engineering

in case of slr- we calculated coefficients by minimisation of ei_sq- we had closed form solution
but now we must use numerical solutions for mlr as we dont have a closed form solution for b0 b1 b2â€¦
using...
gradient descent method (it is similar in principle to newton raphson)
â€œn-dimensional hypersurfaceâ€
capturing all equations (yi = b0i + b1ix + â€¦) in a matrix
y = x.b + e -> a very compact notation

e_t.e is divided by 2m for convenience 

the gradient descent process
for all future ml models we learn from now on we wont have a closed form solution, so we will start at a random point

solvers- take an objective function- and try to max or min it
f value- variance by regression/variance by random error
how do we evaluate the value of error metrics like mse, rmse etc- we either do it in the context of its own model or in comparison to another

where are the metrics used-
sse mse- optimisation
rmse mae- interpretation

we need to check if errors are random even after the r^2 values comes out to be good enough

which of the variables are more impactful- which cause more change in the value of y when changed by the same amount

but
check if they are statistically significant- p value- can they be called statistically distinct from 0?

so on the basis of p value we start dropping variables-  â€œfeature selectionâ€
we start at the highest p value and start eliminating until we are satisfied (p value < 0.05)

remember- more the number of variables in consideration- r^2 is bound to increase
",0.0033359486,-0.025948742,-0.02901169,-0.008751994,0.025904233,-0.03396319,0.015517621,-0.0055809906,0.0568119,-0.039494228,0.11018615,-0.07029184,0.058631193,0.0056916736,-0.02084544,-0.040035103,0.090542614,-0.017024381,-0.10507095,0.0028060565,-0.05957861,0.004427268,-0.04005876,-0.017967943,0.04419676,0.0067406124,-0.0054141586,-0.0076747616,0.07584255,0.0071770838,0.044282418,0.11436691,0.012606207,-0.005619939,0.0018835661,-0.061242532,0.04080448,-0.013952817,-0.03217044,0.011613286,0.08435045,-0.021947676,-0.016395861,0.023705248,0.06222089,0.015077017,-0.035374686,-0.02630301,0.015893215,-0.0020539723,-0.020480271,-0.001386808,-0.06461347,0.044389695,0.00987704,-0.01446824,-0.06347705,-0.04928034,0.004571342,-0.04117597,-0.03210231,-0.026462387,0.05611249,-0.009953268,0.046409264,0.028201684,-0.08448867,-0.035342693,-0.015844097,-0.031949755,-0.110819615,0.025936415,-0.11018562,0.073573165,-0.022600183,0.013622428,-0.009045501,0.00668633,0.02068194,0.011249575,-0.006967362,0.032999087,-0.029842172,-0.0865533,0.02547153,0.019099567,0.042897362,0.07130314,0.04254273,0.050746012,-0.04491909,0.060994156,-0.057264425,-0.0067295814,-0.012943218,0.06121878,0.0403948,-0.0524308,0.06372944,0.055479467,-0.05570413,-0.010896307,0.052913964,0.016058328,-0.09033853,-0.019438758,0.011883057,0.02993445,-0.017882245,-0.025368417,-0.005847978,-0.035127718,-0.05957788,0.025385205,0.018641988,-0.07659339,-0.029002454,0.047474287,0.06075864,-0.021013461,-0.004358584,-0.03759713,-0.027133321,0.056526985,-0.079532616,0.02844819,-0.04091964,6.969883e-33,-0.09624278,-0.0062719136,-0.00046109315,0.016579103,-0.015672786,0.065157555,-0.08218902,0.071054846,0.095395796,0.031288065,-0.04768056,0.02816981,-0.025766863,-0.058445685,0.06943031,0.0956026,0.040290702,0.017280687,0.01482906,0.0024355138,0.112838976,-0.041342743,-0.05650511,-0.016106598,0.053525075,-0.015622727,-0.074073635,-0.0469399,-0.07577078,-0.02385654,-0.066619225,0.041918103,0.08168513,-0.022892425,-0.021596283,0.06135519,0.03225783,0.011241309,0.004228007,0.022927415,-0.08354479,-0.010896109,-0.0074561792,-0.027980128,-0.018022874,0.0473186,0.04659765,-0.061923496,-0.07224736,-0.073674574,-0.006826087,0.031201938,-0.0435892,0.0025492099,-0.060222354,0.037640583,-0.033835717,-0.07401858,-0.08047472,-0.033157818,-0.10952316,0.051519662,0.022050269,-0.06155459,0.04650241,0.020262508,0.008396479,-0.02375947,0.01808221,-0.039394777,0.005688206,0.04513879,-0.03819564,-0.15671101,0.081844054,-0.02504989,0.028043987,-0.056617916,0.059517432,0.033694625,-0.033221137,0.0542607,-0.018777465,-0.0348112,0.04341223,0.021009875,0.067993104,-0.006884201,-0.084586546,-0.06300607,0.030913895,0.029489927,-0.00918648,-0.012817988,0.03643811,-7.41875e-33,-0.09102998,0.012100976,0.0660209,0.027271233,-0.014868134,0.050080787,0.043550488,-0.077654995,0.06746183,-0.09733241,0.012901933,-0.025785444,-0.097321205,-0.03454689,-0.030249415,0.07699836,-0.03248797,0.0058161584,-0.018405598,0.13079794,0.014614305,0.094942994,-0.1310874,0.040820856,-0.096829645,0.045684252,0.0021281391,0.0012031586,0.037697647,-0.055821117,-0.06788788,0.025247192,0.017709058,-0.05021585,-0.03241033,-0.06307883,-0.051605552,-0.049386527,0.064171925,0.08306818,0.07563393,0.06007945,-0.1365407,0.020486645,-0.0010901238,-0.034599848,0.06961106,-0.012222757,0.08519194,0.0068912874,0.043126293,0.086688764,-0.09349674,0.1350109,-0.007577583,0.06405145,-0.040776785,0.0075480035,-0.007390479,0.060224686,-0.009484359,-0.02761906,0.0051202457,0.0328135,0.0019640154,-0.006850143,0.05330348,0.058946118,0.012760445,-0.04210271,0.05773693,-0.016476922,0.029969314,-0.104875036,-0.036237728,-0.07934765,0.023820583,-0.03512763,-0.008284819,0.01989411,-0.055437107,0.09451911,0.0062009646,0.033677585,0.035577994,0.018810228,0.020073725,-0.024409698,0.041944116,-0.0341464,-0.04448635,0.09122408,-0.026487999,0.027351338,0.013735199,-7.209657e-08,-0.040685374,-0.018158197,-0.045718346,0.041181378,0.016478574,0.023848137,-0.013311492,-0.039149325,-0.062237978,0.01439868,-0.011893046,-0.093315266,-0.09371456,-0.06168487,0.07392967,0.07820771,0.04314905,0.016800728,-0.0101825055,-0.009860491,0.014035953,0.012675002,-0.049556524,-0.027267218,-0.07275257,-0.06483137,0.027573038,0.03540812,0.09135581,-0.01801255,-0.017796904,-0.0028988123,0.06007195,0.05878969,0.07637946,-0.032845106,0.0775898,-0.004015213,0.031551905,-0.029151207,-0.010942131,-0.0027047487,-0.060235806,0.041316975,0.02156158,0.0050079166,0.107598014,-0.09299238,0.054561596,0.02936475,0.029764237,0.07915277,-0.05772541,0.039722428,0.014097552,0.05894663,-0.0131930625,0.09966451,-0.065401636,0.021412348,0.03799191,-0.10439166,-0.09903121,-0.038398392,1,-13.28675,-6.4671164,7
306,"in todayâ€™s session, we covered key statistical concepts and techniques used in data analysis and machine learning:

95% confidence interval & interpretation â€“ we discussed the 95% confidence interval for î²â‚ (beta1), which helps us understand the range in which the true value of a predictorâ€™s coefficient is likely to fall, with 95% confidence. this means if we repeat the sampling process multiple times, 95 out of 100 times, the coefficient will lie within this interval.

multiple linear regression â€“ this technique extends simple linear regression by using multiple independent variables to predict a dependent variable. it allows us to analyze the combined effect of multiple factors on an outcome, making it more powerful for real-world data analysis.

feature engineering â€“ we learned how to improve model performance by transforming raw data into meaningful features. this includes techniques like encoding categorical variables, scaling numerical data, and creating interaction terms to enhance predictive accuracy.",-0.017669091,0.015037301,0.03854577,0.07434958,0.07137956,0.036436025,-0.00900979,0.016605526,-0.02233697,-0.050641213,-0.01706623,-0.018235946,0.0034695796,-0.01565679,0.02489828,0.019793503,0.0037682098,0.06393275,-0.1101731,-0.08039309,0.055979107,-0.026373724,-0.016809363,-0.002040901,0.020475967,-0.038630378,-0.031152243,-0.01138168,-0.0019085867,0.006503311,0.004894471,0.054205995,-0.009967044,-0.028218446,-0.13246751,-0.061022013,0.008821454,0.015338116,-0.020006442,-0.017021207,-0.02637351,-0.05876846,0.06690152,0.04881121,0.12499363,0.02166677,-0.009845578,-0.0027873272,-0.017013025,0.04559162,-0.08625482,-0.040435363,0.02248167,0.0030719459,-0.046289816,-0.06956528,-0.0454957,0.016689386,0.055311747,0.045586847,0.024968512,-0.019105215,-0.016232442,0.03762318,0.037712343,0.00037117628,-0.06539511,-0.028478047,-0.053560566,0.012538991,-0.047656164,0.007825049,-0.087938756,-0.0053969,0.02822227,0.036151674,0.007894828,0.015428757,0.041567042,0.08264118,-0.023503313,0.067232646,0.023516344,-0.048878193,0.026236285,-0.01525084,0.01951596,-0.01877604,-0.06534279,0.08064167,0.028549114,-0.043968834,0.06719016,0.015076747,-0.021384193,0.017475357,-0.0052196463,-0.14506269,0.04289723,0.04253644,-0.018763175,0.04697594,0.006361656,-0.01871778,-0.026527291,-0.062728696,0.0761348,-0.033439077,0.044341106,0.020644527,-0.0366171,-0.002927077,-0.050053578,-0.018754337,0.088139325,-0.03293504,-0.02824772,0.059056908,0.056500025,0.0069143614,0.023612304,0.034435783,0.06725244,-0.006503014,0.009136026,-0.010242608,-0.1171815,6.2721455e-33,0.016643489,-0.03378797,0.0013013178,0.11527593,-0.010519804,-0.08597136,-0.12100488,-0.007248771,0.014923745,0.045575444,0.018743038,-0.016512336,-0.005856242,0.09557094,0.037649073,0.088588536,0.026300568,0.04920437,0.0369913,0.07250804,0.038664628,-0.043638244,0.005483765,-0.0126923155,0.029715808,0.038757738,0.012607427,0.022125077,-0.034384,0.027794907,-0.050221,0.03358575,-0.05194178,-0.018771715,-0.030139498,-0.010254513,-0.0070499913,-0.04568964,0.056975864,0.123178564,-0.027582383,-0.030814383,0.048176747,-0.0069427243,0.016395165,-0.013214772,0.008361478,-0.010683283,-0.09648213,0.02370968,-0.05841048,0.044412512,-0.0585464,0.027588367,-0.05394287,0.09213655,-0.04718517,-0.076743245,-0.066046074,0.006158621,-0.11820577,-0.00077225384,-0.035447937,-0.08307238,-0.016879605,0.029137954,0.039317336,0.039130107,-0.005092773,0.050940774,-0.06415752,-0.08208835,0.029325524,-0.083960846,0.0125944745,-0.026077619,0.07077394,-0.050745547,0.045764204,0.014770156,-0.07826253,0.051111933,-0.010261915,-0.09139032,0.03808996,0.0028708081,0.017375132,-0.07918573,-0.059369538,-0.083384946,-0.07963011,0.09053233,0.043790337,0.003539401,0.025547246,-6.638759e-33,-0.0063332254,-0.021099878,0.024805313,0.005947982,-0.0022263138,-0.04349603,0.007224067,-0.05488846,0.024877954,-0.074576356,-0.025888652,-0.041802086,0.04416882,-0.023131222,0.04438893,0.03269789,-0.028885547,-0.018908061,-0.013170158,0.02963947,0.015604355,0.00048580574,0.060850665,0.0030233834,-0.124538645,-0.034709692,-0.048617948,0.03559526,-0.007431191,-0.064173914,-0.090978004,-0.059005834,-0.013650374,-0.04836047,-0.00011029299,0.010005636,-0.03320162,-0.09411678,0.02542588,0.07354805,0.12806292,0.038421154,-0.04703661,-0.022140417,-0.023807352,0.006412455,0.02650117,-0.014007225,0.008986018,0.01023235,0.034011606,0.03777066,-0.090270944,0.023233479,0.012582589,-0.0665678,0.054631725,-0.04654997,0.06778712,0.05031329,-0.02452632,0.018859966,0.028420497,0.091231085,0.01329994,-0.026022,-0.00888378,-0.018176418,-0.03660344,0.02717408,-0.017274322,-0.021552162,0.04013316,0.020379508,-0.078918695,-0.1267455,-0.03811896,-0.09046067,-0.061092958,0.04642673,0.022183888,-0.016011626,0.04092312,-0.042100906,0.040946715,0.05262429,0.034831047,0.15234919,0.08110866,-0.02587532,-0.058417685,0.093715325,-0.08913306,0.07031735,0.01280824,-5.8016884e-08,-0.011111302,0.06956016,0.035389118,0.033757366,-0.025211748,-0.021280566,-0.032070786,0.08501768,-0.062732466,0.0070736744,0.10069672,0.016425898,-0.0419742,0.010930321,0.117759,0.06333721,0.0076124524,0.020833382,-0.019376876,-0.03086742,0.09301561,-0.022234328,0.0016009125,-0.06709475,0.024892721,-0.07658385,0.013499549,0.14547017,-0.026929323,0.04410741,-0.06423299,-0.0031316977,-0.012049001,-0.028123373,0.026663397,-0.0012699488,0.11917864,-0.08753549,0.05043669,-0.010234329,-0.031461306,0.06963095,-0.05567469,0.075777195,0.013333089,0.013179913,0.082694255,-0.038404126,0.041481886,-0.0019554691,0.019533146,0.021050727,-0.009058522,0.14943106,0.07006895,0.10781667,-0.069083296,0.04710848,-0.011590926,-0.008016297,0.032796808,-0.07284063,-0.026532415,-0.070013,5,-12.189144,-4.672542,7
312,"we learned multiple linear regression where there are more that one features. feature selection is important for better models and can be done using domain knowledge.
error metric like rmse and mae have same unit as data so they can be interpreted easily while others like mse are helpful in optimisation.
mlr do have a closed form solution but it is expensive to calculate so we use techniques like gradient descent to find approximate solutions to the precision we want. we start from random solution and make updates to the solution.",-0.014594982,-0.103763275,0.065113954,0.028974716,0.095233075,0.020893356,-0.06275172,0.019152911,0.00816463,0.0039534746,-0.032117072,-0.010816348,-0.034751,0.05012364,0.019018998,0.041724622,-0.03758564,0.01313591,-0.056665454,-0.074794084,0.0031767623,0.04578373,-0.07942857,0.0508894,0.049845,-0.040184464,0.02251719,-0.011926915,-0.030569049,-0.0080353385,0.10717969,0.039314236,-0.0028390395,-0.021059511,-0.122178264,0.006193873,-0.017186819,-0.0043675476,-0.0049315365,-0.04332244,-0.031923264,-0.07307677,0.015486515,-0.041186348,0.07228172,-0.006718713,-0.06867818,-0.08292534,0.08062985,0.030997252,-0.044625543,-0.0806035,-0.04406736,-0.055759493,0.04266119,-0.04494042,-0.0052041435,0.0014766125,0.033391118,-0.02298053,0.030264178,-0.030448122,-0.051281285,-0.017906716,0.06735508,-0.0036156846,-0.05056876,-0.03500114,-0.04738436,-0.018607322,-0.06767984,-0.015354937,-0.057162285,0.048880856,-0.0030383007,0.014759521,0.04569597,-0.017787786,0.060521435,0.05111428,0.0036783083,0.0718336,-0.0012474548,0.007422925,0.060009036,-0.05356738,0.0033718895,-0.03243327,0.022043059,-0.026085997,0.051402196,-0.04417179,-0.043405075,0.015011073,-0.02419073,0.01672839,-0.013970712,-0.039134715,0.025369553,0.0023856657,-0.026847115,0.033571247,0.047033936,-0.018116934,0.021153834,-0.03602779,0.118875846,0.038071398,0.09202805,-0.060697727,0.011981467,-0.010184341,-0.010581593,0.021253034,0.042550713,-0.04311374,-0.026261177,-0.0021089138,0.016603565,0.094209746,-0.066288315,-0.0016825258,0.102337964,-0.01615582,0.0978532,0.041471086,-0.10058277,6.755561e-33,-0.048524708,0.03606015,-0.027840087,-0.014669042,0.0016715921,-0.0065210885,-0.04254039,0.022299921,0.08890556,0.0027098835,-0.023421118,-0.017198948,-0.020261291,0.036943458,0.08282971,-0.022746604,-0.017733783,0.08111793,-0.022458192,0.00044895773,0.04284815,-0.039237645,0.036554843,-0.073864505,-0.010318061,-0.023409592,0.036867067,0.012062503,-0.011744207,-0.015557252,-0.0057787565,0.03951181,-0.032913134,0.098537184,0.015696637,0.015794134,-0.021452796,-0.0475572,0.0099965045,0.013726336,-0.016634118,0.003702782,0.00970052,-0.011843754,0.043889582,0.0504364,-0.06712324,-0.06759264,-0.005082354,-0.03497026,-0.014948501,-0.014401692,-0.13064732,0.015838346,0.005462951,0.06839512,-0.12042655,-0.048227783,-0.028884616,-0.014156791,-0.056823503,-0.048958883,0.008713688,-0.0910156,0.011508493,-0.006358609,0.12299593,0.061477143,0.016966332,0.0043736617,-0.035986047,-0.05605616,0.008744938,-0.07523235,0.006746229,-0.04452838,0.086084194,-0.0503052,0.09259653,-0.04274228,-0.036014806,0.044628307,-0.0558738,-0.050490312,0.019670736,0.023854587,0.031875648,-0.05032793,-0.036035538,-0.024798267,-0.093006335,0.044338915,-0.027102008,0.059923567,-0.007586995,-7.210318e-33,-0.051908974,-0.007762864,0.070736416,0.068552345,-0.015269731,-0.029548598,-0.032927055,-0.080296114,-0.03468077,-0.041679405,-0.0057032234,-0.053507585,0.025978088,-0.0039103297,0.027090792,0.04893374,-0.027530516,-0.04429375,0.028989905,0.065669425,-0.019843534,0.031831723,-0.032262247,0.02646011,-0.04950323,-0.023580864,-0.13639455,0.0503399,-0.03912299,-0.037036974,0.023653666,0.0034453927,0.012572962,-0.0676137,0.030999014,0.040543403,0.034608897,-0.055053614,0.069639206,0.116705865,0.07352544,0.014370441,0.00173775,-0.0369491,0.0070070527,0.031075781,0.005057872,-0.015624625,0.07241276,-0.030805208,0.029664336,-0.060652375,-0.07032737,-0.016774194,-0.06757625,0.0037764811,0.012733364,0.013347167,0.0588367,0.06763227,-0.028133849,0.018422578,0.03793314,0.07090888,0.02124851,0.060785998,0.043838784,0.022795487,-0.113212846,0.018521942,-0.0607021,-0.07822588,0.025082454,0.055295657,-0.03862295,-0.077685475,-0.040745504,-0.09776421,-0.07605896,-0.05008777,0.04059172,-0.014778879,0.0025907261,-0.018723277,-0.010104141,0.0048686373,0.086262986,-0.027695207,0.04177544,-0.1030817,-0.06887002,0.04968015,0.03970457,-0.011362319,-0.010865552,-5.1557457e-08,-0.055250682,0.016572911,0.061114796,-0.063519105,-0.013884483,-0.016993163,-0.059920244,0.16459768,-0.010765092,0.043635916,0.02232138,-0.0034478656,-0.13778964,0.05972586,0.12633413,0.01841843,-0.045410868,0.09980607,-0.0023974243,-0.034288306,0.11187382,-0.023347331,0.0745321,-0.084662616,0.040444102,-0.05844007,0.019859234,0.1317702,0.02180048,0.036637314,-0.013446093,0.06095683,0.09679219,0.018373314,0.11530531,0.06762714,0.10506089,-0.07315298,-0.000236483,0.037241008,-0.035212856,0.06317502,-0.07067289,-0.019426197,0.04179486,0.029944405,0.044860877,-0.048520498,0.006095876,-0.002432183,0.09378979,-0.046554063,0.06592848,0.08664837,0.09832467,-0.01267947,-0.06286895,0.0016734266,0.054776214,-0.019525118,-0.015793629,-0.06352003,0.0030405496,-0.046326976,5,-6.901639,-7.198421,7
344,"gradient descent and newton raphson similarity, concept of multi collinearity causing the matrix  (x^t) x to become non invertible! (while performing mlr, y = (y1 ... ym)^t, x matrix = (1 x1... xk) where xi = (x1i ... xmi)^t and first column of x = (1... 1)^t, î² = (î²1 ... î²k)^t, îµ = (e1... em)^t, y = xî² + îµ. here if any of the xi's are dependent on some xj,... xn (j, j+1,... n âˆˆ {1,...k} then the matrix (x^t)x becomes non invertible since rank((x^t)x) <= min{rank(x), rank((x^t))} and since rank(x) = rank((x^t)) and their columns not being linearly independent, rank(x) < min{m, k+1} and thus the rank((x^t)x) < min{m, k+1}. the square matrix (x^t)x is a square matrix with m rows and columns so it could possibly be invertible but since it's not full rank, it's not invertible). the non invertibility of (x^t)x means the matrix p = x((x^t)x)â¯â¹x^t does not exist (the matrix is p is the projection matrix that orthogonally projects any vector b onto the column space of a if ax = b can't be solved and thus an orthogonal project is required for the best approximation of x, ax = b' where b' = pb) and thus the î²_hat can't be obtained for mlr. training of an ml model - while training ml model using a sample from a population, we shouldn't use entire sample but split it into 80%(training data) - 20%(test data) ratio where training data is used to train the model and the test data is used to test the model. two metrics are obtained ie. râ² for both training and test data where training râ² describes the goodness of fit of model however it being high doesn't indicate a good predictor model as it could be a case of over fitting and thus râ² of test data needs to be considered. adjusted râ² which keeps changing as more variables are added. the best for curve obtained through mlr might not be a straight line as linearity just refers to highest degree of labels in the equation. if the errors are normally distributed, they lie close to the a straight line ina q-q plot. introduced to ols algorithm, aic, bic, skewness and kurtosis, jarque bera test and durbin watson test. ",-0.01322077,-0.14093986,-0.07910885,-0.03754733,0.11673255,-0.04776459,-0.011270841,-0.06422979,0.033778243,0.00867858,0.10421629,0.0675964,0.03041256,-0.016281297,-0.065393336,0.042749133,-0.0026406143,-0.022337275,-0.016656315,0.007493238,-0.045150723,-0.037380423,-0.040617336,0.044074237,-0.020870576,-0.00035960606,0.021841235,-1.2248592e-05,-0.0381074,0.054454587,0.026251934,0.011067775,0.023784127,0.004017093,-0.06107824,0.06761415,0.077877186,0.030435551,0.0233035,0.006556683,-0.036949188,-0.017054874,0.05359513,-0.08152261,0.11660284,-0.06834663,-0.06045518,-0.10179829,0.00061969704,0.026011927,-0.02962562,-0.017911907,-0.053938966,-0.02277166,0.031458825,-0.08549279,-0.035509113,-0.0065241195,-0.013144805,0.057044517,0.048426736,-0.0013947187,0.00079164974,0.003192768,0.015248095,-0.005896596,0.0024406114,-0.0024609512,-0.034326676,0.044875845,-0.014499158,-0.069338426,-0.059659258,-0.03716015,-0.0155706685,0.026805246,0.08054981,-0.0057943417,0.018119352,0.022223653,0.043739546,0.079546064,0.016557403,-0.033442322,-0.06353203,-0.05636892,0.010535919,-0.08552315,0.07958958,0.039227594,-0.0006602431,0.07157332,-0.0813707,-0.03442858,0.010362623,-0.09993303,0.01291918,0.0139815,0.0011028092,0.046774726,-0.016132915,0.0012705534,-0.035728298,0.07169502,0.010477631,-0.010363164,0.0801702,0.04562775,-0.0059533124,-0.053140115,0.026231,-0.01575958,-0.006806039,0.08959176,-0.013290931,-0.030959014,0.05697485,-0.013617138,-0.006062272,0.046265785,-0.07688992,-0.022389494,0.04334655,0.04037902,0.01250366,0.01775888,-0.048038047,8.3773904e-33,-0.05206103,0.07510406,0.0674547,-0.08083376,-0.022120945,-0.059437,0.058043003,-0.04003718,0.008829888,0.0064001,-0.049247775,0.01898807,-0.00050942943,0.048394833,-0.08452306,-0.012932127,0.03939715,-0.02287249,-0.031991236,-0.047546353,0.055742804,-0.036390282,-0.045735415,-0.12996894,-0.06791921,-0.037955955,0.039409116,-0.04109689,-0.05104471,-0.00621424,0.0049947663,0.0019899625,-0.039209098,0.04929133,-0.048876837,0.05194662,0.08963656,-0.036695465,0.09508257,0.025738522,-0.039924204,0.010893545,0.033608362,-0.070603624,-0.056634914,0.027143719,0.022504373,0.017497692,0.015416414,-0.04300491,-0.050361097,0.031918965,-0.026725322,-0.018322935,0.061843086,0.006403036,-0.0068736277,-0.008887058,0.044822134,0.03130757,-0.037806112,-0.09974163,-0.039331928,-0.066067524,-0.025829114,0.0055582025,0.011445416,-0.0774923,0.021335755,0.053773113,0.007460155,0.010969691,-0.0019023429,-0.082172655,0.12692969,0.009767681,0.018206658,-0.036182776,0.0033145465,0.026752733,-0.10393784,0.07797822,-0.010709072,-0.052026037,-0.047041386,0.0014309104,0.058267895,-0.10984448,-0.032940123,0.052223954,-0.054967716,0.028817125,-0.06609053,0.0025074044,0.07114527,-7.948408e-33,0.030111503,0.005359003,-0.03274739,-0.060210958,-0.01895373,-0.022677863,-0.028889066,0.01879897,-0.05692641,-0.048696265,0.03653718,-0.01400997,-0.041715324,0.007872639,0.05571926,0.09173901,0.005775141,0.14786744,-0.062195953,-0.037183225,0.013866084,0.16862115,0.042710066,-0.01581397,-0.073540345,-0.020890657,-0.029247627,0.09979036,-0.00593313,0.0063878424,-0.023621738,0.027354779,0.017408011,0.023286596,0.031025039,0.08234264,-0.105010025,-0.081365235,-0.030676872,0.061124396,-0.007991378,0.03812452,0.0120773325,-0.037844975,0.039684705,-0.013694492,-0.038856175,0.04520487,0.09853205,0.03298914,-0.028519446,-0.019746218,0.031546135,0.07440834,-0.049309187,0.037041485,-0.014902836,0.07928031,-0.0041387174,-0.010919921,0.0028472883,-0.00708013,-0.02050992,0.02493616,0.06320973,-0.035496145,0.047733963,0.03585957,0.07430227,0.04853768,-0.041452095,-0.02281863,0.0077827713,-0.07064967,-0.016423915,-0.04464407,-0.077419564,0.01470162,-0.039508075,-0.10720001,0.14563675,0.102535956,0.0527687,0.06622607,-0.031477235,0.07887979,0.11283049,0.0268781,-0.0006712127,-0.05751649,0.06953925,-0.03347082,0.0518449,-0.04731467,0.03537802,-6.419508e-08,-0.09256427,-0.054401264,0.0014528243,0.018980501,-0.046114434,-0.0067438846,0.0029664522,-0.043563746,0.06400271,0.003481214,0.0557017,-0.021256646,-0.02064158,-0.034041353,-0.081311435,-0.044242933,-0.1206908,-0.016913246,0.009361671,0.0014487327,-0.08790725,0.025215935,-0.005932057,0.009005874,0.028537715,0.01035931,-0.0060996404,-0.03985325,0.0681752,0.040742137,-0.054918054,-0.019179804,0.050521336,0.03975883,-0.010033651,0.10975645,0.04468568,-0.034200057,0.033244766,-0.02649678,-0.048271485,-0.017168147,-0.030390205,-0.0208164,0.105783515,0.060027346,0.018957805,-0.04157621,-0.028870342,-0.011744404,-0.03439368,0.035408307,0.070678875,0.021844836,0.08103414,-0.03347039,-0.12945794,0.051853526,0.104497194,0.066961646,0.064589545,0.038343348,0.023735618,-0.025579717,5,-9.175683,-10.302006,7
369,"we started today's lecture with a recap about what happened in the last class and a bit of discussion about the data analysis done by the professor regarding the responses he was getting as the session summaries. then we started our discussion about multiple linear regression whose formulation consists of more than one independent variable. these independent variables are also called as features. we came to know that for multiple linear regression, there does not exist a closed form solution. hence we need to derive it's solution numerically. for this we use something called as gradient descent. first we write all the mlr equations in the matrix form as the solution for a n featured mlr exists in the n th dimension. writing in matrix form helps us in calculation. then after this we derived its cost function and minimising this cost function provides us with a condition which we use in a numerical method to obtain the solutions. then we hoped onto excel to try mlr ourselves and calculate the regression coefficients and tried to gain some insights by manipulating some of the statistics. ",-0.07108166,-0.049284108,-0.014510053,-0.00077128154,0.055217933,0.05248583,-0.061929606,0.006474154,0.0077508404,-0.0026828283,-0.012059573,0.01972293,0.03545545,-0.022770744,0.00071179494,0.082843885,-0.064224586,0.08014459,-0.08828533,-0.038286805,-0.04112775,0.009980402,-0.1250977,0.047241572,0.0099405,0.0060777166,0.018181069,-0.013712419,-0.006095928,0.054763347,0.110311456,0.014872043,-0.026275491,0.009181318,-0.08799708,0.0030183063,-0.07193595,0.04845519,-0.018288353,0.045886807,-0.013326201,-0.046155136,-0.0206921,-0.03690242,0.08585326,-0.018925637,-0.055288546,-0.07908212,0.0027463909,0.01421553,-0.042329773,-0.022037208,-0.030141763,-0.08783578,0.023607478,-0.14841086,-0.031406604,-0.0033790998,0.02192424,0.045450497,0.04268308,-0.075011946,-0.03304658,-0.000823927,0.03742287,-0.021459294,-0.03611032,-0.031358782,-0.04095054,0.06384006,-0.10528172,-0.027663391,-0.065633014,-0.012222421,-0.017286249,-0.016359592,0.088122785,-0.005781782,0.009782571,0.046323024,0.07290441,0.13497922,0.028600838,0.0010375609,0.011709529,-0.06432594,0.007563199,-0.04060374,0.015104219,0.013825859,-0.017299555,-0.04277109,-0.10394605,0.006972646,-0.015599412,-0.043326654,-0.033823747,-0.030401424,0.1339122,0.02175205,-0.016621083,0.017569812,0.025516782,-0.0009835856,-0.042380176,-0.035276134,0.0772648,0.04365725,0.05807387,-0.054556385,0.029180756,-0.021507295,-0.085232444,0.03003456,7.293584e-05,-0.11030435,-0.0023262552,-0.03205121,0.06227449,0.06717566,0.013630182,-0.04495599,0.08153538,-0.00511698,0.048627224,0.013580059,-0.11225104,3.1369683e-33,-0.050727073,0.029650375,-0.025401337,0.03211036,0.0033206271,-0.022630502,-0.014205166,-0.0035944772,0.0543209,0.02224858,0.027476326,-0.03535099,-0.020889297,0.06177109,-0.0014517118,0.0154094575,0.0188646,0.014024566,0.029643672,-0.01858825,0.048949562,-0.034844495,0.03570314,-0.05502865,-0.04882434,0.017843237,0.06460374,-0.041650016,-0.04186799,-0.015342506,0.051233906,0.0018075154,-0.050142005,0.049679816,0.012771956,0.01985133,0.02762363,-0.042228475,0.07944204,0.04974943,-0.040831298,0.033772074,0.049424842,-0.06674784,-0.053725917,0.053628307,-0.08213451,-0.012114212,-0.0031144498,-0.10546526,-0.06776093,-0.029639076,-0.037141997,-0.032006502,0.011660468,0.04253249,-0.21683198,-0.05730628,-0.022052297,0.028993728,-0.063200325,-0.00561525,-0.0053369026,-0.051120292,0.03185848,-0.041868124,0.086133435,0.05940346,0.023825377,-0.029377429,0.013861414,0.033101764,0.028279599,-0.13331817,0.10698448,-0.009220373,0.051866323,-0.027392441,0.040663946,-0.024024447,-0.016892776,0.09814911,-0.009442741,-0.0029645453,-0.0442033,0.044527385,0.017977096,-0.091810346,-0.018376347,0.011094285,-0.11698168,0.00873328,-0.061283182,-0.03273831,-0.002545811,-5.6280844e-33,-0.009090076,0.04205481,0.015583763,-0.05121201,-0.017179115,-0.035840176,-0.055624887,-0.057592005,-0.0032714352,-0.030850362,-0.014723812,-0.025364911,-0.018111773,0.0025553168,0.03960415,0.046459787,-0.039675657,0.032323886,-0.057990693,-0.03639656,-0.057263535,0.08652677,0.011085041,-0.05838187,-0.04083666,-0.02075254,-0.039399493,0.066306025,-0.007930067,0.014927615,-0.05226975,-0.016467813,-0.010759813,-0.0006767757,-0.033862237,0.10089761,-0.036691148,-0.046103414,0.062564984,0.0024674954,0.050121926,-0.0041988636,0.038687255,-0.10262373,0.08348838,0.010133758,0.015341136,0.030716553,0.054999284,-0.023946984,0.018748842,-0.010254924,-0.0682405,0.06837772,-0.09075497,-0.003653167,0.04177421,-0.031668983,0.057719436,0.006214256,-0.013618188,-0.03939613,-0.013793967,0.09003398,0.03049954,0.03238433,0.05977206,-0.030085105,-0.017401969,0.05976065,-0.018783819,-0.028834298,0.07276332,0.043350857,0.052668903,-0.046642844,-0.091355756,-0.0739188,-0.09770228,-0.016000662,0.057964347,-0.051154733,0.03965063,-0.046227723,0.04560861,-0.049259897,0.10262894,0.0157587,0.016508756,-0.037585996,-0.029898407,0.017122189,0.08489993,-0.013821183,-0.0027760041,-6.401971e-08,-0.03330557,0.009305409,-0.0032220413,-0.034412283,0.00036983987,0.02021965,-0.004644417,0.077319585,0.023041219,0.043421403,0.045900814,-0.025843859,-0.011801728,0.018225899,0.013737445,0.07508939,-0.028735245,0.024737313,0.0022068655,-0.011861791,-0.010308792,0.018714454,0.040066108,-0.030616358,0.041028213,-0.04893996,0.08496815,0.078285806,0.087432034,-0.0077844616,-0.060804833,0.099703535,0.048984755,0.025548158,0.07223077,0.060113985,0.0916722,-0.03808344,0.050640978,-0.009539675,-0.07838939,0.049036663,-0.050835364,0.045218885,0.15525214,0.052381914,-0.034827556,0.01089514,-0.038583476,-0.004919914,0.057717483,0.009399757,0.09085854,0.14558409,0.07272429,-0.058784787,-0.06258196,0.018789198,0.051694468,0.008972361,-0.050356593,0.016438773,0.008065365,-0.09280695,5,-9.464228,-8.949576,7
392,"we started the lecture by having a quick recap of the concepts from the previous class. we discussed what we exactly mean by saying â€˜a given number is statistically significant/ a number is not statistically different from 0â€™. we then started with a new topic- multiple linear regression. before starting, we discussed that before performing mlr, we need to convert the file/ data available to us in a vector form - [x1,x2,x3â€¦]. this vector contains various features of the data. this process is called as â€˜embedding vectorâ€™. sometimes, we also need to derive some new features based on the given ones, which matter more/ are more relevant in the context. so, this process of transforming the already existing features or performing operations on the existing features, so as to get new features, is known as â€˜feature engineeringâ€™.
we saw that we donâ€™t get a closed form solution for the coefficient values in mlr, like that in slr. however, the procedure needed to perform to obtain their values remains the same. like slr, in mlr also, we try to minimize the sum of squares of errors. so, if we have â€˜kâ€™ such independent variables/ features, we get â€˜kâ€™ such equations, on which we perform numerical methods to get the solutions. we also learnt about a statistic called the â€˜f-statisticâ€™. it is the ratio of average variance explained by our regression model to the variance explained by errors. so, we want most of our variations to be explained by the regression model. hence, we want msr to be greater than mse, which means f-statistic should be as large as possible. we learnt that the error metrics that we use to assess the validity of a model, are better interpreted when used to compare different models, rather than using it within the same model. also, since rmse and mae are in the same dimensions as the data, they are easier to interpret or relate as compared to other metrics, like sse, mse. 
in any ml model we first start by assuming that the errors in the predicted and actual data values are random. also, for any ml model, if more independent variables are available then the value of r2 increases, since more variables are available to explain the variability in the data. at last, we saw how we can use the p values for each independent variable to assess whether it has any effect on the data. if we get the p-value >0.025 (for 95% interval) then we can say that this particular coefficient is not statistically different from â€˜0â€™. hence, we can ignore it and reduce the number of independent variables in the regression model. we can continue this until we get only those variables whose p -values are <0.025. this implies that only these coefficients are significant and rest can be neglected. this gives us the true/ actual model. 

",-0.066573456,-0.06307299,0.039245058,0.025065087,0.0682259,-0.023170492,-0.01816657,-0.0125575485,0.04910929,0.0054158703,0.052219454,0.015357808,0.030456642,-0.018083889,0.0076030446,0.05776965,-0.060614802,0.019140704,-0.099582136,0.02975752,0.04262951,-0.007773811,-0.08042476,0.047609266,0.06443161,0.010194832,-0.002059345,-0.012389576,0.08206246,0.009605053,0.062079318,0.13330396,-0.021992441,-0.0083171,-0.03997376,0.033977143,-0.029425656,0.017185915,-0.029979546,0.06407821,-0.0016750048,-0.10268773,0.00055184285,-0.029213538,0.04670206,0.028718516,-0.018268222,-0.12856603,-0.029848129,-0.01953391,-0.047302768,0.0018574623,-0.099996075,-0.009072968,0.023413235,-0.08700977,0.004719727,-0.023797076,-0.01746414,0.016866006,-0.016731605,0.007519881,0.017971069,0.010375765,0.02893392,-0.026734047,-0.055250164,-0.040789336,-0.0018068756,0.060288735,-0.10195702,-0.024068352,-0.04929233,0.028426882,0.027505882,-0.02698979,0.055003524,0.067051,0.033932943,0.06459506,0.011652531,0.058060937,-0.0568589,-0.059425656,0.023112096,-0.0004838503,0.0033204705,-0.087882325,0.04796067,0.019796237,-0.029201303,-0.046242498,-0.016022744,0.07197762,0.01767658,-0.0014068037,0.030592658,-0.027848758,0.11181307,0.025130928,-0.0033624233,0.036502082,0.027748952,-0.03155323,-0.031971704,-0.056152396,0.042825073,-0.015758231,0.059649274,-0.012181615,0.03389422,-0.042095937,-0.041995633,-0.029037332,-0.024260573,-0.099031925,-0.05009278,0.00682491,0.047666784,0.03453031,0.04656347,-0.049177144,0.05327837,-0.00990614,0.03736371,0.031562716,-0.12966952,3.271692e-33,-0.043591466,0.02162715,-0.07004214,0.076454625,-0.038576785,-0.014383843,-0.0052996343,0.07822602,0.06451592,0.022110455,-0.034991264,0.009072505,-0.012872677,0.06361234,0.08451447,0.010970059,0.024892263,-0.008562612,0.03601319,-0.042244703,0.065758206,-0.026557485,0.077948526,-0.01917234,0.006187858,-0.036786273,0.025335431,-0.09131403,-0.12678115,-0.013090149,-0.033320874,0.02503199,0.019887624,0.054735124,-0.041427553,-0.008838195,0.05419471,-0.11059719,0.06608756,0.051882148,-0.046446893,0.014797442,-0.016168348,-0.04288763,0.017589495,0.08530038,-0.0066714427,-0.008111766,-0.04213543,-0.10622513,-0.08583429,-0.005445216,-0.029051395,-0.007700647,0.019695401,0.017649658,-0.13027203,-0.05490764,-0.04895622,-0.0022946666,-0.10265448,0.05130768,0.055660713,-0.063254245,0.020383226,-0.045829806,0.08782713,0.07381101,0.0039899196,-0.03946802,-0.057630405,-0.007513081,-0.05653441,-0.09550654,0.086604774,-0.027391143,0.030168395,-0.024605757,0.078397386,0.009511945,-0.043873373,0.13330896,-0.036142323,-0.07732913,-0.030436873,0.076879375,0.029566657,-0.08477945,-0.015875645,-0.07769396,-0.05606255,-0.03741916,-0.07857281,0.007578877,0.010345644,-7.171449e-33,-0.046973526,0.0008295877,0.01219651,-0.009755588,-0.007864393,-0.0457251,-0.0035299212,-0.08454816,-0.0064851427,-0.01956239,-0.0487807,-0.04698815,-0.029063156,-0.048514046,0.0027658707,0.027131373,-0.0773285,-0.02377892,-0.09335889,0.10454566,0.040131863,0.09373924,-0.0599274,-0.024173157,-0.028211856,-0.030597754,-0.047900062,0.05594241,0.027995184,-0.034185447,-0.023445614,-0.05875041,-0.03132195,8.941987e-05,-0.039791837,0.019990237,0.011340601,-0.054580525,0.060995057,0.052832928,0.06554406,-0.032797664,0.00964065,-0.02871506,0.025447758,-0.057180323,0.022320077,-0.010336288,0.07503485,-0.08541327,0.0951583,0.0010259496,-0.040948644,0.043715093,-0.0042032683,0.0179387,-0.00323008,-0.058880012,0.007844307,0.043174505,0.016712481,0.017200815,0.03417491,0.07497617,-0.06661667,0.041498516,-0.002557582,-0.030291533,-0.075525455,-0.016118972,-0.05070723,-0.006920262,-0.030254515,0.024594013,-0.04273397,-0.07376163,-0.029738892,-0.09742206,-0.07944638,-0.0028483628,-0.014065069,-0.03203554,0.063172214,-0.0052492702,0.04667126,-0.025324685,0.08317681,0.012536302,0.057711717,-0.077733174,-0.042998496,0.04694099,-0.06924982,0.07646702,0.015104402,-7.522549e-08,-0.089332715,0.011805285,-0.007937306,-0.055725463,-0.02437798,-0.02682028,-0.0755441,0.026088718,-0.028612467,0.019142115,0.007917513,-0.025720384,-0.05519813,0.068788104,0.061902817,0.053669594,0.033332527,0.07605831,-0.0013749624,-0.042738892,0.07740271,-0.022174222,0.037664104,-0.030389436,0.019119257,-0.101858325,0.02146274,0.08024354,0.04718712,-0.03017837,-0.039264195,0.06538312,0.06619803,0.063657984,0.14603476,0.03759594,0.121151395,-0.025251918,0.028097779,-0.003145298,-0.06241288,0.041071385,-0.0431314,0.07289926,0.05799772,0.03330633,-0.048571803,-0.08988806,-0.021935059,0.026197469,0.1265301,0.008013777,0.024942834,0.085708044,0.075352915,0.02262468,-0.049436085,0.01673621,0.008253672,-0.030608233,0.025326855,-0.02246058,0.03402879,-0.049840607,1,-9.8557825,-7.418622,7
396,"we learnt about the confidence interval of beta 1. we also learnt that beta1 can also be 0 in the confidence interval range. but we generally don't consider it as a good value because if beta1=0, then it doesn't really make a regression. we discussed about the method which you created to find duplicate submissions in class summary submission. there was a correlation matrix which you used and 1-x represented the similarity between 2 submissions. then it was a discussion that from now onwards we will  shift from excel to python. then we learnt about feature engineering. finally we started multiple linear regression. we saw about the k dimensional hyper surface in the multiple gradient descent method. then we saw its prove and discussed about some solvers. later on we opened excel and then discussed that wether we should use linear regression or not. we saw about feature selection and dropping. we dropped x5 because it had largest p-value. we also saw that larger the f value, better is the  model.",-0.09484099,-0.033871718,-0.011438554,0.046623755,0.012981432,0.011237977,-0.062196475,0.04458632,0.018784674,0.005125172,-0.039503448,0.07636711,0.0738981,-0.03134648,-0.012343994,0.034452327,0.010194811,-0.035786346,-0.026937062,-0.02636909,0.033410363,-0.015697835,-0.05083393,0.029196635,0.08330102,-0.027955202,-0.021085303,-0.004008771,-0.021817442,0.00065760565,-0.0160871,0.04342826,-0.03434406,0.030844303,-0.1057311,-0.03527778,-0.031085446,-0.02518449,0.0041962285,0.012735564,-0.05434378,-0.05398805,-0.017164038,-0.0007991816,0.09008971,0.0068261293,-0.01356831,-0.029519826,0.05475676,-0.06110756,-0.117799364,-0.042310704,-0.03163777,-0.09895373,0.0060252147,-0.09301646,-0.048086666,-0.0066897385,0.039470233,-0.027649881,-0.0013847131,-0.025059666,-0.04910706,0.06128785,0.0007970858,-0.0043461598,-0.1236594,0.0073852385,0.016320402,0.051436093,-0.1196751,0.045826133,-0.06412344,0.033496533,0.047650747,-0.05491213,-0.018233487,-0.001303934,-0.026508247,0.07037768,0.03470791,0.05011298,0.0021743262,-0.004056774,0.006972226,-0.049354017,0.06740065,0.06548298,-0.05735038,-0.06424315,0.11856643,0.008680197,-0.011046206,0.09253782,-0.045561112,-0.036228642,-0.027634732,-0.009772986,0.07215037,0.00973889,-0.055786215,-0.023300707,0.03108006,-0.012462446,-0.02512494,0.001080969,0.06658216,0.0038004369,0.07378076,-0.05025752,-0.04347079,-0.030804032,-0.0622487,0.05604561,0.09967861,-0.03530085,-0.005954029,0.00723217,0.008080643,0.07077494,0.03539654,-0.024121445,0.062273454,0.021541877,-0.024203388,0.015382985,-0.1582647,1.1682042e-33,-0.028930148,0.0041671316,-0.013938919,0.034242976,0.0075601926,-0.0066252644,-0.0073196134,0.024730876,0.05889779,-0.003226691,-0.008348835,0.03689822,0.006601338,0.09967734,0.024676135,0.07391708,-0.00033085272,0.062553205,0.03110645,0.043400396,0.0647588,-0.09953604,0.07180938,-0.029846441,0.0075090844,0.02987961,0.014928459,0.020368632,-0.023205306,0.019339094,-0.04974503,0.009456375,-0.06276417,0.013950699,-0.029440438,0.07051209,0.005616465,-0.076258495,0.013271645,-0.011183393,-0.062936224,0.05608576,0.048128568,-0.06087109,0.05861328,0.045704685,-0.08207694,0.027970377,-0.043180138,-0.040737845,-0.06623501,0.01316617,-0.018307539,-0.014603224,-0.057826973,0.102782816,-0.08384651,-0.068902306,-0.10204263,0.056021564,-0.05983433,0.017376784,-0.05390583,-0.09702162,-0.06979888,0.09831303,0.05185452,0.02430498,0.03535643,-0.03586684,0.0033791834,0.010896165,-0.039931897,-0.11293827,0.08675502,-0.03459091,0.09089625,-0.035835817,0.039654344,-0.051431287,-0.019186491,-0.024708519,-0.03984736,-0.06366854,-0.036582932,0.056479942,0.110429734,-0.015640603,-0.005308097,0.05009798,-0.05051412,0.0043380605,-0.032586273,0.052379858,0.034394193,-2.1626817e-33,0.000334103,0.007702947,-0.014868609,-0.020740082,0.008240868,0.033147845,0.0044398922,-0.11897386,0.027788654,-0.03939416,0.033168178,-0.029285701,-0.024066398,0.022835251,0.027156092,-0.023068558,-0.022421233,-0.065468416,-0.04500948,-0.032805283,0.02928669,0.08041172,-0.020915302,-0.047610108,-0.051953718,-0.030038243,-0.06587234,0.05780141,-0.006267553,0.00067035836,-0.029077886,0.019325003,0.014510317,-0.044200785,0.0344479,0.059687037,-0.021499004,-0.05069907,0.11825472,0.092649005,0.09890973,-0.0088511985,-0.0123906005,-0.080976374,0.053452328,0.051230516,0.04794057,0.023980461,0.08003725,-0.0661215,0.010631677,0.06609208,-0.04643751,0.009755491,0.01653769,-0.020625418,-0.010413172,0.0032900092,0.02165337,-0.0023361524,-0.036664274,0.024599992,-0.021715406,0.07789621,-0.0038917223,-0.0018561721,0.010310985,0.034463014,-0.044778597,0.048442774,-0.031186456,0.019591702,0.04806526,-0.043514546,-0.03786806,0.013467082,-0.09916812,-0.07358221,-0.13842826,0.072192356,0.008903986,0.040988732,-0.0117358845,0.035199426,0.032106586,0.008503308,0.12915148,0.09348355,0.058856517,-0.035404593,-0.081086226,0.044639766,-0.019748561,-0.03569169,0.028059011,-6.391662e-08,-0.025812604,0.07928916,0.10478685,0.009624824,0.022099141,0.047214188,-0.024295,0.11833057,-0.040231094,0.043190926,-0.04102446,0.012353798,-0.061418653,0.050372552,0.045092896,0.05917438,0.0048134103,-0.012580583,0.0024910124,-0.06707585,0.06855341,-0.016472476,0.0070199477,-0.045604203,0.03175778,-0.104368016,0.014925805,0.06945702,-0.0048491815,0.035762906,-0.06068061,0.043070428,0.028178284,-0.05049006,0.10204358,0.018050201,0.10684262,0.0060975756,0.044645075,-0.014447368,-0.05354054,-0.013076976,0.027071023,0.021448735,0.13546295,0.07732971,0.0329532,-0.057854585,0.021648437,-0.06720391,0.057830647,0.02525236,-0.0412095,0.054571785,0.048469484,0.09807187,-0.12756148,-0.013942708,-0.00019592293,0.006325889,0.0047461735,-0.009021528,0.0123027805,0.004909063,1,-12.143276,-3.0173855,7
423,"first of all we recap on the previous part like confidence interval and saw that if we have a sample which has an element called b from lower probability region then that sample is of importance, but in the case of broader 95% region 2 values are not really distinct values and if come distinct then it is by chance. then we saw that how summary is checked if it's similar to other session summary or not. we may get the data in future. we saw around 9 lines which shows that about 9 are pretty similar summaries. we convert text into features of x (vector). we talked about feature engineering and why rate is important and significant. we also saw about f which is equal to msr/mse and it shows behaviour of model itself and in comparision to other. we also dropped different features for our model for multiple linear regression and learnt about how to remove feature which are not important depending on p values. f will be large everytime and r2 remains around constant. multiple linear regression became clear in the class and also feature selection. we talked about a little about topic name like embedding for text to vector conversion also , will discuss in detail later. we saw there are n dimensional dataset in multiple linear regression. also to reduce error we saw we can use matrix to solve and get our matrices. we also saw error matrices like sse, mse , rmse and mae.",-0.0069831586,-0.0006277747,0.040111903,0.008225068,0.15815543,0.057643674,0.031570073,0.07101259,0.06531625,-0.052998155,-0.015163999,-0.00060269306,-0.010487911,0.04713396,0.07513305,0.00744537,0.05663671,-0.040004265,-0.14085826,0.028488977,0.06395608,0.0066463547,0.021791484,0.054916147,0.017888786,0.0006814175,-0.004853329,0.014497734,-0.035173777,-0.0007805378,0.03399998,0.16025569,-0.008764988,-0.0032576371,-0.084906556,-0.051945362,-0.023179654,0.04875549,0.0481868,-0.0329008,-0.039464604,-0.054923344,0.02612665,0.03457136,0.08228242,-0.007082088,-0.05534692,-0.06792576,-0.010889177,0.010796422,-0.04913308,-0.01838387,-0.071774304,-0.0072665494,-0.038290586,-0.05954983,-0.009844754,-0.019807566,0.0073628826,-0.003018899,-0.05058506,0.0026389072,0.015117614,-0.0051705064,0.08576186,0.015528431,-0.060411897,0.021028029,-0.01943225,0.030433053,-0.10276536,0.045549363,-0.024497027,-0.057678174,-0.031202344,0.012748954,-0.041294083,0.04693237,-0.033248406,0.030843511,0.01477537,0.06051188,0.012949519,-0.046107523,0.00030091076,-0.05928028,0.039919246,-0.0977854,-0.106925175,-0.0070378846,-0.008784636,-0.020069528,0.050161913,0.047381956,0.04406919,0.032655574,-0.06676495,-0.06681645,0.12181838,0.02969366,0.01197402,0.046802573,0.029834991,-0.040141866,-0.020171378,-0.013836103,0.045747694,-0.024259625,0.056293223,-0.065864965,-0.00072887324,0.02751033,-0.06839848,0.04219358,0.079602875,-0.057918265,0.027873592,-0.0035589607,0.048157357,0.04260989,-0.024552297,0.02265325,0.061366666,0.02629632,0.05333169,0.043278046,-0.05552171,8.635319e-33,-0.028878495,-0.07746899,-0.06320233,0.10635604,0.068768434,0.0006242553,0.00529132,0.014974737,0.046816807,0.003464056,-0.043920875,-0.011245348,0.048317693,0.008150864,0.010658543,-0.011050329,-0.012564531,0.05020398,0.04967605,0.0015381527,0.05539277,-0.0030193662,0.04930928,-0.028515432,0.0149953645,-0.008545696,-0.0074542454,-0.004702956,-0.06770495,-0.03133743,-0.099569894,0.05164239,-0.015879015,0.05031345,0.033604227,-0.0043783532,0.046949998,-0.0867011,0.018227905,0.007914048,-0.008467131,-0.023148147,-0.026015911,-0.058940344,0.01319596,0.0141721675,-0.059659492,-0.04576895,-0.06300634,-0.06564128,-0.056799132,0.012063678,-0.018280223,0.00013059804,-0.051048588,0.007905823,-0.09861248,-0.02950026,-0.04384138,0.04912332,-0.07355539,0.04211035,-0.07083945,-0.14602964,0.0055682175,0.054328185,0.046437558,0.08934586,0.0007018011,0.0006465858,-0.025837075,-0.02132607,0.005297128,-0.1144034,0.10110566,0.03471305,0.031044591,-0.021708727,0.016816262,0.007230358,0.011765288,-0.021470249,-0.014855595,-0.10212165,-0.037706923,0.016330639,0.12670736,-0.12365992,0.008195677,-0.030113837,-0.01632886,0.005800703,-0.024842719,0.040279713,0.05253641,-9.012238e-33,-0.0054279077,0.012905728,0.051461384,0.011884477,0.007206124,0.009643667,0.01461406,-0.071130484,-0.02509758,-0.13049889,-0.035752896,-0.042897202,0.070025794,-0.061481223,0.01817167,0.030751372,0.005947238,-0.060086407,0.033333663,0.0692503,0.050616644,0.057153568,-0.047822945,-0.018402824,-0.061558936,0.018567812,-0.09126958,-0.005266942,-0.026946705,-0.048953,-0.026575888,-0.015415405,0.018422473,-0.07229169,-0.019595148,-0.033210166,0.007317269,-0.06959622,-0.001429044,0.11416995,0.0944092,-0.014007241,-0.011793006,-0.04635412,-0.061354067,0.039790496,-0.008656516,0.03615375,0.06590723,-0.0120168645,0.01955644,0.011486494,-0.08774714,0.022970753,-0.014509685,-0.027433494,-0.026084047,-0.093494326,0.008888679,0.03185643,0.020303933,0.03499075,-0.04444436,0.016131146,0.093776025,-0.006037774,0.022946792,-0.046533052,-0.020637432,0.04603483,-0.038422998,-0.095667966,0.036646634,-0.006121448,0.018867478,0.02116351,0.013065666,-0.12475822,-0.10517577,0.07626171,0.009910232,-0.022937166,-0.013871466,-0.036516003,-0.026285412,0.034885734,0.02185857,0.07788864,0.04573568,-0.08760828,-0.058242094,0.0272655,-0.066966236,0.03420074,0.08805816,-7.481285e-08,-0.04294291,-0.0004092642,-0.044521578,-0.01920926,0.0037900556,-0.059175335,-0.060638126,0.10249322,-0.017359808,0.07243377,0.011373278,-0.021612471,-0.11196037,-0.011226132,0.058312796,0.039916147,-0.003247035,0.012285443,0.019448217,-0.09534054,0.06084949,-0.046369523,0.050215457,-0.039859995,0.015813896,0.007851337,0.030269204,0.15337083,0.032733656,0.030875722,-0.03456574,0.03626755,0.023479568,-0.0157099,0.0975524,0.043132287,0.09900915,-0.046369243,0.06102647,0.02514029,-0.03826552,-0.0125811035,-0.034623906,0.07977365,0.12087756,0.022079246,-0.021862017,-0.006583805,0.009672307,-0.03275527,0.037131324,0.020653987,0.0064670015,0.1492568,0.040166594,0.0776106,-0.0687083,0.009504316,-0.0015434279,-0.059614405,0.026163014,-0.023156704,0.0020392993,-0.06358081,1,-10.595195,-5.0519648,7
470,"in multiple linear regression, output is calculated based on more than one features. selection of useful features based upon domain knowledge and some other techniques is called as feature engineering. mlr does not have closed form solution. we use gradient descent to find optimal solution. this method is similar to newtown-rhapson method. this method involves start from a random point and then move towards optimal solution iteratively. 
error matrics such as mae and rmse have physical meaning since they have same dimensions as features. sse and mse are used for model selections. we saw an example with all coefficients were zero, but after feature selection we got better outputs. ",-0.018818762,-0.063464664,0.034044877,0.043321155,0.08697344,0.013669094,-0.053150166,0.017584726,0.011832182,-0.029893933,-0.030496405,0.026336374,0.025132537,0.059924442,-0.008362378,0.0503104,-0.021051394,0.031535164,0.002680413,-0.079203114,0.045366965,0.037601244,-0.10031384,0.010144284,0.063659266,-0.04056328,0.012933539,-0.0039807833,-0.020031156,0.011683863,0.09606272,0.03930245,-0.0021354225,-0.022723502,-0.07470764,-0.0016805695,-0.062341366,-0.0046874858,-0.0026015567,-0.014114913,-0.036431827,-0.053523608,0.032102,-0.07606693,0.09280017,-0.0012648284,-0.05822627,-0.07307107,0.060484476,0.0074190446,0.011909108,-0.03449306,-0.055339262,-0.03211989,0.03649717,-0.05880785,-0.011864225,0.0062694433,0.03658264,-0.011993553,0.028081102,-0.03991501,-0.052466944,-0.00011219106,0.080843374,-0.033875838,-0.049896937,-0.03501745,-0.06355236,-0.006328818,-0.05047144,0.011946925,-0.050351292,0.01164704,0.025209056,0.007784313,0.06499037,0.011357686,0.028917903,0.031490184,-0.009455988,0.04254462,0.0008478672,-0.028186569,0.063255385,-0.010762732,-0.032846123,-0.05831667,0.0278097,0.046214223,0.057937443,-0.051922545,-0.071618006,0.0050327075,-0.07916856,-0.026994003,-0.005500138,-0.043866742,0.016956506,0.003580652,-0.036394484,0.03837774,0.082197025,-0.04376423,0.07320865,-0.04031468,0.08756574,0.015271506,0.092673056,-0.06783993,-0.0121921655,0.001973724,0.01577957,0.008734409,0.025964579,-0.014190191,-0.035851017,0.030629616,-0.019677937,0.08191794,-0.06334834,-0.047855817,0.11316749,0.004424645,0.054430727,0.063888624,-0.07087935,8.6034534e-33,-0.044662267,-0.033281364,-0.056053445,-0.056991164,-0.003351505,-0.029700067,-0.03318733,0.007423883,0.112490326,0.008707406,-0.017337257,-0.07765098,0.0068960395,0.050945647,0.07699836,0.0038321652,-0.009069212,0.06999904,-0.0029527606,0.02128339,-0.0081670275,-0.0032402999,0.05333312,-0.09103079,-0.01593373,-0.036417488,0.026397498,0.018294415,-0.11296498,0.0048098816,0.010825534,0.05717018,-0.025406843,0.08457167,-0.0045508277,0.040096402,0.03028115,-0.063820526,0.038342528,0.080491215,0.0011138889,-0.014992838,0.0034369715,-0.0063589793,0.020163652,0.06999388,-0.090822645,-0.041787237,-0.017422706,0.015734771,-0.030512685,-0.017149197,-0.06900039,0.020901123,-0.014253955,0.039646663,-0.13123901,-0.07171433,0.0062106815,0.025026767,-0.0857861,-0.0031393778,-0.034632303,-0.09239525,0.019836387,-0.02711897,0.14090219,0.009244329,0.015812026,-0.0001231167,-0.015720211,-0.059822556,0.05174464,-0.11445508,0.055790246,-0.05800019,0.07065906,-0.029819878,0.08848168,-0.05645804,-0.024449864,0.11652722,-0.061438974,-0.10647203,0.05762559,0.057115827,0.0103293415,-0.09007437,-0.023502948,-0.071590714,-0.12035351,0.0816124,-0.0021764908,0.02932608,0.033633217,-8.729685e-33,-0.019649856,0.014707781,0.06213838,0.028046556,-0.02077808,-0.03538308,-0.0514107,-0.102648295,-0.010553969,-0.060335368,0.0038407343,-0.055873416,-0.0019555087,0.0066061695,0.06260757,0.051223293,-0.023952125,-0.057768732,0.041200846,0.057994753,-0.015522346,0.024951255,-0.015848026,0.034785315,-0.05754432,-0.043534447,-0.11411616,0.046795145,-0.006270214,-0.040176217,0.032823205,0.013896983,0.0039971354,-0.012825787,-0.004166868,0.053868677,-0.022376364,-0.027983392,0.09864722,0.10166908,0.06953982,0.029716175,-0.006607998,-0.009182053,0.013104768,0.002142155,-0.007378504,-0.0006188689,0.09413193,-0.032865983,0.00887645,-0.020641828,-0.05692,-0.002778788,-0.055883046,-0.013885082,0.011350624,0.0018156535,0.043407988,0.017010817,0.0031386318,0.0017056585,0.032326125,0.054500367,0.024379674,0.07036958,0.054406095,0.0430736,-0.077303424,0.00957481,-0.08923297,-0.071997,0.049733896,0.02146625,-0.026990162,-0.061754305,-0.02339788,-0.095127895,-0.08757018,-0.028479526,-0.0061768233,0.019701358,-0.0009896785,-0.018402608,0.0044313828,0.007862338,0.033179775,0.00079154235,0.06855829,-0.11469593,-0.029570822,0.051803876,0.082376935,0.009252559,0.012903953,-5.7730205e-08,-0.05545326,-0.052187555,0.04135099,-0.053179644,-0.00655964,-0.029903335,-0.012172939,0.11579336,-0.035463,-0.034716222,-0.032420035,0.028570944,-0.12224015,0.037131406,0.11850569,0.017754208,-0.044145685,0.063681625,-0.027609644,-0.04772974,0.09660649,-0.031096144,0.067273214,-0.095915206,0.005181349,-0.057791017,0.05109194,0.10427439,0.039110444,0.053787857,-0.018455468,0.033053882,0.048692085,0.024521047,0.1536289,0.03744328,0.1387693,-0.049963504,0.011789352,0.024484472,-0.07655719,0.047858514,-0.042548265,-0.027795145,0.022532564,0.029318007,0.06036928,-0.053266462,0.038987514,-0.019230852,0.06254411,-0.020313712,0.009198823,0.06916649,0.08933885,-0.020210724,-0.079997376,-0.0014183755,0.063560605,-0.0283338,-0.04203918,-0.058097422,0.03188869,-0.052373238,5,-6.8689113,-7.1551623,7
500,"in today's class, the professor discussed the 95% confidence interval for the beta1 value, explaining that if any beta1 value lies outside this interval, it is considered significant. he also covered session summary analysis and how it can be used to detect instances of copying. additionally, he introduced multiple linear regression, explaining that it involves more than one independent variable and emphasizing the importance of feature selection and feature engineering. he also highlighted that the goal is to minimize the sum of squared errors (sse).",-0.061960116,0.010823221,0.016287226,0.045614555,0.04324167,0.06410774,0.047577716,0.037127398,0.015639197,-0.048861623,-0.0052859066,0.0422019,0.01275926,-0.030084485,-0.03673445,-0.034551345,0.03720683,-0.026364965,-0.022691522,-0.067901306,0.03123441,0.010586154,-0.048623938,0.0054091965,0.088361524,-0.0029357183,-0.08040426,-0.017876972,0.014570332,0.0299282,-0.010113999,0.090358436,-0.009548947,0.0289723,-0.10475013,-0.055536423,0.029756486,0.0018253645,0.04927156,0.002750939,-0.030650668,-0.055579595,-0.0044292286,0.047288675,0.028568279,-0.021851297,-0.04052535,0.013839719,0.01769606,-0.0152451545,-0.07260662,-0.08540756,-0.027147485,-0.035129786,-0.009947101,-0.07280741,0.011402808,-0.0022013546,0.041507892,0.117324695,0.02168279,-0.0054559847,0.0031669326,0.06522662,0.113247484,0.041443575,-0.08314398,-0.07206359,-0.043590046,0.0045408094,-0.122163124,-0.0005711578,-0.09664466,0.034942437,-0.012493392,-0.06797446,-0.0002399567,0.055050727,0.013525382,0.0010286687,-0.0328095,0.023592576,0.030929217,-0.015807336,0.0022012263,-0.013917122,0.026015092,0.035753746,-0.045857955,0.066694506,0.05785425,-0.020308299,0.02561478,0.037057474,-0.04655509,-0.0344946,-0.08601787,-0.032641187,0.09268451,0.038852993,-0.019094432,0.0402409,0.08373765,0.013219771,-0.0363781,-0.047397763,0.13066818,-0.102857634,0.01232065,-0.0005404412,-0.08231934,0.04373877,0.000624663,0.038802367,0.08200172,-0.043088574,-0.0132300705,0.069474466,0.025706802,0.012067369,0.082287885,-0.004686048,0.087990984,0.017270701,-0.048015293,-0.026574219,-0.11109363,3.1940106e-33,0.023005852,0.030675326,-0.004800455,0.030196946,-0.011709618,0.014413761,-0.079070106,-0.009347047,0.033416264,0.0076744594,-0.016847443,-0.020792827,-0.01888116,0.040280025,0.028726239,0.111300915,-0.022702282,0.14111488,0.07528291,0.018351411,0.056498654,-0.10025363,0.07430425,-0.0360458,0.061219368,0.027157478,-0.008897109,0.0024717895,0.033990543,0.022234019,-0.014373216,0.015345387,-0.064966165,-0.015337479,0.027280062,-0.010485457,-0.0032434538,-0.03487884,0.05485073,0.10236982,0.034250166,0.019801902,0.039636828,-0.014050161,0.03326431,-0.04536807,-0.014338372,0.00213531,-0.018196993,-0.0042853565,-0.13038045,0.036597677,-0.060299866,0.0043960125,-0.037378605,0.067809805,-0.05183406,-0.044297032,-0.07739,0.01568303,-0.09108877,0.043807343,-0.0662219,-0.12748802,-0.021979498,0.10251389,-0.0044998396,-0.010119595,0.007711363,-0.0014721084,-0.012192443,-0.0781253,-0.023028899,-0.10231238,-0.014327425,-0.08783241,0.07002271,0.022814369,0.071224414,-0.07124029,-0.1326125,-0.02349546,-0.056649968,-0.08544212,-0.009605037,0.018320492,0.07478894,-0.01331242,-0.06684264,0.0026305541,0.040908325,0.00293448,-0.048868887,0.036312003,0.017998422,-3.5280615e-33,-0.03837006,0.011619152,0.05600708,0.010072057,-0.0518972,-0.016842188,-0.021561062,-0.08466627,-0.030527348,0.014593226,-0.034214184,-0.02230838,-0.027933825,-0.057918485,0.0102352975,0.0029065288,0.043050833,-0.09384254,-0.009589732,-0.030879067,0.1003193,0.013074971,0.052098997,0.040674597,-0.07518664,-0.018999225,-0.08077605,0.0035536473,0.014226161,-0.057809085,-0.03275228,0.0012373045,-0.014652193,-0.03608543,-0.02803265,-0.0023190982,-0.014297172,0.01465481,0.046293404,0.10685905,0.09613304,0.033895086,-0.031166464,-0.05315518,0.0038875912,0.09853962,0.06652869,0.016719177,0.009818729,-0.0392389,0.019837303,0.032882098,-0.062010203,-0.008864165,0.0075883213,0.01353388,-0.062709056,0.021709632,0.02700233,0.0007402771,0.07138848,0.056306344,-0.03352105,0.033507247,0.042500403,-0.057246216,0.0077125058,0.07388156,0.0005103415,0.045406062,-0.014683598,-0.015599462,0.07238269,-0.035718966,-0.059377305,-0.056386203,-0.052403264,-0.05660823,-0.021410452,0.055558372,-0.036971103,0.04346369,0.0117700305,-0.018680612,0.004656027,0.03521699,0.067638464,0.110663064,0.020204479,-0.048031926,-0.066647455,0.10252512,-0.0911677,0.010879099,0.055865537,-5.0033663e-08,-0.0028534667,-0.025553454,0.014000345,-0.0103310365,-0.031473923,-0.04893118,-0.04821767,0.0746346,-0.06636942,0.0043744,-0.027522154,0.010548495,-0.034912575,-0.001694398,0.04195616,0.005186949,-0.016569894,0.040108975,-0.0008862285,-0.059712566,0.13061388,-0.036612093,-0.0039107897,-0.041016683,-0.020746421,-0.041229554,0.025171267,0.11687348,-0.0074840966,0.005366483,-0.0036836704,0.06297449,0.01039984,-0.03929418,0.12908432,0.0052954564,0.12426517,-0.015965484,0.06711632,-0.011917592,-0.045627702,0.004263327,-0.016138092,0.052105658,0.06550328,0.03313319,0.082601234,-0.0063231215,0.026030065,-0.10934846,0.09947233,0.008893722,-0.04691582,0.094930634,-0.011554289,0.107792675,-0.041297395,0.012104765,-0.028434226,-0.013207306,0.03165518,-0.039082352,0.0026163233,-0.09383258,1,-12.616018,-2.9555328,7
501,"in today's session we talked about mlr. mlr has more than one independent terms unlike slr. each independent variable has its own coefficient to ensure it's weighted appropriately.
multiple linear regression is suitable when multiple factors affect the outcome. there was also recap of t value . ",-0.019675922,-0.11585514,-0.023494733,-0.033455692,0.07399061,0.032869235,-0.06475831,-0.016364278,0.0788071,-0.007295609,0.07483213,0.034736484,0.007172345,0.07551766,0.045129765,0.018621631,-0.050434604,0.02041269,-0.055340067,-0.032428678,-0.006558486,-0.029700661,-0.06635353,0.05958821,0.07546743,-0.021295128,-0.053196456,-0.00744453,-0.00066158385,0.025953623,0.07080867,0.0028035338,0.028853111,-0.0056733456,-0.102143176,-0.06772451,-0.12546845,0.030092513,-0.01753547,0.061044574,-0.026824014,-0.08992814,0.027032392,-0.06598448,-0.0033991805,-0.019759128,-0.03781981,-0.108781375,-0.05874627,0.04266267,0.06424663,-0.038286585,0.008265212,0.011393083,0.034517247,-0.07587841,-0.071159124,0.007913406,-0.0074207624,0.024518365,-0.049724232,0.022414142,-0.028464971,0.00059158896,0.11046668,0.085650235,-0.07840932,-0.027067043,-0.09227792,-0.0018706234,-0.051889665,-0.06687378,-0.0113673825,0.0147822425,-0.007552371,-0.02191591,0.048157908,-0.042702023,-0.011714276,0.14321415,0.0062976927,0.1703381,-0.0461991,-0.009959647,0.028339772,-0.09168225,-0.04224722,-0.02989857,0.04440161,0.06955971,0.000768664,-0.019831156,0.008020104,0.06319331,-0.051089473,-0.04113954,-0.051471174,0.019116845,0.085513666,-0.060261756,-0.04021,0.0031701745,0.025178764,0.03949571,-0.016792389,-0.0526512,0.033674862,-0.042198014,0.013411294,0.0010158827,-0.016619485,0.029991677,0.06979791,0.024155926,0.013658397,-0.11443128,-0.03385998,0.034325235,0.080508746,0.034255788,0.031669408,-0.042757254,0.09831727,-0.06579625,0.1066196,-0.020766428,-0.07729338,3.170328e-33,-0.069073185,-0.010689003,-0.026290933,0.0510055,0.0034128393,-0.0075062024,-0.054692995,0.053293943,0.013325848,-0.042656597,-0.006120743,0.030940672,0.009516597,0.046113417,0.025213113,0.03650805,0.06858993,0.04667018,0.0135297375,0.018576216,0.0076539526,0.0075113885,0.037011735,-0.031462323,0.015451221,-0.042732418,0.06314261,-0.00085627864,-0.011588839,-0.008680143,0.03467598,0.031198926,0.0006950659,0.011001964,0.046365906,0.008227411,-0.04763351,0.023340905,0.014570619,0.00030647483,-0.06288723,0.024559366,0.05790945,-0.02741557,0.026743079,0.08232517,-0.046167962,-0.05657311,-0.08749058,-0.002414697,-0.037274763,-0.048164867,-0.0707088,0.010268644,0.05308112,0.05884849,-0.13697118,-0.028647302,-0.07709563,-0.008483398,-0.13387196,0.035061486,0.017306827,-0.10074822,0.13036448,0.0005406683,0.031110361,-0.0002143404,0.025222179,0.00092636485,0.017681988,0.015745347,0.023540957,-0.034397226,0.012025111,-0.0022431978,0.017561387,-0.06896078,0.03912026,-0.022249518,-0.08629914,0.076937035,0.041420795,0.018163107,-0.05543758,0.067902744,-0.023287192,-0.05940612,-0.017691553,0.0029456718,-0.05119408,0.027782964,-0.058198597,-0.0022813936,-0.0058640065,-4.7350242e-33,0.040981594,-0.035907026,0.018028228,0.01956992,0.025221467,0.016276048,0.0066936845,-0.08097248,0.04873453,0.039834756,0.035902645,-0.032976747,-0.028488431,0.058500424,0.123756364,0.035161544,-0.053465426,-0.0039942153,-0.058502845,0.027382867,-0.008172834,0.053127628,-0.011935502,0.009437483,-0.044634752,-0.01608668,-0.12419296,0.04270699,-0.036900774,0.07953045,-0.051107623,-0.042737458,0.10260773,-0.07819551,0.034900133,0.067252494,0.045646418,-0.062075704,0.009027481,-0.0051630125,0.057770208,0.02289762,0.119203694,-0.04312953,0.009928305,-0.0044710976,0.0228185,-0.04446286,0.0801763,-0.020152027,-0.033924755,-0.04990623,-0.07016099,0.030936373,0.016993241,-0.07358234,-0.0030094949,-0.0578156,-0.028053677,-0.03789488,0.04272226,0.041885104,0.018478815,0.082714856,-0.057857215,0.001987728,0.031937562,-0.11219742,-0.035809517,-0.009283448,-0.06303087,-0.025726374,0.047215234,0.058456607,-0.006983107,-0.046040434,-0.05337316,-0.057328325,-0.11076737,0.04539327,-0.00761957,-0.07243603,0.024675634,-0.05057393,-0.04247178,-0.006658423,0.04700278,-0.003720167,0.022184514,-0.06854546,0.003359559,0.04448159,0.026327886,0.04545725,0.030018127,-4.525499e-08,-0.013390266,0.01599244,-0.0022562917,-0.013148015,-0.07902718,-0.0329854,0.028111422,-0.014986631,-0.0067337877,0.10551618,0.030506965,-0.016079456,-0.07801959,0.0078062396,0.07271256,-0.019549401,-0.02659352,0.009521892,-0.0068099606,-0.034709968,0.05957006,-0.0010595107,-0.011153948,-0.057155043,0.055921026,-0.037727453,-0.004144677,0.074632764,0.0014440088,-0.039878704,0.0454627,0.070733026,0.04020665,0.006068391,0.08633677,0.025331004,0.08206525,-0.050509617,0.096817404,0.05883887,-0.022170158,0.036068037,-0.042399935,0.052743465,0.09280372,0.09132833,-0.019863795,-0.06202536,-0.05407586,-0.043311026,0.0365318,-0.0023115133,0.044219494,0.05979919,0.111843765,0.015912037,0.04038179,0.0269228,0.015599031,-0.014725609,0.042808276,-0.09634313,0.017239494,-0.070764124,5,-11.925335,-9.284253,7
527,"in todays class (29/1/25),
first the discussion started with the doubts i raised in my last summary form about the real significance of statistically 0 which was clearly explained using the 95% confidence bound and marking out points a, b, c and d where we said d as statistically insignificant as it was out of the confidence interval whereas a and b being required interval and if there's a point c = 0, a and b can also be statistically 0 due to the parametric estimation in the confidence interval. then sir showed us how he calibrates and check out the submissions using the natural language processing. 
then we moved out to understanding feature engineering where we learned what exactly does feature mean about extracting data from the basic data values we have. like pixels from the images, embedded vector in the texts. 
lastly we started the discussion about the multiple linear regression (mlr), discussing about multiple features and equal number of variables (coefficients).
the closed form solution which we used into the slr was not on much into the fitting the mlr, thus sir initiated discussion on gradient descent algorithm engaging from the newton-raphson method by moving towards the minimizing loss moving towards negative gradient. ",-0.051640034,0.0013306849,0.026490211,0.022674203,0.116467506,-0.0006822577,-0.052767936,0.06497888,0.012603573,-0.03640327,0.01422244,-0.0149990795,0.03918984,-0.006573843,-0.019260496,0.03458347,0.0056150323,-0.04477198,-0.11597625,0.0010957543,0.017133527,-0.010089288,-0.034411557,0.035702754,0.036327202,0.010923137,-0.048759628,-0.07509476,0.03360936,0.053557765,0.017271085,0.083642855,0.0022579988,0.013938307,-0.024320686,0.018871041,0.04895374,0.06726963,-0.02464308,-0.017474737,-0.014710726,-0.115987256,0.044238128,0.01831496,0.14458172,0.05585571,-0.03204933,-0.056820597,-0.051026363,-0.023682786,-0.09133279,0.02169383,-0.08822973,0.016362812,-0.046506822,-0.011315233,-0.017217658,0.014032928,0.03152952,0.056404043,0.01713383,-0.037425563,-0.036437694,0.02950642,0.065706976,-0.044062052,-0.045049194,-0.12025164,0.02933125,0.055911966,-0.083977915,0.025930837,-0.008185263,0.013071314,-0.01797382,0.0061833444,-0.031235397,0.07707816,0.0066217794,0.028995134,0.05672089,-0.00028473308,0.028563537,-0.008682874,-0.02065675,-0.001272753,0.039070394,-0.043266557,-0.014854219,0.05703903,0.009205977,-0.06503508,-0.07887573,0.04573202,0.024564669,-0.004173195,-0.02106014,-0.07002499,0.0116798105,0.027785288,-0.021900501,0.011872626,-0.0018163079,-0.06129972,0.028678128,-0.010755179,0.006245097,0.0006046495,0.049846396,0.024288476,0.009938069,-0.010861999,-0.07125567,-0.006166637,0.056903213,-0.08992032,-0.0375656,0.026190342,-0.033881117,0.019546887,-0.038689602,0.024364978,0.030085294,0.0023461916,0.06524728,0.017306862,-0.07645159,1.0778414e-32,0.003636412,-0.026035558,-0.026089622,0.052948866,-0.033854228,-0.006801492,-0.060083352,0.03368362,0.10431951,0.030159257,0.041677356,-0.07534321,0.03115241,0.038168862,0.07122915,0.08347302,0.0032932288,-0.055473417,0.027310424,0.009181184,0.059907496,-0.0048594377,-0.013342878,-0.04465513,-0.032272525,-0.011503244,-0.02938148,-0.0061679836,-0.1289056,0.007286145,-0.09119737,0.029026538,-0.022086235,0.02718719,-0.019574324,-0.00854775,0.033438545,-0.120281,0.041803524,0.08792047,-0.026590513,0.035903756,-0.0024438163,-0.035981413,0.012047143,0.023817657,-0.0033308999,0.025135519,-0.09887907,-0.03366583,-0.08418275,0.07410186,-0.047874022,0.012877547,-0.052352257,0.026213102,-0.10229777,-0.075664,-0.091640204,-0.00250842,-0.04144807,0.040722176,-0.03806798,-0.07827177,-0.027673092,0.027051788,0.011103517,0.07785131,-0.027434522,-0.007346406,-0.004155569,0.027528785,-0.010497134,-0.1506339,0.04619853,-0.00031694808,0.10422967,-0.042521626,0.065848164,0.0063685067,0.035014894,0.0718733,0.028382221,-0.03331376,-0.028983094,0.029623145,0.08832706,-0.058185715,0.0058779367,-0.06969032,-0.047498733,-0.035450112,-0.08531156,0.010067034,-0.040554293,-9.7976426e-33,-0.019479359,0.012478114,0.00061244,0.046278834,-0.05216904,0.031911932,0.017601782,-0.07109538,0.04804436,-0.08344255,0.0176594,0.002306018,-0.054847766,-0.05630304,-0.008883906,-0.01799776,-0.026483925,-0.06483485,-0.022353176,0.069075696,0.055590767,0.098003976,-0.12630513,-0.014587847,-0.07588283,0.021307401,-0.042680517,0.054750655,-0.052368637,-0.01934055,0.04433461,-0.015722943,-0.035306424,-0.050073527,0.034381647,0.018245187,-0.01578501,0.009808417,0.101929955,0.054365363,0.106402844,0.006621,-0.09768246,-0.061228804,0.018397683,-0.04515425,0.036158502,0.04309058,0.066875085,0.005385513,0.026621087,0.038887825,-0.039396156,0.05784609,-0.030581275,0.004078398,-0.010705073,-0.03194038,0.055513658,0.07762626,-0.020537438,0.048572674,-0.047682066,-0.01750239,0.026265144,0.045740884,0.022538617,0.0027079517,-0.033512674,-0.028997889,-0.053909075,-0.0592307,0.052350454,-0.052326273,-0.032366283,-0.0047152922,-0.00952925,-0.08998323,-0.09879087,-0.008542706,0.07195138,0.0054695494,0.02858906,0.022277253,0.10339374,0.01988365,-0.0038980132,0.025433322,0.085351475,0.0035011226,-0.004672651,0.05452713,-0.05075213,0.03840468,0.044881843,-7.97117e-08,-0.024534231,0.04943963,-0.029130561,-0.00973473,0.0030499662,0.032234415,-0.00782817,0.053326033,-0.030990321,0.062657535,0.055799775,-0.0070785847,-0.15536846,0.003684584,0.08726686,0.09528509,0.01844042,-0.011071235,-0.037717797,-0.04874335,0.076931305,-0.10106844,-0.025124941,-0.068195,-0.0032025683,-0.046553932,0.050182138,0.10974544,0.021532267,0.00953077,-0.051126752,0.048212484,0.05523402,0.033284523,0.1171263,0.04244324,0.1577242,0.00508002,0.06635912,-0.025676232,-0.095147245,0.022232745,-0.0068845595,0.0686471,0.08487406,-0.015191799,-0.0039704386,-0.028301518,0.0026196153,0.067582235,0.054358795,0.061870184,-0.011970213,0.16626798,0.0550521,0.01327165,-0.037226975,-0.013782716,-0.03847974,0.03578003,-0.012456359,0.01778513,-0.03445318,-0.08335945,1,-10.396755,-5.8628473,7
540,"statistical significance vs. statistical similarity:-
statistically significant: indicates that the observed result is unlikely to be due to random chance, suggesting the effect or difference is meaningful.
statistically similar: implies no significant difference between the groups being compared, meaning the results are likely due to random variability rather than a true effect.

multiple linear regression (mlr):-
embedding vector: in machine learning, text is converted into vectors to allow models to process and learn from textual data.

feature and feature engineering:-
understanding and creating features is essential for building effective models. this involves selecting, transforming, and generating features that improve model performance.

slr (simple linear regression) vs. mlr (multiple linear regression):-
unlike simple linear regression, mlr does not have a closed-form solution for parameters. instead, an iterative approach is used to estimate the parameters.

gradient descent:-
this method is used to minimize error by selecting an initial random value for parameters and adjusting them iteratively in the direction that reduces error (e.g., adjusting beta values). the formula is:
beta_new = beta_old âˆ’ âˆ‡(ð½)ã—î·
where âˆ‡(ð½) is the gradient and î· is the learning rate.
solvers: solvers are algorithms used to optimize parameter values and minimize model error.

good regression model: a good regression model is indicated by a large f-value, suggesting that the model explains a significant portion of the variance in the data.

model evaluation context:-
model evaluation can be approached in two ways:
between two models: comparing the performance of two models to determine which performs better.
single model evaluation: assessing a single model's performance by analyzing its error metrics (e.g., mean squared error, r-squared, etc.).",-0.046692807,-0.08429441,-0.0064454935,0.0040051546,0.071037985,0.009180135,-0.1202976,0.047381435,0.066343926,-0.083282754,0.037369758,0.05707141,-0.0016221168,0.005313849,0.010497539,-0.019236289,0.048659395,0.00068748894,-0.096218176,-0.047768638,0.0044965697,-0.039433006,-0.04183153,0.08206686,0.054416206,-0.013055586,-0.03274218,0.029208703,-0.014800664,0.039599918,0.08188559,0.011721313,-0.011803984,0.018919567,-0.10461868,-0.0020301782,-0.07644153,0.045402937,0.01635413,-0.028764412,-0.0203813,-0.10835158,-0.0020331128,-0.008149002,0.03381256,-0.012567402,-0.06213333,-0.06853262,-0.026553052,0.042505987,-0.024545042,-0.0664294,-0.031656947,-0.0057254788,0.011444717,-0.039647188,-0.010793031,-0.03746143,0.012038828,-0.025940612,-0.030593446,-0.07057494,-0.008557456,-0.0018537533,0.01739834,-0.014857213,0.013682683,-0.08232161,-0.04230136,0.031099342,-0.115266055,0.04426569,-0.045545157,0.101378076,-0.029947009,-0.011111718,0.025964024,0.036569737,0.0076441793,0.043945365,0.05445412,0.102990754,0.056346428,-0.008756476,0.054064024,-0.024494395,0.081244655,-0.08603984,-0.014968061,-0.01280704,-0.024739986,-0.0020553006,-0.051139083,0.002892736,-0.08643699,-0.018081646,-0.03815342,-0.039379418,0.03863269,-0.012413602,0.0006280559,0.052331157,0.013386695,-0.03395729,0.061616898,0.034147885,0.037531074,-0.034645103,0.019030772,-0.008616993,0.00046541917,0.055505957,-0.023837253,0.070448734,0.01900902,-0.10378806,-0.05156928,-0.003436599,-9.943165e-05,0.09191218,0.014899093,-0.03667318,0.035099078,-0.022987014,0.02974407,-0.006552153,-0.10345241,3.8745396e-33,-0.055260275,-0.010421728,-0.050702922,0.06927325,0.032652088,-0.032935288,-0.07393229,0.0756674,-0.0021530911,0.036457796,-0.011101233,-0.021832298,0.014599361,0.09956484,0.032803413,0.07174843,-0.0006751198,-0.07563672,-0.0048575816,-0.0052499203,0.03750613,-0.022651052,0.03294481,-0.01014618,0.008092479,-0.060056157,0.049987134,-0.029467672,-0.102139615,-0.037830763,0.010634253,-0.04772672,0.005685494,0.053891785,0.033258975,-0.014983411,0.065153345,-0.10520397,0.03500403,0.0419085,-0.055373833,0.03877398,0.015970303,-0.0044084443,0.05236882,0.10698968,-0.04920182,-0.0171904,-0.036846496,-0.074891225,-0.028587142,0.022006102,-0.09412429,0.050836515,-0.0017466531,0.03320928,-0.118171185,-0.025336953,-0.0403077,-0.005113449,-0.055168945,0.043674704,0.010131915,-0.073808156,0.08366018,0.009314455,0.04718062,0.1024902,-0.0087964125,0.015435379,0.0455697,-0.0076382365,0.013503701,-0.01480411,-0.00060494704,-0.02444482,0.031022875,-0.09176804,0.07814298,0.054265916,-0.059412543,0.11175304,-0.0450273,-0.03699502,-0.12211747,0.022704232,0.03598776,-0.0963497,-0.0007263207,-0.014619359,-0.012775479,0.028819494,-0.05296264,0.009310878,-0.034183487,-5.610134e-33,-0.011469547,-0.0011107026,0.070027985,0.07583392,-0.021240463,0.004152382,0.033120293,-0.052685466,-0.013375309,-0.003885488,0.019014737,0.001959456,-0.012931589,-0.023717294,0.08513324,0.045375925,-0.060283992,0.030334394,-0.062108744,0.085731536,0.064985804,0.08804072,-0.0314293,-0.055432502,-0.054336336,-0.049424563,-0.00030841204,0.049042173,0.0010134743,-0.07603023,0.016390715,0.03301676,0.018103786,-0.09471185,-0.014976753,0.088455796,0.021370642,-0.03547351,0.067805156,0.005360017,0.013726101,0.017612936,0.054254636,-0.07800201,0.039112356,0.039395534,-0.012622302,-0.020504821,0.084426165,-0.056717917,0.05388296,-0.029523483,-0.123541534,0.06259791,-0.0011824659,-0.06717949,-0.04137304,-0.03904025,-0.047132097,0.046692334,0.02886059,0.03670099,0.0057829795,0.06381888,-0.0354253,0.020510476,0.025255276,-0.077271976,0.0031349927,0.020086128,-0.06255293,-0.036768824,0.034593955,0.05070141,-0.012842484,-0.11560437,-0.020267032,-0.052380055,-0.118190326,-0.0074994606,0.012857141,-0.06881083,-0.03165169,-0.06723322,-0.02412236,0.025691252,0.025472725,0.05242036,0.03149654,-0.053458124,-0.008995144,0.07581114,-0.040122878,0.02252969,0.046253603,-6.0593266e-08,0.022695886,0.04210104,0.02982335,-0.04840452,-0.07928265,-0.018091235,0.014359643,0.044291917,-0.076311976,0.00036701126,0.018749153,0.0038501907,-0.104359776,0.020131,0.037193533,0.042499702,0.018956803,0.037917294,0.059379257,-0.00010487335,0.021681925,-0.011117947,-0.0066667395,-0.008175151,0.041306954,-0.077654764,0.035086017,0.11272318,0.031339813,-0.05694272,-0.013575763,0.055731546,0.12655888,-0.013896247,0.10892743,0.07389774,0.048789915,-0.062092353,0.12971616,0.0541965,0.025624488,0.05784458,-0.05396114,0.03783196,0.054495756,0.035886835,0.06534285,-0.07152716,-0.027683882,-0.0032586625,0.11350392,-0.018719397,-0.0005256229,0.09901618,0.026175646,0.052148953,-0.054087996,-0.004393207,0.07882584,-0.03431613,0.0814368,-0.051112104,-0.016747842,-0.0917068,5,-11.929094,-8.515366,7
550,"the concept of statistically significant and statistically similar was discussed in today's lecture. in a linear regression equation of form y=ax+b, there will be a gaussian distribution of ""a"" value and a particular zone (95%) symmetric around the mean. if any value lies in that particular zone, then that value is not significant and is called a statistically similar value, while those values lying outside the zone are called statistically significant. 
multiple linear regression (mlr) was discussed. multiple linear regression deals with more than one independent variable and talies the form as : y=a+bx1+cx2+dx3+.... . on the other hand simple linear regression deals with only one independent variable and talies the form as : y=ax+b. then mlr gradient descent approach was introduced, an optimization algorithm that helps to reach an optimal value. ( similar to newton raphson method)",0.008755094,-0.051526528,-0.014508158,-0.017360637,0.061451595,0.023252562,-0.06526935,0.027585376,0.071833886,0.035349812,0.036476906,0.073041745,0.039118756,0.03924741,0.05390153,-0.05268533,0.013456669,-0.044944163,-0.064137734,-0.008669544,0.017740658,-0.042736206,-0.07476037,0.031284474,0.038797457,0.007036969,-0.027234364,0.006421435,-0.008217342,0.046387475,0.09396161,0.056356296,0.04329671,0.02025859,-0.09079815,-0.0335097,-0.086531274,0.09710799,-0.002258016,-0.02571376,-0.021485472,-0.048289686,0.06681604,-0.016675249,-0.016927743,-0.0097099645,-0.05491744,-0.084347926,-0.009278866,0.019693956,0.0133156,-0.043259654,-0.021930307,0.006611543,0.059664886,-0.09223182,-0.041495465,0.046902776,0.05337382,0.049560834,0.012736218,-0.0015674289,0.028110016,-0.0036738913,0.11166432,0.011501824,-0.085012354,-0.08047011,-0.098608166,-0.0008018714,-0.05180782,0.0070605557,-0.023391899,0.029451588,0.002657539,-0.02404593,-0.0020694481,0.026875122,-0.058605384,0.096890524,-0.008685447,0.111212514,0.005014815,-0.017405944,-0.010757412,-0.04243055,0.031842623,-0.08191277,0.0111840805,-0.021277755,-0.0061713434,0.015106216,-0.07706583,0.020278413,-0.021360287,-0.042003773,-0.01591915,-0.049100474,0.07005393,-0.044570375,0.012365204,0.030639477,0.03850445,0.011896707,0.041761383,-0.0335035,0.06537069,-0.02588533,0.03520126,-0.0202376,-0.054975953,0.01102786,-0.0023341733,0.019060554,-0.031425513,-0.12287522,-0.041673813,-0.008405107,0.001931262,0.021692049,0.049092088,-0.018265305,0.049228866,-0.04430752,0.054341264,0.020123497,-0.09307384,5.5429966e-33,-0.07100865,-0.028764624,-0.10779296,0.049204826,0.035263866,-0.04055104,-0.054780204,0.007323476,0.0684705,0.04136765,0.03541076,-0.06581788,0.052522782,0.08719694,0.022068601,0.090726174,0.046356756,-0.05026876,0.016457181,0.016526498,-0.0058584237,0.035874303,0.04592986,-0.0689135,-0.013356632,-0.03813422,0.051745802,-0.008516144,-0.08059998,-0.03315165,0.03318401,0.03480496,-0.047636207,0.030700583,0.021143902,-0.018420802,0.041093055,-0.05743273,0.035225224,0.0484797,-0.05324674,0.036462523,-0.021763057,-0.019661782,0.12494178,0.08129805,-0.09147599,0.0032657837,-0.04078225,-0.13410889,-0.07310005,0.05035597,-0.030714637,0.0039762165,0.009185721,0.082657635,-0.15357238,-0.06523954,-0.09162362,0.029484417,-0.056571845,0.041500054,-0.009880939,-0.073718816,0.040670995,-0.0062672817,0.078572296,0.06659935,0.018851196,0.04460557,0.04524937,0.031971764,0.027306503,-0.07222439,0.056531608,-0.028630534,0.048534747,-0.06633888,0.03476714,-0.04109735,-0.06798325,0.07980064,-0.011440428,0.02152622,-0.08946279,0.06856568,-0.02155935,-0.026397876,-0.004178643,-0.053699333,-0.0012377029,-0.0018349615,-0.09384098,-0.037374973,-0.025172308,-7.093061e-33,-0.016638735,0.020405198,0.019377055,0.0035221777,-0.021613574,0.0479734,0.013523471,-0.10003026,-0.030569676,0.006106394,-0.037444063,0.028390402,-0.017649207,-0.009465632,0.06426677,0.04148793,-0.0031170931,0.0482685,-0.065511584,-0.004915777,0.05396083,0.0208984,-0.009751044,-0.08782742,-0.07439114,-0.018905701,-0.12562756,0.10674997,-0.08424639,-0.033362333,-0.019066697,0.033887997,0.03800448,-0.0938153,-0.021190587,0.115356445,-0.03031317,-0.027417207,0.0021709441,-0.004282357,0.007427832,-0.005587964,0.0979895,-0.13088931,0.03056429,-0.003841037,0.008226041,0.016331859,0.049758457,-0.019869551,-0.02074805,0.0025998903,-0.1092666,0.04573883,0.0107428,-0.008184611,-0.05559366,-0.011920372,-0.011160809,0.057546027,0.04070332,-0.002468751,0.007641915,0.11064974,-0.04328395,0.074281834,-0.0035988484,-0.07239458,0.036700666,0.015464295,-0.033096768,-0.026469272,0.04772826,0.04818892,-0.0053146305,-0.008894265,-0.031238873,-0.088209294,-0.110446416,0.05314088,0.06309704,-0.02258353,-0.016062235,-0.07861422,-0.0018341668,-0.017962093,0.011792815,0.0066929483,0.068674974,-0.10213704,-0.025912097,0.025541773,-0.05059733,-0.044113725,0.06403262,-5.739202e-08,-0.005217061,-0.001103702,-0.017188357,-0.06713022,-0.048771862,0.012497609,0.033114977,0.030019103,-0.034248833,0.026341407,0.0058076307,-0.0015618289,-0.07205989,0.00574511,0.008850803,0.016872844,0.0038675277,0.019858731,0.033850685,0.008238261,-0.012139916,-0.03994388,-0.061444957,-0.021820314,0.0755153,-0.04044974,0.02179485,0.10405392,0.0143773435,0.0701079,-0.016556134,0.045819357,0.10979818,-0.005562443,0.100583695,0.065767355,0.055063926,-0.0048105572,0.073142484,-0.06214947,0.010546153,0.01170369,-0.04809216,0.059902217,0.10623551,0.05656827,0.07231993,-0.029868884,0.008196545,-0.035066392,0.08808437,0.025598861,0.07144096,0.079301886,-0.014893095,0.022424612,-0.09729484,-0.0076431595,0.020339174,-0.005157171,0.060979385,0.016132204,0.025183406,-0.10243675,11,-11.971356,-8.6168165,7
570,"multiple linear regression there are multiple predictor variables also called features. what 'stuff' to use as features, how to calculate / derive features from some other data and using domain knowledge form what is known as feature engineering.
some error metrics like rmse and mae may have a physical meaning as they have the same dimension as the data but others like sse and mse are used only form 'optimization purposes'. also, these values themselves may not be very helpful, but they aid in model selection; we decide which model is better based on these metric (lower is better for error and higher is better for something else).
even when metrics such as r-squared and f-statistic are high, the model may still be bad. saw an example where the regression coefficients were statistically equal to zero. after performing some feature selection (successively dropping the ones with the highest p-value) we become more confident with the model even though the value of the metrics may have decreased slightly.
mlr doesn't have a closed form solution hence we need to use gradient descent to arrive at a solution (that is potentially correct). it is similar to newton-rhapson method. we start with a randomly initialized value as the answer and the iteratively move towards the 'optimal' solution. multiple solvers are available that implement some form of gradient descent and it forms an integral part in the functioning of many machine learning algorithms / techniques.",-0.008493662,-0.08646071,0.013475277,0.053159114,0.06882357,0.06833023,-0.0749476,0.060005818,0.051555388,0.0031841223,-0.06002203,0.0073900013,0.012387647,0.10857083,0.04044536,0.0018867115,-0.0062307,0.09141139,-0.08289958,-0.008663779,0.018773971,-0.031109583,-0.06258251,0.06424301,0.0021098836,-0.05613238,-0.034646742,-0.0017502423,-0.09475675,0.020748978,0.11230441,0.02192766,-0.008955552,-0.01046029,-0.061996836,0.008333324,-0.018709894,0.029363543,-0.010030195,-0.059926342,-0.04984595,-0.1024875,0.040614065,-0.044221394,0.055509906,-0.037859876,-0.00976791,-0.06815822,0.051932566,-0.03339195,-0.011314768,-0.005896519,-0.021949654,-0.03423948,0.054494295,-0.04983903,0.0060174316,0.0012561332,0.003903518,-0.00046102394,0.017287139,-0.022112299,-0.049649123,-0.007744571,0.035138354,0.011796047,-0.06195474,-0.057236064,-0.04273309,0.08521741,-0.03969108,-0.010817826,-0.04679488,0.0055304104,0.025714004,0.017245071,0.06280413,0.030635023,-0.00657463,0.034682445,0.058966614,0.061073504,0.0015869848,-0.048015885,0.04483428,-0.02460136,-0.009818024,-0.093903095,-0.020038104,0.041489042,0.07702159,-0.019918457,-0.0443839,0.029382145,0.022404514,0.0023191222,-0.018268934,-0.076379575,0.040130027,0.021347312,-0.07048509,0.035247505,0.037653234,0.0015482233,0.063946895,-0.02542893,0.062434476,-0.015837487,0.05703618,-0.022892052,0.053700257,0.0029478779,-0.030378142,0.052809983,0.014600014,-0.042588532,-0.057687484,-0.014433993,-0.013876872,0.078647666,-0.05636101,-0.05708403,0.123835556,0.010621603,0.07989943,0.047407374,-0.1616351,9.341326e-33,-0.030055951,0.022397077,-0.038929347,-0.022300014,-0.018636694,-0.044665698,-0.03788949,0.029143875,0.1188193,0.018291753,-0.01408735,-0.0436152,0.0022671733,0.035715826,0.07620947,0.044503905,0.009225276,0.025867803,0.03880897,0.043021183,0.034354422,-0.08563972,0.04550238,-0.04594446,-0.007394464,-0.030485189,0.019596502,0.05110993,-0.09554445,0.0012956102,-0.0174836,0.018012341,-0.007831356,0.084496714,-0.00048337737,-0.015360779,-0.061314993,-0.05162887,-0.00041542092,0.04562205,-0.030323204,0.008838914,-0.019444719,0.027230058,0.043038536,0.07900268,-0.070511796,-0.05317071,-0.06863685,0.0033795037,0.0010353333,0.022286255,-0.06432713,0.007862719,-0.049954046,0.019963522,-0.15526651,-0.08167568,-0.0444512,-0.024234157,-0.07409243,-0.024197154,0.006655371,-0.12271955,0.024793267,-0.020121254,0.115171224,0.07292651,-0.034151837,0.021214781,0.0132504115,-0.053420495,0.022374835,-0.11802478,0.092936955,-0.008319988,0.11624449,-0.056954756,0.07259379,-0.03516599,-0.013856592,0.09967591,-0.008197039,-0.005598607,0.026351431,0.011008927,-0.01852819,-0.07253353,-0.0043783914,-0.012124084,-0.09378735,0.04549481,-0.030177237,0.034939118,-0.046938065,-9.6488676e-33,-0.02371788,-0.003243859,0.016983617,0.012456847,-0.061544467,-0.014738391,-0.006835588,-0.058321822,-0.03868987,-0.09576112,0.035094175,-0.03828595,-0.0075522335,-0.0043062805,0.05561634,0.020126035,-0.015449373,-0.034803677,0.035099853,0.04929304,-0.008946259,0.052463327,-0.029361682,0.020772703,-0.08187952,-0.030910522,-0.16961128,-0.028498013,-0.0064780335,0.00036876075,0.05787952,0.07694661,0.027685368,-0.031461112,0.027324112,0.047354285,-0.01706081,-0.031938326,0.06236231,0.08770187,0.06997395,0.06331415,0.0021046575,-0.039717086,0.026628602,-0.00907459,0.021550376,0.011378448,0.10212981,-0.013778044,-0.00082884205,-0.051780656,-0.09271361,-0.004207624,-0.042233415,-0.039998803,-0.01624061,-0.0043790024,0.0367431,0.038305935,-0.010749927,-0.009195985,-0.02167054,0.09478937,0.01992019,0.043435693,0.061587173,0.002269118,-0.035757147,0.053719077,-0.06315114,-0.10909437,0.06538196,0.047970664,-0.030523006,-0.044117276,-0.028060302,-0.070826545,-0.08897337,0.04529334,0.05821584,0.032345988,-0.029457929,-0.032312665,-0.022707827,-0.039154544,0.024429465,0.061352536,0.06194951,-0.108232774,-0.020346612,0.054310165,0.02549898,0.017951736,0.04939754,-7.415057e-08,-0.01784044,0.02398234,0.02828296,-0.042419977,-0.069167815,-0.07857681,-0.04359354,0.081723526,-0.007190011,0.018398779,-0.0047615822,0.0077445465,-0.12829153,-0.0048060454,0.08898019,-0.0026229755,-0.039369836,0.06845679,-0.015091167,0.0055818693,0.043530535,-0.046763405,0.02529309,-0.080661364,0.053315036,-0.084053144,0.059128843,0.062197644,0.00838536,0.031718202,-0.020128803,0.049935542,0.07306674,0.07587463,0.07801641,0.023128228,0.08588812,-0.055852003,0.031392064,0.03470107,-0.033177536,0.08494314,-0.058572777,-0.0029940233,0.05364815,0.029124033,0.04487203,0.019546853,-0.014070229,-0.00935803,0.037416395,0.0468001,0.025948208,0.09078568,0.04535383,0.012882254,-0.07109166,0.051577363,0.043769248,-0.029971866,-0.024493229,-0.11199018,0.008250043,-0.09017498,5,-7.08539,-6.681693,7
600,"consider a sample s1 from a population. we used this data to fit a line using linear regression. we have a value of a (y=ax+b). we then estimate the 95% confidence interval of a. any two values which lie in the 95% confidence interval are considered statistically similar and not significant. because a value outside the 95% confidence interval has very low chance of occurring it is statistically significant. in linear regression we don't want value of a to be zero . so for a good model we want zero to lie outside 95% ci. 
multiple linear regression deals with more than one independent variable y=a0+a1x1+a2x2+a3x3+......+anxn...   . converting text into a vector is called embedding vector. for sales--->(age, earnings, location,.... ). (x1,x2,x3,....)-->features. feature engineering is calculating useful additional features obtained by operating on existing features to draw meaningful conclusions. we use mlr gradient descent to find the features. matrices are used in derivations y(mx1)=x(mxl)b(lx1)+e(mx1). e-error term. here we describing all points not  the regression line.(in slr we describe y=ax+b, but here y=xb+e where e is error term).m= number of observations, k= number of features. we need to minimise the cost function.  the gradient descent process: 1) assume some value for b.(generally initialised to all 0s or 1s) 2) using y_hat=xb evaluate y_hat. 3) calculate dj/db as per the above expression by assuming some value for n. $) calculate new values for b using the following expression b_new=b_old-n(grad_j). 5) repeat steps 2 to 4 until abs (b_new-b_old) reaches a threshold level(eg:0.0001). finally we end up with a b(column vector) which are our features. f value=msr(mean square due to regression)/mse(mean square due to random error). sse, mse, rmse, mae are useful for comparing between two samples. to qualitatively make some conclusion from data, we need to calculate some other values from these. in mlr based on the coefficients we can understand which features effect the output most. anova- analysis of variance. we drop variable with highest p value. more variables mean more r squared value. when we drop variables with higher p values we see a decrease(very less) in r squared value and f value increases. we want p value to be less than 0.05 for the coefficients to be significant.  ",-0.021973899,0.014845246,0.010920035,-0.0022542311,0.07687787,0.063125625,-0.07678481,0.024576839,0.0543679,-0.06108478,0.08374856,-0.019739887,0.072577134,-0.029668532,-0.0021485526,0.038561624,0.0462917,0.013562595,-0.08516238,-0.059842464,0.041191526,-0.00013198939,-0.055803087,0.0022155463,0.021113463,-0.001140638,-0.06685944,-0.050935175,0.016517082,0.07355035,0.0716559,0.08589559,-0.016474051,0.036461532,-0.061669093,-0.0712525,0.0064964863,0.08084623,0.0104354955,0.03721268,0.0041629244,-0.05102686,0.022966864,0.041771423,0.11017417,-0.0009867555,-0.025637856,-0.06087308,0.017876357,0.06497203,-0.042499483,-0.012085395,-0.06495115,-0.0056308536,-0.025938377,-0.104264446,-0.034736674,-0.010376595,-0.004652829,0.001416934,0.042190645,-0.039344914,0.07477548,-0.021269538,0.039789572,-0.019849058,-0.10463139,-0.022394743,-0.105238415,0.03492353,-0.062447056,-0.021546118,-0.083007246,0.014786122,-0.04773006,0.009298548,0.036729746,0.020640895,0.030393695,0.086676255,0.033745583,0.11218918,-0.026732508,-0.049819093,0.015730944,-0.028379409,0.046960652,-0.08390708,0.003306758,0.047129773,0.011924612,0.010703441,-0.059401486,0.050499108,-0.0063344026,0.005212103,-0.09062127,-0.027959555,0.033354584,0.041853126,-0.014623668,0.043677848,0.05937888,-0.017361907,-0.10146048,0.0042539607,0.04604426,0.033922333,0.0141880615,-0.036648788,-0.0017258894,-0.037172616,-0.081856735,0.044942074,0.042017195,-0.11734921,-0.0052320315,0.036505524,0.061594438,0.060703192,-0.0024341792,0.05309899,0.078151986,0.050626587,0.0024213498,0.0054946886,-0.07642972,5.866473e-33,-0.07935794,-0.03680819,-0.03372821,0.060272314,0.014875448,0.004511257,-0.05733677,0.03745163,0.038121894,0.051133838,-0.04866718,-0.08952823,0.027223399,0.053281136,0.06925017,0.07674693,0.070514105,-0.018706027,0.057869416,0.0022150872,0.021137763,-0.021301825,0.017279115,-0.018868025,-0.013710821,-0.03101938,0.020981066,-0.032254364,-0.11018818,-0.010813457,-0.0741384,0.038322274,0.065307066,0.0037195326,-0.025292264,-0.014548704,0.021296566,-0.044546675,0.04981582,0.09573338,-0.07438731,0.015180017,0.046141192,-0.032476377,0.048091866,0.03985273,-0.05072874,0.0015868859,-0.08400304,-0.023773452,-0.06567038,0.05071759,-0.06621468,0.003088346,-0.06345631,0.028997706,-0.12551422,-0.045555484,-0.090489335,-0.06739798,-0.12828265,-0.0076836143,-0.041485332,-0.09319906,0.038082357,0.042150903,0.07614275,-0.007964368,0.017617455,0.019355094,0.0021403146,-0.022722242,-0.008437566,-0.120560445,0.06079991,-0.02450123,0.056866217,-0.027984299,0.0953046,0.0060259164,-0.016819948,0.071526095,-0.010647209,-0.054892763,0.007927533,0.029362135,0.0406503,-0.047378596,0.067627184,-0.022717446,0.020918837,-0.008057844,-0.11845254,0.033901073,0.03459002,-5.6078413e-33,0.0033512732,0.014557061,0.022594294,-0.031743262,-0.025827613,0.045982912,0.034451164,-0.117865674,-0.0013052771,-0.059501346,-0.03230977,-0.044422485,-0.011025001,-0.02180863,0.04037195,0.04359603,0.03378121,0.0032809575,-0.021312708,0.074309945,0.047625743,0.061165515,-0.04591683,0.025826229,-0.07760956,0.022944547,-0.110773295,0.08526729,-0.031354953,-0.053498752,-0.043989494,-0.029439086,0.026084127,-0.0922362,-0.040802225,0.05679352,-0.06679677,-0.0037306312,0.05997498,0.038317747,0.09731233,0.040499147,-0.004117921,-0.056729555,-0.04557251,0.02164538,-0.00078681915,-0.010525478,0.09299273,-0.042799614,0.0069003273,0.08899903,-0.079290465,0.080591954,-0.020849213,-0.008115192,-0.060371663,-0.006032324,0.015728641,0.035115182,0.015365709,0.029274873,-0.029051142,0.059096437,0.028898146,0.017718837,0.045179915,0.05573444,-0.011150871,-0.018413147,0.0036057243,-0.020186951,0.018878214,-0.040869,-0.035960574,-0.04246102,-0.030536296,-0.09808716,-0.07824065,0.025548272,0.028036809,0.0012412958,0.0055644303,-0.014433428,0.03406034,-0.045970317,0.014940487,0.09602337,0.06409733,-0.040037148,-0.011770144,0.020210626,-0.09256814,-0.0124549195,0.061497696,-7.632929e-08,-0.04195206,0.047652557,-0.004918356,-0.03202891,-0.015713368,0.014352929,0.032624327,0.048576403,-0.034449775,-0.0066465484,0.05020808,-0.027335072,-0.1133405,-0.032580234,0.039389208,0.06443395,-0.027006181,0.030240182,0.04639393,-0.027945185,-0.0036185205,-0.02509761,-0.041742947,-0.012518503,0.0005180389,-0.056300465,0.050802182,0.10191208,0.08278281,0.033232402,-0.054467883,-0.008380355,0.09688704,0.0033294156,0.08329315,0.0213239,0.14788009,-0.036581185,0.048863564,-0.06358346,-0.017330058,0.03725528,-0.026863018,-0.0019232372,0.09019601,-0.012601534,0.07809707,-0.05560982,-0.011964288,0.0019781445,0.08594073,0.045507204,-0.030877726,0.12961936,0.0032869785,-0.013330693,-0.056089904,0.0065222615,-0.03366866,0.0118445335,-0.008255359,-0.029580072,0.04669139,-0.097510345,11,-12.10012,-6.045641,7
