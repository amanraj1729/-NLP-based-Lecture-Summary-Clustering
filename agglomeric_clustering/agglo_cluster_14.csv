SerialNo,Session_Summary,BERT_Feature_0,BERT_Feature_1,BERT_Feature_2,BERT_Feature_3,BERT_Feature_4,BERT_Feature_5,BERT_Feature_6,BERT_Feature_7,BERT_Feature_8,BERT_Feature_9,BERT_Feature_10,BERT_Feature_11,BERT_Feature_12,BERT_Feature_13,BERT_Feature_14,BERT_Feature_15,BERT_Feature_16,BERT_Feature_17,BERT_Feature_18,BERT_Feature_19,BERT_Feature_20,BERT_Feature_21,BERT_Feature_22,BERT_Feature_23,BERT_Feature_24,BERT_Feature_25,BERT_Feature_26,BERT_Feature_27,BERT_Feature_28,BERT_Feature_29,BERT_Feature_30,BERT_Feature_31,BERT_Feature_32,BERT_Feature_33,BERT_Feature_34,BERT_Feature_35,BERT_Feature_36,BERT_Feature_37,BERT_Feature_38,BERT_Feature_39,BERT_Feature_40,BERT_Feature_41,BERT_Feature_42,BERT_Feature_43,BERT_Feature_44,BERT_Feature_45,BERT_Feature_46,BERT_Feature_47,BERT_Feature_48,BERT_Feature_49,BERT_Feature_50,BERT_Feature_51,BERT_Feature_52,BERT_Feature_53,BERT_Feature_54,BERT_Feature_55,BERT_Feature_56,BERT_Feature_57,BERT_Feature_58,BERT_Feature_59,BERT_Feature_60,BERT_Feature_61,BERT_Feature_62,BERT_Feature_63,BERT_Feature_64,BERT_Feature_65,BERT_Feature_66,BERT_Feature_67,BERT_Feature_68,BERT_Feature_69,BERT_Feature_70,BERT_Feature_71,BERT_Feature_72,BERT_Feature_73,BERT_Feature_74,BERT_Feature_75,BERT_Feature_76,BERT_Feature_77,BERT_Feature_78,BERT_Feature_79,BERT_Feature_80,BERT_Feature_81,BERT_Feature_82,BERT_Feature_83,BERT_Feature_84,BERT_Feature_85,BERT_Feature_86,BERT_Feature_87,BERT_Feature_88,BERT_Feature_89,BERT_Feature_90,BERT_Feature_91,BERT_Feature_92,BERT_Feature_93,BERT_Feature_94,BERT_Feature_95,BERT_Feature_96,BERT_Feature_97,BERT_Feature_98,BERT_Feature_99,BERT_Feature_100,BERT_Feature_101,BERT_Feature_102,BERT_Feature_103,BERT_Feature_104,BERT_Feature_105,BERT_Feature_106,BERT_Feature_107,BERT_Feature_108,BERT_Feature_109,BERT_Feature_110,BERT_Feature_111,BERT_Feature_112,BERT_Feature_113,BERT_Feature_114,BERT_Feature_115,BERT_Feature_116,BERT_Feature_117,BERT_Feature_118,BERT_Feature_119,BERT_Feature_120,BERT_Feature_121,BERT_Feature_122,BERT_Feature_123,BERT_Feature_124,BERT_Feature_125,BERT_Feature_126,BERT_Feature_127,BERT_Feature_128,BERT_Feature_129,BERT_Feature_130,BERT_Feature_131,BERT_Feature_132,BERT_Feature_133,BERT_Feature_134,BERT_Feature_135,BERT_Feature_136,BERT_Feature_137,BERT_Feature_138,BERT_Feature_139,BERT_Feature_140,BERT_Feature_141,BERT_Feature_142,BERT_Feature_143,BERT_Feature_144,BERT_Feature_145,BERT_Feature_146,BERT_Feature_147,BERT_Feature_148,BERT_Feature_149,BERT_Feature_150,BERT_Feature_151,BERT_Feature_152,BERT_Feature_153,BERT_Feature_154,BERT_Feature_155,BERT_Feature_156,BERT_Feature_157,BERT_Feature_158,BERT_Feature_159,BERT_Feature_160,BERT_Feature_161,BERT_Feature_162,BERT_Feature_163,BERT_Feature_164,BERT_Feature_165,BERT_Feature_166,BERT_Feature_167,BERT_Feature_168,BERT_Feature_169,BERT_Feature_170,BERT_Feature_171,BERT_Feature_172,BERT_Feature_173,BERT_Feature_174,BERT_Feature_175,BERT_Feature_176,BERT_Feature_177,BERT_Feature_178,BERT_Feature_179,BERT_Feature_180,BERT_Feature_181,BERT_Feature_182,BERT_Feature_183,BERT_Feature_184,BERT_Feature_185,BERT_Feature_186,BERT_Feature_187,BERT_Feature_188,BERT_Feature_189,BERT_Feature_190,BERT_Feature_191,BERT_Feature_192,BERT_Feature_193,BERT_Feature_194,BERT_Feature_195,BERT_Feature_196,BERT_Feature_197,BERT_Feature_198,BERT_Feature_199,BERT_Feature_200,BERT_Feature_201,BERT_Feature_202,BERT_Feature_203,BERT_Feature_204,BERT_Feature_205,BERT_Feature_206,BERT_Feature_207,BERT_Feature_208,BERT_Feature_209,BERT_Feature_210,BERT_Feature_211,BERT_Feature_212,BERT_Feature_213,BERT_Feature_214,BERT_Feature_215,BERT_Feature_216,BERT_Feature_217,BERT_Feature_218,BERT_Feature_219,BERT_Feature_220,BERT_Feature_221,BERT_Feature_222,BERT_Feature_223,BERT_Feature_224,BERT_Feature_225,BERT_Feature_226,BERT_Feature_227,BERT_Feature_228,BERT_Feature_229,BERT_Feature_230,BERT_Feature_231,BERT_Feature_232,BERT_Feature_233,BERT_Feature_234,BERT_Feature_235,BERT_Feature_236,BERT_Feature_237,BERT_Feature_238,BERT_Feature_239,BERT_Feature_240,BERT_Feature_241,BERT_Feature_242,BERT_Feature_243,BERT_Feature_244,BERT_Feature_245,BERT_Feature_246,BERT_Feature_247,BERT_Feature_248,BERT_Feature_249,BERT_Feature_250,BERT_Feature_251,BERT_Feature_252,BERT_Feature_253,BERT_Feature_254,BERT_Feature_255,BERT_Feature_256,BERT_Feature_257,BERT_Feature_258,BERT_Feature_259,BERT_Feature_260,BERT_Feature_261,BERT_Feature_262,BERT_Feature_263,BERT_Feature_264,BERT_Feature_265,BERT_Feature_266,BERT_Feature_267,BERT_Feature_268,BERT_Feature_269,BERT_Feature_270,BERT_Feature_271,BERT_Feature_272,BERT_Feature_273,BERT_Feature_274,BERT_Feature_275,BERT_Feature_276,BERT_Feature_277,BERT_Feature_278,BERT_Feature_279,BERT_Feature_280,BERT_Feature_281,BERT_Feature_282,BERT_Feature_283,BERT_Feature_284,BERT_Feature_285,BERT_Feature_286,BERT_Feature_287,BERT_Feature_288,BERT_Feature_289,BERT_Feature_290,BERT_Feature_291,BERT_Feature_292,BERT_Feature_293,BERT_Feature_294,BERT_Feature_295,BERT_Feature_296,BERT_Feature_297,BERT_Feature_298,BERT_Feature_299,BERT_Feature_300,BERT_Feature_301,BERT_Feature_302,BERT_Feature_303,BERT_Feature_304,BERT_Feature_305,BERT_Feature_306,BERT_Feature_307,BERT_Feature_308,BERT_Feature_309,BERT_Feature_310,BERT_Feature_311,BERT_Feature_312,BERT_Feature_313,BERT_Feature_314,BERT_Feature_315,BERT_Feature_316,BERT_Feature_317,BERT_Feature_318,BERT_Feature_319,BERT_Feature_320,BERT_Feature_321,BERT_Feature_322,BERT_Feature_323,BERT_Feature_324,BERT_Feature_325,BERT_Feature_326,BERT_Feature_327,BERT_Feature_328,BERT_Feature_329,BERT_Feature_330,BERT_Feature_331,BERT_Feature_332,BERT_Feature_333,BERT_Feature_334,BERT_Feature_335,BERT_Feature_336,BERT_Feature_337,BERT_Feature_338,BERT_Feature_339,BERT_Feature_340,BERT_Feature_341,BERT_Feature_342,BERT_Feature_343,BERT_Feature_344,BERT_Feature_345,BERT_Feature_346,BERT_Feature_347,BERT_Feature_348,BERT_Feature_349,BERT_Feature_350,BERT_Feature_351,BERT_Feature_352,BERT_Feature_353,BERT_Feature_354,BERT_Feature_355,BERT_Feature_356,BERT_Feature_357,BERT_Feature_358,BERT_Feature_359,BERT_Feature_360,BERT_Feature_361,BERT_Feature_362,BERT_Feature_363,BERT_Feature_364,BERT_Feature_365,BERT_Feature_366,BERT_Feature_367,BERT_Feature_368,BERT_Feature_369,BERT_Feature_370,BERT_Feature_371,BERT_Feature_372,BERT_Feature_373,BERT_Feature_374,BERT_Feature_375,BERT_Feature_376,BERT_Feature_377,BERT_Feature_378,BERT_Feature_379,BERT_Feature_380,BERT_Feature_381,BERT_Feature_382,BERT_Feature_383,kmeans_cluster,TSNE_1,TSNE_2,agglo_cluster
2,"in this session, we explored various feature encoding techniques, essential for converting categorical and textual data into numerical representations suitable for machine learning models. initially, we discussed vectorization and one-hot encoding. vectorization is a general approach to transform textual or categorical data into numerical vectors. one-hot encoding specifically converts categorical variables into binary vectors, creating separate columns for each category. this method is widely used in multiclass and multilabel classification problems but can introduce the curse of dimensionality when dealing with variables having many unique categories.

we then examined label encoding and integer encoding. label encoding assigns each categorical class a unique integer value. while this method is straightforward, it implies an ordinal relationship between categories, which may not always be appropriate. integer encoding is particularly useful when the categorical variable represents ordinal data (categories with a meaningful order), as it preserves the inherent ordering.

due to the limitations of one-hot encoding, such as increased dimensionality, we introduced binary encoding (compact encoding). binary encoding efficiently represents multiple categories using fewer columnsâ€”for example, just three columns can encode up to eight distinct classes. this approach helps mitigate dimensionality issues while retaining meaningful category distinctions.

additionally, we covered frequency encoding and target encoding. frequency encoding involves assigning each category a numerical value based on its frequency within the dataset. target encoding replaces categories with values derived from the target variable (such as the mean target value for each category), effectively capturing relationships between categories and the outcome variable.

finally, we briefly touched upon methods for converting textual data into numerical vectors through vectorization techniques, setting the stage for deeper exploration in future sessions.
",0.064315565,-0.017309297,-0.03495265,-0.03392214,-0.008965653,0.084191196,-0.05334848,-0.08130464,-0.010383625,-0.08575638,-0.04769419,-0.056518,0.029229691,0.023511168,0.02405323,0.016847508,0.038234565,0.11052629,-0.098765664,-0.045082785,0.03699567,0.01438896,-0.0127853155,0.030817887,-0.02018749,0.012107763,-0.029584039,0.029794784,0.0015186338,-0.03843033,-0.0092262365,0.01975149,-0.023530973,0.03249527,-0.08196292,-0.0065854923,-0.05596749,0.023488399,-0.047403112,0.03650898,-0.034319174,-0.01942872,0.060484253,0.029664662,0.04294593,0.056446347,-0.03959661,-0.067425266,-0.04159888,-0.0067893183,0.003970575,0.016628288,-0.022764482,0.090238445,-0.12027711,-0.04808796,-0.011890465,-0.034527592,-0.031719297,0.05059133,-0.12739688,-0.014515973,0.04980854,-0.003307034,-0.028641172,-0.04399474,-0.035548076,0.035317652,-0.020429274,0.048436962,0.018664096,0.029455753,-0.06912344,0.073525414,0.047234237,0.04226945,0.027769806,0.010472934,0.08425549,-0.0086152805,-0.06215549,0.055188507,0.0016303856,0.010958043,0.12903938,-0.029580433,0.011739848,0.048750658,-0.0967844,0.068278246,-0.056337554,0.012711378,0.1836454,-0.0061930316,-0.013584354,0.00038959287,0.029927181,-0.011445856,0.019615373,0.01812876,-0.03844954,0.0038375931,-0.030760225,0.022820558,0.009134734,-0.059948128,0.068750314,0.0017029396,0.10586872,-0.11951895,-0.0019843245,-0.012996677,-0.12483963,-0.023491064,-0.013012547,0.037968226,-0.043638118,0.03494984,0.07707304,-0.002847448,-0.030617977,-0.031127892,-0.018733129,0.048287366,-0.02620892,0.0013319742,-0.025080757,6.107929e-33,-0.0001294755,0.01096251,-0.049579754,0.0026459645,0.047548458,-0.06632034,-0.09276042,0.030886184,0.053839963,0.0628835,0.0014993849,0.06284356,-0.007997635,0.12488317,0.019212916,0.00052056415,0.01680593,0.0689341,-0.06561932,-0.06236966,0.024504308,0.0287547,0.05545335,0.022657316,0.025752949,0.02837341,-0.01627408,-0.07047158,-0.044857044,0.0048670257,-0.04718431,-0.049493488,0.03581485,-0.020163279,-0.003968665,-0.059879653,0.012043314,0.037964616,0.04885602,0.036667388,-0.03702572,-0.012962852,-7.041542e-05,-0.004320996,0.051373124,0.08479368,0.04871576,0.013402908,-0.06291343,-0.012761261,0.020147178,-0.018421628,0.027576596,-0.002001792,-0.03237191,-0.011100253,0.035630386,-0.042195648,-0.07951477,0.006168415,-0.07290385,0.07749096,0.10091134,0.00071077794,0.018054359,-0.053382263,0.07109798,0.0028408777,0.06055629,0.030408867,-0.05339338,0.06153742,-0.042928625,-0.1013476,-0.024973078,0.064513795,0.07557921,-0.09570497,-0.034267217,0.057993036,-0.0072549386,-0.041538235,0.07332219,-0.03978655,-0.05956041,-0.03566954,0.05217822,-0.08969375,0.04404146,-0.0128100375,-0.06396577,0.056714095,0.013309984,-0.045472827,0.055809684,-6.306825e-33,-0.08186981,-0.003926831,-0.018729549,-0.025191415,-0.034494232,-0.013631768,-0.031493057,0.010091401,-0.047085587,-0.039651975,-0.01815746,-0.0062447623,0.008603042,-0.03907947,-0.0030662732,-0.0052360627,-0.14758076,0.06978869,-0.026926152,0.062889375,-0.000910187,0.10774681,-0.08435408,0.0658252,0.0135913845,0.007947735,0.02543989,-0.058190774,0.042913016,-0.012139209,-0.026807629,-0.04540995,0.022255657,-0.03888034,0.027121738,-0.01669779,0.04217166,-0.05040151,-0.02095703,0.08916053,0.029422207,-0.018254194,-0.063432485,0.023870932,-0.027285447,-0.022547718,-0.026366202,0.0026586533,0.03778186,0.03321453,0.026209932,0.041763846,-0.034781452,-0.007349892,0.0800326,-0.026606431,-0.06764428,-0.049477067,0.028353192,0.0120382495,-0.027037561,-0.014416787,0.12286645,-0.043213505,0.0019820908,-0.08096087,-0.0016181255,-0.05878907,-0.08585099,-0.046667036,0.038576536,0.019435955,-0.042197883,0.049938098,-0.109826945,-0.065936364,0.034040138,-0.0013289824,-0.09223373,0.03869577,0.040001728,-0.0086804135,-2.2409624e-06,0.12214959,0.0043222867,0.0011970231,0.1166567,0.045364577,0.058192674,0.010489235,0.009623426,0.1391772,-0.016587874,0.12787822,-0.029041173,-4.833203e-08,-0.0077816597,0.014887033,-0.065140866,0.0076978183,0.005089198,0.026844492,-0.044435818,0.06547486,-0.043438498,0.06430076,0.044107527,0.056245975,-0.05436715,-0.021020573,0.022711597,0.0150491465,0.032764707,-0.061110925,0.05608884,0.05510123,0.052337237,0.043191135,0.032162853,-0.06979968,-0.040518567,-0.1074998,-0.00644413,0.14355919,0.12253035,0.0112171285,-0.043400254,0.04381991,-0.031431638,-0.030508535,2.3559958e-05,0.04661543,-0.023925025,-0.026145408,-0.022883158,-0.07671665,0.011213461,-0.004504616,-0.06363249,0.011101463,-0.0037967067,-0.027291201,0.017831763,-0.014363169,0.038886033,0.07695851,-0.11355457,0.02107751,0.04177552,0.04352275,0.0069587394,0.015051664,-0.077357486,-0.058972124,0.03847085,-0.029283501,0.06164211,-0.021426834,-0.01840365,-0.034825366,12,32.597057,-4.103371,14
52,"today's class started with a brief discussion on our upcoming projects and assignments. then we moved onto encoding of categorical variables. we studied in brief various encoding methods, their advantages and disadvantages.
1. one hot encoding: good if the independent variables are not ordinal. if the output is not ordinal, this is not preferred. but this increases the dimension of the dataset, bringing along the problems of high dimensionality.
2. binary encoding: instead of giving separate columns to each of the bit, combine all the columns into one binary number column.
3. integer encoding: has a sense of order. it can't be used for nominal labels.
we also discussed other methods of encoding like frequency and target encoding. then we started discussing about text processing. any text processing application involves converting text into numbers of some sort and then make meaning out of them. mostly words called stop words that don't add much meaning to sentences are removed and a dictionary is made out of the rest of the words for further processing.",0.06426875,0.016037561,-0.07863354,-0.048438136,-0.042935643,0.06573073,0.0038690495,-0.04350671,0.023155656,0.006686785,-0.008061834,0.034260992,0.014002986,0.021832349,0.042595293,0.00026202042,0.073054954,0.060932778,-0.053322863,-0.05956126,0.061984703,0.008430614,0.024405964,-0.0059406213,0.0023367237,0.039154887,-0.07289356,-0.0020380854,0.041955266,-0.015651563,-0.02703003,0.06922067,0.043298766,0.033369467,-0.06325485,0.0573589,0.03604523,0.027830416,-0.059847847,0.0005937324,-0.06852007,-0.038495213,0.05487769,0.038106382,0.05736232,0.02040556,0.003978661,-0.0153707955,-0.1558344,-0.035007723,-0.019733496,0.031693507,-0.017795583,0.09073951,-0.0742951,-0.011444022,-0.03270845,-0.021340538,-0.07950995,0.018325351,-0.1492273,0.005990999,-0.0059563387,-0.009777821,-0.010635055,-0.0657548,0.0102913,0.004843142,0.04784861,-0.011115726,-0.020425234,0.0140913455,-0.06309735,0.05659276,0.022669403,-0.012876309,0.028627923,-0.018475832,0.058904044,0.009213961,-0.0015084627,0.001593703,0.031117294,0.08891154,0.04657557,-0.04391841,0.036393594,0.054563392,-0.13253355,0.031718027,-0.012807743,-0.022931619,0.18671922,-0.0002323055,0.024843348,-0.0112741655,0.023984866,0.0036636824,0.06888628,-0.03966171,0.046907555,-0.0349455,-0.08454747,-0.03263974,-0.03972706,-0.07860302,0.098228864,0.037153237,0.03210334,-0.07822369,0.0046632676,0.01191952,-0.09869876,-0.09423487,0.002905608,-0.006382447,-0.028672734,0.007467775,0.09325814,-0.04270374,-0.04240981,-0.053464983,-0.067460686,0.03604434,-0.0068115583,0.004200289,0.012326749,3.931291e-33,-0.0048873015,0.03695096,-0.10681008,0.022032578,-0.029671667,0.0018159213,-0.043970212,-0.011749351,0.080215,0.028122393,0.023095177,0.060791377,-0.026609499,0.12179553,0.0900084,-0.04605486,0.016481798,0.07001761,-0.05142794,-0.001374015,0.04216519,0.095607445,0.085291155,0.029508261,0.049998827,0.022532785,-0.028555624,-0.15008645,-0.03663698,-0.0069732917,-0.055888318,-0.046913043,0.021255996,0.023279311,-0.050670873,-0.0636185,0.013364895,0.037857756,0.012402118,0.015628414,-0.012661061,0.0462086,0.005448393,-0.00010709521,0.040440522,0.05335505,0.042261444,0.029781625,-0.05427909,0.009147564,0.019150054,0.010469626,0.027026393,0.041367665,-0.0014274218,0.023367904,0.037232544,-0.12865454,-0.07537971,0.043325856,-0.09734683,0.019829476,0.14380442,-0.0319333,0.0236503,-0.070927896,0.05760747,-0.07613057,0.022532566,-0.0015438187,-0.06728501,0.05327325,-0.053034756,-0.09329015,-0.00031406886,0.08467959,0.019164301,-0.09682618,-0.06813906,0.023590203,0.03616124,-0.023618141,0.028631208,-0.05233794,-0.038214922,0.02959966,0.0021387956,-0.10823284,0.03505205,-0.010618052,-0.04097241,0.044595666,0.004175714,-0.022192238,0.05348994,-5.05252e-33,-0.026907299,0.011775002,-0.062375624,0.008926563,-0.08705793,-0.014897969,0.029163253,-0.013689678,0.016635511,-0.06715455,-0.088080235,-0.013498695,-0.012995844,0.016470518,-0.029682852,-0.045149807,-0.07098986,0.0526919,-0.012307651,0.02107947,-0.02336801,0.10634213,-0.12720855,-0.00028324197,0.05263705,0.03287589,-0.055078864,-0.044631947,0.015505903,0.038861424,-0.041760843,-0.042418443,0.058309942,-0.046924084,-0.024222186,0.014458846,0.07487605,-0.01339506,-0.07875638,0.04851622,0.019081628,0.027718108,-0.02131095,0.020244304,0.022166638,-0.024481585,-0.07659904,0.011520543,0.025301145,0.07140328,0.10441897,-0.0058601983,-0.021139216,-0.028695203,0.06409755,-0.07014797,-0.05418532,-0.087932065,-0.02778198,0.035801798,-0.036465153,0.001817544,0.112696335,-0.041352898,-0.031179648,-0.046522457,0.015364113,-0.036624674,-0.05248528,-0.05546849,0.07357045,0.033327796,-0.0069436654,0.055146188,-0.07524793,0.024407204,0.03476144,0.022433072,-0.082183294,0.10271816,0.04180278,0.012705045,-0.0011248814,0.0512665,-0.031256273,0.04358649,0.10475209,0.03377636,-0.03194205,0.0025427914,-0.009106901,0.07171921,-0.012937258,0.0956168,-0.05187855,-5.6012286e-08,-0.0016806355,-0.06063678,-0.08788954,-0.00445636,-0.032818843,0.032385763,-0.029553419,0.053518414,0.0030280543,0.025297279,0.049562294,0.045523927,-0.0767828,0.016455425,0.050291136,-0.0040036677,0.083059534,-0.061712556,0.017879618,0.036436558,0.07107693,0.050296042,-0.12442144,-0.039213657,-0.029174,-0.034487423,0.029006764,0.090512216,0.080874816,0.0025684072,-0.025958622,0.07858661,-0.008689579,0.018939009,0.011498301,0.022642184,0.009996803,-0.022747397,-0.008118919,-0.05080615,0.02768009,-0.024604756,-0.052114982,0.00066754327,0.01949111,-0.059787743,-0.019724248,-0.0076680696,0.011288047,0.09766375,-0.07257595,0.03524933,-0.0120051075,-0.0090745175,0.03900992,0.01377018,-0.046654753,-0.03843023,-0.035630926,0.004294351,0.03500523,0.0072759395,-0.015141878,-0.036370322,12,32.744877,-5.588552,14
54,"we talked about the plan for the remaining 10 lectures and the group project.

started the class by learning about function encoding.

saw how to change categories like red, blue, green into a function with three variables.

there are two types of problems: multiclass and multilabel, and the approach depends on that.

learned about binary encoding, which is a very compact way to encode data.

discussed how to convert data using binary encoding in detail.

frequency encoding replaces category values with how often they appear in the dataset.

in target encoding, category values are replaced by the average score above 2.5.

other encoding methods include label encoding, one hot encoding, and image encoding.

",0.010451573,0.018354258,-0.06887495,-0.012573586,-0.07558583,0.019224513,-0.020158796,-0.057797715,-0.049252134,-0.04379509,-0.014400778,-0.06065981,0.0075575225,-0.017628428,0.054770615,0.018497098,-0.024429671,0.063830696,-0.090744376,-0.11003645,0.047148943,-0.030779341,0.07834541,-0.012332568,-0.0021814108,0.031552885,-0.032696195,-0.008393528,0.03514883,-0.07214952,-0.03838778,0.04391889,0.086173944,0.05746007,-0.06328681,0.07808566,-0.009283538,-0.023737112,-0.033391297,0.076292574,-0.099189594,0.048409153,0.036061272,-0.0045559364,0.03179601,0.06562037,0.0033491298,-0.040401753,-0.056701105,-0.03980259,0.0470572,-0.015814945,-0.074208766,0.023410264,-0.031704202,-0.0018533347,-0.001199772,-0.073498376,-0.05375103,-0.03511121,-0.0965802,-0.0011615801,0.004464497,0.08521985,0.018320512,-0.03665949,-0.0070167882,0.053368174,0.03840148,-0.031241588,-0.031026859,-0.0027590394,0.018091243,0.012592641,0.100468285,0.029128643,0.019255796,0.014418493,0.01953799,-0.07918105,0.053771507,-0.010474548,0.013549201,0.10779432,0.13721688,0.0027213357,-0.030688917,0.0967002,-0.1628968,0.02129989,-0.0499463,0.06152355,0.1137496,-0.008203175,-0.0425162,0.019940818,0.025827527,-0.09060059,0.07553251,0.016446413,-0.041686404,-0.002774182,-0.011865606,-0.121713385,-0.11186544,-0.0865847,0.070786394,0.094829254,0.04626077,-0.08622025,0.0016706747,0.007265409,-0.09346069,-0.092780106,-0.025459561,0.003221113,0.018542508,-0.0148145,0.08092594,0.012562352,0.042689387,-0.024704162,-0.009820022,0.031958397,-0.037029278,-0.002266802,-0.10906812,2.3340812e-33,0.021841802,0.011571528,-0.027880441,0.020522337,0.031919476,-0.018058868,-0.025797905,0.0112271095,-0.028156577,0.008630197,0.028144062,0.04677658,-0.022279741,0.122661255,0.07632258,-0.06671815,-0.047468662,0.026997918,-0.067601986,-0.023347622,-0.029870085,0.07316694,0.09891253,-0.040989123,0.021919614,0.09533084,0.021147398,-0.06154277,0.059093278,-0.0042348546,-0.0053637563,0.032936107,-0.06819768,0.01956738,-0.018000908,-0.012899191,0.06598332,0.01911364,0.032139063,0.042180777,0.034514856,0.06594702,-0.016129596,-0.016247096,0.09170402,0.09473147,0.09285283,-0.0017672117,0.017731871,0.031748503,-0.048405148,-0.025812455,-0.018778162,-0.027837543,-0.021849722,-0.0020452943,0.049054444,-0.06020408,-0.054469045,0.009122875,-0.009522203,0.10214898,0.091765516,0.008928289,-0.0035071922,0.0016797703,0.038749576,-0.065590814,0.04943958,-0.0068220836,0.0026409777,0.035334285,-0.05802246,-0.08166162,-0.014600848,0.03784103,0.0018557266,-0.05543438,-0.10288179,0.006127929,0.018659296,-0.059042905,0.016099244,-0.062640324,-0.07259418,0.013255696,0.06198207,-0.08198657,-0.021871159,0.044477254,-0.021726169,0.016723832,-0.008520965,-0.00038970576,0.046747353,-4.2072085e-33,-0.03892861,0.10818374,-0.056230217,0.02294852,0.031944584,-0.009618572,-0.032328732,0.02066358,0.04286206,0.0625709,-0.036380544,-0.011864097,-0.0015184731,-0.027130466,-0.05591379,-0.05826214,-0.0574705,0.013971979,-0.02868079,0.04556246,-0.025094127,0.1357809,-0.061802723,0.03501598,-0.002899768,0.060532767,0.007579502,-0.03493575,0.060810238,0.009834969,-0.056509607,-0.08659694,0.002540312,-0.036953233,-0.03457653,0.012919674,0.0735041,-0.015232135,-0.094986886,0.08971443,0.088185795,0.00031992618,-0.081822425,-0.02362591,0.013466921,-0.0027673324,0.011002228,0.026224105,-0.02808005,0.041166056,0.057113726,-0.04110308,-0.03486937,-0.08919272,0.084416196,-0.023633646,0.022162475,-0.13366355,-0.013513443,0.08241885,-0.079072066,-0.035272744,0.07224745,-0.027421253,-0.06167318,-0.07972506,-0.016011268,-0.03947372,-0.043653186,0.016331349,0.018704925,0.0014297379,0.0064194645,0.0006805218,-0.049178418,-0.046222135,-0.052722707,0.035199825,-0.050890796,0.06731285,0.0363551,-0.055878907,0.0445771,0.13898697,0.004066779,0.009397667,0.098751575,0.038964152,0.010510562,-0.022371283,-0.01980997,0.08723372,0.052164312,0.09833672,-0.01959626,-4.7059245e-08,0.014504313,-0.03687966,-0.018619005,0.014165961,-0.028412012,0.0073712496,-0.07316973,0.029285748,-0.022478947,0.053874545,0.014141808,0.05306724,-0.014867014,0.031595234,0.026629046,0.05388893,0.05054954,0.028731171,0.03155386,0.037353,0.01484763,-0.02083989,-0.065084286,-0.06158446,-0.03717286,-0.03562746,-0.018645577,0.055381086,0.049181722,-0.005389646,-0.047450576,0.06600794,-0.027464792,0.028229749,-0.024546972,-0.082731895,-0.081979685,0.014307657,0.024247797,0.017670091,-0.003808227,-0.020661697,-0.056819934,0.018228967,-0.0033022002,-0.025704646,-0.00056316523,-0.021283636,-0.0141612645,0.080056906,-0.047312606,0.010191172,-0.09633447,0.016422462,0.043920975,0.00937992,-0.02692121,-0.019037627,-0.0050897896,0.00904426,0.010317744,0.027382232,-0.08643355,-0.00515,12,37.102505,-4.5885043,14
63,"in todayâ€™s class, we covered different techniques for encoding categorical data. we started with feature encoding, where we learned about one-hot encoding and vectorization. one-hot encoding is useful for categorical data, but it can lead to the curse of dimensionality when the number of unique categories is very large. vectorization, on the other hand, helps convert text data into numerical form, which we only touched on briefly toward the end of the lecture.  

we also explored label encoding, which assigns a unique integer to each category. however, if the labels have a natural order (making them ordinal rather than nominal), using integer encoding would be more meaningful. for multiclass and multilabel problems, different encoding approaches may be required depending on the complexity of the data.  

binary encoding was another method we discussed, where each category is converted into a binary format, which helps reduce dimensionality â€” for example, three binary columns can represent up to eight classes. we also learned about frequency encoding, where the frequency of a category is used as its encoded value, and target encoding, where the encoding is based on the relationship between the category and the target variable.  

overall, the session introduced us to the challenges and trade-offs involved in different encoding methods and gave us a basic idea of how to handle text data through vectorization.",0.046598017,0.032033388,-0.06761065,-0.06468093,-0.024083028,0.0538005,-0.036026165,-0.04111555,0.015772667,-0.03741988,-0.05344941,-0.013449599,0.0050542983,0.047430772,0.05070222,-0.010527314,0.027628794,0.102714516,-0.07358762,-0.053598113,0.005341568,-0.017830607,0.052003384,0.01604127,-0.013026862,0.056175403,-0.055493835,-0.04115254,0.010060575,-0.03893064,-0.004727095,0.07955261,0.014128595,0.048028324,-0.090200484,-0.004072822,-0.008114555,0.021471988,-0.04407298,0.0724668,-0.06527848,0.0149904005,0.057485823,0.02865225,0.015755063,0.05014668,-0.017897196,-0.06211651,-0.07990654,-0.026724402,0.0018438589,0.017877407,-0.040610593,0.07319691,-0.09418318,-0.024170138,0.0037516486,-0.03157969,-0.053516295,0.06014459,-0.15482318,-0.01889598,0.060857534,0.0075732884,-0.0126324585,-0.03606921,0.00645752,0.0055870125,0.015792888,0.047865026,0.020622563,0.03182919,-0.043944232,0.059153333,0.057235327,0.021253519,0.037358638,0.0097648455,0.048129644,-0.00074260466,-0.011888683,0.021127349,0.009843723,0.04541474,0.13141274,-0.030675663,0.0043838047,0.051490474,-0.10180236,0.047771774,-0.01946673,0.0139380945,0.13780591,0.025892874,0.009273794,-0.017914085,0.028252725,0.03933097,0.052373867,-0.027791511,0.009962722,-0.022546124,-0.04469688,-0.008527051,-0.0044881683,-0.07216822,0.07924916,0.009652703,0.08732026,-0.10793765,0.0050239945,0.007906169,-0.1325647,-0.027639635,-0.037825305,0.0014710319,-0.015969072,0.0110674435,0.107373536,-0.010544975,-0.03799908,-0.06901822,-0.0265717,0.02548385,-0.018597513,0.040832795,-0.02474182,2.171984e-33,-0.0077095744,0.03629774,-0.06294245,0.031369705,0.0067473957,-0.03975277,-0.06575991,0.0225713,0.048932362,0.054522537,0.04406618,0.085771635,0.0043465314,0.124977805,0.052618165,-0.032034762,-0.00055969646,0.057106204,-0.06899605,-0.048116334,0.034786835,0.05248873,0.08964363,0.0024328467,0.070183836,0.04298701,-0.023540206,-0.09449214,0.022253511,-0.0010980241,-0.062360544,-0.07941618,0.026306953,0.020091943,-0.014901277,-0.042158682,0.02994462,0.018931825,0.008086869,0.0073544495,-0.020701932,0.013001571,0.024349885,-0.023538245,0.055595316,0.08242332,0.030044217,0.026233712,-0.06100906,-0.04472259,-0.01913996,-0.0035306006,0.0069858576,0.00084457063,0.012664351,-0.002008126,0.022522012,-0.074706584,-0.07385738,0.019476026,-0.08415071,0.06580187,0.13733697,-0.018438885,0.0029703719,-0.018233879,0.04591447,-0.02451849,0.06522458,0.0014845197,-0.036279798,0.047411174,-0.089949846,-0.09534067,0.001022228,0.059170116,0.051307652,-0.081902884,-0.042906966,0.061322108,0.0095743975,-0.06171729,0.06712696,-0.022503337,-0.05875003,-0.0098571945,0.05283601,-0.09513399,0.049305286,0.015162219,-0.03252935,0.045579217,-0.0115550915,-0.021508392,0.06121555,-3.526433e-33,-0.08538587,-0.0046272767,-0.040297877,-0.0014661377,-0.047921035,-0.020554861,-0.013627098,-0.0028114324,-0.038585674,-0.05241008,-0.03299851,-0.0074405507,-0.022146892,-0.023747627,-0.019972716,-0.0338642,-0.12791088,0.043222334,-0.006852222,0.023434155,-0.002083998,0.09474837,-0.09111363,0.049436517,0.019786593,0.022966068,-0.019899849,-0.064157695,0.042353693,0.0017670639,-0.021469109,-0.10002159,0.047791835,-0.04156379,-0.0024230655,0.018574452,0.07367967,-0.012986387,-0.04743598,0.08696062,-0.010855618,-0.013128489,-0.044246934,0.0147310095,0.0064286105,-0.01592641,-0.040313408,0.034127697,0.044812556,0.07774907,0.066776656,-0.0033884577,-0.01747254,-0.0400208,0.09807525,-0.050760694,-0.068914406,-0.07036103,-0.015047556,0.041930925,-0.049659953,0.0005074659,0.10796341,-0.01967977,-0.05602834,-0.05465868,0.005795522,-0.06287716,-0.10473987,-0.043313947,0.054371916,0.057713166,-0.056416854,0.063675866,-0.088663556,-0.010652161,0.025561804,0.023751192,-0.09604994,0.04814946,0.048057273,-0.012934135,0.008563942,0.12233198,-0.0019540212,0.00010400474,0.13660513,0.042909246,0.033718605,-0.027390981,0.018417878,0.08761066,-0.053061463,0.11698777,-0.027448764,-5.196731e-08,-0.0074007004,-0.007797792,-0.0672892,-0.026505765,0.0010681,0.021830136,-0.05099694,0.05254159,-0.046149127,0.068881094,0.038199775,0.063301295,-0.060788516,0.004478883,0.053725958,-0.013918058,0.06339869,-0.07294737,0.058334563,0.07683551,0.045908775,0.031462897,-0.037015557,-0.068430334,-0.031431127,-0.056236986,0.03133635,0.10129819,0.113427795,-0.025640544,-0.068477206,0.05442635,-0.050937917,0.019968234,0.006968353,-0.006912466,-0.03925438,-0.025186261,-0.017148618,-0.06882487,0.01625294,-0.03197479,-0.044737,-0.010808837,0.010386191,-0.017058168,-0.052085843,0.0064315903,-0.012803021,0.109561265,-0.112945214,0.028307978,-0.0045307917,0.030518277,0.025433252,-0.020236053,-0.089176916,-0.06733508,0.008601077,0.009048345,0.058850925,-0.014137243,-0.07688178,-0.030498855,12,32.81461,-4.8416743,14
70,"in class today, sir explained various feature encoding techniques. he began with label encoding, which assigns unique integers to each category. the second method is integer encoding, suitable for ordinal variable categorization. then came binary encoding, a more compact representation, in which three columns can depict eight classes. frequency encoding involves replacing categories with their occurrence counts, while target encoding allocates values based on the target variable statistics. one-hot encoding is particularly useful when it comes to multi-class and multi-label problems, but it suffers heavily from the curse of dimensionality. in conclusion, a brief introduction to text vectorization techniques, which are techniques to convert text into numerical representations, was also introduced. ",0.029227434,0.027984008,-0.09223321,-0.04759346,-0.045616385,0.09129354,0.011362118,-0.03250711,-0.046294194,-0.047427673,-0.03895592,0.049436904,0.06638691,0.00871763,-0.04742625,0.020195963,-0.003171405,0.10686102,-0.05195528,-0.08618691,0.022547731,0.02556935,-0.0017266038,0.0010894969,0.029357428,0.0784206,-0.046274923,-0.010838747,0.006721557,-0.022215351,-0.011276638,0.049730472,0.04848649,0.050181825,-0.0905419,0.023159629,-0.03549662,0.055044685,-0.091164246,0.050927494,-0.031649116,-0.026749426,0.055843234,0.053084057,0.09486524,0.03621226,-0.09046808,-0.012516979,-0.026276281,-0.05419519,-0.015152268,0.03398956,-0.044600047,0.044642955,-0.11683468,-0.029480569,0.009506062,-0.004463029,-0.029047485,0.016524838,-0.10827404,0.001079644,0.06252842,0.02314038,0.019181432,-0.060082816,-0.00352747,0.033255856,-0.0128377145,0.03455308,-0.036371388,0.024910215,-0.054131202,0.059152253,0.03179549,0.043078035,-0.01883556,0.006529819,0.069017045,0.0044658184,-0.017575853,-0.020835152,0.07686812,0.035169147,0.10404483,0.016620465,0.015494022,0.026592348,-0.11101962,0.031416167,-0.014033583,-0.012034937,0.143957,-0.0038516922,-0.062144037,-0.024403252,0.0151101295,0.032252178,0.03811975,-0.019676778,-0.009731219,-0.045797717,-0.048779357,0.011849805,-0.043770213,-0.063953795,0.10465769,-0.00066507666,0.103289925,-0.12195081,0.0037185058,-0.047779553,-0.16119316,-0.06700024,-0.04075402,-0.028573837,0.0061453804,0.029435612,0.055328738,-0.009492105,-0.051571872,-0.067975566,-0.06073476,0.029381387,-0.08216852,0.015244385,-0.04076798,2.7353992e-33,-0.018629663,0.039581027,-0.09112266,0.050668254,-0.0128165055,-0.023664724,-0.057633817,0.0040717763,0.06403346,0.040438175,0.035915032,0.024002613,0.0007579253,0.1271494,0.041158505,-0.04234498,0.0069486927,0.034372196,-0.038593516,-0.10762797,0.013807554,0.0679818,0.09120249,-0.026810957,0.029919825,0.080219164,0.008095181,-0.11121648,-0.0022445961,-0.0030514565,-0.0657934,-0.021110352,0.008777352,-0.01743346,-0.017961334,-0.029149234,0.047053915,0.013666588,0.0310872,0.035907015,-0.032288104,-0.009332188,0.03870776,-0.057431694,0.017284824,0.10971683,0.04416875,0.050018948,-0.030183215,-0.0046790126,-0.02839212,-0.008015699,-0.012297678,0.028470272,0.024136858,-0.02870433,0.055595905,-0.026292702,-0.06806729,0.0419841,-0.059185218,0.062432535,0.11419105,0.0010287534,0.01669453,-0.09738662,0.04225329,-0.055263806,0.039251536,0.06200744,0.012996209,0.06872971,-0.073812164,-0.07098252,-0.074591056,0.05256539,0.032046337,-0.03851261,-0.06305239,0.029617954,-0.007864254,-0.054242056,0.04754161,-0.03827858,-0.05038031,-0.01570558,0.033512242,-0.09754142,0.067420684,0.030502453,0.00050430605,0.022462906,0.018201493,-0.03347484,0.043524034,-3.4765954e-33,-0.07925363,0.04725659,-0.034421556,-0.01627544,-0.07502149,-0.01155649,-0.04037775,0.029198602,-0.020479226,-0.035442114,-0.040182546,-0.011177664,-0.008295506,-0.049941592,-0.009073296,-0.041104134,-0.08753094,0.060815938,-0.025195349,0.04333963,0.008565269,0.056704026,-0.044966113,0.02631643,0.047474466,-0.013465367,0.047919642,-0.038202878,0.036590654,-0.006050544,-0.035017144,-0.079889484,0.038296208,0.020608323,-0.02907905,0.026401652,0.0677311,-0.004916915,-0.0014847177,0.082453,0.04448137,0.0061971657,-0.029082159,-0.03988341,0.010244547,-0.002360721,-0.040200416,0.041350145,0.05191147,0.03157985,0.075237915,-0.018710155,-0.017085196,0.01863806,0.019252634,-0.02850266,-0.043385074,-0.03758916,0.018852685,0.011848034,-0.032731574,-0.020789085,0.11593847,-0.036827046,-0.04095482,-0.0708983,0.014161517,-0.008334794,-0.13365905,-0.024915544,0.068490155,-0.008275686,0.015535701,0.063158154,-0.08520041,-0.06737962,0.017788084,0.009071175,-0.09048628,-0.005214715,0.042576794,-0.01175622,0.013812138,0.13943991,0.01232349,0.0026647302,0.13834444,0.0006810293,0.0393642,0.008381867,0.01071884,0.15000229,-0.02405058,0.13107362,-0.0067129424,-4.396043e-08,-0.034828346,-0.09271372,-0.10608223,-0.01602876,0.021195589,0.023798263,-0.07477144,0.03359337,-0.025773462,0.023600727,0.031572897,0.00072607805,-0.040811576,-0.038908612,0.09498963,-0.010604218,0.06649182,-0.0016567715,0.063433915,0.061338842,0.05916554,0.04414163,0.010628141,-0.050411314,-0.050686892,-0.025098845,0.043319415,0.10798543,0.1382868,-0.021710105,-0.0341489,0.08735878,-0.026782839,-0.020544222,0.030211749,0.015924463,0.029519351,-0.000841245,-0.035036664,-0.030937118,0.0058126748,-0.013791975,-0.08077898,0.014491911,0.021628091,-0.024460852,-0.015726272,-0.03412774,0.029358303,0.07227442,-0.066492215,0.010317561,0.0140183475,-0.02076424,-0.013602074,-0.023556728,-0.00714972,-0.040834323,0.023725467,-0.0061042337,0.021786444,0.05156996,-0.02673816,-0.037358344,12,31.896498,-4.8649793,14
71,"in today's session, we first discuss what we are going to cover next in the course, including feature engineering, data engineering, data preparation, big data, cloud computing, and tools in the cloud, etc. we further discussed the upcoming project and exercises. then we discussed the feature encoding irrespective of the kind of dependence of y, whether it is dependent on categorical variables, they must be converted to a numerical value. there are 6 types of encoding as follows:
label encoding: we assign a unique integer to each category, like red:0, blue:1, or green:2, but these already assign an ordinal relationship in the data, and we should apply this to y rather than x. 
one-hot encoding: converting categories into vectors like red:(1,0,0), blue:(0,1,0), and green:(0,0,1), but here we increase the dimensionality in the data. 
binary encoding: converts the categorical values into separate binary columns as a:00, b:01, c:10, d:11, this is similar to pseudo one-hot encoding, but here we reduce the dimensionality comparatively, still has various columns. 
integer encoding: somewhat similar to label but used in tree-based models mainly. frequency encoding: assigning the frequency of the categorical variable in the dataset. target encoding: replaces categories with the mean of the target variable for each category. 
for the y=f(x) we simply use one-hot encoding for the x-side and the y-side, consider everything, one or multiple labels based on the problem, and apply appropriate models. we further learn about feature binning, where we change continuous numerical features into categorical features and then use the classification model. at last, we see about llm in which statistical processing can generate deterministic output and we will mainly use it for code generation.
",0.039327424,-0.039618395,-0.024304312,-0.022334063,-0.01019292,0.04548214,-0.025195694,-0.104482405,0.011342622,-0.06048083,-0.020016622,-0.043798707,0.015102755,0.04738817,0.056641914,0.044910498,0.041854143,0.034951862,-0.0707248,-0.025142634,0.03885785,-0.007983409,0.0075322264,0.03444517,0.024698352,-0.0042059957,0.0005172627,-0.0024272809,-0.015759079,-0.04376328,-0.042371463,0.09724404,0.0019340875,0.041039288,-0.030295497,0.02975188,0.029450167,-0.011285249,-0.058915425,0.020274244,-0.025875324,-0.028631905,0.06805027,0.030082662,0.02649233,0.060905177,-0.021768125,-0.023508959,-0.12515341,-0.031770073,-0.004179818,0.058241364,-0.076540135,0.09450685,-0.011467597,-0.027028708,-0.04478635,-0.021179711,-0.026408482,0.01796952,-0.13624468,0.0161301,0.041294485,0.011606992,0.025537875,-0.05541067,-0.0119423205,-0.0005413388,0.018831408,0.0139037445,0.010879405,-0.026014065,-0.068372615,0.029922402,0.027544508,-0.0022201678,0.08279724,0.019762347,0.04586348,-0.014947403,-0.0122654205,0.07748959,0.03481832,0.08850797,0.057469737,-0.034725215,0.01323042,0.027084682,-0.12566267,0.08145986,-0.029526766,0.013976056,0.12861738,0.02456946,0.05517206,-0.023153119,0.064634725,-0.015218497,0.08226245,-0.03377948,-0.020348115,0.013558879,-0.044521302,0.01301115,0.022155445,-0.047966566,0.07090027,0.03290684,0.048889127,-0.08710328,0.0012426433,-0.030472102,-0.11170803,-0.08389379,-0.054042008,0.0023302422,-0.061052848,0.043751277,0.050033264,-0.044194993,0.010110973,-0.031841468,-0.031035958,0.053639285,-0.03488498,-0.033869695,-0.05606048,4.7177327e-33,0.0011531665,0.009362439,-0.07870194,-0.0033788118,0.019415343,-0.011869415,-0.030126689,0.040211145,0.0119747985,0.074697375,-0.061570928,0.10447991,-0.05616338,0.11532212,0.06414544,-0.042237964,-0.0059823818,0.08245981,-0.059543397,-0.009688023,0.03831172,0.059456546,0.0486755,-0.0063084746,0.04369856,-0.030466327,-0.03814645,-0.07721062,-0.031553503,0.0070502106,-0.055671092,-0.022235978,0.0006821391,-0.008231267,0.0007995407,-0.06013063,0.031244632,0.011905915,0.024145413,0.047857467,0.048540484,0.05735951,-0.030868113,-0.03853159,0.03039941,0.051217377,0.10469509,-0.0059607825,-0.08337855,-0.010361581,-0.008964703,0.008673182,0.01570991,-0.0077603236,-0.02508587,-0.026222544,0.069075234,-0.068232186,-0.07559389,0.08355592,-0.13851374,0.03279198,0.12036566,-0.083841704,-0.01941174,-0.059313487,0.060404286,-0.062628016,0.056485586,0.0144258,-0.059032384,0.08866679,-0.048545144,-0.1422595,-0.023563651,-0.0031636178,0.03239144,-0.07280945,-0.019753544,0.016742243,-0.016462175,-0.0020301803,0.020833055,-0.03732382,-0.07828317,0.0079194065,0.038490433,-0.07092806,-0.023340544,-0.023815542,-0.026808979,-0.026640994,0.042654242,-0.061866052,0.034013994,-5.233927e-33,-0.098880865,-0.014342233,-0.07524215,0.024452683,-0.053041644,-0.023334863,0.042987745,0.0041252966,-0.002350934,-0.018580887,-0.02058115,-0.024737751,-0.0070821717,0.0021988954,0.005689378,0.014382406,-0.1359084,0.04617742,-0.07553486,0.07675363,0.0021138594,0.113943055,-0.09078174,0.054485507,0.048900615,0.06856359,0.0016727367,-0.021241354,0.07562715,0.016896453,-0.045279294,-0.045525424,-0.018152962,-0.06454073,-0.004674373,-0.0050728484,0.06412493,-0.042504717,-0.046355646,0.036882587,0.04532137,-0.015575813,-0.056541834,0.04073799,0.04440204,0.032353476,-0.025320403,0.03568207,0.031871647,0.032059975,0.074645,0.049292788,-0.007513051,0.011464098,0.037353467,-0.043539338,-0.10311054,-0.045208633,0.002257168,0.01720588,-0.0047075436,-0.042817846,0.10453412,0.017616201,-0.04248489,-0.06511148,0.021038432,-0.077450514,-0.06360772,-0.049903646,0.034325104,-0.0028880625,-0.028595172,0.01715286,-0.06862895,-0.092907354,0.004154143,-0.027209748,-0.043859728,0.13705844,0.012355026,0.07950861,0.052085478,0.11651868,0.028934002,0.026842494,0.12253282,0.08087724,0.0142952,0.013244895,-0.006288667,0.05769595,-0.093779065,0.113098174,-0.01406354,-6.2089e-08,-0.055552147,-0.03672025,-0.04229834,0.009622598,-0.008369156,-0.009388138,-0.05041287,0.0536393,0.01162163,0.041707784,0.044112816,0.0549376,-0.06822764,0.0055037946,0.09257359,0.010956908,0.043740198,-0.09413221,0.03050718,0.060050245,0.049851302,0.03836062,-0.08130188,-0.061338257,-0.023796162,-0.06948977,-0.00955226,0.10593187,0.0918329,0.0067660883,-0.018235048,-0.00010471789,0.024533136,-0.0045132665,0.004015195,0.00082307734,0.017324561,-0.012806752,-0.012645314,-0.06862737,-0.018797334,-0.03651386,-0.048896205,0.016165866,0.02073365,-0.045471,-0.028206248,0.04993081,-0.00070438033,0.09666933,-0.06289506,-0.00046654718,-0.03125319,-0.0010097075,0.043452505,0.0130464565,-0.087041736,-0.032929607,0.044569824,0.01369865,0.061817113,0.01608363,-0.029537171,-0.062274646,12,33.783947,-5.0330553,14
85,"the session focused on feature engineering techniques, particularly feature binning and various encoding methods used in machine learning. feature binning is a technique that converts continuous numerical features into categorical features, making it useful for classification models by grouping values into bins. this can help improve model interpretability and performance.

the discussion then moved to different encoding techniques for categorical data. label encoding assigns numerical values to categorical variables and can be used for both dependent (y) and independent (x) variables. one-hot encoding, another common method, creates binary columns for each category but can significantly increase dimensionality. binary encoding provides an alternative by converting categories into binary format and mapping them to new columns, thereby reducing dimensionality compared to one-hot encoding. additionally, frequency encoding was discussed, where categorical values are replaced by their occurrence count in the dataset. target encoding was also covered, which involves replacing categorical values with the mean of the target variable for each category. the session emphasized the importance of selecting appropriate encoding techniques based on the problem at hand, as different approaches impact model performance and interpretability in various ways.",0.08872619,0.0018687615,-0.05867549,0.001744627,-0.013634492,0.054206412,-0.0025804713,-0.04855723,-0.06981769,-0.09093357,-0.023341162,-0.057872042,0.01628764,-0.0016215681,0.026255812,0.02086333,0.017659321,0.088356614,-0.071471505,-0.0853253,0.056690086,0.00012470683,-0.017703185,0.0030455808,0.009200838,-0.0073046414,-0.029135806,0.027168078,0.012808328,-0.026601836,-0.03193902,0.11827075,0.018784883,0.00764146,-0.07056579,0.028441252,-0.018917197,0.03932628,-0.03367381,-0.019824032,-0.0787991,-0.063100465,0.072001204,-0.0024336309,0.061329037,0.07055916,0.0093217045,0.00053200126,-0.062012494,-0.037448697,0.03177939,0.026766926,-0.010662959,0.048781462,-0.06359173,-0.069066174,0.0023325111,-0.0058831507,0.01513062,0.027039306,-0.09229022,0.030403929,0.031446368,0.01655322,0.011316125,-0.060769893,-0.026038611,-0.012559511,0.055466566,-0.027729088,-0.00594726,0.02438952,-0.07796841,0.016661858,0.014193382,0.0143002225,0.05336431,0.044562124,0.074855365,0.0111329695,-0.07331963,0.022170229,0.044017497,-0.0015709767,0.08779687,-0.005996704,-0.0078829415,0.030131595,-0.14122015,0.065503575,-0.033458937,0.0069763847,0.16097493,-0.013321993,-0.011877957,-0.029372737,0.0036998887,0.0015857611,0.032796327,0.02975525,-0.038206816,-0.015672918,-0.037405115,0.008714467,-0.0009181197,-0.046020344,0.08035957,0.031731877,0.091128476,-0.054062434,-0.011587488,0.039464403,-0.0921744,-0.080676384,0.004944544,0.04226042,-0.0600983,0.051808674,0.052622415,0.027131762,0.0145925945,-0.042380005,-0.03478458,0.030796789,0.0047020214,-0.02352462,-0.035982057,5.0502687e-33,0.002962661,-0.052892227,-0.09813056,-0.033174668,0.045067858,-0.017253686,-0.078778744,0.051458552,0.09082193,0.034731355,-0.043168932,0.05450679,-0.029907022,0.13741095,0.058745123,-0.027883472,-0.030497542,0.06338251,-0.06404185,-0.042635463,-0.013167342,-0.005378211,0.05200141,0.025649443,0.024225771,0.041599073,0.0023924406,-0.05932221,-0.07009324,0.040529232,-0.04824827,-0.027391076,0.006449039,-0.003911242,-0.021015907,-0.05757458,0.026746953,0.022708831,0.054299567,0.086672775,0.01457492,0.0010507255,-0.0036539922,-0.013507031,0.014714157,0.06462198,0.07549111,-0.024459682,-0.103798896,0.035813127,0.034169674,-0.010203336,0.031113023,0.009568187,-0.05880068,0.0066805515,0.011940619,-0.07874571,-0.065502785,0.08255691,-0.072066315,0.031387795,0.07516259,-0.018603723,0.022783844,-0.06581845,0.10391256,0.009193263,0.015675021,0.027784668,-0.06301965,0.054234393,0.005302884,-0.057972617,-0.04099545,0.020176332,0.05946257,-0.0060867765,-0.018791901,0.03362363,0.0077903797,0.043432947,-0.013716291,-0.09402905,-0.012499621,0.030237708,0.010734539,-0.0927254,-0.020209549,-0.024612349,-0.057559315,0.063811325,-0.002029339,-0.036845308,0.06596782,-5.8334822e-33,-0.090896554,0.037838176,-0.057589337,0.018820623,-0.06944358,-0.008432751,-0.0003439076,-0.06664634,0.0018653928,-0.053749066,-0.045093928,-0.068643324,-0.01465605,-0.04173027,-0.050403282,-0.026374497,-0.17866097,0.029630274,-0.045901075,0.10633975,-0.022402365,0.098005295,-0.11603749,-0.0012813144,-0.013495512,-0.030594125,-0.013870967,-0.057524364,0.089496836,-0.0028008204,-0.009890449,-0.03196367,-0.019964404,-0.043528352,0.026789736,-0.014947602,0.0548756,-0.049118463,0.018794676,0.08638933,0.07156731,0.030860223,-0.10410644,0.01761747,0.019352147,0.021806223,-0.032274332,-0.003074683,0.05611805,0.0064587183,0.06639949,0.010425102,-0.0124518,-0.0042828294,0.031013397,-0.018438572,-0.04866473,-0.03825742,0.0067150933,0.02775807,-0.05927594,-0.044451695,0.14270854,-0.036608882,-0.008581318,-0.0702747,-0.010877692,-0.005784678,-0.058761124,-0.029284935,0.02396717,-0.059350397,0.0224795,0.067830235,-0.07623082,-0.11246435,0.0112937605,-0.07905741,-0.09338469,0.037478365,0.012304283,-0.036285028,-0.01522277,0.10904084,-0.0008888888,0.035296272,0.05666694,0.05053666,0.017144665,-0.0057666264,-0.041044876,0.13268003,-0.08079339,0.13307324,-0.02603651,-4.7586596e-08,-0.0007129619,-0.045598626,-0.028217603,-0.0045403307,0.014565496,0.050203774,-0.07422209,0.080987416,-0.03818189,-0.02151526,0.07112025,0.04117092,-0.08253797,0.0024730088,0.036571532,0.044434533,0.06049831,-0.026810005,0.041594718,0.044909492,0.062783405,0.029341025,-0.00027772144,-0.070118904,-0.03880951,-0.10342135,0.017224561,0.15981492,0.08865241,-0.017629068,-0.01042786,0.056204367,0.03600096,0.0024819053,0.05050377,0.015718617,0.001848267,-0.009677356,-0.04455285,0.008050754,0.03028968,0.018137755,-0.042971257,0.02814838,-0.034168415,-0.015052871,0.060126826,0.021761047,0.0034541385,0.04417056,0.0006024743,0.030123016,0.0140889995,0.05608057,0.031768285,0.07915518,-0.045127295,-0.06737482,0.07599654,0.007592066,0.01108709,-0.009988201,-0.027989022,-0.06764901,12,33.837837,-3.0592144,14
143,"feature encoding techniques

during this session, we reviewed several techniques used to transform categorical data into numeric representations, which is an essential step in getting data ready for machine learning models. the discussion began with the introduction to feature encoding techniques such as vectorization techniques and utilization of one-hot encoding, label encoding, integer encoding, binary encoding, frequency encoding, and target encoding.

one-hot encoding:
one-hot encoding converts categorical features into vector where each category has a separate column. one-hot encoding is particularly helpful when encoding the input features (x) in the case of multiclass or multilabel problems. one-hot encoding is less ideal for encoding target variables (y). one of the primary disadvantages of one-hot encoding is that it can cause the curse of dimensionality. for example, if you have a variable such as pincode with thousands of distinct values, applying one-hot encoding will lead to a dimensionality explosion, making the dataset sparse and computationally intensive.

label encoding vs. integer encoding:
label encoding gives each category a specific integer. the method is well-suited to encoding target variables (y), especially for classification tasks, but may not be suitable for input features where the model may mistake the numerical ordering as an ordinal relationship. on the other hand, integer encoding is used when the target variable is naturally ordinal instead of nominal, such that the order of the categories holds important meaning.

binary encoding:-
binary encoding, or pseudo one-hot encoding, offers a more compact representation compared to one-hot encoding. by converting categorical values into binary code, a few columns (for example, three columns) can represent multiple classes (up to eight classes in this example). this method helps mitigate the issue of high dimensionality while preserving the distinctiveness of each category.

frequency encoding:
frequency encoding substitutes each class with its frequency in the dataset. while it reduces the representation, it is not necessarily good for target variables because two classes can have the same frequencies, causing possible loss of useful information.

target encoding:
target encoding allocates to each category the target variable's mean for that category. target encoding can be especially useful when there is high correlation between the target variable and the categorical feature. however, caution needs to be exercised to prevent data leakage while training the model.

simplification strategies:
tackling complicated regression questions by converting them into classification questions using methods such as feature binning is common. also introduced briefly was converting text into numeric vectors through vectorization methods as a key step in handling unstructured data.

in general, the suitable encoding method is determined by the type of data, if the variable is utilized as a feature or target, and possibly the effect it may have on dimensionality. every approach has its strengths and weaknesses, and the proper choice is critical to constructing efficient and effective machine learning models.",0.052159745,-0.038736004,-0.03396509,-0.008442898,0.015718048,0.088734604,0.0012980872,-0.081979446,-0.020470347,-0.09879598,-0.04159152,-0.05658535,0.0132965315,0.016190672,0.04037223,0.010530856,0.06604246,0.1180237,-0.05740177,-0.06697901,0.035253502,-0.04306474,-0.03595321,0.01723404,0.0020094058,-0.042093452,-0.035639007,0.019666463,-0.04104979,-0.025953267,0.0100104185,0.050355062,-0.036066297,0.02260281,-0.06306191,0.031117057,-0.06690743,0.0007725732,-0.06354434,-0.009556342,-0.019911252,-0.04132894,0.05625242,0.031890795,0.04679045,0.05849517,0.0085406685,-0.0074251555,-0.06366617,-0.053804927,0.022024378,0.005342251,-0.0030771303,0.078523934,-0.08353531,-0.08190203,-0.0074830246,-0.0024129872,-0.024282193,0.02655455,-0.12346073,-0.012959472,0.059423458,-0.024535974,0.035107616,-0.08694192,0.029897168,0.010499465,0.008589376,0.029170485,0.027163092,0.018310385,-0.06805061,0.0017632936,0.05788606,0.022624288,0.05304503,0.019416025,0.08920517,0.013442806,-0.01635617,0.019186288,0.0097674625,0.002733608,0.12164973,-0.004826845,-0.0077750473,0.041901253,-0.09005105,0.048460457,-0.018919732,-0.0029250986,0.16813366,-0.021780724,0.014387671,-0.05141781,0.010258343,0.027279712,0.012493475,-0.022357428,-0.027706534,0.0009812048,-0.020576963,0.019631464,0.04914064,-0.032932002,0.08158879,0.022581734,0.09611273,-0.11795505,-0.012367563,-0.0048823194,-0.06988041,-0.03673591,-0.033141315,0.005706127,-0.043614343,0.020842874,0.05463841,-0.0013643289,-0.05415063,-0.060377296,-0.0595676,0.047932725,-0.042240642,0.0013795942,-0.06526846,5.429634e-33,-0.0030943882,0.013614702,-0.122036666,-0.027356282,-0.0112572005,-0.096702024,-0.053914927,0.021392802,0.07284111,0.07949159,-0.013178946,0.025936808,0.006783049,0.14143442,0.029425135,-0.007207835,-0.006068229,0.071119875,-0.06357997,-0.011973269,0.013954631,0.038014334,0.036599558,0.0054511093,0.026145874,0.014933808,-0.026099252,-0.08908905,-0.062031846,0.02954921,-0.10864265,-0.056379285,0.021644227,0.017329108,-0.014284866,-0.07580424,0.01272572,0.032413017,0.036103666,0.0365062,-0.0018550993,0.006712643,0.026357315,0.0075066993,0.03495585,0.07765747,0.03155577,-0.010934737,-0.10634351,-0.022117345,0.023051972,-0.001304028,0.014710899,-0.02866384,-0.05377797,-0.022740558,0.027517026,-0.078654975,-0.048148964,0.02193418,-0.13791326,0.023834594,0.06955603,-0.017518917,-0.0013549757,-0.060050238,0.09336025,-0.01697211,0.045909002,0.020187983,-0.047649,0.044957712,-0.052302524,-0.123603664,0.0012181803,0.036613632,0.07325882,-0.056209113,-0.024812007,0.04904161,0.018471902,-0.00084096583,0.038026102,-0.027888812,0.002720052,-0.03388146,0.017725728,-0.09664582,0.0015118634,0.028659295,-0.025517832,0.058455028,0.0147103565,-0.048996214,0.052853316,-5.0060084e-33,-0.08548887,0.020610576,-0.038592033,-0.008063798,-0.08989881,-0.026056275,-0.009441105,-0.019661663,-0.010169795,-0.05982165,-0.04168945,-0.049618386,-0.02443267,-0.044772513,-0.026691439,0.009037922,-0.13542159,0.08264992,-0.01708281,0.06484778,-0.027186034,0.06469946,-0.065678306,0.06537119,-0.008663937,0.017392661,0.01411436,-0.06486277,0.07202892,0.009338876,-0.052916344,-0.032642864,0.0088292835,-0.034397155,0.0027843828,0.023398414,0.058275614,-0.004261307,0.029645693,0.093349755,0.062271383,0.006062544,-0.05180766,0.05934798,-0.012836336,0.006380656,-0.032326564,0.01786363,0.03720856,0.05603377,0.07343025,0.013759131,-0.020793192,-0.0038214151,0.03809415,-0.04783929,-0.0131156575,-0.05496936,0.004475469,0.03781735,-0.04808571,-0.020973865,0.13422506,-0.026471302,-0.00038473832,-0.068918735,0.027200654,-0.034514476,-0.07024287,-0.04642793,0.08035215,0.042665087,0.00037143967,0.06727737,-0.08496229,-0.081521645,-0.001675662,-0.02184628,-0.07264057,0.051290955,0.029504126,-0.0062835235,-0.02290601,0.11139706,0.06401233,0.047676094,0.11417095,0.05487812,0.020561121,-0.062871784,0.036384027,0.12086884,-0.039119627,0.12417426,-0.01960026,-4.8895668e-08,-0.006073366,0.0072771856,-0.027854618,-0.015569589,-0.014054478,0.01648638,-0.058981158,0.0962554,-0.004557586,0.05554161,0.057100978,0.069624014,-0.044145975,-0.0012258216,0.068175144,-0.0020577589,0.033936057,-0.024725074,0.02658925,0.077918254,0.06311903,0.045870785,-0.012635416,-0.11211431,-0.010362641,-0.08570985,0.017624889,0.12795249,0.13346988,-0.025738418,-0.040188856,0.044897325,-0.00730527,0.004762204,0.03662363,0.014758603,0.011476637,-0.023736887,-0.025025608,-0.038314708,0.03277619,0.017142024,-0.054806516,-0.015896326,-0.045423876,-0.026015144,0.032681413,-0.008873897,0.0066718445,0.10799882,-0.047311142,0.016940659,0.024215916,0.06799488,0.036165256,-0.002150262,-0.059621125,-0.03996988,0.07690035,0.018726103,0.06446092,0.028004095,-0.07216198,-0.042420723,12,32.963787,-3.3358276,14
146,"in this session, we covered key concepts related to dimensionality reduction, categorical data encoding, and classification modeling techniques.

we began with t-sne (t-distributed stochastic neighbor embedding), which is widely used for visualizing high-dimensional data. however, since t-sne is stochastic in nature, it produces different clusters each time it runs, making it unsuitable for building predictive models. in contrast, pca (principal component analysis) can be used for dimensionality reduction but is more suited for continuous numerical data rather than categorical data.

moving on to categorical data encoding, we discussed one-hot encoding, which converts categorical variables into binary vectors. for example, if a variable y belongs to categories red, blue, and green, one-hot encoding would represent them as:
red â†’ 100
blue â†’ 010
green â†’ 001
while one-hot encoding ensures that categorical variables are properly represented, it significantly increases the number of columns, leading to the curse of dimensionality, especially when dealing with high-cardinality categorical features. this can make models computationally expensive and difficult to train.

we then examined different types of classification problems:

multi-class classification, where the output belongs to one of many classes (e.g., classifying an image as either a cat, dog, or bird).
multi-label classification, where the output can belong to multiple categories simultaneously (e.g., tagging an image with labels like ""outdoor,"" ""sunny,"" and ""people"").
since one-hot encoding can sometimes be inefficient, we discussed alternative encoding techniques, such as:

binary encoding: a more compact representation for categorical variables.
frequency encoding: uses the count of each category's occurrences but is applied only to input variables (x), not the target variable (y).
target encoding (mean encoding): uses the average value of y for each category to encode the variable. this method is useful but may introduce data leakage if not handled properly.
lastly, we explored feature binning, a technique used when continuous features need to be converted into categorical variables. once transformed, the problem can be approached using a classification model instead of a regression model.",0.054477558,-0.0784758,0.0013326484,-0.0052603837,0.03308569,0.04530121,-0.059284583,-0.064479694,0.064365335,0.0024952923,-0.036916252,0.0056984876,0.016540412,0.03262658,-0.028097766,-0.023887325,0.06364952,0.063764885,-0.06037256,0.007235649,0.002365221,-0.010702187,-0.0047024568,0.052787468,-0.008506302,-0.024715528,0.07831097,-0.007946289,-0.08713801,0.009278514,0.005062704,0.0828821,-0.026367843,0.04413096,-0.08009877,0.05470488,-0.01529795,0.056278467,-0.06726843,0.048763942,0.02076315,0.0025258267,-0.030850396,0.053338192,0.06623035,0.015100772,-0.027815375,-0.08642737,-0.09250699,-0.08297418,0.0015111215,0.004875314,-0.077730775,0.06339014,-0.09992546,-0.07783181,0.03558835,-0.07493715,0.035486333,-0.043284353,-0.006793688,-0.032965455,0.041014664,-0.030142238,0.029275376,-0.02153411,-0.00563908,0.05128793,0.037050422,0.003793279,0.053852264,0.03689565,-0.09619473,0.05509226,0.024838008,-0.027675679,0.098569,0.005997444,0.07913405,-0.015477438,-0.010288363,0.051695246,0.01649397,-0.017079638,0.13059875,0.042247098,-0.041353762,0.023139494,-0.09790544,-0.02167815,-0.009262205,0.05469517,0.12892158,-0.04830957,0.019481707,-0.052199647,0.055696793,-0.08190924,0.07159218,0.020901084,-0.018674998,0.023896625,-0.02949923,0.0037353507,-0.00806547,-0.06413572,0.0530045,0.0065619065,0.06523934,-0.07851855,-0.032717075,0.011445802,-0.11866002,-0.025573991,-0.022316866,0.028703073,0.0056131626,0.03958112,0.08155284,0.010664688,-0.024927342,-0.028922368,-0.02152178,0.008914357,0.07355363,-0.05925389,-0.113937534,4.664184e-33,0.05895038,-0.017060151,-0.09035399,-0.008821631,0.055193257,-0.04195645,-0.016897006,-0.0024184526,0.066939935,0.08243797,-0.069201864,0.1145599,-0.03074767,0.109849945,0.07234035,0.016011076,-0.023504678,0.06738346,-0.06002446,-0.05714263,-0.0027068562,-0.0060961843,0.0727109,0.040761974,-0.015648767,-0.042051576,0.0026496202,-0.11160766,0.008331666,0.010669289,-0.08769968,0.0052859657,-0.019399242,0.0040442706,-0.0016643434,-0.0630599,-0.03872122,0.010012807,0.05492612,-0.017622031,-0.03133448,-0.016311545,0.03025007,-0.05652848,0.003146057,0.059392646,0.059242655,-0.04470856,-0.12588316,-0.033271957,0.0831225,-0.02046663,-0.0028747157,-0.0056113075,-0.11336973,0.0027953887,0.06700773,-0.0648273,-0.026826685,0.018309282,-0.101132765,0.046994478,0.075257085,-0.037559904,0.0028915792,-0.0930848,0.0054338058,0.048807856,0.03777845,-0.012964348,-0.0037032505,0.043014046,-0.040537406,-0.14111075,0.022264227,0.0061744046,0.06417035,-0.026552217,-0.040042873,0.07455398,-0.027203312,-0.05151463,-0.008756351,-0.067209855,-0.040251814,-0.052314524,0.038032934,-0.106058024,-0.0054627,-0.041427784,-0.06350163,0.047258716,0.033688154,-0.051666714,0.052770644,-4.977599e-33,-0.13002905,0.01134329,-0.051034957,0.052452117,-0.026545342,-0.039181415,-0.021694759,-0.013019426,-0.033441048,-0.06270448,0.022702431,-0.046332873,0.03833232,-0.0043929433,0.0077452315,-0.011180579,-0.06466481,0.102742195,-0.030103981,0.05877403,-0.029914599,0.046599973,-0.08650771,0.02310893,-0.035220236,0.067447975,-0.021382831,-0.040928934,0.006909911,-0.0032402663,-0.049108878,-0.009783427,-0.025922846,0.026683656,-0.04480948,0.042734355,-0.0236345,-0.014400731,-0.0054937066,0.061604194,0.054578222,-0.012210768,-0.019813672,0.07117526,-0.0029287823,-0.037786484,-0.02320818,0.034020834,0.06506192,0.056518678,0.060460567,0.067591235,0.024062442,0.014634562,0.09150527,-0.03486761,0.006546662,-0.064649,0.0021477286,0.020873418,0.012377819,-0.041793816,0.0895971,0.051124383,-0.026276832,-0.11357259,0.028378913,-0.004591472,-0.0054724235,-0.026888693,0.033244956,0.083559915,-0.043881513,0.035342712,-0.03312075,-0.07264849,0.013267505,0.027750323,-0.012941728,0.051924042,0.017607363,-0.020721532,0.023689274,0.030763267,0.057364494,0.028017348,0.08193574,0.015185994,0.07431933,-0.0491087,0.01805855,0.0892873,-0.014147518,0.104372874,0.0062390054,-4.751556e-08,-0.010987973,0.004412534,-0.016269255,-0.066655986,0.025408626,-0.037183113,-0.029357817,0.12785783,-0.011691273,0.03463252,0.06488821,0.021609787,-0.06345742,-0.02821409,0.085103564,-0.009477271,0.0012103731,-0.0463693,0.035678867,0.06929905,0.016058393,0.026872762,-0.020859862,-0.050452117,0.027699225,-0.06947438,-0.01628156,0.1574317,0.065610334,-0.048392586,0.017545473,0.018408393,-0.0026537154,-0.03208656,0.026323989,0.02293604,0.055632427,0.026182244,-0.053673342,-0.025721373,-0.028340288,0.019765832,-0.04701782,0.022348668,-0.012550945,-0.0032087835,-0.004695399,0.016280321,0.06523515,0.107431844,-0.090312324,0.008155312,-0.015780402,0.02864722,-0.022267113,0.061540812,-0.04807723,-0.019220881,0.050568413,-0.03287885,0.039431628,-0.032654013,-0.10341438,-0.017310793,12,31.677294,-1.9770141,14
151,"today's lecture discussed dimensionality reduction and feature encoding methods. t-sne is good for visualising high-dimensional data but not for model building because it can lose information, while pca retains variance and can be applied to modeling. in feature encoding for multiclass and multilabel problems, encoding must be done with care to avoid ambiguity. target (y-values) but not feature (x-values) are label encoded since x affects predictions. integer encoding is applied when there is a natural ordering of categorical values, whereas one-hot encoding adds dimensions, which may lead to the curse of dimensionality. frequency encoding substitutes labels with counts of occurrences but is inappropriate for y-values because of potential overlap between classes. target encoding utilizes statistical aggregates of the target variable and may enhance model performance in certain scenarios. these methods assist in managing categorical data effectively for machine learning models.",0.05619106,-0.02064331,-0.0069255307,-0.027656162,0.037451684,0.0996548,-0.030158542,-0.047115706,0.041331425,-0.09951301,-0.015921807,-0.030538047,0.04322133,0.009734624,0.024277244,0.007744791,0.068558276,0.0844538,-0.047856044,-0.085369505,0.05718337,-0.033338636,0.014275381,0.06495533,0.0103182895,-0.020845572,0.00040490273,0.018897932,-0.005075573,0.027979175,-0.009203915,0.08087159,-0.009166646,0.02108744,-0.08313357,0.0152921425,-0.034958586,0.07079222,-0.0021365155,0.007793178,-0.0076030213,-0.032280456,0.017215287,0.048986867,0.04666693,0.042122994,-0.05408758,-0.085241005,-0.12741782,-0.04950222,0.02986038,-0.049130358,-0.0722214,0.0674193,-0.052238468,-0.05845476,0.01332195,-0.035804167,0.0186008,0.026957968,-0.08239807,-0.027434045,0.04188449,0.006457715,0.096910104,-0.064633325,0.011943857,0.05429516,-0.007796359,0.02133545,0.03530085,0.03312326,-0.045729924,0.045778703,0.05687488,0.048566226,0.014214238,0.0231233,0.07344061,-0.0044352924,-0.029034073,0.010739983,-0.025261046,0.012923242,0.14060426,0.012084687,-0.03222084,0.011703096,-0.08666101,0.02578941,-0.020119602,-0.0068944404,0.12589376,0.004929565,-0.026605021,-0.06497651,0.03749557,-0.026310941,0.022361988,-0.035272002,-0.027243294,-0.0115104,0.00091862254,0.007994457,0.020938419,-0.073064856,0.07996523,0.0058020502,0.06650762,-0.13327378,-0.02602531,0.0104873255,-0.06982359,0.0056125685,-0.0070926994,0.020739388,-0.021613514,0.015868684,0.08208017,-0.0755768,-0.070841976,-0.048559494,-0.03450892,0.0343807,-0.0020694446,-0.0075431587,-0.07488384,4.4635493e-33,0.06813279,0.015186955,-0.10545993,-0.009683994,0.012928292,-0.05239144,-0.049232494,-0.013090688,0.081370704,0.10398426,-0.052413214,0.028267076,-0.009858232,0.15045387,0.03494581,0.009166931,-0.0044097933,0.09740054,-0.09558042,-0.011682573,0.005801782,0.03453318,0.049300395,0.017369539,0.03554416,-0.01805777,-0.05552365,-0.09990442,-0.05385984,0.025463127,-0.09682789,-0.008533732,0.032489676,0.022085639,0.010275866,-0.06294757,0.016846286,0.012720515,0.027145807,0.03537066,0.020916078,0.0125234425,0.036946334,-0.025061883,0.004341695,0.05277463,0.026368216,-0.06359496,-0.122700185,-0.028012367,0.04445571,-0.041897994,-0.025118357,-0.04220896,-0.09614759,-0.023221895,0.01724777,-0.09385591,-0.02741463,0.04495454,-0.11184623,0.002537341,0.1058655,-0.043894015,0.044672377,-0.019389125,0.033915825,-0.025645474,0.041518647,0.017567627,-0.023273608,0.056283534,-0.076748826,-0.07118687,0.030779844,-0.010131847,0.03688047,-0.037753455,-0.0077295145,0.06738726,0.0030249755,-0.017238269,0.0006783796,-0.06492639,-0.01589222,-0.057399858,0.046400588,-0.10504025,-0.010923928,-0.013196849,-0.0089529,0.03961302,3.7997783e-05,-0.02580461,0.046583537,-5.4060342e-33,-0.10364198,0.062343426,-0.020086654,0.006408197,-0.05781684,-0.027724128,0.021826917,0.0017518494,-0.0069069425,-0.048965015,0.0027706337,-0.08382079,-0.03692637,-0.042172555,-0.0016740427,-0.020054648,-0.07831554,-0.005321874,-0.02258802,-7.5387834e-05,-0.016086986,0.023413297,-0.051567793,0.05733029,-0.0040682717,0.012198612,-0.032069847,-0.056581676,0.06263342,0.010160017,-0.018334616,-0.07209249,0.02641417,-0.0061039813,-0.019652879,0.029844321,0.054258913,-0.00013313044,-0.0099422345,0.09977828,0.046009287,-0.03579128,-0.04282679,0.12031496,-0.0059789415,0.015970506,-0.00032531,-0.003529017,0.0688788,0.01497072,0.07136848,0.026451886,-0.00036210354,-0.026931202,0.046756417,-0.06884562,0.0012660916,-0.044550728,-0.028405761,0.0020104926,-0.014809356,-0.04928728,0.07486225,-0.05033157,-0.046374224,-0.057137545,0.017673586,-0.0075522596,-0.0560118,-0.075779915,0.048032034,0.0027377289,-0.02115351,0.050601117,-0.12841551,-0.16288514,0.012691154,0.0052348482,-0.04771331,0.050054546,-0.002771287,-0.038958367,0.0037354743,0.09891605,0.03532188,0.056438122,0.080994986,0.06955156,0.0020162708,-0.064653605,0.036984116,0.108178034,-0.07409094,0.14940053,0.03239492,-4.6628973e-08,-0.034721807,-0.011114777,-0.023553772,-0.008881701,-0.0056056273,-0.013929326,-0.03248287,0.13934694,0.013219326,0.01986359,0.04291953,0.01062814,-0.076484665,-0.0071284757,0.062035438,-0.016686104,0.025082817,0.021679265,0.030461987,0.030998614,0.013320693,0.0114455875,-0.0066966256,-0.110977225,0.019857919,-0.05777384,0.0073241484,0.15978354,0.10774621,-0.038938902,0.03328505,0.037189014,0.007483664,0.0016627227,0.045976363,0.011331462,0.024329063,-0.0046748933,-0.0027045363,0.019311476,-0.026026899,-0.005789074,-0.06808301,0.032558974,0.036452703,0.017715922,0.040945448,-0.040293302,0.04151719,0.08450661,-0.042251345,-0.009746058,0.036938343,0.072131395,0.044168085,0.026284244,-0.037967354,-0.040500436,0.06796369,-0.04597031,0.0732216,-0.039583348,-0.07622347,-0.052167796,12,32.337784,-2.1199062,14
181,"
todayâ€™s class began with a revision of the t-sne plot. then, we moved on to feature engineering, starting with feature encoding, which is essential when either dependent or independent variables are categorical. these variables must be encoded before being used in a machine learning model.  

we first discussed one-hot encodingwith an example where the dependent variable was categorical, while three features were numerical, and one was categorical (e.g., ""blue""). we then covered multiclass and multilabel problems and discussed the drawback of one-hot encodingâ€”it increases the number of columns, leading to the curse of dimensionality and making the dataset sparse.  

for nominal data, one-hot encoding can be used if the number of classes is small. however, for ordinal data, assigning specific weights to categories is necessary, so one-hot encoding is not recommended.  

next, we explored binary encoding, where categorical values are first converted into numerical values and then expressed in binary notation. we also covered frequency encoding and target encoding as alternative methods.  

towards the end, we started discussing llms (large language models)and how words are converted into numerical data or vectors for processing in machine learning models.",0.0035580462,-0.07383407,-0.019268025,-0.04201243,0.02376298,0.05317472,-0.03518993,-0.034425892,0.03859787,-0.021692632,-0.03929444,-0.062955044,0.046468105,0.01714566,0.07215816,0.010784253,0.080367945,0.05686983,-0.07560297,-0.04941135,0.068219975,-0.0066555515,-0.017940288,0.017971033,0.030274311,-0.035326093,-0.02675591,0.021523397,-0.0140271885,0.005988584,-0.029176531,0.05666746,-0.022836762,0.053680938,-0.0772028,0.0013538499,0.016654877,0.04670473,-0.07395037,0.030136403,-0.03685695,-0.06890862,0.03849153,0.031516574,0.10834636,0.033699915,-0.011046967,-0.03211536,-0.13033864,-0.016125979,0.0026007716,-0.023420122,-0.034750063,0.08535589,-0.07565469,-0.05436712,0.0015847211,-0.02022215,-0.023168964,0.006659471,-0.12934317,-0.06562865,0.020947082,0.0015209321,0.04378062,-0.07089136,-0.009549739,0.033527188,0.022466937,0.06680293,0.013348165,-0.019671109,-0.04796987,0.057719298,0.01028077,0.005747661,0.060048368,0.0028945499,0.10030218,-0.06082558,-0.019823745,0.029320868,-0.022272132,0.037737798,0.1051388,-0.03255565,-0.018887984,0.07002229,-0.07919592,0.036695212,-0.04099005,-0.025375688,0.10682019,0.00092930265,0.032439545,-0.0050302288,0.0081426585,-0.020305134,0.049874973,0.008150008,-0.0034078055,0.022544673,-0.009642711,-0.00027217355,0.0031844066,-0.06692025,0.033152327,0.012210039,0.10260375,-0.11524268,-0.009115802,0.0343411,-0.10039364,-0.025946409,0.0009905421,-0.01703361,-0.010483443,-0.019095017,0.082853116,0.014104493,-0.06168046,0.024512721,-0.015906768,0.029006284,0.01512784,-0.0046505546,-0.057234913,3.3096223e-33,0.059306685,0.04297841,-0.108208574,0.029166041,0.020559102,-0.05801563,-0.015774658,0.028541602,0.035732664,0.06897217,-0.021600496,0.08282978,-0.04263801,0.14018837,0.056630325,-0.009451493,0.05606526,0.020632606,-0.057123657,-0.016266264,0.00021134617,0.08485895,0.072041675,0.018079136,0.033502977,0.023048852,0.030400973,-0.089236155,-0.030905992,0.043424293,-0.08082168,-0.030127428,0.02984484,0.0048367777,-0.0045087067,-0.07656421,-0.03987252,0.02445766,0.05230718,0.010720113,0.01563249,0.0032240301,0.026060283,-0.019879352,0.015115224,0.05121558,0.06246607,-0.0028250355,-0.082170926,-0.059432246,0.04843112,-0.059303712,-0.03922518,-0.0057164594,-0.00067267,-0.00013452774,0.014711289,-0.07427621,-0.09094281,0.006243698,-0.08540535,0.040736776,0.13309625,-0.027959188,0.054617755,-0.043503437,0.007729024,0.015078455,0.017707521,-0.028980024,-0.034073576,0.08855527,-0.058706716,-0.12203593,0.013110049,0.04090893,0.11264169,-0.07535057,-0.027208718,0.063195705,0.000630015,-0.059096746,0.037619732,-0.08904807,-0.064462155,-0.0026595592,0.06050041,-0.09396543,0.032339543,-0.031913478,-0.06760656,-0.002348849,-0.0014561876,-0.04886491,0.013406794,-4.525691e-33,-0.1305914,0.021422353,-0.048503842,0.055105824,-0.06170223,-0.033838052,0.008124162,0.021163385,-0.026197024,-0.04212907,-0.039950244,-0.067284234,0.015270782,-0.043536305,0.041548233,-0.012558089,-0.08969672,0.01916307,-0.016281739,0.0658585,-0.051126968,0.079526946,-0.11177887,0.06844524,-0.010069112,0.016861616,-0.055689678,-0.035942506,-0.009655395,0.06842388,-0.024320459,-0.05606777,0.015828043,-0.037391007,0.0015022687,0.052927088,0.09713252,-0.018899214,-0.03391712,0.050831717,0.06488045,-0.02822478,-0.035460074,0.059656937,-0.020225547,-0.0011440781,-0.08466211,0.019398473,0.047592457,0.012614724,0.07130876,0.004365292,0.029575305,-0.014537751,0.043387532,-0.08332704,-0.020464264,-0.10104589,-0.024716822,0.0019264778,-0.0453327,-0.029910384,0.06544512,-0.02701702,-0.032372683,-0.049288545,-0.044653382,-0.04182946,-0.07043479,-0.06634454,0.061603483,-0.011463561,-0.025937969,0.051607583,-0.08004698,-0.039478213,0.00070070784,-0.026609713,-0.08055621,0.027304525,0.009620293,0.0035372498,0.014204718,0.08894972,0.018823674,0.036514662,0.13415043,0.082512006,0.040506583,-0.07127126,0.009992651,0.11672377,-0.07043511,0.11917466,-0.021348422,-5.3480584e-08,-0.033513475,-0.016035352,0.008824724,0.0011309108,0.04571245,-0.00019695364,-0.0018042773,0.11478449,0.0031468917,0.060399223,0.03231425,0.057632033,-0.03298614,-0.038809955,0.05704584,0.025440086,0.037625026,-0.05192267,0.029258324,0.043424353,0.048383478,0.049734026,-0.024857245,-0.067826636,0.01881117,-0.07870248,-0.02582071,0.13196625,0.12728313,0.009863403,-0.050165854,0.019795792,-0.05945092,0.029840589,0.037291802,0.02421899,0.022237437,-0.040013902,0.0010089234,-0.052601043,0.02081099,0.016757231,-0.049207024,0.017293127,0.021268452,-0.023645297,-0.03380889,-0.029778183,0.009739428,0.08153931,-0.090987206,0.05380612,0.00988168,0.06543694,0.00140366,0.007492472,-0.05016127,-0.04192914,0.039599925,-0.0006048398,0.03875694,-0.0007522693,-0.04466252,-0.025840435,12,31.932701,-3.0822217,14
221,"discussed about the plan for the next remaining 10 lectures and about group project, learnt about function encoding in the start of class. saw an example of converting y(red, blue, green) with three column to be encoded to a function f(x) with three variables y1, y2, y3. there are two types of problems multiclass and multilabel, based on that we can have the approach ahead. then we saw binary encoding which results in a very compact encoding. we learnt that conversion in detail, how to get the answer. frequency encoding is in which we replace the category values with it's frequency it occurs in dataset. in target encoding, all the r values in the column are replaced by average calculated score above 2.5 in this case. ",0.03937566,-0.008791975,-0.10691319,-0.046354733,-0.013441186,0.057223182,-0.02920629,-0.030494472,-0.026497964,-0.054651838,-0.029843459,-0.04872576,0.018921772,-0.02404831,0.04043371,-0.012836298,-0.03798654,0.04459382,-0.120434895,-0.08325201,0.07558557,-0.0022053956,0.07663752,-0.00089662836,0.01668374,-0.009265794,-0.08071348,0.0055392687,0.03799224,-0.044546373,-0.030809902,0.07722351,0.09481331,0.060832776,-0.086434126,0.039666697,-0.011561362,-0.01264795,0.033142123,0.07676575,-0.10138915,0.026200088,0.08796852,0.017288065,0.011786004,0.08530032,0.0064949957,-0.018795665,-0.09057291,-0.03843101,0.050686713,0.078334294,-0.10088651,0.073343754,-0.009540614,-0.050210457,0.0012448528,-0.05716359,-0.07627048,0.035876915,-0.17634715,-0.0241811,0.03956981,0.07689075,0.03369209,-0.040453393,-0.010293509,0.0019520387,0.00579454,0.0067861187,-0.035169683,-0.047400713,0.010263247,0.0038263006,0.09718195,0.102485105,0.009410504,0.040347777,0.03697216,-0.02036257,0.06029766,-0.0507908,0.047403846,0.045423094,0.12609977,-0.010627821,-0.007613162,0.062207174,-0.08878191,0.1041276,-0.04640179,0.07069821,0.057967108,0.004300173,-0.0821415,-0.02635891,0.039264772,-0.027222693,0.097000435,-0.0075446037,-0.05837507,-0.0018450542,0.031743426,-0.096349,-0.09310393,-0.07132347,0.06721011,0.08577769,0.038237017,-0.13588491,-0.039449956,-0.008179115,-0.09310476,-0.054177783,-0.06290658,0.024689386,0.0072655585,0.0036642323,0.06259236,-0.022916881,-0.011695077,-0.048155222,-0.0031344432,0.038646422,-0.0804917,-0.0037307038,-0.07988744,1.1883953e-33,-0.0519008,0.020370455,-0.047801815,-0.055736367,-0.016393626,-0.025050422,-0.036619082,-0.0029446285,-0.015528727,0.036768567,-0.01401135,0.03225565,0.013926177,0.10880888,0.061086666,-0.007575484,-0.025806522,-0.0055130054,-0.08002653,0.012497294,0.075099245,0.10849982,0.09542442,-0.04971811,0.002930257,0.07226842,-0.014702134,-0.09882585,-0.016290616,0.011677469,-0.07324004,0.00021755966,-0.033833995,-0.0036778722,-0.019768646,-0.042090207,0.08864409,0.040788215,-0.022195118,0.053629626,0.07066169,0.053134635,-0.01683922,-0.03168484,0.062067755,0.024032839,0.061297167,0.031243594,0.009252004,0.070227556,-0.04618358,-0.03331932,0.014392779,-0.00066773314,0.020811468,0.029965352,0.07390305,-0.022707298,-0.01968228,0.060502652,-0.075962774,0.047738034,0.11360145,-0.019993564,0.0030703237,-0.026177265,-0.0014830643,-0.10071988,0.054987572,0.042031337,0.08196392,0.039695643,-0.072677426,-0.057455722,-0.0036434147,0.056113757,0.008918469,-0.07865578,-0.05324406,0.008900995,0.030597053,0.03814191,-0.017883094,-0.07234103,-0.06256835,0.010824922,0.046909064,-0.08892268,-0.040276766,0.03126648,-0.010492708,-0.0070864744,0.010356836,-0.023263922,0.033382315,-3.5777236e-33,-0.038092673,0.12252038,-0.045142956,0.04959584,-0.017883454,0.0050700377,0.0030839737,0.020325366,0.034085564,0.069590844,-0.017881837,-0.07463012,0.03164822,0.011025339,-0.031856835,-0.06798747,-0.047928505,0.019049002,-0.0034206107,0.042379417,0.046980496,0.10755259,-0.003341687,0.053448875,0.033367418,0.034249913,0.0047362824,-0.014062386,0.036538117,-0.004146642,-0.02831591,-0.06998508,0.010402527,-0.006498113,-0.04010515,-0.008126398,0.059226587,0.019992659,-0.087050624,0.081176884,0.047581505,0.041507967,-0.08576512,0.040433347,0.040903278,0.009827727,0.0025888144,-6.318597e-05,0.017789748,0.012294965,0.029712807,-0.018552015,-0.047881667,-0.07669589,0.02554309,-0.022419536,-0.025129689,-0.060447928,-0.07271554,0.0059392676,-0.001512681,-0.03824461,0.057444975,-0.0730186,-0.0165446,-0.051521305,0.05544034,-0.038378622,-0.029924287,0.036463846,0.01658383,-0.02213763,0.04015644,0.024021775,-0.049741294,-0.021617444,-0.055223078,0.0137449,-0.03848812,0.06911551,-0.0044479957,-0.018720705,0.0542103,0.12712626,-0.037271403,-0.033649415,0.086189225,0.055837475,-0.00491719,-0.043146882,0.021412054,0.10339678,-0.02417816,0.083824076,-0.018162549,-5.475994e-08,-0.0067926883,-0.066765,-0.09799314,0.0011772041,0.008302518,0.017300757,-0.0793224,-0.008034418,-0.0046560327,0.02067997,0.012739118,0.025899766,-0.015529249,-0.016066674,0.011479016,0.0356073,0.05736922,0.040394913,0.04000683,0.03489764,-0.003200492,-0.0044046124,-0.066779956,-0.09011788,-0.093442045,-0.027801912,-0.04381063,0.018675078,0.057498746,-0.01913915,-0.029863862,0.03718766,0.025812775,-0.0128110275,-0.049148474,-0.03971785,-0.034843396,0.029418467,0.004974413,0.0014350768,0.024378285,-0.048460312,-0.079674914,-0.007113452,-0.0033910258,-0.051834386,-0.0031085175,0.03658601,0.0010619153,0.03385592,0.029285131,-0.0037630477,-0.12731157,0.014486203,0.040129434,-0.022028726,-0.06820701,-0.061636392,-0.0068564294,-0.02554596,-0.036228042,0.024099529,-0.038889267,-0.051375546,12,37.292877,-5.4892664,14
230,in today's lecture we first learnt about feature encoding and we learnt about multi class problem which we use when one expected output of many classes and multi level problem that is expecting output that suggest all possible labels. then we learnt about different types of including. first one is label encoding when independent variable is categorical in nature and if you want to do label based classification we can do this for y but we have to be careful on x side and we have to look for labels. then comes integer encoding which is when numbers assigned will mean something and when there is some original variable. then one hot encoding in which we increase number of columns hence inviting the curse of dimensionality it leads to sparseness of data we should not use this for ordinal variables we can only use this for variables with nominal levels of measurement. zen binary encoding which is like for three columns we can use 8 levels and we use binary system in this then comes frequency encoding that is the category values are replaced by its frequency in the column. in target encoding we replace the values by their average. then we learnt about feature bending that is when continuous features need to be converted into categorical features for example when height and weight we can categorise them like underweight under height and so on. then comes text processing which includes the methods to convert text data into vectors for machine learning like by making dictionaries,-0.0070254556,-0.02404387,-0.047551468,-0.005292121,0.017002001,0.11308594,-0.020215943,-0.029124552,0.025084201,-0.049614407,-0.01256818,-0.010477918,0.03914685,0.053855717,0.012400854,0.006382929,0.038750973,0.086815245,-0.084095776,-0.068335466,0.07528471,-0.032764137,0.04488967,0.06337373,-0.01975714,0.0029572302,-0.028985461,0.001705935,0.055170227,-0.012950225,-0.010514159,0.084304914,-0.017852888,-0.007379956,-0.09193035,0.006000288,0.0034396888,-0.018797165,-0.092368715,0.003924726,-0.033589993,-0.00024252049,-0.023816325,0.06238407,0.03688166,0.066544056,0.010445626,-0.07577019,-0.15238184,-0.058484163,0.021165699,0.06099005,-0.052106928,0.089274965,-0.018875921,-0.015011679,-0.046772867,-0.017423918,-0.08032019,0.06826942,-0.15374227,-0.027845243,-0.033401217,-0.012164617,0.027604159,-0.03488953,0.0030141617,0.03605974,-0.015437744,0.010570185,0.001970753,-0.034299467,-0.031596065,0.056680735,0.046967458,-0.012534591,0.08280526,0.02569154,0.038682714,0.03785056,-0.033992805,0.061611626,0.019768449,0.0010967769,0.106171206,-0.02797845,-0.012224094,0.05360945,-0.15432934,0.050685685,0.03424424,0.026396705,0.0917123,0.03016616,0.036869444,0.011595292,0.069551684,-0.008818901,0.08450444,-0.030593079,-0.017340574,-0.016916012,-0.017274195,0.024711385,0.023723647,-0.026399814,0.056863133,0.048457384,0.033931468,-0.069846615,0.0021537913,-0.04414924,-0.094028406,-0.08908249,-0.038832482,-0.00882154,-0.024375187,0.018548405,0.05581099,-0.040212937,-0.04807479,-0.07906459,-0.05505683,0.02658169,0.014285253,0.06721557,-0.05096331,8.38995e-33,0.01115669,0.023800746,-0.10788305,-0.059237484,-0.043327782,0.0058834623,-0.009883731,0.033392057,0.048704606,0.044306085,0.02430853,0.03620706,-0.014015851,0.10578515,0.0381922,-0.05796768,0.039482806,0.093426704,-0.047510527,0.0068305475,0.038583934,0.051596545,0.0869781,-0.04652613,0.050565235,-0.016172998,-0.06313492,-0.12903266,-0.07660919,0.0066730124,0.010539378,-0.02823474,-0.016845535,-0.021368958,-0.046273995,-0.08442232,0.03494667,0.042652152,-0.013067429,-0.029423824,0.04075127,0.00826895,-0.0018574692,0.016053569,0.05601623,0.07431934,0.058960285,0.015010091,-0.11205365,-0.010358831,0.0002635053,-0.0007855017,-0.0052021802,-0.034607854,0.0037981188,-0.013350553,0.011285113,-0.107159,-0.11925932,0.015863137,-0.108668305,0.007608833,0.10414371,-0.04907934,0.037863847,-0.11817289,-0.0135342395,-0.01619626,0.06725784,-0.0035580099,-0.0038320648,0.082968205,-0.091878265,-0.07034024,-0.016108043,0.016908891,0.053538498,-0.0447873,0.0074765664,0.027039595,-0.050812583,0.023170566,0.061613206,-0.01511165,-0.02620764,0.04530075,0.030964315,-0.08773056,0.0039364807,-0.0014948905,-0.0046735755,0.0029133344,0.054798722,-0.04265642,0.00917245,-8.685192e-33,-0.06728705,0.050109096,-0.06558824,-0.03677049,-0.031078624,-0.021010559,-0.044930603,0.03696551,-0.034921113,-0.05653798,-0.014494639,-0.021257771,-0.013904581,0.015071678,-0.0046285726,-0.0026273583,-0.1468663,-0.028754659,0.0313585,0.07324815,-0.0041450053,0.10678146,-0.11526299,0.03447695,0.017501444,0.025645813,-0.01104771,-0.017030267,0.05515905,0.044317853,-0.005953311,-0.070959724,0.025225671,-0.033161428,0.009069376,0.0038614809,0.06108397,-0.037656914,-0.07809896,0.038972486,0.0058833146,0.0014448328,-0.07169784,0.024447983,-0.005034433,-0.043159388,-0.005456,0.046375398,0.023220621,0.06415909,0.0647471,0.004957152,-0.02619194,0.02363271,0.024712587,-0.020292036,-0.02355788,-0.06592944,3.8988237e-05,0.026292231,0.027776321,0.0013873725,0.06085577,0.00808671,-0.038585782,-0.025583392,0.050965164,-0.02660214,-0.069429025,-0.06076763,-0.06291669,-0.0027272752,0.00866582,0.038487237,-0.0618847,-0.014954264,0.012538442,0.0029140308,-0.09360835,0.105167225,-0.02105595,0.0057318844,-0.01600567,0.10297896,-0.025938949,0.008341636,0.14161673,0.04671551,0.031199904,-0.031096347,0.05140492,0.06749472,-0.07148341,0.15386875,0.0035730393,-6.568567e-08,-0.031865295,-0.046378404,-0.043354817,-0.029254973,0.017817667,0.0070924037,0.006919225,0.040500112,-0.046909723,0.05969395,0.023627657,0.030003764,-0.039831355,0.037049882,0.12674488,0.031737536,0.05790291,-0.008606183,-0.0065553635,0.04402751,0.025904067,-0.0060607134,-0.036206592,-0.058704797,-0.0219822,-0.08032991,-0.0181502,0.12273312,0.1153148,0.022901403,-0.03725201,0.0500068,0.028114349,0.012782854,0.05763534,0.032750085,0.0396095,-0.07716171,-0.07254213,-0.05659249,0.01623997,-0.041258126,-0.0054983106,0.013233309,0.04001127,-0.05510119,-0.0044435784,-0.019357659,0.052175812,0.09589985,-0.087800756,-0.00052697625,0.0071633076,0.006622956,0.050532646,0.035015937,-0.05481596,-0.056685667,0.003373413,-0.0014439626,0.0010029279,-0.0074637462,-0.016851295,-0.036088094,12,33.628853,-5.8543005,14
231,"feature encoding. when either the dependent variable or some of the independent variables are categorical then, they have to be appropriately encoded prior to being used for training ml models. the project will be related to assessment of exercises which we have done. label encoding, one hot encoding, binary encoding, integer encoding, frequency encoding, target encoding. multiclass problem- mnist dataset- there are 10 classes(0-9), multilabel problem- there are multiple labels associated with a single object, a image with both cat and dog. how we encode a variable depends on domain knowledge. label encoding- take categorical variables and assign numerical values to these values-this for nominal variable. we can use label encoding to encode output variable, but we should avoid using it on input variable. integer encoding is for ordinal variable- the numbers carry a value and have a meaning. one hot encoding- converting the output into a vector of dimension of number of classes. this increases the number of columns in data and introduces the curse of dimensionality. they choice of encoding depends on number of classes. one hot encoding can be used for nominal variables and classes are not too many. binary encoding- it is just a different notation of one hot encoding. we are converting vectors in one hot encoding into binary values(pseudo one hot encoding).  frequency encoding- the category values are replaced with its frequency in the column. class takes on value of occurance we have to check whether two classes have same frequency. target encoding- we collect all y values corresponding to a particular class take their average and use this to represnt x in input. this is used to encode the input variables. for encoding input variables, we can use one hot encoding generally. for output variable encoding we can use other techniques. eda is important as if we are not able to capture what data is properly we will not be able to perform the further processes correctly.  we need to think multiple times before applying a particular encoding. feature binning- many times we want to convert a continous problem to a discrete problem. we will divide the continous variable into bins and then assign categories to this. this now becomes a classification problem. it can be used for several reasons including problem simplification, reducing the impact of outliers and noise in the data, handling non linear relationships. how to process text data- natural language processing(nlp)- examples of the manner in which statistical processing can generate deterministic output- code generation. how to convert text into numbers so that analysis is useful. method1- drop the common words(stop words); convert to lower case. - create a dictionary. -express the document using the dictionary. in a sentence we cant know exact meaning until we know the context in which it is being used as each word might have multiple uses. ",0.060898267,-0.060615733,-0.042461395,-0.01683383,-0.027433822,0.07879651,0.020138472,-0.056692626,-0.040678132,-0.039020058,-0.027732277,-0.06812408,0.08009768,0.055778123,0.02762607,0.03241175,0.005960304,0.075609826,-0.10447496,-0.053840935,0.10127847,-0.00586052,-0.011136994,0.010876141,-0.048497938,0.04247768,8.0116435e-05,-0.01842543,-0.007264995,-0.008463658,0.023449589,0.041754518,-0.0014730145,0.031844277,-0.025214013,0.04781174,-0.06391707,0.020389771,-0.015696663,0.022444105,-0.056898173,-0.020389223,0.079019636,-0.031898305,0.06048117,0.06078278,-0.0134924,-0.043413647,-0.08427177,-0.018997211,0.005784287,-0.033251554,-0.039804883,0.036034867,-0.06745624,-0.05820867,-0.009963848,-0.006175086,-0.0821497,0.051415227,-0.115112655,0.0013839318,-0.0065336963,0.00032133784,0.041303825,-0.068578385,-0.020560011,0.038165603,0.015733765,-0.035156198,0.028144863,-0.0019068856,-0.051189255,-0.02896396,0.056679945,0.009995557,0.1252809,0.01095274,0.08186069,-0.052749977,-0.022435235,0.05073829,0.029604658,0.024534365,0.1516932,-0.03508385,0.021742282,0.07183113,-0.11844015,0.046879824,-0.05022508,-0.066024646,0.11748135,0.011178551,0.052984465,0.018001674,0.012513974,-0.018737065,0.045411207,4.242985e-06,-0.059682466,-0.0022754902,-0.033620074,0.030140921,0.03860986,-0.04486564,0.09239916,0.032308713,0.054844867,-0.08411123,-0.01477591,0.03438221,-0.084480025,-0.028685117,0.004522519,-0.020821651,-0.031428505,0.0035316895,0.031022916,-0.020526094,-0.06400986,-0.06338444,-0.035728727,-0.013360125,0.017090768,-0.012934267,-0.051662296,4.467178e-33,-0.0053865244,-0.013129509,-0.08846154,-0.008551291,0.0008699533,-0.030461416,-0.015027706,0.019230608,0.060219318,0.017774304,-0.016333036,0.051519156,-0.015734194,0.1522231,0.05061078,0.012568414,-0.004777217,0.09311957,0.0010160212,0.017893745,-0.003310866,0.069268204,0.05237499,0.006982995,0.03796154,-0.010929878,0.011125556,-0.111415856,-0.08845356,-0.0055185705,-0.016255789,-0.046949074,0.037063345,0.0032973043,-0.044546418,-0.076555744,0.021111509,0.028757298,0.014393036,0.0106253205,0.04079406,-0.008263982,0.051130354,0.026595907,0.037267197,0.04260798,0.029131562,-0.033242594,-0.10655588,0.006403358,0.032046467,0.009380755,0.0020021237,-0.09639611,-0.023996936,-0.029057791,0.010795999,-0.08218082,-0.062144022,-0.0032604376,-0.0780101,0.030345507,0.11229226,0.027083902,0.081181414,-0.07015619,0.04475427,-0.05228161,0.04978185,0.00036611574,-0.0495351,0.03345535,-0.04297474,-0.14069438,-0.021668717,0.047038663,0.0021047152,-0.07676451,-0.01629934,0.05943955,0.003224498,-0.014300228,-0.0055658952,-0.060590185,-0.046971515,0.08747982,0.00759883,-0.09387617,0.0035234694,-0.016956009,-0.0052825203,0.04783258,-0.053799964,-0.041434787,0.08771571,-6.1243575e-33,-0.047482457,0.09022638,-0.068305664,-0.015053489,-0.07390549,0.045874048,0.018717455,-0.020229202,-0.04387505,-0.0381029,-0.057529453,-0.018983534,-0.024741987,-0.018343724,-0.074520074,-0.025281282,-0.15809326,0.014949543,0.050924305,0.033628285,-0.048189957,0.123478025,-0.07094382,0.031350236,0.009193184,0.027792983,-0.06333887,-0.008326131,0.077365264,0.0644367,-0.019343134,-0.092020616,0.05344542,-0.02822702,0.0030386748,0.034329493,0.06438845,-0.07132616,-0.032509286,0.07962884,0.03950212,0.008770303,-0.06285407,0.03780081,-0.011692823,-0.0014816644,-0.063371636,0.0142523125,0.06621872,0.04738749,0.074022256,-0.015433652,-0.044407886,-0.046057712,0.009051284,-0.03836371,-0.053545076,-0.06420888,0.024539359,0.049503684,0.0061105066,-0.040979672,0.100902595,0.031091664,-0.080658816,-0.024507366,-0.03533424,-0.0468645,-0.061105188,-0.027523464,0.00188871,0.050880525,0.021699196,0.015167705,-0.06305273,-0.0068448028,-0.014829714,0.025728147,-0.039253216,0.029694237,0.01242562,-0.034023672,-0.008852724,0.11648957,0.0072101466,0.043805033,0.14278838,0.06664774,0.048083328,-0.051145736,0.029384991,0.09500063,-0.03564351,0.08343699,-0.028122371,-5.993007e-08,-0.009516833,-0.033739295,0.005496537,0.014536848,-0.028744766,0.025685417,-0.061321404,0.024965268,-0.0007909068,0.07222955,0.058867637,0.03235319,-0.07784611,0.034431003,0.07305789,0.025794992,0.050608322,0.010693724,0.032419182,0.05408317,0.06107071,-0.040873423,-0.03952005,-0.06017456,-0.004090073,-0.08812089,0.011666978,0.10378117,0.041602906,0.017951474,-0.013978097,0.029504409,-0.012356295,-0.007115949,0.02035468,0.017934943,0.024907967,-0.09159107,-0.011212649,-0.032010183,-0.016697897,0.020315265,-0.085356146,-0.011748284,0.033842243,-0.030883698,0.019497346,-0.022443883,-0.009062898,0.11108342,-0.107977554,0.03317422,0.019547764,0.021069422,0.047893275,0.02468363,-0.022694154,-0.07754477,0.055044603,0.07545293,0.014169016,0.04644138,-0.060202327,-0.06243626,12,34.625645,-4.04367,14
245,"discussed bird's eye view of next month. then we started feature encoding. no matter what the form of y be, when passed to algorithm, y should be numerical. 1st method to change categorical data is one hot encoding. which is just creating additional columns. dealt with multi class( like showing 8) where expected output is one of many classes. 2nd is multi label(a same picture having a dog and cat), output suggest all possible artifacts of the outcome. next approach we saw is label encoding where suppose red is 0 green is 1, need to remember that y doesn't not get effected by these values. but x can't be label encoded it will affect it's values and will affect the entire algorithm. next was integer encoding if suppose like grades, there is inherent order in it so can directly increaseencode it to that. in one hot encoding we  number of columns just pave way for curse of dimensionality, data becomes sparse. binary encoding convert in binary form then .next up was target encoding like averaging of the category values. to convert continuous problem into discrete we can use feature binning. we lastly saw how text data is getting converted to categories.",0.044631105,0.0011990169,-0.04392408,-0.06113296,0.013015257,0.0061194343,-0.0047426117,-0.09460902,-0.010031255,-0.0753026,-0.05076412,-0.040082723,0.022464432,0.049119372,-0.0075911726,0.060424846,0.004936981,0.085545,-0.084711865,-0.0826614,0.04058011,-0.0007312504,-0.028577723,0.007812047,-0.027505526,0.1284382,-0.019978078,-0.062304106,0.039393432,-0.06622546,-0.043668535,0.074745536,-0.020941572,0.045749925,-0.09069456,0.029313935,-0.032362584,-0.005500614,-0.04643004,0.010004107,-0.06533651,-0.0015280526,0.02914561,-0.023660868,0.077657625,0.05639573,-0.059507564,-0.050355908,-0.0745863,-0.08419282,-0.059470516,0.016036566,-0.0737336,0.034695286,-0.052967843,-0.019652126,0.02542862,-0.07362434,-0.013543899,-0.021936802,-0.094362035,0.018205376,0.0461686,0.013986448,0.038783167,-0.0481094,-0.004586211,-0.019502103,0.050063357,0.024399176,0.029822178,0.05653622,0.01265316,-0.03322433,0.02604225,0.034916956,0.08323612,0.042472124,0.084500924,-0.0013154141,0.038851455,0.023404118,0.032442104,0.040279336,0.1093893,-0.040839884,-0.09118866,0.047755156,-0.14638709,0.06192643,-0.03803865,0.006240826,0.05731539,0.00037677368,0.041211817,0.014442702,0.008900105,-0.056716304,0.09995808,0.020841677,-0.054418687,-0.021389311,0.011507941,-0.108452424,0.032429814,-0.048225474,0.12701376,0.042952698,0.028028466,-0.0989392,-0.007423587,-0.060520735,-0.09286341,-0.019060606,-0.051244464,0.04231013,-0.031550746,0.0203648,0.032737438,0.013564522,0.009961429,-0.016719699,-0.021116953,0.034675933,0.013297941,0.022082034,-0.008821155,8.944399e-33,0.0045471913,-0.06038205,-0.05437508,-0.04768383,0.07814912,-0.047413986,-0.038732287,0.010242876,0.051494587,0.035107993,-0.0023902494,0.05456688,-0.011757994,0.092671275,0.110941544,-0.07166689,0.0058969557,0.07836788,-0.048136253,-0.03969224,0.009551233,0.038917873,0.0538637,-0.0317877,0.016605211,0.040467523,-0.0409754,-0.09649368,-0.03617689,0.025752928,-0.069177434,0.02021118,0.007051956,0.022369804,-0.01966776,-0.028949236,0.10081934,0.025431102,0.0142685855,0.025545083,0.017679414,0.035738803,0.05054324,0.014014056,0.0150495535,0.11454071,0.03417775,0.06682133,-0.13076012,0.03675991,0.031965703,-0.03262801,0.031664517,-0.04778555,-0.07409537,0.0016835755,0.035282154,-0.061128672,-0.072044864,0.040619947,-0.038690638,0.013010609,0.08234778,-0.009907984,0.020027587,-0.04896604,0.022609336,-0.06132922,0.012696714,0.04396822,-0.078037046,0.03585469,-0.056025177,-0.13667952,0.015606983,-0.012248303,0.0741844,-0.03039221,0.0035446852,0.0077071427,0.007642237,0.062843524,0.045027148,-0.08675313,0.0019355679,-0.0077733127,0.034215502,-0.030561512,0.0009838869,0.0013803485,-0.029500166,0.016852448,0.050423987,-0.034153614,0.0841431,-9.249993e-33,-0.11145633,0.07128739,-0.05529776,0.039892357,-0.044911165,-0.03430426,0.009271134,0.0028247656,0.069995396,-0.05792729,-0.054627813,0.047406152,0.015567315,0.019165816,-0.06916814,-0.046287477,-0.117359355,0.032266434,-0.014402507,0.08062267,0.03149551,0.11951318,-0.10056195,-0.022864928,-0.044659406,0.10412361,-0.0240776,0.009516488,0.09048634,0.032030012,0.00270179,-0.08142476,-0.023875775,-0.036742188,-0.017224543,0.007259593,0.03150864,-0.035114456,-0.061311234,0.05449556,0.049916673,-0.019769754,-0.15246701,0.04337932,0.06058861,-0.01608863,-0.006553586,0.0066771987,-0.009436711,0.039515994,0.019081593,0.05578398,-0.062178247,0.016898459,0.021326443,0.0014437627,-0.08994803,-0.028576665,-0.027197018,0.062281754,-0.06701578,-0.035359185,0.051755887,0.0036983863,0.020370211,-0.05943357,0.016392732,-0.066601746,-0.05870677,-0.029628567,0.06387442,0.054281782,-0.05632564,0.043778174,0.00039817192,-0.07637054,-0.006752579,0.0060430565,-0.0050862147,0.07313176,0.0465372,-0.0016590306,0.02372548,0.11453582,0.0016456221,-0.014342011,0.10762516,0.027328944,0.059890196,-0.018375825,0.020395763,0.048052378,-0.057356067,0.07597145,-0.015164893,-7.053206e-08,-0.03652333,-0.040453855,-0.054536864,0.028082568,0.07251385,-0.004476309,-0.048095424,0.032587256,-0.005933493,-0.004233787,0.030203564,0.05653737,-0.029710077,-0.019405346,0.019306546,0.06584629,0.047848914,-0.07573581,0.0033529056,0.0718619,-0.0052025164,0.0124354735,-0.036893513,-0.019655038,-0.029642323,-0.08617507,0.011151424,0.09850925,0.08126323,0.037881196,-0.021117862,0.040881045,-0.013339584,0.049393535,-0.00076526,-0.053045064,-0.0056332666,-0.010523744,0.002976356,-0.06734165,-0.030843271,-0.022561545,-0.060243785,-0.028187292,0.0030679093,-0.009150474,0.025547232,-0.029362252,0.02947294,0.08758619,-0.07457253,0.038684834,-0.053462833,-0.009889728,0.06251564,-0.03697598,-0.06580302,-0.05261033,0.04480701,0.01216565,0.043598056,-0.027432209,-0.02052217,-0.061956108,12,34.938942,-5.3688087,14
255,"in today's lecture, the discussion was on feature engineering and encoding. depending upon the type of measurement (nominal/ordinal/interval/ratio) the inputs has to concerted into appropriate numbers that machine can understand (i.e., there is a need for encoding). we learnt different types of encoding methods. 
one-hot encoding: this splits the main dependent variable y into more than one variable which are independent of each other. but this method invites the curse of dimensionality (as it explodes into many number of columns) and gets into trouble in case of multiclass problem & multilabel problem; and hence should be used only in case of nominal data with less number of columns. 
after that sir explained label encoding and integer encoding. in integer encoding when we want to categorise things based on integers, there is an inherit sense of ordering where as in case of label encoding, numbers only represents different categories, it does not mean any kind of ordering. 
then sir explained binary encoding: binary encoding also accounts for combination of variables like (1,1,0) etc. unlike one-hot encoding where such combination tend to create (multilabel) problems. frequency encoding and target encoding were also explained along with this methods.
at last, sir explained feature binning, when we have low value of râ² (all the variance is not possibly explained by the regression model), in such cases me form bins and create classification model on them,  thus even if the regression model fails to predict the values effectively,  classification model works well in this case with high accuracy, precision,  recall, and f1 scores. towards the end of the lecture, we had and short introduction about how to process text data. how we can store it (use a number for each word or use an array to store that number; how the meaning of word can change with respect to its surrounding, etc. and how yo process text considering all this things).",0.04906681,-0.020760966,-0.075225554,-0.056946956,-0.020579817,0.06298538,-0.029609142,-0.05356799,0.03807728,-0.02582372,-0.03894386,-0.04610319,0.049206268,0.03376297,0.025947489,-0.019787403,0.025567723,0.062098444,-0.07580402,-0.049667723,0.079790585,-0.02336843,0.010901828,0.008526455,-0.030744774,0.022053426,-0.07272262,-0.015000272,0.0065201703,-0.029940592,0.01786912,0.11993605,0.014670245,0.004646979,-0.06747314,0.0073431553,-0.014194736,0.020154698,-0.03389905,0.027567258,-0.044672277,-0.0071980353,0.050319877,0.05116413,0.00950846,0.07028957,-0.011778083,-0.035059072,-0.13508831,-0.07488051,-0.01764811,0.073600166,-0.020133896,0.09930456,-0.062227093,-0.0474289,-0.03657143,0.015958592,-0.060307905,0.06471666,-0.16323315,-0.034245744,0.016254898,-0.02944058,0.052438222,-0.05672463,0.009982601,-0.04406341,-0.00083339977,0.02708005,0.037608717,-0.023815988,-0.023387354,0.046186287,0.07572981,0.027893992,0.060326178,0.03598365,0.049760442,0.009322537,-0.015092029,0.016337432,0.026812965,0.043750446,0.1175164,-0.06554312,-0.0023279428,0.011758371,-0.1123258,0.03897393,-0.0032787053,0.007840161,0.11490433,0.0265249,0.03299168,-0.058870394,0.032090586,0.060017876,0.076097496,-0.049064912,0.008126868,-0.03648476,-0.042673025,0.026237214,0.028938744,-0.085689,0.058139727,0.03249134,0.085545376,-0.1078418,-0.0050049494,0.005804404,-0.08535014,-0.037144545,-0.048916742,-0.008828359,-0.04765673,0.011045822,0.090001315,-0.05440455,-0.069324836,-0.09146029,-0.054647576,0.024975058,0.009978488,0.030679863,-0.012764682,3.255045e-33,-0.049180854,0.019872999,-0.087971225,2.4239715e-05,-0.032262277,-0.02330208,-0.0708735,0.024846684,0.08839098,0.054111697,0.020430706,0.067818135,0.022174558,0.1308451,0.06969768,0.023430025,-0.0070396774,0.06835019,-0.048683174,-0.02904915,0.033189297,0.06964269,0.076810494,-0.006999575,0.035720907,0.020689348,-0.025918191,-0.09168173,-0.04043688,0.01993036,-0.06118432,-0.043997485,0.05301763,0.009909686,-0.030357568,-0.06320747,0.00018113828,0.042481523,-0.018697603,0.007920634,-0.0015668283,0.020112302,0.026018307,-0.0329506,0.06589966,0.0760829,0.026536765,0.0053539104,-0.10805433,-0.026889957,-0.01680735,0.008885223,0.05378489,0.003080065,0.039210435,0.0022081658,0.02242328,-0.08127763,-0.041150738,0.06391052,-0.10505916,0.04525786,0.11581629,-0.033361275,0.023220507,-0.06045168,0.059520062,-0.039081555,0.03969948,0.065440044,-0.017028563,0.049435697,-0.09994542,-0.10855597,-0.017268782,0.035794903,0.0670076,-0.06115076,-0.0015990272,0.029721001,0.0028300413,-0.016524827,0.037824202,-0.0037197154,-0.08185737,0.0043448918,0.012773515,-0.06266271,0.012690575,-0.018670306,0.010638655,0.00547293,0.0076720407,-0.018800136,0.020797022,-5.3739594e-33,-0.07321986,0.018266596,-0.05974371,-0.020727018,-0.0816843,-0.023031618,0.010279749,-0.01974061,-0.01571052,-0.045122586,-0.03633921,-0.033779822,-0.028395005,-0.008709531,-0.025325306,-0.00540766,-0.12755069,0.026833253,0.022293538,0.032771017,0.024015428,0.08621248,-0.09131566,0.027053045,-0.014051126,0.017513525,-0.03697945,-0.039010923,0.02925093,0.06505643,-0.03682863,-0.095212445,0.046835538,-0.010936335,0.0139668025,-0.016905494,0.057634283,-0.00026301102,-0.029021975,0.0762677,-0.017547373,-0.003157594,-0.03250879,0.04380203,0.047721483,-0.0032827966,-0.018498305,0.025437463,0.025634259,0.05058803,0.116998434,0.03678269,0.007044026,-0.041922912,0.02828091,-0.03561302,-0.07502715,-0.042060584,-0.016294008,0.042067416,-0.014467121,-0.020738563,0.09609037,-0.009911942,-0.033971526,-0.0031966073,0.026779745,-0.057068884,-0.086871915,-0.039463732,0.02010171,0.026952066,-0.01301338,0.037552595,-0.098697044,-0.016094223,-0.028249338,0.0032114317,-0.07709648,0.06818033,-0.0021965858,0.025830388,0.022579541,0.088083565,-0.034539863,0.016721228,0.13042891,0.043554515,0.012584392,-0.05305112,0.048977092,0.08022468,-0.077869006,0.093019724,-0.04603867,-5.7986405e-08,-0.008168191,-0.05177696,-0.061648298,-0.061938852,0.015457865,-0.00065991963,-0.060066104,0.09214663,0.016153714,0.07798815,0.028330881,0.06712069,-0.072493374,0.014659935,0.08908404,-0.023024958,0.012661271,-0.06259048,0.012432444,0.060968548,0.056450084,0.018864082,-0.048329968,-0.07741361,-0.03999821,-0.04912505,0.002463133,0.070682555,0.09297491,0.005929118,-0.010987763,0.050494906,-0.03127836,0.035655573,-0.010255651,0.0105034225,-0.010547365,-0.027804513,-0.03177166,-0.1037176,-0.020591078,-0.052596178,-0.063556455,0.0020004604,0.05651216,-0.033001076,-0.030194303,0.025019566,-0.017086852,0.118740804,-0.098264664,0.03904915,-0.013852169,0.020170435,0.030978143,-0.039573908,-0.09835807,-0.097476505,0.022119505,0.024050966,0.05891656,-3.9957416e-05,-0.023640582,-0.06740701,12,33.315052,-5.1196313,14
258,"today's lecture was very interactive since we planned next ten lectures and briefly discussed group project. we started by diving into function encoding, which we demonstrated using an example. here, we used to transform one function with three color variables (red, blue, green) into one single function with variables y1, y2, and y3. we became familiar with two major types of problems: multiclass and multilabel with different solutions each.

we studied more about binary encoding, which is a very space-efficient way of representing data. how to convert data through this process was discussed in depth so that it would be understood how to get to the answer. we also studied frequency encoding, in which values of categories are substituted by how often they occur in the dataset. target encoding was also mentioned, where every value in a column is mapped to the average score computed for a given condition, like scores greater than 2.5.

we also learned about some other encoding methods such as label encoding, one-hot encoding, and image encoding. we had a hands-on example of how to do feature binning through an example involving a random scatter of data. and lastly, we briefly discussed test data processing, wrapping up a thorough session with a series of key topics that deal with data encoding and processing.",0.05462997,-0.009128005,-0.06413754,-0.015016753,0.012784108,0.0018631873,0.018036764,-0.043977335,-0.014115817,-0.044574663,-0.07054618,-0.07972269,0.07407434,0.024896491,0.07052683,0.03111208,-0.010220963,0.07538061,-0.10949428,-0.0799269,0.050781578,-0.056475107,0.03979684,-0.03119823,0.026266837,0.0007368254,-0.041390743,-0.044312812,-0.0002845049,-0.02786849,-0.011300595,0.070202544,0.058178235,0.062688015,-0.060301773,0.034837585,0.012324164,0.01935195,-0.02902853,0.026213473,-0.10417804,0.016528487,0.06278517,0.011006299,0.043905646,0.06315668,0.0052042333,-0.033061467,-0.10412951,0.01007069,-0.0045813303,0.032961544,-0.06812542,0.0646075,-0.04764298,0.0116075985,0.05360768,-0.044854637,-0.009506996,0.00842246,-0.15033293,-0.026037527,0.032143015,0.04515116,0.041747183,-0.046726268,-0.005963328,0.01218001,0.049989328,-0.031148503,-0.04473473,-0.015026681,0.0013464617,0.026904356,0.096320674,0.04457025,0.013116188,-0.0007338017,0.04045087,-0.062410057,0.022827538,-0.016485209,0.027114237,0.08337425,0.14673239,-0.0080589885,-0.020598287,0.0898745,-0.1123599,0.088542685,-0.072132036,0.0241072,0.07711535,-0.0090846475,0.0332358,0.003968524,0.074080974,-0.092431754,0.12107701,0.036331825,-0.043512803,0.0228172,0.013850095,-0.12853296,-0.03479142,-0.07750835,0.05737603,0.039538007,0.04027084,-0.1097288,-0.014277343,0.027461594,-0.10773606,-0.053817987,0.004139171,-0.02017801,-0.027980763,0.0126082245,0.028088603,0.037782166,0.015933858,-0.016195044,-0.012190548,0.035325736,-0.009159736,0.0037968769,-0.06964117,4.576789e-33,0.014837005,-0.05104406,-0.0666489,0.05026213,0.008813948,-0.041305277,-0.030937737,0.02008588,-0.036298502,0.055631887,0.026392097,0.086244345,0.0073567308,0.14278182,0.05091128,-0.03657015,-0.057104517,0.02306065,-0.081474036,-0.003707825,-0.014252773,0.051788885,0.11115985,0.007975728,0.0299743,0.024660556,-0.045354884,-0.08744656,0.021459334,-0.001837792,-0.076328106,0.015739538,-0.027130589,0.027776992,-0.026268546,-0.05318535,0.041189473,-0.00221298,0.02487249,0.10649558,0.0339789,0.020386059,-0.0056805606,-0.0028630625,0.010360352,0.05914355,0.06562146,0.005378002,-0.03818228,0.025813056,-0.0038427133,-0.0063263695,0.022264551,-0.025324192,-0.03447238,0.03641014,0.050893787,-0.1016702,-0.039845113,0.047808796,-0.07770827,0.07790813,0.061285745,-0.009961757,-0.03554113,-0.020490669,0.03158542,-0.023157213,0.05543505,0.012073308,-0.04009597,0.06708498,-0.07274748,-0.11879907,0.0039149225,0.054197717,0.028038803,-0.06814654,-0.08839462,-0.011625304,0.02023293,-0.051342085,0.002140281,-0.07387486,-0.05547,0.02939209,0.0088500045,-0.13892876,-0.044026777,-0.05213189,-0.03816811,0.02733491,-0.057773426,-0.041754033,0.04308136,-5.8352704e-33,-0.07374724,0.0914073,-0.0510726,0.06595881,-0.0042539774,-0.026190955,0.020452451,-0.05418785,0.04280696,0.0062389243,-0.07241165,-0.055844508,0.0048733917,-0.01561092,-0.0912113,-0.016071407,-0.08164643,0.058387104,-0.051066272,0.039522517,-0.013114425,0.14985178,-0.05280099,-0.01921118,-0.062355485,0.058102645,0.019101508,-0.057437878,0.043369696,0.038963675,-0.02872947,-0.048509497,0.035253204,-0.041279856,0.0051274966,-0.0010423216,0.09985307,-0.040370595,-0.06058326,0.072119445,0.07930924,0.047737,-0.069861345,0.022902012,0.020048153,0.022701,-0.053081125,0.054588817,0.008882922,0.02211754,0.025791729,0.026752545,-0.05743653,-0.041257802,0.055061303,-0.066538304,-0.06404736,-0.08740106,-0.037867814,0.0702581,-0.075276926,-0.0412507,0.084822685,-0.027207004,-0.026656073,-0.070097156,-3.8544895e-05,-0.0924121,-0.05529186,0.04028685,0.03286565,-0.022147555,0.01692633,0.0076551423,-0.003488066,-0.027660979,-0.016199142,-0.012101382,-0.06966933,0.071416825,0.01286404,-0.06355516,0.043883286,0.09964984,-0.030000808,0.045148578,0.080574,0.06917904,0.04031376,-0.04751579,-0.010780145,0.08407792,-0.019697763,0.06289449,0.015932348,-6.579885e-08,0.00092316046,-0.0215101,-0.016011158,0.015664743,0.00047067826,0.060477734,-0.025789425,0.07822449,-0.03230551,0.0046571638,0.06986085,0.032265328,-0.039156515,-0.03833884,0.06344955,0.04960491,0.07089237,0.012321751,0.019256737,0.06481007,0.022472547,-0.0051784823,-0.06364636,-0.05562743,-0.06459501,-0.024943855,-0.017414674,0.08734289,0.06900282,-0.019439913,-0.020474736,0.02729135,-0.0049535106,-0.03933307,0.019890305,-0.06192811,-0.03461258,-0.0050634323,-0.0188822,0.019476825,-0.025439028,0.015262119,-0.05293073,0.010887743,0.028535025,-0.011508313,0.004705573,0.048943784,0.013964597,0.04844193,-0.032516856,-0.009734347,-0.05750093,0.0466964,0.03255446,0.021856839,-0.0750089,-0.016498074,0.015591424,0.029311545,0.011769711,0.017162096,-0.059293456,-0.06548694,12,36.39471,-4.852425,14
274,"in todays class (12/3/25)
the class started with understanding differences between t-sne and pca, where t-sne can't be used while changing dimensions (projected features accounts into data loss and thus model can't be strained on) whereas pca is consistent (principal components contains into all the data while reducing dimensions eliminating the problem of data loss)
next we discussed about what will be the structure of last 10 classes and exercises. accounting the remaining class into feature engineering, big data and cloud computing, exercise 5-6 will be normal. 7 will comprise of project with due 13th april while 8-9-10 will be based in big data and cloud computing.
later, the discussion entered into the feature encoding, we initiated the discussion the importance of numerical value of y while passing for any algorithm/ model. there are two methods of which one is one hot encoding ( basically including creation of additional columns) dealing with multi-class objectives and other is multi-label classification where the output suggests the possible outcomes labelling each of them. 
later we delved into the discussion of label encoding where we understood the effects of x and y. followed the discussion on integer encoding where we used the examples of grades where there is a specific order in the output which can be directly implied into the code.
finally after discussing about one hot encoding ( number of columns help us to overcome the curse of dimensionality), binary encoding ( conversion of outputs into the binary form), and target encoding ( includes averaging the category values), we dived into the discussion of feature binning which is used for conversion of continuous problem into the discrete cases and ended the class understanding forming categories from the text data. ",-0.07243676,0.04442483,-0.014091725,-0.06694345,0.05100955,0.042439196,-0.012496533,-0.006992061,-0.018329065,-0.01813441,-0.063494705,0.03750276,0.022498338,-0.029890053,0.0065277745,-0.00560129,0.034708392,-0.0003339757,-0.11995637,0.0019076656,0.049360093,0.004212382,-0.040452775,0.09989968,-0.0037224775,0.033241224,0.026397312,-0.018467726,0.007934304,0.020314928,-0.06416633,0.12581207,-0.028952805,0.054432288,-0.05644869,0.04262116,0.02573768,0.03235741,-0.024372257,0.010799001,-0.050484892,-0.102236025,-0.026421804,0.021388195,0.08865447,0.024339981,-0.02148583,-0.14086342,-0.0785569,-0.038605228,-0.0074063144,-0.02537353,-0.11420501,0.031022837,-0.092967115,-0.031979464,0.029589238,0.020707848,-0.012394183,0.019140938,-0.06544994,-0.0599344,0.06144099,0.024261622,0.08941624,-0.07601963,0.018600596,0.012707957,0.0076058614,0.025487846,0.011472048,0.02095139,-0.046906795,0.025613965,0.011670181,0.009736627,0.0011444469,0.09688024,0.06870355,0.0006574413,0.023147034,0.06368044,-0.04737862,0.0039458694,0.073056996,-0.0004431103,-0.025751572,0.01565242,-0.049201634,-0.0023418278,0.032905247,0.012647338,0.062462337,0.021525983,0.042495295,-0.044270437,-0.029019991,-0.041545685,0.072264224,0.016547566,-0.043198697,0.06650637,-0.0038636806,-0.04074543,0.014708496,-0.038935598,0.05243085,-0.010832943,0.08342651,-0.14337684,-0.04942357,-0.013280667,-0.09810532,-0.031946316,0.016642934,0.049665872,0.0029730776,0.065268494,0.019830342,-0.004885926,-0.0853715,-0.040193405,-0.007692578,0.010339571,-0.010986281,-0.010654035,-0.11677052,8.181224e-33,0.056491878,0.0074235126,-0.092312634,0.025258489,0.0011760441,-0.05574288,0.019965617,0.042788886,0.06418387,0.06738233,-0.048510734,0.06095862,0.034795843,0.06559635,0.12369386,-0.03201188,-0.05336621,0.07922906,0.00889135,-0.045882955,0.05435319,-0.005313486,0.11706625,-0.037619803,0.061317634,-0.01564284,0.003018236,-0.07708433,-0.04815565,0.012764299,-0.059787523,-0.050853807,0.0026616224,0.013005015,-0.0013041425,-0.024310635,0.02893811,-0.009148177,0.053725883,-0.007299102,0.0129921865,0.051533658,0.01707145,-0.064868614,-0.008881444,0.044684906,0.0639437,-0.025344772,-0.052486673,-0.05414646,-0.0026249299,-0.061682202,-0.013116763,-0.06639565,-0.05828323,-0.0115678115,0.06527184,-0.026060468,-0.011783379,0.06857429,-0.07385138,0.006272972,0.051885188,-0.05917856,-0.07172334,-0.027788619,-0.026084917,0.026525851,0.015875796,-0.035679676,0.005899456,0.02205061,-0.07176304,-0.094772376,0.110674925,-0.030079227,-0.00045572082,-0.03564794,0.0012206259,0.037635453,-0.012753921,0.04525424,0.027409483,-0.05868117,-0.061116878,-0.032287505,0.09032597,-0.084396064,0.029831393,0.011476174,-0.05864235,-0.033489887,0.037358705,-0.00435367,0.039869834,-8.8081834e-33,-0.121541984,0.05503341,-0.10375871,0.036240067,-2.5320162e-06,-0.03733496,0.03733401,-0.011449699,-0.018638961,-0.005543879,0.028566549,-0.054537553,-0.034541667,-0.0022631597,0.0023341523,-0.063570224,-0.100964986,-0.052787032,-0.03732671,0.05799728,-0.008987166,0.06015036,-0.039346192,0.0036206618,0.011232368,0.02848335,-0.011698811,0.009286164,0.12042066,0.0066062543,-0.018584164,-0.08577337,-0.03598147,0.07159714,-0.042232122,-0.018000543,0.047513653,-0.023222463,-0.020722203,0.065016374,0.06565469,-0.056407165,-0.06231524,0.082431495,-0.012556054,0.009522825,0.020196648,0.07657565,0.013363766,-0.0052482053,0.11667861,0.034482043,0.0018746782,0.027422985,0.055872835,0.014511179,0.016032906,-0.057857323,-0.1167839,0.018828189,0.015512807,-0.014439201,0.05893307,0.021186953,-0.0028383036,0.0032663057,0.005765128,0.015043404,-0.0647477,-0.005623183,-0.0030207867,-0.021209637,-0.07335102,-0.012667324,-0.08801061,-0.100374654,0.0045976946,0.036070205,0.043056607,0.06558049,-0.060542375,0.06120968,-0.009036036,0.055066213,0.010719563,0.006658275,0.12820289,0.024786565,0.02236197,-0.055010535,-0.027211916,0.090264715,-0.06704644,0.097257525,0.012286647,-6.729333e-08,-0.05275786,-0.049030032,-0.024602713,0.033788785,0.048024733,-0.09038788,-0.04843642,0.12628984,0.041611854,0.08820822,0.039800517,-0.016163234,-0.036386676,-0.04338591,0.041853588,-0.009911894,0.043617282,-0.034332708,-0.010669017,-0.028146569,0.051069662,-0.046521455,-0.02008475,-0.060201272,0.012804799,-0.048246007,0.023676177,0.15267041,0.061152708,-0.023072243,-0.01962039,0.012725425,-0.061919235,-0.031123653,0.06569329,-0.016829094,0.06674715,0.014334939,0.067154594,0.04936873,-0.07851324,-0.025528451,-0.024314996,0.03462078,0.091353,-0.041052498,-0.05731497,0.0029212504,0.018964026,0.08105959,-0.07070488,0.013028477,-0.039165553,0.058971345,0.060523953,-0.0142387645,-0.059955087,-0.08037892,-0.012010283,0.020086026,0.041685216,0.005412627,-0.08148399,-0.0072672395,12,31.723804,-1.0917165,14
296,"discussed about the plan for the next remaining 10 lectures and about group project, learnt about function encoding in the start of class. saw an example of converting y(red, blue, green) with three column to be encoded to a function f(x) with three variables y1, y2, y3. there are two types of problems multiclass and multilabel, based on that we can have the approach ahead. then we saw binary encoding which results in a very compact encoding. we learnt that conversion in detail, how to get the answer. frequency encoding is in which we replace the category values with it's frequency it occurs in dataset. in target encoding, all the r values in the column are replaced by average calculated score above 2.5 in this case. other methods are like label encoding, one hot encoding, image encoding, etc. then we saw feature binning with an example of random spread of data. how to process test data. ",0.04975818,-0.005443605,-0.108583264,-0.034113556,-0.00063420093,0.021480009,-0.020486897,-0.06433002,-0.051087543,-0.042082366,-0.03984222,-0.063807614,0.02639765,-0.014062858,0.032254115,-0.041626636,-0.02966868,0.06848085,-0.12364354,-0.1102437,0.07193048,-0.043347605,0.06884459,-0.0039282297,0.0058122016,0.0031705743,-0.051141255,-0.02203562,0.03913667,-0.042608317,0.023148995,0.08823072,0.06521811,0.06969145,-0.030221904,0.03893126,-0.006054311,-0.016105521,0.029136565,0.060333848,-0.089680746,-0.006051524,0.06943009,0.005880746,0.04461165,0.075666554,-0.008134117,-0.0046177916,-0.107568994,-0.052809045,0.022050895,0.05708714,-0.0809974,0.072848454,-0.038193244,-0.0656984,0.02429791,-0.060103558,-0.050131805,0.050477825,-0.14896198,-0.0462906,0.023724863,0.06386257,0.019931665,-0.027474377,0.011951377,0.015814865,0.054714926,-0.060762085,-0.04303995,-0.032078076,-0.0028964826,0.031299837,0.10037191,0.09071613,0.0059619085,0.018793298,0.053199127,-0.037095465,0.035646513,-0.046237543,0.050083596,0.060064185,0.11846258,0.020747824,0.024951434,0.09505166,-0.14321831,0.08628731,-0.04990092,0.06090608,0.01909999,0.00885333,-0.038505998,0.0032582502,0.05744112,-0.045447655,0.1364277,0.002055658,-0.03583819,-0.032324083,0.007456706,-0.09075743,-0.09258669,-0.06646559,0.08442529,0.046606403,0.059122663,-0.10471876,-0.032056782,0.019350505,-0.10626112,-0.051451508,-0.033693757,-1.4255355e-05,-0.04248107,0.025047394,0.002323713,-0.011044072,-0.028457372,-0.037051093,-0.03184631,0.018316688,-0.051221874,0.026676267,-0.068751074,2.365242e-33,-0.045501314,-0.008488108,-0.056843672,-0.024622763,-0.03326807,-0.012014748,-0.040659513,0.03475796,0.013966399,0.05292645,0.018160671,0.0220251,-0.0044574784,0.1038419,0.046924252,-0.028823717,-0.05922495,-0.002458353,-0.069441006,0.031926256,0.06365136,0.09364692,0.093632445,-0.033648137,0.0026789855,0.027216276,-0.051571257,-0.10846369,0.0017420504,0.030049259,-0.09056207,-0.004669446,-0.045890775,0.04766428,-0.030361913,-0.043419737,0.07098573,0.05846223,-0.03657872,0.061697003,0.08556928,0.021223426,-0.022867536,-0.016367203,0.06674791,0.030982474,0.05651873,-0.008546626,0.007402579,0.04458753,-0.0067205313,-0.018343667,-0.0085413195,-0.008007524,0.005334608,0.047864094,0.05400601,-0.07505976,-0.0012054702,0.084508516,-0.06270564,0.03572106,0.080493525,0.0062336586,0.0042839614,-0.05076605,0.0085022235,-0.11483335,0.053910203,0.04563216,0.048065573,0.031030616,-0.07960478,-0.06304559,-0.032043047,0.043945607,0.010423017,-0.0077731996,-0.05047114,0.0022794635,0.03871405,0.028344877,-0.0061118975,-0.08993134,-0.06251049,0.019479027,0.0050897393,-0.10886831,-0.029404532,-0.01371145,-0.0055009234,0.007511806,0.00376612,-0.037198894,0.051903263,-4.6595674e-33,-0.025327107,0.13574086,-0.055033036,0.076876216,-0.011831549,-0.028829688,0.013154177,-0.011150263,0.044143826,0.030064683,-0.03620838,-0.087979056,0.04144381,0.012365099,-0.08368468,-0.069839224,-0.05114715,0.02281062,-0.015848534,0.0565413,0.010166097,0.13334267,-0.0027566033,0.0077455146,0.00040362732,0.017559648,0.0030363724,-0.00013306648,0.08535361,0.008682655,-0.017709652,-0.034134507,0.010436473,0.008828712,-0.02595921,-0.00031565042,0.09823693,0.014749811,-0.062321093,0.08208261,0.050941404,0.06653176,-0.1021745,0.03631344,0.05242894,0.02238361,-0.013433923,0.015618883,0.017769884,0.012935863,0.066165686,0.014865704,-0.031381607,-0.060604803,0.025584737,-0.034421436,-0.052111417,-0.050366454,-0.07201024,0.04959511,-0.02296362,-0.050840378,0.06434496,-0.02960612,-0.03298168,-0.050655015,0.04411314,-0.057464574,-0.031895675,0.052073237,0.032532148,-0.021442799,0.03047783,0.025865791,0.009776105,-0.055920683,-0.09305611,-0.015354684,-0.036370218,0.047046244,-0.0018112548,-0.048644107,0.0425065,0.12687126,-0.03125722,0.01043121,0.08079125,0.02778161,-0.012952698,-0.03749486,0.012283546,0.071272604,-0.04705988,0.07817693,0.021664256,-6.07754e-08,0.0048125987,-0.121092714,-0.056026734,0.0012767025,0.007863051,0.053293273,-0.09799186,-0.0025644628,0.029741123,-0.009994652,0.0035717303,0.031754605,-0.05270094,-0.0009414828,0.04214655,0.03824161,0.04172591,0.04170361,0.04774753,0.050060876,-0.005608683,-0.008135209,-0.024388993,-0.07120736,-0.07379506,-0.006011134,-0.016452616,0.035703186,0.073464535,-0.018884785,-0.0022720431,0.020963915,0.009057801,-0.053439338,-0.02689102,-0.03426784,-0.021852868,0.031916883,0.011882058,0.011434553,0.007652484,-0.0055078347,-0.09516113,-0.026164796,-0.014508626,-0.045085836,0.01032447,0.042853635,-0.0011311098,0.025838532,0.030455181,-0.0019379954,-0.12204503,0.0018297638,0.0044080373,0.0040152017,-0.07015789,-0.043272834,0.013388707,-0.015397711,-0.008778377,0.04917445,-0.038097184,-0.08611057,12,37.169144,-5.3942294,14
313,"today we started the discussion with feature encoding methods. when either of the dependent or independent variables are categorical we have to use feature encoding techniques to convert them into numerical forms with which we can work with. 
one hot encoding method creates columns which are equal to the number of unique categorical labels in the original data and fills each of these columns with respective true(1) and false(0)  values. this method should be used very cautiously as it increases the dimensionality of the data significantly and becomes cumbersome to work with. for one hot encoding treated data, the classification models should be tree based or neural network based as logistic regression cannot handle such data. to overcome the curse of dimensionality of one hot encoding we can follow another method which is similar to onh but uses bits instead of true or false values to encode the categories for example for categories of weather related data like hot, very hot the encodings can be 001 and 011 respectively. other most widely used data encoding method is label encoding in this integers are allotted to different categories present in the columns this is best suited for nominal scale entries of the columns as in this method there is no ordering of the integer values that is for example red:1 and blue:2 does not mean that blue is greater than red and its also important to note that this method works well for y and using this method for encoding x features must be avoided. integer encoding is another type of encoding which is very similar to label encoding but the only difference is the integer values have a sense of ordering and thus is suitable for ordinal scale valued columns. frequency encoding uses the number of occurrences of  particular category in a column and sets as the label for that particular category should only be used for features and not target variables, because there might be possible that two different categories might have same frequency but this issue some how doesn't affect the feature variables and thus can be used. target encoding this also used for encoding the feature variables where for a particular category all the corresponding y values are noted and then average of these noted values is set as the category label. we then discussed feature binning where the features with continuous values in discretised by making bins or categories so that the regression problem can be converted into classification problem , such conversion might be very useful when the actual data variance is very high and the r squared value of the regression model is very low. began discussion about processing text and how the text data is converted into numerical data which can be used to do some useful analysis. the common stop words are removed and dictionary of all the other words is made and a vector is associated to each sentence of the document based on whether the given word from the dictionary made is present in that particular sentence or not.",0.018330203,-0.06153885,-0.026211344,0.03696851,0.01016704,0.045474153,-0.023745032,-0.07847889,-0.04257246,-0.03872983,-0.009667797,-0.044693965,0.010481471,0.047447,0.0412507,0.04223761,0.041875064,0.08944321,-0.0669152,-0.03553743,0.029001143,-0.017850127,-0.087767795,0.04003101,0.011852531,-0.04672549,-0.0195872,0.055582345,-0.026892437,-0.0071734926,-0.027192492,0.051021136,-0.04478576,0.011560183,-0.08738011,0.00027250103,-0.091128245,-0.02084763,-0.0671473,-0.000983776,-0.036403004,-0.06576696,0.05100933,-0.027745325,0.09660903,0.061240777,-0.022203373,0.010826674,-0.09675508,-0.04525923,0.006640413,0.07379294,0.03289082,0.059607178,-0.059692126,-0.081553176,-0.040454745,-0.01347324,-0.038648676,0.033294033,-0.081423946,0.029161105,0.005279208,-0.02055603,0.004686794,-0.015852498,-0.038127776,0.0033317567,0.0611027,-0.017912587,0.037039045,0.011827071,-0.059631664,-0.010598158,0.04221073,-0.0034408153,0.12617134,0.04898811,0.096499614,0.004991319,-0.009582354,0.033182405,0.032003634,0.016488858,0.07608146,-0.011429345,-0.029277695,0.06813069,-0.13242441,0.028705435,-0.015628608,0.010289047,0.17172736,-0.030507905,-0.0052404883,-0.025789155,0.015982762,0.014123683,0.028262744,-0.041688938,-0.060449846,-0.05905332,-0.06784024,-0.027156744,0.08013407,-0.015317307,0.08077098,-0.0117585035,0.11075551,-0.06546076,-0.006919224,0.0039410857,-0.08928154,-0.09454651,0.009687531,-0.0050005466,-0.028940713,-0.0047802674,0.015275786,0.010379869,-0.019070124,-0.042868376,-0.05619493,0.019388212,0.037069075,-0.02965393,-0.045051154,9.459049e-33,0.012733798,0.04444978,-0.09491095,-0.042800356,0.013108088,-0.01786995,-0.0873427,0.03771974,0.08718433,0.05850363,-0.047827072,0.096565925,-0.007629181,0.08786865,0.06805929,-0.04405346,-0.017473562,0.0403878,-0.045368023,0.005453417,-0.023214024,0.015320949,0.07325066,0.0009668123,0.044308115,-0.01651354,0.010240471,-0.062694035,-0.060243797,0.024258526,-0.026098544,-0.062022515,-0.011123872,0.019038316,-0.025596296,-0.043647565,-0.03582178,0.048171684,-0.011496566,0.058280814,-0.0140456455,0.012141446,-0.017770618,0.0011654883,0.05133984,0.063386776,0.02918181,-0.027592925,-0.10554427,-0.015257829,0.045147974,0.020972462,0.011591051,-0.008154279,-0.057634477,0.0021886604,0.049351107,-0.084345676,-0.010040018,0.061974958,-0.1034061,-0.040817328,0.07072274,-0.043134127,0.005901293,-0.04105661,0.10192943,-0.047289338,0.0013617477,-0.04401696,-0.0098415725,0.036030177,-0.013743181,-0.09298944,-0.051668767,0.026455691,0.07463717,-0.03551096,-0.020608315,0.061310343,-0.008998629,-0.014135088,0.031317443,-0.01507196,-0.026576212,-0.0033879692,-0.017677775,-0.012832332,-0.046949558,0.014319801,-0.03815646,0.051442325,0.07015397,-0.07023574,0.09592908,-8.9579216e-33,-0.07508704,0.025975052,-0.043454286,-0.011035649,-0.088466614,-0.022445649,-0.0029485982,-0.0036428426,0.017532619,-0.02699866,0.021454003,-0.07122157,-0.004746972,-0.07254398,-0.008991629,-0.03237607,-0.16173276,0.058302548,0.0051647653,0.12928358,0.010476972,0.09928885,-0.16064873,0.04434928,-0.0402254,0.0029076913,-0.024531173,-0.0082488805,0.060220458,0.029388124,-0.03121826,-0.032872986,-0.019120445,-0.07988436,0.018632174,0.03103004,0.07355819,-0.042272266,-0.0023653936,0.04221798,0.059697874,0.014285486,-0.068249054,0.03268783,-0.013933017,0.0030156323,-0.047506385,0.021518998,0.036442596,0.020513386,0.110602096,-0.014095376,-0.026370317,0.048475496,0.054818507,-0.029010743,-0.045108795,-0.035556424,-0.062621854,0.041162442,-0.047080487,-0.0336067,0.12925088,0.016537728,-0.038952205,-0.05617286,0.0017775947,0.006641191,-0.072508335,-0.050544705,0.041643333,0.0043870662,0.023285942,0.06098759,-0.04854193,-0.10505016,-0.008276745,-0.0103227515,-0.017311377,0.086469784,-0.008746637,-0.012467201,0.008703826,0.12360885,0.06764638,0.026411403,0.07405583,-0.0013295567,0.065702096,-0.035591274,0.0041421186,0.11124633,-0.048481792,0.12290721,-0.033131953,-6.1661446e-08,0.0087261405,-0.059899334,0.006618368,-0.010037709,-0.01127912,-0.040786117,-0.028193133,0.05063472,-0.027200378,0.032434124,0.029081028,0.09288285,-0.06701259,-0.021004885,0.09864653,0.0055461563,0.060002852,-0.03938605,0.028655699,0.09526751,0.08092451,0.034255374,-0.056747522,-0.0496494,0.04533601,-0.07658373,-0.015437697,0.05391372,0.0895559,0.018301439,-0.010989093,0.018449962,-0.0050306837,0.021695353,0.05021108,-0.005808506,0.082904406,-0.015849467,-0.065029226,-0.032412473,0.020396732,-0.005692201,-0.043620773,-0.008941505,-0.028561162,-0.023656309,0.0211964,-0.014512716,0.06796208,0.082969315,-0.086885534,0.0421902,-0.029190378,0.03106,0.04212028,-0.013158709,-0.037748065,-0.033011,0.067927,0.028075956,0.07096476,-0.0076090894,-0.026956432,-0.022915946,12,33.14171,-2.7995884,14
326,"today we started our lecture with brief discussion about some confusions related to tsne and looked how it is to be used for visualisation only. we then started learning about feature encoding. we looked at an example where we had to classify the entities into red, blue and green. we tried using one hot encoding where we vectorize the features. but it comes with it's own disadvantage, mainly the excess column generations. we then looked at frequency encoding and target encoding. there are 2 main issues with encoding. one is the multiclass problem and the other as mutlilabel problem where an image can contain both cat and dog and it should not be labeled for only one of them. ",0.011994736,-0.022160089,0.0023122302,-0.024850782,0.002885985,0.076385535,-0.008674898,-0.06702016,0.008795727,-0.031554088,-0.04259034,-0.067984745,0.07999655,0.04447592,-0.031671505,0.0039528674,-0.03001558,0.10100757,-0.060158722,-0.09371661,0.043307096,-0.068825856,0.03031209,-0.03270704,-0.0041288915,-0.016744621,0.018118616,-0.0042263865,0.07406289,-0.026922617,-0.026715964,0.049319703,-0.07833814,0.077001825,-0.121018104,0.033757262,-0.039846815,0.025387561,-0.019001823,0.037640817,-0.0104587255,-0.0130558275,-0.050280277,0.0003236748,0.065073945,0.032343328,-0.042758357,-0.052132793,-0.080464505,-0.081593186,-0.03689016,0.009178528,-0.07550517,0.08733959,-0.050443336,-0.05743567,-0.042572733,0.011328138,-0.024791101,0.037316453,-0.047552478,0.006958333,0.055805914,-0.016550442,0.08231001,-0.045756932,0.015467272,0.054789323,0.04226935,-0.028226508,0.020042364,-0.022964465,0.00051104085,0.0156049235,0.039070345,0.035561427,0.05858172,0.05090657,0.070524015,-0.032841288,0.012355817,-0.053189393,0.033502787,0.0029650903,0.16214252,0.03454957,-0.07270741,0.0058742696,-0.14208841,0.054089993,0.0033154858,0.009525831,0.10222513,-0.026773183,0.033596978,-0.030797523,0.03254666,0.030562272,0.041654665,-0.035909723,-0.054669186,-0.05272831,0.002462172,0.03552344,0.013444333,-0.07971,0.113583334,-0.031179463,0.052901965,-0.15163039,-0.01449332,0.019238899,-0.078190856,-0.052176047,-0.07816112,0.022984587,-0.096243374,-0.021974208,0.101065986,-0.053516828,-0.06520247,-0.07506943,-0.071704105,0.051227927,-0.004705123,-0.07760518,-0.062831,3.185728e-33,-0.021768756,0.055657577,-0.098755956,-0.015889054,-0.028348051,0.02854436,-0.06505918,-0.04509641,-0.03307543,0.029291285,-0.043239746,0.01916811,-0.034659218,0.14365698,0.07250649,-0.09064455,-0.061391044,0.06022356,-0.025278257,5.2786952e-05,0.014802596,0.09569354,0.05746956,-0.008299152,0.0031505171,0.004663435,-0.015830247,-0.13343593,-0.031064155,0.026877953,-0.07261774,-0.006558253,0.027892424,0.042599395,-0.05294831,-0.035578877,-0.0035971114,0.043636613,-0.043581437,0.047897715,0.04727456,0.021868413,-0.02694886,0.02177336,0.013064087,0.055616826,0.039982367,-0.04361569,-0.08371243,-0.0033643486,0.021454755,0.0453211,0.009400443,-0.047534037,0.053800937,0.006217873,0.042311046,-0.08098512,0.021498943,0.043488227,-0.049029507,0.010978108,0.08133308,-0.012964397,0.058675963,-0.043351788,0.0121453265,-0.018644474,0.0198966,0.0019863525,0.025894277,0.05863522,-0.020180164,-0.15203597,0.0030709999,0.051626753,0.05713176,0.008607761,-0.04915963,-0.004159084,-0.00585695,-0.03534782,-0.020992704,-0.002840307,-0.06633326,0.028578421,0.043777883,-0.06587307,0.01806178,0.088878416,0.05660613,0.059873,-0.014642431,-0.08044615,0.029988986,-3.3478244e-33,-0.047563918,0.10743396,-0.085341215,-0.007084229,-0.089391366,-0.03689993,0.0104682455,-0.045248535,-0.01037671,0.045159195,0.031466372,-0.06902208,-0.056506936,-0.047055155,-0.062988505,-0.01788885,-0.0051635737,6.134055e-05,0.028172154,-0.011209165,-0.015644025,0.07599211,-0.1005589,0.047064204,0.0024699578,0.084946245,-0.029447503,0.0038277819,0.03408211,0.003131915,-0.015738446,-0.0654154,0.057880294,0.010193929,-0.002159919,0.049210124,0.059845828,0.010768548,0.015468724,0.08710255,0.052149355,-0.020514416,-0.1025194,0.07691211,0.027410155,0.012963962,0.0062348866,0.00336157,0.047022294,0.020018198,0.0918666,0.00035390927,-0.013847881,-0.099042304,0.025775705,-0.02403728,-0.006243278,-0.012621955,0.017295334,0.020496534,0.02786672,-0.03306966,0.03840349,-0.037046768,-0.03323457,-0.0712786,0.020925762,-0.0022153894,-0.018242342,-0.038268004,0.038425464,-0.008073045,0.0032167304,0.032534186,0.0012589206,-0.018964324,0.0076489565,0.049289826,-0.024119377,0.021399869,-0.09288706,-0.044774644,0.036004014,0.12409819,0.034807503,0.04744592,0.031970765,-0.003576844,0.06961459,-0.078731075,0.05064479,0.07713206,-0.06806467,0.09869193,0.060825966,-5.306361e-08,-0.02839039,-0.113938555,-0.051815633,-0.051010016,0.008597699,-0.043607365,-0.044546984,0.065898165,-0.0013760824,0.038834576,0.016786113,0.0075377333,-0.08440501,0.005820342,0.1255871,0.032282826,-0.0316936,-0.006840578,0.040580314,0.07469861,-0.02028056,0.022762155,-0.054759067,-0.045259874,-0.059884124,-0.010441191,-0.0038558987,0.065494455,0.07010129,-0.046299003,-0.015504583,0.060267128,-0.00047881517,0.00948818,0.032056946,-0.05159898,-0.017328946,0.013239741,-0.058852155,-0.0072469665,0.040480122,0.018721439,-0.03928767,0.012731044,0.05265212,0.031642772,0.03796665,-0.044965032,0.022519302,0.07670956,-0.05270314,0.028367091,-0.0029415763,0.07708736,0.010499033,-0.037730765,-0.023341483,-0.021316526,0.059356038,0.0019330215,0.028030407,0.06969283,-0.004338478,-0.05483782,12,34.417793,-1.4490135,14
338,"todayâ€™s lecture started off with the birdâ€™s eye view of the next one month, and we briefly discussed about the project. we then started off by discussing about feature encoding. both the independent or dependent variables might need to be encoded in order to convert them into numerical forms for training the ml models. basic feature encoding includes one-hot encoding, which involves vectorisation i.e. creating vectors of the size equal to the number of classes and then assigning 1 to one of the vector components based on the class. we then talked about different types of encoding like label encoding, which involves taking all possible values of the categorical variable and assigning integers to them sequentially. this kind of encoding works for the dependent variable, but not for the independent variable as the independent variable defines the model and hence any kind of ordering or hierarchy in its values will lead to errors. integer encoding has an inherent sense of order, where the assigned values may not be sequential and may have some meaning. one hot encoding has an issue that it increases the number of columns, and thus invites the curse of dimensionality. hence, one hot encoding should be used carefully and with data having lesser number of categories. we also discussed binary encoding, which uses bits for encoding. frequency encoding involves assigning the frequency of a class as its encoded value. this method is not very useful for the dependent variable as we need unique values for categorising y, whereas frequency encoding may lead to assigning same values if the frequency of two classes is the same.  we also have target encoding, where the average of the y values for a given class, are assigned to all the class values in the data columns. we then moved on to problems where we want to convert continuous data into discrete data such as height, weight, etc. in such cases, we might be moving on from regression to classification, which could lead to improvement in our metrics. we then talked about text processing, where we discussed large language models, which are examples of the manner in which statistical processing can generate a deterministic output. text processing basically involves converting text to numbers so that processing is efficient. usually the methodology includes dropping some very common words called stop words, which do not add to the meaning of the sentence. we then create a dictionary of words and then express the document using the dictionary.  ",0.021216268,-0.088123344,-0.035652224,0.014321637,-0.006673212,0.08298417,0.0074641714,-0.10247242,-0.00024532562,-0.077531636,-0.019763071,0.0036240274,0.04840409,0.029773617,0.00895094,0.023400934,0.01774468,0.070778646,-0.08106298,-0.03986988,0.04849374,-0.0328486,-0.050070286,0.037560795,-0.013893301,0.013778066,-0.010694298,0.006324555,0.049436674,-0.03744288,0.011603328,0.033883695,0.02427356,0.020055832,-0.06469553,0.010122674,-0.04477618,-0.04433368,-0.05968315,0.0068475804,-0.016289208,-0.028502174,0.04597418,-0.018450893,0.06924644,0.05128081,0.014486481,-0.029106416,-0.14532253,-0.059209593,-0.018100567,-0.017629487,-0.041546408,0.054122224,-0.078598306,-0.057872918,-0.041969195,0.001087899,-0.05380933,0.033134904,-0.114896655,0.016215768,0.025054285,-0.032387625,0.015968174,-0.0914351,-0.020822683,0.030290889,0.04659503,0.0074084452,-0.011398716,-0.011561905,-0.08089537,0.0025000765,0.023388013,0.048061486,0.13989785,0.015911398,0.07194152,-0.012439913,-0.06723025,0.032986812,0.029847829,0.0045254617,0.10504993,-0.054525778,-0.009344892,0.0495824,-0.09261554,0.042679064,-0.013776527,-0.019893976,0.1716768,0.0024786084,0.09687915,-0.004882059,0.07702026,0.0031880387,0.039337132,-0.0013432365,-0.029536342,0.0013720165,-0.030783443,0.019247534,0.0674,-0.08735628,0.07372402,0.006312497,0.053937368,-0.090114884,-0.010767602,0.00062424457,-0.07143677,-0.07709021,-0.019728653,-0.022094246,-0.08125938,0.006225061,0.037345696,-0.013801508,-0.023338895,-0.08018187,-0.03669996,0.04207791,0.000327377,-0.0153987445,-0.054423064,6.673815e-33,0.0058281347,0.014545646,-0.09305727,0.044999857,0.029895525,-0.028721767,-0.042778756,-0.0021236727,0.07280064,0.058404397,-0.041229602,0.04707169,-0.026216093,0.14263055,0.06968957,-0.0049777064,-0.0012951564,0.065456875,-0.03973605,-0.04480944,0.03530583,0.045772392,0.050529778,-0.022049801,0.044162687,0.033193216,-0.020234749,-0.13135883,-0.07332359,0.025523646,-0.07744999,-0.04422283,0.0009242486,-0.009490543,-0.011190754,-0.048216164,0.02157805,-0.011744835,0.0075255446,0.019985711,0.04675119,0.017138058,0.028101666,0.0003447441,0.019435847,0.032642756,0.058559578,0.018720636,-0.090757124,-0.004555951,0.00038651325,0.006962172,0.001072928,-0.04212245,-0.029385623,-0.006051007,0.0057190205,-0.082300946,-0.0670285,0.0032953478,-0.09767546,0.044617373,0.10610697,-0.022636903,0.03695567,-0.070817985,0.06949824,0.0036247708,0.026475204,-0.025196418,-0.04617533,0.05654427,-0.07024901,-0.15609851,-0.051388305,0.043753974,0.02312104,-0.11263541,0.010563575,0.05547586,0.007820092,0.021736396,0.04658573,-0.030483484,-0.040449955,0.012402677,0.026396234,-0.06534726,0.016810687,-0.018643865,-0.0064500934,0.03756952,0.073340245,-0.06090873,0.06425766,-7.27525e-33,-0.104220584,0.029983439,-0.059879526,-0.026605388,-0.06910383,-0.03191017,0.023331022,-0.013711328,-0.024580535,-0.022162365,-0.05424911,-0.021096509,0.019411,-0.00025767687,-0.03559105,-0.0034794828,-0.16005452,0.011537609,0.011835583,0.06043509,-0.0034094264,0.093214504,-0.095928185,0.029453598,-0.013116774,0.052864406,-0.009231199,0.0066009313,0.045050915,0.038956802,-0.018700691,-0.02974409,0.0074255182,-0.040711142,-0.00871159,-0.0041831993,0.050250944,-0.042332727,-0.02623812,0.056084864,0.07054117,0.0029783216,-0.05924013,0.04771715,0.033298682,-0.0031254329,0.004399006,0.023845658,0.029224453,0.010133167,0.082968965,0.024424817,-0.05260711,0.0059371507,0.009747971,-0.022232912,-0.07033672,-0.0024888034,0.039990578,0.054394763,-0.020907667,-0.031517476,0.09992786,-0.027580468,-0.061418597,-0.02067068,-0.0076764054,-0.06743855,-0.01325754,-0.06537205,0.065808475,0.0071356534,0.023116928,0.026979648,-0.08309902,-0.03751279,0.023183797,0.026556779,-0.031414058,0.04820501,0.011933353,-0.019149113,0.032044925,0.11395889,0.023358464,0.00793544,0.13315225,0.037639584,-0.012648788,-0.021998448,0.011478174,0.108703755,-0.03980183,0.16495085,-0.009495165,-6.3826825e-08,-0.024875032,-0.019322336,-0.04065834,-0.005882507,0.009839579,-0.05398618,-0.02654065,0.029126335,-0.015332521,0.058056884,0.030929366,0.04667571,-0.040346175,-0.0029837799,0.108414195,0.059424743,0.050899092,-0.06899859,0.0022861876,0.04779306,0.08084492,0.018973652,-0.002988333,-0.06396928,0.0077770515,-0.11379933,-0.017040286,0.14150776,0.087535866,-0.01245995,-0.02265826,0.095052324,-0.01029799,0.018764244,0.013257232,0.041851293,0.05686766,-0.05411883,-0.028086746,-0.06757509,0.0035062083,-0.012556147,-0.059276428,0.00838532,0.015894834,0.004459099,0.01170924,-0.0549288,-0.0072757346,0.07718836,-0.07820287,0.031167274,0.038584553,0.01440303,0.056136236,0.022389248,-0.03152508,-0.037804957,0.035410665,0.03316384,0.042437688,0.024892779,-0.025225919,-0.086363554,12,33.871212,-4.2586384,14
343,"firstly we started discussing about feature engineering, which is when either the dependent variables categorcial  item they have to be properly encoded in oder to be used in ml models. and we discussed about the plan for upcoming lectures and also discussed about the group assigment which will be done in groups of 4 and the dataset will be given and test will be given later we also learnt about one hot encoding",0.00076512597,-0.0578355,-0.016625233,0.031358667,-0.0054321704,0.029482616,-0.013312496,-0.076217964,-0.076456465,-0.047593374,0.052749112,-0.0073718536,0.010271686,0.00831334,0.0386278,0.053479623,0.022043364,0.063921295,-0.11759678,-0.08981991,0.059343856,-0.06305942,0.00020937563,-0.014670468,-0.044702888,-0.011738969,0.06122239,-0.017958436,0.040485002,-0.045280453,-0.024896901,0.057955682,0.02127926,0.01615623,-0.0037044461,0.052721173,-0.0055292104,-0.0720655,-0.024019286,0.04381005,-0.055328947,-0.06732994,0.038816784,-0.03714262,0.08751488,0.044107605,0.03293734,-0.041761987,-0.08691137,-0.029074995,-0.011137494,-0.06267334,-0.042134855,0.02568863,-0.04927887,-0.088424504,-0.05274016,-0.031206489,-0.059040673,0.023518443,-0.04204837,-0.018786296,-0.058421567,0.030633647,0.01973827,-0.028126776,-0.0074726776,0.06987112,0.035506338,-0.03008247,-0.019056404,-0.019447286,-0.07895325,-0.040974796,0.0461394,0.028144367,0.10247022,-0.050603986,0.08889459,-0.07224739,-0.04042608,0.04151029,-0.020686185,-0.0041090273,0.060833327,-0.04258421,-0.031942915,-0.012839133,-0.12631075,0.021814609,-0.005915997,-0.00012456808,0.11543216,0.00456936,0.016724182,0.057882264,0.08112629,-0.062195398,0.051247545,0.0108786095,-0.03644612,0.053434815,-0.043253448,0.016554339,-0.02987924,-0.06794391,0.08924117,0.006243671,0.06590817,-0.061415836,-0.013461945,-0.0018725961,-0.044334326,-0.104558274,-0.046566773,-0.08383586,0.0029439698,0.033310276,0.022714078,-0.00949151,-0.008175635,-0.014775749,0.056786243,0.0044200756,0.052744403,0.004076088,-0.07924981,3.0771342e-33,-0.00064426335,0.0076298444,-0.08233091,0.08103998,0.02257755,-0.05645802,-0.03689245,0.04644857,0.01798439,0.01025171,-0.027515663,0.022618456,-0.06814,0.1268554,0.060564835,-0.027358895,-0.030938242,0.0917622,-0.035335746,0.011904237,0.013841521,0.043643646,0.075353235,-0.033989515,-0.030899717,0.045833413,0.005343792,-0.095592566,-0.05389328,0.055443965,-0.07637588,0.028066324,-0.053977307,0.03717398,-0.07002223,0.027494567,-0.0485692,-0.027422022,0.05010293,0.069181375,0.04993507,0.004368556,0.024380961,-0.03951446,0.043633223,0.04108411,0.022648025,-0.04347767,0.020558702,-0.016832607,-0.03469197,-0.042740904,-0.007957985,-0.06066529,0.012329693,-0.009654347,-0.06415539,-0.115773655,-0.018939512,0.06572098,-0.10262164,0.09895611,0.068511024,-0.034594093,0.006572527,-0.03429105,0.055882055,-0.04549325,0.03899825,-0.011047753,-0.03536961,0.06002298,-0.026744945,-0.095220394,-0.028287007,0.033370838,0.014388516,-0.023062468,0.012384362,0.022015039,-0.0070334435,-0.014477285,0.023435555,-0.07168053,-0.011334966,0.06459002,0.026752198,-0.0677596,-0.051738996,-0.0035135336,-0.061587363,0.060882844,0.013304541,0.019856732,0.11957926,-6.3237834e-33,-0.053575847,0.07825659,-0.054356556,-0.013429453,-0.0026618922,-0.040664632,-0.00787507,0.0068469923,-0.026348526,0.063531466,-0.006363645,-0.07473724,-0.0032596504,-0.028299809,-0.03899101,-0.009401603,-0.076279186,-0.094898716,0.023992792,0.10022164,-0.02569595,0.07968561,-0.1396306,0.05380825,0.0003090272,-0.023301296,-0.032351054,-0.04824956,0.0036608847,0.056037396,0.015148527,-0.050517656,-0.042853024,0.018239763,-0.016727269,-0.001912132,0.062383786,2.14959e-06,-0.010990698,0.09849514,0.13588837,0.031308822,-0.054058474,0.04854665,0.027170992,-0.027023118,0.028114006,0.011097669,0.023178436,-0.012866629,0.09797792,-0.018715516,-0.0019445998,-0.0020062278,0.010521496,0.017637044,0.0010623516,-0.12907906,0.010935069,0.083563745,-0.038573977,-0.025734924,0.10349844,0.015440411,-0.051864237,-0.04760788,-0.0060794475,0.010840435,-0.06733188,-0.04277193,0.028323818,-0.07667082,-0.023916546,0.004549505,-0.035035174,-0.044734247,0.008245337,-0.020924728,-0.03613434,0.044811517,-0.008428355,-0.08263952,-0.0009143754,0.09371533,0.04001062,-0.018028904,0.1051243,0.06480588,0.026769556,-0.032931805,-0.04374261,0.06410306,-0.011998393,0.13244627,0.009834667,-5.0579334e-08,-0.05014309,-0.002818975,0.05248516,-0.008075234,-0.03104655,-0.039619878,-0.097197466,0.06982061,-0.013381997,0.08475715,-0.0043864595,0.06705302,-0.061616488,0.054719783,0.10862272,0.061285086,0.021458283,0.01427174,-0.0148588605,-0.01823407,0.10454344,-0.011244071,0.029434899,-0.07011935,0.029047232,-0.058745164,-0.015088965,0.039116718,0.004632629,-0.020598048,-0.035154372,0.06785055,-0.06370225,0.015246549,0.07386539,-0.0062028947,0.05361631,-0.05670412,-0.016806712,-0.0034285788,-0.0014067949,-0.039283562,-0.053541973,0.04735909,0.03563089,0.0067887176,-0.0032766727,-0.08176219,-0.03041303,0.104895644,-0.071716085,0.020207305,-0.029551525,0.068689175,0.07206809,0.0625338,0.019978816,0.0019070854,0.055050973,-0.07038695,0.08468135,0.048668064,-0.05744383,-0.0056467094,12,35.93492,-2.6292715,14
411,"feature encoding converts categorical and textual data into numerical form for machine learning. one-hot encoding is common for multiclass and multilabel problems but can cause the curse of dimensionality. label encoding assigns unique integers, while integer encoding is used for ordered categories.

binary encoding reduces dimensionality by converting categories into binary formâ€”three columns can represent eight categories. frequency encoding replaces categories with their occurrence rates, while target encoding assigns the mean target value to each category.

lastly, vectorization techniques like tf-idf, bag-of-words, and word embeddings convert text into numbers for nlp tasks.",0.07538795,-0.0014085303,-0.03511974,-0.024145544,0.004065465,0.06271204,-0.01637462,-0.078823775,0.014686438,-0.055244137,-0.01331769,-0.03786396,0.034026403,0.026523,0.033617124,0.05601033,0.021967508,0.117861256,-0.06577421,-0.08207132,0.053642854,0.013253982,-0.0010921699,-0.014091206,0.0033676925,0.014512557,-0.0834676,0.0054941527,0.004998028,-0.004582038,-0.005205244,0.060102638,0.019879961,0.053236756,-0.07233514,-0.009886341,-0.07793964,0.031981952,-0.079463705,0.050800454,-0.052049223,-0.038208377,0.028085401,0.02756293,0.047884773,0.08429131,-0.060437497,-0.0045929668,-0.04938073,-0.018041497,0.0033132683,0.03240857,-0.03403754,0.10785455,-0.0722434,-0.06594389,-0.027012717,-0.012458303,0.00107826,0.0010896059,-0.086489074,-0.015964327,0.067086205,0.005508558,0.04149659,-0.033879716,-0.057930745,-0.0132277515,-0.029030675,0.009043157,-0.012903533,0.040380396,-0.04841726,0.040297225,0.063872114,0.0077957283,0.005312519,-0.019270549,0.106435224,0.029123155,-0.053041935,0.0101435715,0.027672548,0.005594787,0.1458253,-0.026723411,0.060066342,0.014772354,-0.107595414,0.03068236,-0.046508703,-0.041479506,0.16854875,0.0051454045,-0.06551011,-0.040279597,-0.0071867565,0.024611246,0.008315981,-0.022572713,-0.031656317,-0.014858682,-0.028217401,0.036374457,-0.0097561125,-0.043310404,0.0949127,0.0063763913,0.10747191,-0.09485234,-0.0032545808,-0.0002844638,-0.11178281,-0.03113783,-0.012784216,1.3306608e-05,-0.02915902,0.027445804,0.06470706,-0.003362101,-0.065524794,-0.04022128,-0.042711005,0.02726813,-0.06765678,-0.009791429,-0.0066847294,4.6423824e-33,0.0020120621,-0.012892358,-0.10101343,-0.048546392,0.03267325,-0.05467068,-0.07515589,0.035973337,0.052566044,0.056266885,-0.036314856,0.032968074,0.015303963,0.16437243,0.029942347,-0.03214748,0.016787438,0.06625694,-0.011982992,-0.040936127,0.0033701316,0.053761553,0.03833828,0.008965225,0.02000212,0.02797646,-0.010305039,-0.0798137,-0.02952125,0.011362579,-0.06346283,-0.034085024,-0.014274962,-0.00362813,-0.016536014,-0.031879716,0.04076806,0.05821753,0.032978065,-0.002238884,-0.029317936,-0.00510912,-0.0027926203,-0.03356511,0.0589936,0.06603185,0.054068744,-0.026648873,-0.0689289,-0.025735186,0.023557136,0.0004308348,0.019215064,0.019968353,0.014869613,-0.008454007,0.058985338,-0.05814916,-0.042006686,0.035450768,-0.057093937,0.032059766,0.108741924,-0.037947148,0.062031273,-0.0548738,0.09757407,-0.026763259,0.05427916,0.039282434,-0.010047118,0.06684834,-0.04471938,-0.05312229,-0.051438987,0.04271432,0.06671832,-0.111710824,-0.016652696,0.07811298,-0.0051323306,-0.07989898,0.04200197,-0.07391825,-0.045046877,-0.023333218,0.0049669403,-0.12156786,0.050177887,-0.0070523615,0.0025021243,0.042589728,-0.011885433,-0.016641859,0.06576708,-4.3893466e-33,-0.10768113,0.029238809,-0.070254594,0.0021385718,-0.05379972,0.0015636159,-0.019095352,-0.014743678,-0.009244011,-0.0010515413,-0.025371626,-0.065691024,-0.027969938,-0.056992322,-0.007726479,-0.043125056,-0.13569096,0.06816021,-0.015804049,0.07191531,-0.006152504,0.09444535,-0.09152523,0.072387315,0.026549857,-0.008493178,0.002758294,-0.05525346,0.036000818,-0.042701345,-0.03749094,-0.06566259,0.03691026,-0.0076175327,0.0004095092,-0.02356153,0.052978963,-0.002434674,0.014476407,0.057829816,0.0466652,0.01482773,-0.041111227,0.019970853,-0.04397339,0.03167936,-0.041496467,-0.016941585,0.08580148,0.0032707835,0.070901155,0.027270788,-0.060120016,-0.00036705198,0.039191976,-0.005330401,-0.038442075,-0.074276365,-0.02110785,0.027582292,-0.049546484,-0.0025578893,0.15108337,-0.036318664,0.014988239,-0.06649187,-0.006587499,-0.011724685,-0.12595116,-0.046468273,0.08718637,0.012409719,0.006997806,0.0552468,-0.089719,-0.10811097,0.029140977,0.00077061605,-0.0791498,0.0053818705,0.04399123,-0.028238324,0.0035617214,0.11373249,-0.015675154,0.018975073,0.11460722,0.0056167045,0.021499949,0.0040013287,0.012785882,0.12804092,-0.042484988,0.12168035,-0.015595544,-3.8702353e-08,-0.033621535,-0.04959905,-0.06877537,-0.032573413,0.0017415399,0.02815471,-0.04136543,0.07407063,-0.03712635,-0.019864528,0.059135765,0.041426416,-0.071022496,-0.012457702,0.069176145,0.0039506303,0.054475233,-0.042667426,0.07462487,0.02587299,0.054884005,0.039981183,0.022036381,-0.04513743,-0.0328944,-0.05461508,-0.0095506925,0.13531576,0.103976876,-0.03580308,-0.025122507,0.065058045,-0.005628731,0.006396058,0.044474743,0.020578248,-0.008576887,-0.038078904,-0.053977672,-0.03358823,0.03352787,0.022778489,-0.051452473,-0.02366586,-0.012582197,-0.012128381,0.012632388,-0.03952462,0.017813215,0.074810445,-0.0085428655,0.025571056,0.03432966,0.05571115,0.034297545,0.004036298,-0.052469637,-0.061274942,0.06423915,0.0057670074,0.08135459,0.0031242697,0.019084778,-0.048084803,12,32.615543,-3.8356433,14
445,"we started with feature engineering where we focused on feature encoding. since machine learning models work with numbers, categorical data needs to be converted.

we discussed one-hot encoding with an example but also highlighted its drawbackâ€”adding too many columns, which can lead to sparsity and the curse of dimensionality. while it works for nominal data with few categories, ordinal data requires a different approach, like assigning numerical ranks.

next, we explored binary encoding, which converts categories into numbers and then into binary form, along with frequency encoding and target encoding as other techniques.

before wrapping up, we briefly touched on llms (large language models) and how textual data is transformed into numerical vectors for machine learning.",0.039616823,-0.030795747,0.016923657,-0.022385824,0.028783428,0.03324159,-0.057175446,-0.055245705,-0.013564515,-0.04267367,-0.06115633,-0.026781663,0.028899353,0.025770372,0.04725011,0.045614135,0.06559482,0.10687399,-0.029434074,-0.06948038,0.028544039,0.048009582,-0.0084265685,-0.0028122442,0.0359085,-0.024050657,-0.025474465,-2.2617962e-05,0.011498896,-0.04271533,-0.030421617,0.03746042,-0.032780983,0.029831946,-0.06586934,0.013419568,0.0351419,-0.006009858,-0.08939388,0.01150607,-0.0071782162,-0.06930327,0.07410046,0.02502979,0.09486885,0.060418513,0.0016749281,-0.022155363,-0.06742252,-0.00688663,-0.048552036,0.018408587,-0.015252177,0.063735455,-0.10740206,-0.05473995,-0.005244541,-0.012250833,-0.0050237593,-0.009090038,-0.10570548,-0.054792274,0.005736035,1.198745e-06,-0.00551512,-0.08267533,-0.026192872,0.04594254,0.020159977,0.04815278,0.010843657,0.022637988,-0.062943324,0.038656235,0.01927932,0.006038418,0.010814029,0.005515895,0.09622606,0.018602965,-0.053998813,-0.004489515,0.029932585,0.050596345,0.05895621,-0.034682762,0.04253059,0.014868243,-0.118295975,0.04659496,-0.033620283,-0.033945505,0.16067466,0.0065184296,5.935622e-05,0.0017011417,0.011172095,-0.03724961,0.010970242,0.012844285,-0.027345397,0.036886968,-0.048236735,-0.044396758,-0.012705199,-0.033998743,0.07886824,0.030956492,0.11574035,-0.10527993,0.038516365,0.024407843,-0.082434565,-0.056281883,0.0025373779,-0.069942124,-0.0046975394,-0.019179717,0.054245222,-0.014625901,0.007325665,0.004209133,-0.05923123,0.057978004,-0.011094076,0.024336455,-0.02428985,2.2167272e-33,0.058832146,0.01608825,-0.07197444,0.028813217,0.038628437,-0.049435943,-0.0013100995,0.040985204,0.04786252,0.059174538,-0.039614603,0.04947805,-0.036128014,0.10755516,0.064425625,-0.054125853,0.03568854,0.038588855,-0.039024565,-0.014858456,0.04929998,0.045576423,0.043464955,0.021405783,0.047595378,0.059609048,-0.0061140363,-0.11155287,-0.06074588,0.023254506,-0.07021837,-0.03226682,0.007956368,0.029423026,0.003711999,-0.0051335967,0.004726871,1.343768e-05,0.07057263,0.05611934,0.0018760726,-0.012205422,-0.010489974,-0.016378133,0.012492774,0.053949926,0.06951149,0.00334062,-0.06513654,-0.05912395,0.036032304,-0.016639417,-0.046507128,0.018446933,0.015245596,0.004191233,0.020566551,-0.07661772,-0.098797455,-0.006493785,-0.04319995,0.06902971,0.13233775,-0.055791628,0.0071426956,-0.07602855,0.059750654,-0.04111273,0.022092229,-0.015805405,-0.043205768,0.08526941,-0.07587536,-0.06357932,-0.021802384,0.025465088,0.08174052,-0.11343877,-0.016987845,0.05017513,0.033072885,-0.042438522,0.05445513,-0.083555065,0.009878235,-0.013835205,0.03292263,-0.09384625,0.0014622576,-0.023912417,-0.048788283,-0.026045514,0.016124215,-0.03134019,0.039392192,-2.8648235e-33,-0.14418806,-0.009938967,-0.029269649,0.035374265,-0.080847144,-0.044676255,-0.02751946,0.034817863,-0.033726472,-0.026750386,-0.052481636,-0.0672172,0.024682028,-0.06284467,-0.026581326,-0.034699332,-0.07781444,0.03120906,-0.021079734,0.04498976,-0.007636784,0.1477353,-0.10245067,0.09020005,0.037137657,-0.0007115991,0.002846031,-0.039127227,0.018271934,0.025829552,-0.027848585,-0.043393854,-0.011657771,-0.034543943,-0.024718897,0.013398511,0.0781394,-0.010662228,0.0045310343,0.044667274,0.03942192,-0.030478142,-0.047716364,0.027447326,0.013465612,-0.020561567,-0.060523994,0.008420787,0.06524423,-0.010132895,0.08523787,0.041075855,0.013303547,-0.0060340376,0.0050209262,-0.047179665,-0.03308858,-0.08468078,0.031685043,0.017894763,-0.061288368,-0.022946443,0.10988697,-0.06254068,-0.025057778,-0.05249689,-0.009950076,-0.011624679,-0.1318559,-0.08270931,0.04238537,-0.06146632,-0.020858187,0.094330795,-0.08561224,-0.08382356,0.054590933,-0.028892005,-0.09266591,0.024854418,0.040468626,0.0030226016,0.043776967,0.091299154,0.07223972,0.038782764,0.13690318,0.059481457,0.022009805,0.017681155,-0.03591463,0.08505458,-0.03874032,0.07740582,-0.03011481,-4.254764e-08,-0.05039901,-0.010852587,-0.03380914,0.004313882,-0.0095880795,0.030250186,-0.04633544,0.094207644,-0.030715477,-0.014622624,0.004581536,0.084514305,-0.04397796,-0.018496195,0.107350715,0.07133884,0.051249444,-0.038321216,0.030850496,0.040055417,0.06996715,0.080091,-0.015809404,-0.03750912,0.009956799,-0.08271714,-0.013107308,0.15934327,0.13117664,0.036324296,-0.06463777,0.03764152,-0.011827514,0.03372354,0.036756203,0.029247183,0.06957676,-0.022035642,-0.035374653,-0.12078089,-0.0055172765,0.009556103,-0.051858008,0.03274411,-0.018622298,-0.021108644,-0.021087091,-0.012453966,0.027024688,0.067291476,-0.008676491,0.021264369,0.040153354,0.012851541,0.021648543,0.022461534,-0.04044982,-0.038948618,0.023471592,-0.021223957,0.05810141,-0.0018825593,-0.019612063,-0.02387279,12,31.593208,-3.6964188,14
472,"in today's class our discussion on feature engineering was continued upon .feature engineering transforms raw data into meaningful inputs for machine learning, with feature encoding playing a key role in handling categorical data. common encoding techniques include label encoding (assigns unique integers to categories), one-hot encoding (creates binary columns for each category), binary encoding (converts categories into binary format), integer encoding (assigns integers, suitable for ordinal data), frequency encoding (replaces categories with their occurrence count), and target encoding (uses the mean target value for each category, prone to overfitting). in classification, a multiclass problem assigns each instance to one of several categories, whereas a multilabel problem allows multiple labels per instance. feature binning helps manage continuous data by grouping it into discrete bins, reducing complexity and overfitting. for text data, key techniques include tokenization, stopword removal, stemming/lemmatization, and vectorization methods like tf-idf and word embeddings to convert text into numerical features.

",0.0450424,0.016715618,-0.020160465,-0.040403932,0.023670489,0.029152684,-0.03244225,-0.03739853,-0.007179043,-0.056824427,-0.051639367,-0.05212008,0.030147891,-0.007919938,0.04683387,0.04122882,0.001908075,0.11411998,-0.11409073,-0.13084371,0.07335343,0.022391843,0.039857265,0.03131051,-0.030022198,0.023347778,-0.048650127,-0.04147996,0.04410482,-0.03209455,-0.01219193,0.10041442,-0.02969699,0.059703227,-0.07145822,0.045181092,-0.03202338,0.020859923,-0.009589867,0.018585375,-0.06996169,-0.029438784,0.009604055,-0.0002651477,0.08572458,0.07826486,-0.02521986,-0.050453424,-0.007820567,-0.04077592,0.00039981032,0.010950348,-0.039209016,0.07443563,-0.058707967,-0.051774584,0.037826713,-0.0018335971,0.032189224,0.037673578,-0.05142849,-0.047428444,0.06820468,0.012312776,-0.0054890863,-0.03862887,0.0034764656,-0.01251317,0.030732201,-0.0033691798,0.025389358,0.03159931,-0.05406935,0.005697918,0.027483251,0.03883336,0.04574289,0.027462514,0.06041051,0.016019246,-0.050737683,0.006506534,0.04228717,0.0065964237,0.1517375,-0.044670213,-0.0412285,-0.0027804803,-0.11490857,0.08686262,-0.047546387,-0.07986008,0.12406499,-0.030356131,-0.0385383,-0.057572678,-0.026021298,-0.017893124,0.02045709,0.007406631,-0.06393441,0.013776111,0.020725373,-0.04871921,0.03337363,-0.022861587,0.07708229,-0.016084624,0.12563857,-0.07993972,-0.008865045,0.013486818,-0.13565306,-0.008068333,-0.003423119,0.0290958,-0.024728423,0.032367654,0.0630586,0.07342401,-0.11228292,-0.07474176,-0.017716441,0.02940239,-0.027658537,0.022657001,-0.033379827,4.9366006e-33,-0.020671755,-0.05314198,-0.08859053,-0.008709068,0.016019266,-0.05889264,-0.082459085,0.05310135,0.021081205,0.010859495,0.046797674,0.033867285,-0.011682964,0.13186371,0.037559245,-0.083987795,-0.046726193,0.05134069,-0.018146338,-0.009389943,-0.0054294695,0.0029128091,0.057386342,0.013073105,0.052489065,0.028524196,0.0002633527,-0.06966443,-0.021615107,0.040903755,-0.07652734,-0.014538684,0.07436121,0.0808503,-0.048157852,-0.04260322,-0.0010404169,0.028847933,0.040749647,0.08097015,-0.04374277,-0.02314576,0.023924513,-0.06686311,0.03555508,0.08570682,0.0038668925,-0.0504714,-0.08828169,-0.038492743,-0.0012070128,-0.00037410538,0.011533578,-0.00034652805,-0.022040352,0.010107302,-0.010313229,-0.08012517,-0.044203937,0.038799796,-0.06316215,0.07237105,0.063441336,0.020879071,0.05242809,-0.01657091,0.093053475,0.024208736,0.060292777,0.017988358,-0.047693484,0.045932222,-0.062232964,-0.10893312,-0.016994258,0.021959895,0.046233747,-0.04274919,8.078561e-05,0.08253519,-0.0024841374,-0.0026801757,0.05691984,-0.058212038,0.01846536,0.028869761,-0.028759446,-0.08688263,0.023271788,0.001637785,-0.06480797,0.06308138,-0.03234374,0.006984739,0.018834071,-5.6389224e-33,-0.06457988,0.037106115,-0.044102866,0.019899005,-0.015559334,0.010007499,-0.048080944,-0.055328313,-0.012325766,-0.05079868,-0.060311686,-0.03609216,0.010229969,-0.06626986,-0.08833913,-0.046065535,-0.09117871,0.02244276,0.0020427946,0.068927325,0.035876714,0.08973838,-0.09126572,0.0187799,-0.025395982,0.0065095467,-0.093437985,-0.03557842,0.091629066,0.0045457324,0.0076557933,-0.057757,0.03871815,-0.071339205,-0.009567019,-0.027116615,0.01247914,-0.011466873,0.037656486,0.10224556,0.06645695,0.013807313,-0.05968375,0.047268406,0.011953918,0.006128366,-0.06043128,0.02250112,0.04790862,0.03137958,0.019322777,-0.0080075255,-0.027561484,-0.012913095,0.018508408,-0.029369475,-0.009955749,-0.037183933,-0.04516642,0.018453952,-0.062348578,-0.02132731,0.13044716,0.0067628594,0.020754611,-0.05718922,0.026034487,-0.02087065,-0.11753362,-0.049289837,0.050717458,0.0014786068,-0.0536381,0.026111763,-0.057548735,-0.018180026,-0.026391387,-0.092835225,-0.09277265,-0.006103658,0.025921252,-0.048245434,-0.007856,0.13287473,0.034807157,0.009929322,0.092951275,0.09268472,0.069704816,-0.03283869,0.004774428,0.111703575,-0.0903963,0.11210414,-0.016116807,-5.4938905e-08,-0.0009448386,-0.02458598,-0.038054217,-0.030021464,0.03976592,0.008404617,-0.060300548,0.13365828,-0.025029602,0.014146014,0.013972855,0.050915077,-0.09230267,-0.011681984,0.0444045,0.0150124235,-0.004087132,-0.00819082,0.06798573,0.02222649,0.06142342,-0.036805607,0.0548569,-0.044849936,-0.046634164,-0.08709076,0.012068928,0.09654739,0.061979637,-0.020523997,-0.055358272,0.051536422,-0.021935659,0.021548029,0.040634513,-0.0040340945,0.04050402,-0.022370476,-0.027577903,0.017271424,0.014338328,0.051518407,-0.029720481,0.00696319,0.0016333744,0.031078208,0.020971354,0.0174416,0.006627004,0.09244402,-0.025011716,0.033550758,0.02553712,0.081149794,0.015081852,0.039753485,-0.05886123,-0.08683846,0.097381055,0.02038354,0.026159724,-0.024914347,-0.013861914,-0.04264077,12,34.01833,-2.7065485,14
487,"today, we covered different ways to encode categorical data for machine learning. we started with one-hot encoding and vectorization, then looked at label and integer encodingâ€”useful when the target variable has an order. we discussed the downside of one-hot encoding (curse of dimensionality) and alternatives like binary encoding, which uses fewer columns, and frequency encoding, which assigns values based on class occurrence. target encoding, which factors in relationships with the target variable, was also introduced. finally, we briefly touched on turning text into numbers using vectorization.",0.05847703,0.014332319,-0.023191618,-0.01942195,-0.006340615,0.07260832,0.0023913814,-0.102056436,-0.022012088,-0.0510742,-0.034076575,0.042674076,0.0072191837,-0.0010697864,0.04963808,0.023464538,0.050941378,0.10576807,-0.020644873,-0.053024255,0.012068602,0.007874829,-0.029125918,0.012427875,0.016308201,0.0022496725,-0.004999823,0.030255642,-0.009927384,-0.016383676,-0.0274887,0.028938096,-0.0035680414,0.05121215,-0.08492294,-0.003166744,-0.050855372,0.015306548,-0.07047531,0.048351135,-0.028506396,-0.027850622,0.08475666,0.039989937,0.06499737,0.036294624,-0.037073817,-0.021030746,-0.068872616,-0.016305307,-0.0051327953,-0.01748238,-0.011044035,0.060886092,-0.11690505,-0.062942356,0.0007703707,-0.022873513,-0.005565858,-0.0042876666,-0.08985342,-0.031610493,0.076221585,-0.02716288,-0.0056476155,-0.091785006,-0.006266172,0.06570666,-0.023240486,0.050206102,0.039429277,0.04227349,-0.062492393,0.02394183,0.031853244,0.057065673,0.00179971,-0.049261406,0.06907382,0.01324033,-0.04364835,-0.00800379,0.023584737,0.07773386,0.10644357,-0.03621873,0.031850368,0.039302446,-0.07332516,0.056039166,-0.034337398,0.0021391786,0.20122044,-0.0055935383,-0.031108756,-0.02125828,0.03371382,-0.012700338,0.010586848,0.0014402563,-0.05398067,0.013028039,-0.047025207,0.019935425,-0.014724759,-0.072591595,0.122833684,0.00027188245,0.09437441,-0.119117245,-0.0028294807,-0.005840787,-0.08862858,-0.09680512,-0.04006526,-0.04742824,-0.035275467,0.01715865,0.065880746,-0.017621005,-0.035943665,-0.02086087,-0.06330102,0.05650086,-0.057381682,-0.031369247,-0.037195507,2.3736127e-33,0.020543043,-0.015310816,-0.056610778,-0.005321112,0.035739314,-0.07181088,-0.053987123,-0.030050794,0.0543811,0.057701666,-0.02470185,0.055198655,-0.022357887,0.1610277,0.037634615,-0.025601033,0.029292865,0.054798573,-0.08251049,-0.033906892,0.019601328,0.025923146,0.012569563,0.015858667,0.020053351,0.029202223,-0.0034446053,-0.088704,-0.0087256385,0.0019764726,-0.053581644,-0.03935984,0.01888093,-0.017655734,0.006625101,-0.043765757,0.028809149,0.056012094,0.05446722,0.07057764,0.015842557,0.017542586,0.04608651,-0.021743765,0.03731585,0.07149994,0.0440057,0.024293069,-0.039145067,-0.055492077,0.011841579,-0.010594858,-0.011714404,0.006698285,0.010587337,-0.017784096,0.043853328,-0.056038886,-0.08763158,0.024010621,-0.073817015,0.027633116,0.14197226,-0.04239803,-0.016511371,-0.059133228,0.06728563,-0.055689566,0.04644051,0.016330061,-0.039751958,0.08807079,-0.058253758,-0.059198793,-0.004416366,0.088622905,0.050611157,-0.09125602,-0.06315241,0.021780336,0.023741072,-0.022881305,0.054981116,-0.040438823,-0.004493776,0.004805866,0.009940682,-0.10763235,-0.006920104,0.013552974,-0.06808617,0.038712744,-0.002568797,-0.020220036,0.05908115,-3.0274493e-33,-0.11476425,0.02494333,-0.034058847,0.0113385655,-0.04775595,0.018496782,-0.03019946,-0.001220363,0.00895772,0.03255336,-0.031701285,-0.011951918,0.023854392,-0.04169483,-0.011523342,8.10999e-05,-0.124993,0.06112549,-0.039998934,0.025867766,-0.032009445,0.056120556,-0.070775434,0.06348745,0.053633273,0.028708955,0.022567075,0.005693734,0.0433609,-0.012534145,-0.06747133,-0.05059747,0.022839323,-0.039449614,-0.03323498,0.03793567,0.05949084,-0.03961894,-0.033248764,0.09696421,0.04800017,0.016648946,-0.05215125,0.026768947,-0.0024076235,-0.020975424,-0.034740776,-0.010693753,0.07146624,-0.009736694,0.05851061,0.027515547,-0.042889465,0.0053776135,0.056747776,-0.049819417,-0.028865105,-0.04043189,-0.007589333,0.06935442,-0.04267006,-0.014084241,0.13664551,-0.022177769,-0.05283526,-0.08984814,0.012888562,-0.029080857,-0.10210166,-0.07369665,0.094323084,0.045414183,0.0031783495,0.09520268,-0.10252616,-0.09267126,0.042877816,0.02468084,-0.09548504,0.054846086,0.077835485,-0.047346607,-0.0004917417,0.12497124,0.059560135,0.011926373,0.07775567,0.015500762,0.0015320105,0.00045760523,-0.045942795,0.08275369,-0.057192184,0.10381195,-0.04421385,-4.787722e-08,-0.0016600019,-0.030360477,-0.040491804,-0.004213533,-6.454865e-05,0.031199535,-0.044901747,0.05371522,-0.034854062,0.014396713,0.056004457,0.0613357,-0.04359851,-0.029107288,0.056480538,0.04646688,0.08236789,-0.081347324,0.061558403,0.08663579,0.05489516,0.07712936,-0.037906803,-0.065833345,-0.004512292,-0.09099177,-0.00758507,0.124714755,0.106183864,-0.019873802,-0.03783706,0.034001287,-0.0014539991,-0.011081882,-0.00055279693,0.04356107,0.0026993072,-0.010861081,-0.041975055,-0.06412718,0.045381453,-0.00012830312,-0.07441074,-0.029457213,-0.044496518,-0.009620514,0.02325273,-0.046851266,-0.010167751,0.035155185,-0.036918566,0.0016756922,0.05279922,0.033894405,0.027135186,0.0074924473,-0.05671885,-0.041149214,0.016264046,-0.009816254,0.085243545,0.005613583,-0.010437935,-0.03971668,12,31.960014,-4.1661396,14
548,"in today's lecture, we started with the plan of the schedule for the next 10 lectures and the details regarding our next group project. we then proceeded to function encoding, beginning with an example wherein a categorical variable (red, blue, green) was encoded into a function of three variables (). we considered two main types of problemsâ€”multiclass and multilabelâ€”and how they influence our approach. next we discussed binary encoding, a dense method that tidily encodes categorical data. we learned the sequential process of converting data with the help of this approach. proceeding, we discussed frequency encoding, where the category values are substituted with the frequency in the dataset, and target encoding, where all the occurrences of a category are substituted with the mean score over a cut-off point (2.5 in our case). other encoding techniques such as label encoding, one-hot encoding, and image encoding were discussed as well. finally, we examined feature binning using an example of data distributed randomly and discussed the method of processing test data.",0.07785821,-0.0005158164,-0.05109617,-0.034568474,-0.02377997,0.0147580635,-0.023790574,-0.055081014,-0.036241945,-0.05905103,-0.04366038,-0.051843368,0.03854187,0.021015517,0.046088215,-0.020219939,-0.015726766,0.07218702,-0.0894415,-0.08312895,0.07686209,-0.034848373,0.019389171,-0.011113255,0.055142052,-0.012286108,-0.060755476,-0.032184158,0.04951781,-0.017764932,-0.010180754,0.077737786,0.07238437,0.06451632,-0.03253347,0.04506468,-0.022988163,-0.0053602066,-0.036374737,0.034394264,-0.119417384,0.011592697,0.054714162,0.00053259195,0.050932582,0.07780561,0.023684278,-0.01231941,-0.11037202,-0.04455439,-0.004342981,0.0028313217,-0.060211435,0.03906155,-0.058729444,-0.036090635,0.06460267,-0.053185634,0.004677742,0.02682413,-0.13548613,-0.038162146,0.015992641,0.046438992,0.007842728,-0.035133302,0.0018425914,0.0050341664,0.056270164,-0.06624756,-0.032327913,-0.026547244,-0.04005095,0.04969731,0.071650825,0.049071845,0.055403695,0.027438449,0.057802144,-0.08380287,0.0067645716,-0.03447211,0.045794006,0.045946643,0.11256048,0.013514263,0.003135857,0.113633305,-0.15072952,0.07267286,-0.050847717,0.011762893,0.07686847,-0.0009782057,0.00769914,0.013577302,0.023727426,-0.054737747,0.12538378,0.05667507,-0.039393377,-0.019699186,-0.004434287,-0.086092055,-0.030979821,-0.04726104,0.100388296,0.021670524,0.06792192,-0.08361205,0.00059922686,0.041090507,-0.10500813,-0.07922976,-0.0051416503,0.018518832,-0.035170034,0.042413604,0.029152425,0.040656243,0.0055250656,-0.036330275,-0.034563128,0.02064238,-0.009400073,-0.0054327114,-0.045495573,1.2923104e-33,0.019397542,-0.06813447,-0.059027098,-0.011806695,0.04632728,-0.003721039,-0.049695667,0.025429053,0.014053865,0.037944853,0.021158379,0.0062655704,-0.050327048,0.123696916,0.07635295,-0.069369435,-0.015788414,0.05417699,-0.09426492,-0.00072642666,-0.0052721086,0.0046328744,0.063163824,-0.00045811484,0.008076354,0.04953263,0.00041932397,-0.10802879,0.021820316,0.024910493,-0.07181372,-0.0027173897,-0.012987281,0.03673754,-0.030989587,-0.05577883,0.037387006,0.04640481,0.009430938,0.07811969,0.058182187,0.03571916,-0.019859152,-0.022420844,0.034952946,0.023311038,0.08380587,0.014099403,-0.072258666,0.043631375,0.010141432,0.026242472,0.009885548,-0.042869613,-0.058800574,0.048926007,0.028900865,-0.12507942,-0.038393017,0.06618816,-0.04234872,0.072423756,0.06597885,0.011530946,-0.01847082,-0.060924485,0.0416665,-0.031030582,0.038475916,0.047290593,-0.034518808,0.04738196,-0.055081286,-0.10095614,0.001348744,0.05734672,0.0132021075,-0.026452731,-0.084317625,0.014814437,0.03990228,0.0003052219,0.0047617974,-0.09565729,-0.057798166,0.04462085,0.0119064655,-0.10928539,-0.0061215763,0.00055226503,-0.07210707,0.07433509,-0.046534564,-0.02745265,0.06360885,-2.8892468e-33,-0.07779054,0.106830426,-0.0377091,0.05478914,-0.00091855606,-0.042439144,0.028656194,-0.042260118,0.014032363,-0.0053744526,-0.041995823,-0.064251676,-0.016912747,-0.034233328,-0.091868736,-0.051124737,-0.061323244,0.04969301,-0.028574692,0.065140024,-0.00852176,0.13897537,-0.12535381,-0.019503685,-0.042306654,0.051691428,-0.0035174596,0.013137967,0.07673638,0.020513698,0.005163883,-0.052592557,-0.010575374,-0.0061036823,0.0075903786,-0.015026984,0.072148785,-0.01126072,-0.0545504,0.09793817,0.074701756,0.058805928,-0.10974489,0.0375465,0.01918782,-0.0188465,-0.043192666,0.02787139,0.03375713,0.011522294,-0.0049900613,0.021643002,-0.032218788,-0.03125538,0.04307567,-0.046227526,-0.054838143,-0.067728594,-0.043710034,0.071312025,-0.045929175,-0.04403439,0.1000613,-0.05238176,-0.015820842,-0.060239892,-0.0048913495,-0.07072863,-0.053549938,-0.0039772573,0.036027394,0.0109961545,-0.010551286,0.023842912,-0.031984013,-0.05717245,-0.02820363,-0.0046290285,-0.050679225,0.023737056,0.0008313079,-0.072735436,0.01111238,0.13742593,-0.006229632,0.017872363,0.040092714,0.058301378,0.02173321,-0.014271101,-0.04920749,0.10200706,-0.019722624,0.102242194,0.0068935086,-5.9413605e-08,0.022261087,-0.04217903,-0.040704086,0.009948293,0.02168868,0.046109118,-0.056597568,0.044579674,-0.016111901,-0.06202871,0.063060515,0.034112643,-0.0774168,-0.0007317734,0.06227298,0.012327619,0.086693905,-0.010112627,0.041144315,0.064472176,0.024460362,-0.0074014007,-0.03279328,-0.0858181,-0.076092705,-0.049667656,0.026719252,0.13018331,0.0855085,-0.023487411,-0.013479427,0.04517908,0.021629164,-0.052848008,0.01283816,-0.046293825,-0.022162097,0.019734668,0.019457052,0.021078097,0.044006128,-0.015194706,-0.012686091,0.016095025,-0.026232073,-0.035436705,0.044021636,0.0081849545,-0.006321908,0.027833555,-0.023281863,0.022788744,-0.043584373,0.047645677,0.049693312,0.026597288,-0.030308113,-0.0678632,0.011511324,0.031171469,0.01075164,0.012537494,-0.069679365,-0.06648074,12,36.22882,-4.8606563,14
552,"sir began the class by summarising the progress of syllabus till date and also discussed future plans, announcing the group project which is going to be based on assessment of assignments submitted by the students themselves. the main focus of the project is feature engineering.
then we were taught feature encoding. here one hot encoding was taught. this was don't by taking an example of classification of colours via logistic regression further multi class and multi label problems were discussed and the application scenarios for these problems.  further label encoding was discussed were the independent variables are not supposed to be label encoding whereas the dependent variable works were with label encoding next a similar type of encoding ie integer encoding was also introduced where specific integers are encoded when the variable are associated to those specific values based on domain knowledge. effects of one hot encoding were discussed where the dataset becomes more sparse as more columns are added in one hot encoding and examples of use cases were discussed.
next binary encoding was discussed where it's a type of sudo one hot encoding as it eliminates the extra addition of columns. next other methods like frequency encoding and targent modelling were surfaces upon and their use cases were also explained were frequency modelling is the variables are assigned the frequency of their occorance and in target encoding the average of target value is taken. here emphasis was put on choosing the correct encoding method while considering repercussions. next the classification on continuous problems was discussed and the application scenario where the r2 value is less in a dataset independent of the fitting model was explained as the variance of the dataset is to high to be explained by curve fitting and r2 values drop here binning is done. next sir starting discussion on text processing ie how to convert text into numbers so that processing is efficient and useful here sir briefly introduced us to characterisation of words which mainly depends upon the context. tree bank and word net dictionaries are used to find synonyms, words with same meanings or different meanings for the same word. further the overall approach was discussed via an example were a dictionary is formed and the document is represented via the dictionary formed.",-0.009846636,-0.032578096,-0.10909076,-0.039180424,-0.020230634,-0.0019407483,0.04853067,-0.03480134,-0.056823425,-0.0048361793,-0.007206457,-0.015175114,0.0955045,0.009806112,0.03270672,0.015193499,-0.022849215,0.038665045,-0.12667602,-0.097062446,0.06730748,-0.011237786,0.011151021,0.024892157,-0.03634485,0.022920048,-0.010637977,0.005389198,0.045127463,-0.058621906,-0.024139836,0.09391942,0.059263565,0.038564146,-0.051553316,0.009731316,-0.01923942,0.03528386,-0.042601123,0.015379098,-0.10078384,-0.045600247,0.043619175,-0.015976593,0.076096766,0.014467284,-0.0010053052,-0.05028866,-0.13059978,-0.046636354,-0.044205,-0.038014192,-0.026589159,-0.013364009,-0.10776779,-0.029852372,0.021267269,-0.037228473,-0.007099118,0.026700065,-0.14335193,-0.015688745,-0.044950828,0.026427126,0.042239416,-0.03000183,-0.046263184,0.02402842,0.03805876,-0.018313596,-0.029983925,-0.054413136,0.02329824,0.010775936,0.09984153,0.05743,0.060466185,0.08629329,0.0050869565,-0.063616306,-0.004865836,0.0008866363,0.029258998,0.04160547,0.13592993,-0.021114547,-0.01955126,0.05826182,-0.13616672,0.008370726,0.009874501,0.004040966,0.104047865,0.0022105556,0.0043066284,0.009623284,0.037682004,-0.022857986,0.07745868,-0.01780815,-0.05331897,-0.0041095447,-0.02052128,0.024695866,-0.06300189,-0.08958554,0.07835589,0.027975002,0.05294915,-0.054209676,-0.02859309,0.049894787,-0.15481874,-0.06886801,-0.006317023,0.0071518887,-0.017557004,-0.003827479,-0.025958037,-0.0033122632,0.0018403592,-0.039624885,-0.050595544,-0.02810474,0.008287737,-0.022206208,-0.10351068,9.280294e-33,0.017235955,0.06274943,-0.104641505,0.007762967,0.029854959,-0.023803206,-0.08654332,0.04314192,0.0736973,0.019436644,0.03618808,0.06553573,0.0023246417,0.12542982,0.017970704,-0.018850023,-0.02460061,0.07346146,-0.037885442,-0.005885828,-0.04487364,0.061372116,0.14111787,-0.04926889,0.022425769,0.08254425,0.012813583,-0.042911027,0.008254017,0.037212603,0.03515233,0.006784672,0.0029139,0.007635429,0.0010828906,-0.004011668,0.027865076,0.007123791,0.05774502,0.0776006,-0.0092316745,0.056692168,0.028664377,0.0023392946,0.021971777,0.047715507,0.07286294,-0.0060327565,-0.04088467,0.04388857,-0.090234555,-0.073265664,-0.013795362,-0.07602844,0.0036671807,0.044684466,0.03644107,-0.068130635,-0.0808273,0.017011484,-0.096214026,0.06546435,0.08738778,-0.015199521,0.0004046891,-0.037548013,0.06155088,-0.0040615997,0.07755776,-0.020414526,-0.022915099,0.049588323,-0.05409412,-0.065221176,-0.0076506096,0.03214216,0.045485076,-0.011153724,-0.031750105,0.043502297,0.004042602,-0.028567463,-0.030508092,-0.08965673,-0.05475006,0.056697804,0.015138351,-0.059648138,-0.020239998,-0.03907618,0.007298318,0.0325695,-0.003838351,0.008536612,0.0441283,-1.09277746e-32,-0.046278264,0.09776853,-0.085702665,-0.003822419,-0.003582345,-0.025472023,0.009696847,-0.014796946,0.00052261795,-0.043016024,0.011640178,-0.029521927,-0.06747865,0.030423155,-0.08475499,0.004712683,-0.110636,0.012268396,-0.026980003,0.04015543,-0.058055446,0.1058335,-0.110287495,-0.014923241,-0.055634372,0.0028899966,-0.027075363,-0.00886452,0.029231582,0.05513309,0.011661642,-0.081760764,-0.013966126,-0.02358275,0.02588316,0.005564923,0.08492,-0.033823125,-0.04193853,0.07282436,0.06876234,0.0039789043,-0.024189511,-0.032779824,-0.00024171037,-0.036237452,-0.003634065,0.013997807,0.025199063,-0.007936273,0.087196395,-0.04291045,0.009939741,-0.050696224,0.046307165,-0.01427928,0.011850524,-0.041813813,-0.027849713,0.076575436,0.001092702,0.011340791,0.10179457,-0.012050136,-0.034379493,-0.054490674,-0.021000935,-0.005608803,-0.06986472,-0.025767062,0.028402256,-0.019070031,0.025891146,0.023193296,-0.06659963,-0.05811176,-0.07241211,0.005736816,-0.090471976,0.072928995,-0.02300155,-0.039123796,-0.033761755,0.1286134,-0.053223178,-0.0222271,0.10564633,0.030141652,0.04892171,-0.09947105,0.00541615,0.12979046,-0.035489384,0.15582974,-0.0007844999,-6.6883686e-08,0.0073891273,-0.0888516,0.012976675,-0.0029860118,0.02346041,-0.0052454653,-0.07185591,0.036446087,-0.02353005,0.10276459,0.009022068,0.054849308,-0.053707086,0.014668167,0.047128372,0.039550282,0.0757871,0.010549491,0.01126456,0.009726177,0.07572939,-0.037248343,0.00045472843,-0.05430785,-0.01650203,-0.072442584,-0.0064361678,0.06664862,0.04771996,-0.0030469997,-0.01936687,0.0579147,-0.007066572,-0.05803917,0.023290118,-0.024842648,0.021744061,-0.052072123,-0.0140075935,0.036100563,-0.025698077,-0.014272656,-0.07647899,0.04002135,0.09182722,0.027265415,-0.053052265,0.008044659,-0.0048664603,0.07118058,-0.117929354,0.042126723,-0.014917069,0.02587146,0.015377734,0.0032842122,-0.01856147,-0.02752067,0.024335278,0.004430305,-0.011240748,0.030916002,-0.01751313,-0.024227867,12,36.011528,-3.6093483,14
567,"today's class was about feature engineering and different encoding techniques. we learned about one-hot, label, integer, binary, frequency, and target encoding, along with feature binning and how to handle text data.",0.02518784,0.0015077888,-0.025138544,-0.003372751,-0.013547967,0.031243172,0.059497092,-0.029584806,-0.096032016,-0.056868386,-0.019789765,-0.02210901,0.040502567,-0.023009675,0.028749142,0.011716342,0.024914406,0.046842612,-0.041496053,-0.09172752,0.055474766,0.006778258,0.0046010776,0.013078261,0.032422904,0.048880327,-0.021834675,-0.011256634,0.05127003,-0.035568014,-0.075219184,0.11209966,0.054145157,0.056484442,-0.046213094,0.022004843,0.034888633,-0.010988536,-0.060551807,0.018470611,-0.058231395,-0.04151357,0.06434658,0.035990003,0.07092042,0.035498213,-0.019256193,-0.009814394,-0.05556688,-0.054202303,0.0077955243,0.005986785,-0.04612928,0.033178367,-0.037891556,0.003759336,0.044817764,0.03147595,0.0067325598,0.040666457,-0.05265093,-0.032595888,0.011770682,0.02429206,0.0060688285,-0.078209504,0.04294092,-0.009287826,0.047394276,-0.09055086,-0.038601503,-0.017698308,-0.06560899,0.02928023,0.071638934,0.016832707,0.030686613,0.0048564547,0.05581395,-0.0017982166,-0.092384025,-0.08611962,0.07390037,0.048347294,0.10163723,-0.00079125684,0.0071347295,0.053106114,-0.07870325,0.0785207,-0.051214494,-0.046063956,0.12085465,-0.023233248,-0.009171177,0.0078225555,-0.0188884,-0.017826924,0.014414342,0.01583735,-0.05282201,-0.04470739,-0.02954647,0.02509678,0.0006417266,-0.06292847,0.11331268,-0.013508254,0.030658636,-0.06412096,0.014053118,-0.021340214,-0.08805769,-0.11217231,0.04397202,-0.03828291,-0.05715844,0.02012598,0.052770752,0.008700675,-0.028760944,-0.060113795,-0.07315942,0.027565002,-0.03899678,0.013750976,-0.04908737,-3.340138e-33,0.043521147,0.018224806,-0.079102375,0.044723857,-0.013212322,-0.028702449,-0.06468279,0.052315332,0.02583835,0.02748739,0.023803005,0.020787966,-0.025865138,0.12212055,0.046544354,-0.0791246,-0.0701522,0.06640612,-0.025389303,-0.0229582,-0.007556437,0.066137545,0.079959214,-0.04738694,0.044299617,0.041289035,-0.04542785,-0.101347215,-0.05664868,0.035078697,-0.08299729,-0.04294037,0.033999227,0.044505276,-0.0028573128,-0.048738554,-0.024026774,-0.010285154,0.025932703,0.05585629,0.032903798,0.02057763,-0.013607316,-0.083500735,0.02744056,0.06330753,0.031280156,0.013283687,-0.052224677,-0.025488032,-0.0006156808,0.0116786575,-0.004787189,-0.012695364,0.007258747,0.00068067625,0.007886559,-0.054765098,-0.050504915,0.104056425,-0.0670596,0.0762096,0.10074713,-0.097828925,-0.00022475839,-0.012727582,0.052771207,-0.04810542,0.010232896,-0.015647437,-0.05383334,0.05997883,-0.06601356,-0.07031169,-0.01791,0.03779388,0.06721877,-0.046961874,-0.031349357,0.010543259,0.004109318,-0.058399595,-0.003762646,-0.031981785,-0.010802046,0.05302496,0.0043790666,-0.10863794,-0.028061429,-0.03315065,0.01565562,-0.0009889435,-0.010526822,-0.021087406,0.07345593,6.394089e-34,-0.08543878,0.06638239,-0.0587743,0.017575966,-0.0665785,-0.016672237,-0.004120643,-0.010128876,0.015080018,-0.012553372,-0.020609578,-0.056090172,-0.04593899,-0.0920159,-0.07509551,-0.0155264735,-0.103819795,-0.038888324,-0.0090782,0.090237424,-0.021164877,0.04383926,-0.12641199,0.014620094,-0.0017991676,-0.016242642,-0.008794438,-0.02111946,0.078867845,0.025064254,-0.029864093,-0.025242614,0.02090307,-0.018556336,-0.04148013,-0.028815728,0.091007136,0.00081989315,0.04839693,0.07946701,0.11444351,-0.015080536,-0.017451027,0.07012286,0.02087942,0.023777854,-0.05428711,0.04774156,0.024999645,-0.020920146,0.08966392,-0.0019496622,0.004065142,-0.005935838,-0.032425255,-0.022633623,-0.026372623,0.014293379,0.016657522,0.07349715,-0.0708104,-0.020556057,0.09773132,-0.0017918505,-0.051898073,-0.03410405,-0.013352988,-0.012498555,-0.16187784,-0.048984688,0.07617483,-0.04132545,-0.0105400905,0.042151816,-0.07778593,-0.07807895,0.01153877,-0.01992884,-0.06786475,0.07103111,0.03240907,0.017794179,-0.028890021,0.15130867,0.059251416,0.055081747,0.05413271,0.031767815,0.029206643,-0.045923088,-0.068844534,0.13766953,-0.09554242,0.102347836,-0.008704097,-3.319831e-08,-0.05379152,-0.08184281,-0.033623133,-0.013761893,0.029315846,0.059383277,-0.093973465,0.057096723,0.010523683,-0.031204022,0.05148654,0.059166662,-0.078346394,-0.004144253,0.11963271,0.0032631345,0.025380112,-0.0046875807,0.034207057,0.0034136975,0.08739779,0.023716982,0.06416057,-0.06506109,-0.062894166,-0.045902148,0.015031496,0.116374075,0.11003651,0.01684284,0.016814949,0.06874407,-0.008276485,-0.034417722,0.056187823,0.02531602,0.039608087,-0.04386455,-0.008301509,-0.006637128,0.03849241,0.009530259,-0.053613815,0.0005066055,0.007212423,0.034303334,0.034653414,-0.05671699,-0.006371292,0.043858904,-0.0074689076,0.041598905,-0.010398734,0.06722723,-0.011871081,0.050864685,-0.03918067,-0.08231421,0.039135683,0.031173779,0.0057617202,0.0754104,-0.020016499,-0.049909614,12,34.79288,-2.6972265,14
571,"feature encoding
if any of the independent variable or dependent variable is categorical, it needs to be encoded before pushing it into the model.
we then got into one hot encoding, where we saw the difference between multiclass and multilable problems, where multiclass aims to process one of the classes and multilable processes and gives many of the possible labels 
we also understood that it is more covinient of encode the dependent variable and not mostly encode the independable variable as it is directly present in the algorithm
whatâ€™s the problem with one hot encoding, it invites increase in the dimensions and we all tend to reduce the dimensionality
we then saw about binary encoding, label encoding, one hot encoding, frequency encoding and target encoding 
then we gone into feature binning which essentially converts continuous quantities into discrete items
we then delved into text processing, the whole sole basis of text processing is statistics",0.030125495,-0.08504246,-0.043143246,0.0020775038,0.015661167,0.060801804,-0.020999331,-0.025728686,0.051585108,-0.029582208,-0.022371339,-0.027547758,0.065332115,0.018194176,0.08272564,0.013430172,0.055440035,0.060673527,-0.14418344,-0.0997087,0.08409815,-0.017490419,-0.01814271,0.008807286,0.0113413185,0.035402972,-0.045246195,-0.004464904,0.014220093,0.005523551,0.030632269,0.09348167,-0.010536722,0.019820875,-0.05986083,0.029863423,-0.07826691,-0.024627518,-0.032943383,0.0032109094,-0.055984143,-0.007961824,0.025855754,0.01511956,0.090072826,0.039259072,-0.058353357,-0.036616758,-0.117630735,-0.05616653,0.005543754,0.009040561,-0.041546162,0.06696472,-0.039669737,-0.05561335,-0.0030485408,-0.01252591,-0.045701124,0.011873967,-0.100885645,-0.0462075,0.03420301,0.013363949,0.066259354,-0.062617555,0.014957121,-0.009988064,-0.0013831875,0.008262579,-0.010610677,0.00469087,-0.10904015,0.003562142,0.08151958,-0.020038458,0.068665475,-0.00070614315,0.050984517,0.006138555,0.0054171067,0.03229823,0.072845764,0.023250047,0.123489894,-0.035151783,-0.0076167234,0.027116114,-0.12113514,0.020751547,-0.057969026,-0.04836841,0.09305313,0.02526728,-0.03788573,-0.048346866,-0.039456207,0.037149236,0.07475199,-0.044923276,-0.017880397,-0.02474809,0.003177891,0.013218631,0.04349856,-0.04372842,0.11803378,0.016300764,0.044494547,-0.09979868,-0.013550702,0.010392391,-0.078639805,-0.002472191,-0.011270062,-0.013323449,-0.016235936,0.02209858,0.081370704,0.055279575,-0.10087197,-0.06411915,-0.040260185,0.00957874,-0.019358031,-0.018759347,0.0022940296,6.165504e-33,-0.01139116,-0.021835012,-0.0807625,0.017684987,0.0360413,-0.0023728893,-0.07551784,0.016253963,0.087474376,0.049939863,0.0066305306,0.067543775,-0.005204775,0.15763474,0.08247476,-0.036631268,-0.029949056,0.10149824,-0.017075783,-0.005029023,0.025493255,0.0876322,0.08315178,0.014666624,-0.009699122,0.062293436,0.008029645,-0.08941185,-0.08524775,0.017646955,-0.064738,-0.0045800703,0.044427954,0.028945558,-0.02797676,-0.044246133,0.0253619,0.032101415,0.008746313,0.0010933406,-0.018740185,0.00847066,0.04621548,-0.034414735,-0.0056534423,0.04999756,-0.012264337,-0.05041775,-0.1091906,-0.041731786,0.01137287,0.011176039,0.09935078,-0.018128455,-0.01482534,0.0026895113,-0.029787544,-0.10118343,-0.04030402,0.062098794,-0.13527754,0.06141583,0.08626646,-0.024832197,0.08036934,-0.03919436,0.04585721,-0.0362553,0.04914012,0.0077975034,-0.032441445,0.07855732,-0.10514033,-0.12609,0.01985007,0.017411591,0.04059152,-0.07784038,-0.036590666,0.07583094,-0.049674414,-0.048384767,0.039959606,-0.08850421,-0.0064743254,0.008121677,-0.006753264,-0.075665824,0.021967031,0.028819392,-0.012042037,0.043691967,0.022744637,-0.031350598,0.088705786,-7.123747e-33,-0.086340494,0.027290381,-0.08434967,0.01374593,-0.053202085,-0.00044288972,-0.0009476994,-0.041413352,-0.023683349,-0.063024655,-0.066021346,-0.029604321,-0.026452132,-0.043886796,-0.0453237,-0.0020028516,-0.09146637,0.037556358,0.024928562,0.06366049,0.0371568,0.087807596,-0.09641011,0.016881611,-0.022689456,0.017355615,-0.066075705,-0.0117874965,0.06956887,0.071346454,-0.013393098,-0.05913639,-0.009871921,0.001826783,-0.017261097,-0.0115023,0.03930795,0.02074683,-0.009256445,0.04738425,0.041434724,0.012233356,-0.08956786,0.04167733,0.042584848,0.017916124,-0.06326167,-0.006916503,0.050469555,0.011509598,0.07095663,0.015347007,-0.021007327,0.01940136,0.0144099165,-0.020743288,-0.050888952,-0.090962484,-0.025598895,0.046907637,-0.011326203,-0.0057478035,0.112168774,-0.030135786,-0.009322367,-0.04249822,0.02471359,-0.039595105,-0.06507044,-0.046258368,0.053214107,0.008071888,-0.043709654,0.038317073,-0.043816898,-0.0049770237,-0.004903931,-0.027923563,-0.09851234,0.040921334,0.0370024,-0.035682153,-0.018297743,0.0701873,0.027543679,-0.021488436,0.10923381,0.03244025,0.05985249,-0.08492973,0.0038168419,0.12616241,-0.06570527,0.11110615,-0.027823567,-5.8203597e-08,0.0021932654,-0.046622526,-0.061311003,0.0011216135,0.008258385,0.005812657,-0.031391002,0.08720828,-0.026624111,0.021443611,0.022819178,0.049613953,-0.05542944,0.008558551,0.08877906,0.0011864321,0.009168952,-0.030626599,0.07297174,0.04125588,0.056926306,-0.0019212213,-0.032374498,-0.06863875,-0.043755896,-0.051335253,0.017647814,0.057421464,0.07483827,-0.0031458319,-0.0055362624,0.04768621,-0.021357093,0.034759577,0.037273828,-0.0034654508,0.072110355,-0.058431473,-0.04628095,-0.07311608,0.040861323,0.00021513455,-0.05288563,0.038292736,0.037330754,-0.019352533,0.04835403,-0.036514167,0.05009223,0.101086855,-0.046537004,0.032645248,-0.0071799965,0.024833089,0.037324887,0.013876485,-0.024100997,-0.0391311,0.035749163,0.014263985,0.046353,0.011854879,-0.021772834,-0.08722862,12,33.873405,-3.7020173,14
573,"first 10 lecs schedule was discussed. we started the course with discussing function encoding, where we saw an example of turning a categorical variable (y) with three variables (red, blue, and green) into a function f(x) using y1, y2, and y3).  additionally, we learnt about the various problem typesâ€”multiclass and multilabelâ€”that influence our strategy.

binary encoding, which produces a more condensed form, was the next topic we discussed.  we looked at how to determine the final product and the conversion process step-by-step.  with the introduction of frequency encoding, category values are swapped out for the frequency with which they occur in the dataset.  target encoding substitutes the average score over a predetermined threshold for data in a column. 
",0.031185105,-0.0039494587,-0.067336656,-0.042501625,0.008950586,0.03747476,-0.04841318,0.005357782,-0.026366975,-0.05176942,-0.074101985,-0.04589423,0.0296221,-0.019536993,0.07876644,0.036524504,0.0054471954,0.07085499,-0.104250155,-0.049348626,0.08934431,-0.042154342,0.026978543,-0.0013652849,0.022777563,0.008879509,-0.036573626,0.0014469739,0.012161,-0.033414736,-0.019836621,0.049008906,0.094653144,0.038319543,-0.053065427,-0.015397021,-0.042449724,-0.04246719,-0.012318696,0.07327526,-0.060155243,-0.015464785,0.03940814,-0.028116746,0.026377875,0.049073797,-0.006039229,-0.00053602556,-0.13022654,-0.005866847,0.0031986774,-0.0075724223,-0.13390633,0.07285347,-0.032898437,0.014894069,-0.0053345743,-0.05868353,0.006670357,0.03154051,-0.20079944,-0.010959223,-0.03294802,0.053349536,0.07873726,-0.02447135,-0.06235709,0.033092547,0.015950954,0.034844868,-0.04387105,-0.041285485,-0.03889034,0.026075536,0.12576668,0.0463491,0.06778729,-0.013254145,0.0067066457,-0.03803328,0.0017463478,-0.0042489655,-0.0015239505,0.054023124,0.115282066,-0.056630608,0.0075650266,0.10880687,-0.059711132,0.08255524,-0.020285832,0.011174189,0.10959113,-0.02961982,-0.0448647,0.03257738,0.02968994,-0.037881996,0.14505257,0.020036653,-0.041349728,-0.018254174,0.0089495145,-0.092770815,-0.09467436,-0.07028204,0.09315714,0.097635455,-0.00066193234,-0.11062834,-0.032390427,0.038303994,-0.09782855,-0.0856967,-0.045011792,0.051694736,-0.010982711,0.0052222186,0.025693014,0.010130115,-0.014101037,-0.056092437,0.014996462,0.041590195,-0.07137694,0.016858445,-0.08456838,1.0429787e-33,-0.0497864,-0.049465615,-0.06796882,-0.04010862,-0.010318928,0.00871454,-0.018870879,0.022463623,0.001882024,0.04983031,-0.00043373197,0.059638962,-0.05131185,0.12937054,0.06734863,-0.050902557,0.028488154,0.05536358,-0.057583768,-0.0026916538,0.057157878,0.020379752,0.08135251,-0.019639188,0.018573686,0.025160331,0.0072308443,-0.08149174,0.020641858,0.01907799,-0.034081392,0.017800538,-0.00016493164,-0.018206317,0.01026346,0.0008250703,0.045765758,0.022608666,0.03873412,0.082593985,0.0266829,0.017226702,0.0022615502,-0.0114174625,0.043333173,0.06343341,0.07101358,0.026230386,-0.041194286,0.033637695,-0.03130156,-0.025193904,0.03514303,-0.0062664566,-1.9252297e-05,0.027869754,0.020489374,-0.12878211,-0.06470293,0.07612249,-0.053534545,0.09413819,0.101078235,-0.003426735,0.014604519,-0.03447171,0.022981493,-0.06774759,0.048011318,0.014122419,-0.008554744,0.0342045,-0.06115358,-0.092250414,0.01831544,0.03542373,0.018262247,-0.03219401,-0.038271874,-0.024061788,0.063042484,0.060582884,-0.019553091,-0.1108969,-0.02108178,0.02540535,-0.005571375,-0.1262598,-0.075765215,0.020971185,-0.060390916,-0.009565327,0.015232627,0.0042552147,0.05081235,-2.0946826e-33,-0.061023623,0.04725722,0.00023299626,0.077132896,0.0013699717,0.0023125622,-0.0034314857,-0.0217397,0.08294089,0.08627669,-0.020077266,-0.045612875,0.028130606,-0.063154794,-0.07394207,-0.032281287,-0.064512506,0.01997694,-0.002532352,0.04780829,0.027753305,0.11533693,-0.058900803,-0.009160926,-0.0039678873,0.044280257,0.018310722,0.04669865,0.044819728,0.00751522,-0.068999976,-0.07726725,0.019168932,-0.022363324,-0.0063679875,0.013980951,0.05484438,0.00053950323,-0.05248586,0.045385428,0.032716148,0.01977758,-0.08174312,0.017815743,0.004407049,0.017819569,-0.037700925,0.054821327,0.08205705,-0.008228353,0.014781158,0.013220101,-0.043953195,-0.032670975,0.041337445,-0.03982072,-0.035543732,-0.098262556,-0.058254622,0.01764169,-0.038298514,-0.0090330085,0.0905178,-0.086199865,0.007010249,-0.046888232,0.041706517,-0.06828476,-0.055808477,-0.0018059991,0.068963766,0.024567299,0.029801464,-0.0067836517,-0.0804964,-0.05684272,-0.051980242,-0.032494623,-0.07271295,0.07563779,-0.017891113,-0.040367696,0.04663631,0.08899346,-0.05776102,0.031098079,0.09415592,0.06376372,0.03297643,-0.030350644,-0.017751498,0.08285425,0.011356874,0.110371664,0.0044544195,-5.5696603e-08,-0.016023872,-0.037470877,-0.04158505,0.03330566,-0.00019377543,0.023447577,-0.024640292,-0.02300951,0.00703472,0.011645544,0.057093985,0.04809628,-0.017643115,-0.003139675,0.061528616,-0.010847306,0.10619648,0.05282564,0.01605594,0.049981374,0.053535406,0.0016738924,-0.003751631,-0.06274581,-0.03844977,-0.0884255,-0.015314663,0.08244922,0.098384105,0.030769031,-0.03443251,0.015618544,0.012297695,-0.0015904496,0.0040667146,-0.025315998,-0.035190858,-0.01598158,-0.024925794,-0.006660011,0.04108593,-0.031325366,-0.09156359,0.005836479,-0.00025578035,-0.067146815,0.0015921575,0.042054858,-0.010820834,0.056647126,-0.0059702955,-0.01736865,-0.0561108,0.00575308,0.09951463,-0.0073741996,-0.017761115,-0.089294456,-0.017151773,0.017537462,0.020413129,-0.0055578463,-0.020641765,-0.024617115,12,36.511166,-5.6946883,14
596,"the class started with the recall of t-sne algorithm. though we can create clusters but we should not create prediction models based on this. because t-sne converts n-dimensional data into two  dimensional and then form clusters on top of it. 
sir, then discussed about the project and we started a new topic: feature encoding.  
when y is categorical and some of x is categorical, they need to be encoded prior to training model. we need the values to be numerical and not categorical. one way to encode is vectorize (one hot encoding). if y has three values red, blue, green then we have y1, y2, y3 and assign 0 and 1 depending on the value of y. it works like y1 = f(x), y2 = f(x) and y3 = f(x). if we use logistic regression, then we predict three models. but the problem with this is that the three models are independent of each other. the main problems that arise are: multiclass problem, multilabel problem. 
when dealing with multilabel problem: expected outcome is one class out of many class. when dealing with multilabel problem: expected outcome is the total labels that are possible. the way in which we encode y is different for both the problems. in multiclass we encode with differential ways, like 0, 1, 2â€¦, in multilabel problem we encode such that values depend on one another. 
when we are working in multiclass problem and the dependent variable is categorical, we go with label encoding. giving numerical values to data. we should not do this for x, only y should be encoded in this since. another way is integer encoding: it has inherent sense of hierarchy. 
one- hot encoding has the problem that it brings more dimensions. example: when we are dealing with pin-codes. when we have nominal data we can use one hot encoding. we do not use one hot encoding for ordinal data. 
we then move to binary encoding. this is similar to one-hot encoding except there are more options to describe data. 
then we have frequency encoding. it uses the frequency measure as the encoding. we do not use this encoding for y, because we need distinct values for y. it can happen that two different y values get in same frequency. in target encoding, the values are replaced by the average value. 
then we looked at feature binning. it is used when continuous features need to be converted into categorical features. 

then we started: how to process text data? 
most llm are the examples of the manner in which statistical processing can generate deterministic output (code generation). the basic question is: how to convert text to numbers so that processing can be made efficient. 
",-0.024157694,-0.079234704,-0.08188121,-0.04174086,0.053664796,0.046950884,-0.043070253,-0.052146062,-0.025054907,-0.058096733,-0.022392765,-0.054273587,0.1095552,0.021254586,-0.007839472,-0.017378988,-0.023904553,0.06908107,-0.069390066,-0.051060937,0.07193162,-0.001243436,0.0478893,0.10763766,-0.00898821,-0.05213048,0.030775823,0.0011327902,-0.06868138,0.0006822479,0.012775944,0.08275765,-0.015191298,0.06781549,-0.08873422,0.012932768,-0.060689177,0.028534848,-0.04690026,0.025641026,-0.06659664,-0.0458043,-0.01990618,-0.03561222,0.090988,0.011000296,-0.057532173,-0.055856925,-0.1052306,-0.107637316,0.021195238,-0.010037771,-0.08407107,0.012364646,-0.08271774,-0.07124883,0.045001194,-0.057220306,-0.04282987,-0.04730283,-0.009063434,-0.050750796,0.025987606,-0.015425368,0.054253455,-0.044605374,-0.009581827,0.0775494,0.016849542,0.00013747635,0.033894427,0.00088531565,-0.06253595,0.01627304,0.069456816,0.074788176,0.1592713,0.093087845,0.061078686,0.021981712,-0.0022678238,0.0008159785,0.00042873598,0.06553354,0.1381837,0.0065683364,-0.102379344,0.046312947,-0.047188595,0.032152414,-0.064638264,0.032487553,0.026353909,-0.036683735,0.039075747,-0.04839695,0.016960409,-0.04590414,0.11146957,0.0020761709,-0.051423516,0.028460808,0.046820708,-0.04702691,0.056105606,-0.01404538,0.08963806,-0.025904976,0.10479834,-0.11431066,-0.00025513294,-0.010823187,-0.1002051,0.0040265624,-0.041956652,0.08711163,0.01770616,0.027675552,0.008732544,0.02623581,-0.06445914,-0.012991071,-0.02545904,0.06442921,0.043518253,-0.081612006,-0.09229432,5.948941e-33,-0.025013367,0.010311479,-0.05151494,0.02859226,0.03126127,0.010091848,-0.06203056,0.014536986,0.0050932136,-0.024192194,-0.09645404,0.03697423,0.05031765,0.10423396,0.06780136,-0.045027588,-0.050017007,0.036815595,-0.03571578,-0.049124945,0.020396603,0.058056958,0.045279562,-0.044899955,-0.02720175,0.03167211,0.032277443,-0.1103096,0.01892933,0.019234562,-0.031938426,0.007970319,0.016231967,0.017654132,0.037003804,0.054963775,0.03747029,0.036209952,0.008675177,0.038383096,0.05161629,0.020227274,0.049849145,-0.05026536,-0.0014039712,0.030674776,0.040706627,-0.068925664,-0.07538393,-0.037300527,0.0087276185,-0.07703446,-0.06378753,-0.02882563,-0.042396054,0.027234532,0.026951188,-0.06370111,0.01626901,0.051483996,-0.09924095,-0.029532226,0.07148476,-0.022150788,0.013488936,0.009824624,0.03640759,-0.01315246,0.09016535,-0.015230299,-0.008348031,0.042871993,-0.006230869,-0.07218842,0.040416397,0.038202617,0.05802926,-0.03680936,-0.041946113,0.029399835,0.0022333455,-0.051361524,-0.05523271,-0.022617713,-0.0074192397,-0.0038322888,0.020452743,-0.08544373,-0.06598876,0.07860924,-0.026315484,0.03488532,0.03452262,-0.015819458,0.03947169,-7.804004e-33,-0.08405283,0.07588249,-0.06408749,-0.032537617,-0.026175149,-0.03447939,-0.04588765,-0.011667665,-0.03647001,-0.05380828,0.0072073136,-0.079383075,0.08337132,-0.02117254,-0.0378133,0.0062815,-0.026502905,0.0071893246,0.0011015256,0.05248737,0.03564821,0.059277736,-0.093258694,0.008238787,-0.04168891,0.07450287,-0.056387935,0.017396156,0.037293,0.029472277,-0.009597753,-0.1328473,-0.013781268,0.01489469,0.0023364539,0.0903915,0.00028757867,-0.0006132433,-0.021668727,0.0075133597,0.0344536,-0.023602184,-0.09415222,0.05646113,0.03418038,-0.037217375,0.011717446,0.019111576,0.054354057,0.025013734,0.02381213,-0.026927775,-0.033571083,-0.008567445,0.0028704412,0.004265668,-0.024095064,-0.0765675,-0.05021591,0.027216021,0.022202432,-0.024535296,0.037764367,0.0054822764,-0.009232332,-0.096761025,-0.015436969,0.0046091126,-0.008492371,-0.021119783,0.0085586645,0.08708916,0.00029284513,0.06278327,-0.022268366,-0.07338328,-0.07727818,0.020760385,-0.038881343,0.002758447,-0.028788038,-0.012850147,0.04144712,0.1282218,0.05564459,-0.021016417,0.13042055,0.07749032,0.05737074,-0.09220062,-0.0032746915,0.14151177,-0.0018309684,0.12258999,0.0040649767,-6.406723e-08,-0.013942989,-0.060444456,-0.01412996,0.038473997,-0.0071209283,-0.033181757,-0.031170534,0.06718757,-0.009621196,-0.01010032,-0.022812571,0.035413284,-0.08327852,-0.038925946,0.04386655,-0.018052692,0.0097197695,0.024559563,0.017282879,0.017049111,0.0021399395,-0.040646493,-0.02403943,-0.0536195,0.0010787644,-0.06906593,-0.04105478,0.067356974,0.047438875,-0.032138493,-0.021469217,0.009259321,-0.026663147,0.028181856,-0.00070443633,0.01424778,0.031987872,-0.009127387,0.021376507,-0.059302207,-0.016639827,0.03492081,-0.063695915,-0.009720571,0.059502993,0.023534797,0.01840307,-0.03325361,0.07035779,0.09597185,-0.06467775,0.06704993,-0.025346398,0.08051423,0.09617611,-0.03041638,-0.06799697,-0.05695627,0.08018488,-0.0020633359,-0.023793153,0.021163795,-0.06323363,-0.046974346,12,33.26103,-1.113698,14
598,"this lecture was mostly based on feature encoding. feature encoding is done when the dependent or independent variables in your data is categorical. you need to convert this categorical data type to numerical so that it can be processed by the computer. there are different ways by which this can be done. we can expect two types of output through this. one is that there are multiple classes, out of which there is only one class which the y represents at a time. this is known as â€˜multiclass problemâ€™. another is the one in which two or more labels can be present in a single y value. this is known as â€˜multilabel problemâ€™. for example of multiclass problems, we can consider a situation in which the y value is some color. so, the color can be any of the decided number of color options. we can use label encoding for this type of problem. it just assigns labels to each of the possible options (in this case, the colors). these labels donâ€™t have any inherent value, they just represent the different classes. 
for multilabel problems, we can consider the situation in which the y value is an image which consists of both dog and cat. so, here for single y we can associate it with two different labels, cat and the dog.
for such data sets where y values are nominal, we use label encoding. 
but, if the y values have ordinal level of measurement, then we can use integer encoding. it assigns integer values to each of the class and these integer values also have inherent value associated with them, by which we can compare between two distinct values. 
the other types of encoding methods include:
1)	one- hot encoding: in this, we assign values of 1s and 0s to all the available classes. we basically create vectors, whose components can be either 1 or 0, depending on the actual class. so, for one feature, we create n different columns. â€˜nâ€™ depends on the number of classes associated with that feature. this increases the dimensionality. so, it shouldnâ€™t be used when there are large number of classes associated with a feature.
2)	binary encoding: it is also known as â€˜pseudo one-hot encodingâ€™. instead of using separate column for each class and unnecessarily increasing the dimensionality, we can reduce the number of columns used by using a combination of 1s and 0s to represent the different classes, instead of using just one column with 1 for each class.
3)	frequency encoding: in this type of encoding, the value associated with each class is just equal to its frequency in the data. this is generally not used for y variable as it may happen that two different classes in y have the same frequency. so, in this case, we wonâ€™t get â€˜distinctâ€™ label for each class.
4)	target encoding: in this, the value for each class is just the average of the corresponding y values associated with all the occurrences of that class.
5)	feature binning: in this, the numerical values are categorized (divided into categories) to turn the problem into a classification problem.
next, we just discussed a bit on how we can process text data. all the above techniques were used when the features were already known to us. but in case of text data, we donâ€™t have the features, we need to create them.
also, we need to convert a word to some numerical value, so that the computer can deal with it. a problem which can be encountered here is that one word can have multiple meanings in english language. so, we should be very careful while determining the meaning of a statement. the context of the statement should be clear before making any kind of predictions.
",0.02528139,-0.020064095,-0.017516239,-0.023751544,-0.005580186,0.060544223,0.029396223,-0.019789798,-0.006511899,-0.07034019,-0.005647719,-0.065241024,0.08979064,0.03523226,0.0447235,0.06375291,-0.03364323,0.016991258,-0.10814138,-0.092482075,0.07827333,-0.06287321,-0.01283381,-0.00017770812,-0.06698344,0.020753033,-0.00927036,-0.04702379,0.026762992,-0.026493842,-0.014725252,0.07714988,-0.035659745,0.058312967,-0.054519117,0.022496494,-0.0528923,-0.020697242,-0.036418628,0.04492509,-0.048714675,0.0060166996,0.037613463,-0.013577394,0.02675785,0.057541806,-0.03228351,-0.038981445,-0.08834178,-0.054089796,-0.020827862,0.035700154,-0.084371775,0.027936842,-0.027975593,-0.048724536,0.033058934,-0.012202031,-0.06080689,0.0569892,-0.09230163,0.015125742,0.029441006,0.013117434,0.08212633,-0.030035174,0.016530642,0.009757355,-0.030314999,-0.012178049,-0.017519403,-0.009103302,0.011039887,-0.015055979,0.05429963,0.018092657,0.09544535,0.0424512,0.075714186,-0.020755757,-0.0015280803,0.017798673,-0.008192571,0.07867613,0.17698218,-0.0036455104,-0.084873244,0.050740432,-0.080839224,0.07019702,-0.033452015,-0.06234609,0.0426649,0.03254937,0.058324356,-0.0055605886,0.045302827,-0.008189426,0.073728725,-0.041081954,-0.024170812,-0.050627712,0.062385254,-0.042513765,0.020159552,-0.07052407,0.06954152,-0.0012756374,0.06737502,-0.105678424,-0.020107562,-0.06816542,-0.08737596,-0.012038533,-0.057593003,0.027041875,-0.012868176,-0.025278453,0.03120425,-0.042882003,-0.053613137,-0.06503845,-0.01705053,0.027233236,-0.018433442,-0.022539679,-0.01926489,3.904774e-33,-0.032107934,-0.05411021,-0.071468666,0.0033731365,0.03718597,-0.059097476,-0.072519794,0.0031715136,-0.026737804,0.049646646,-0.0029616726,0.034928545,-0.014734011,0.11432879,0.055758517,-0.023709217,-0.023116436,0.012125034,-0.0070977164,0.02039383,0.030940337,0.12563707,0.10179265,-0.007922621,0.017430635,0.048832003,0.004941409,-0.078349255,-0.032624863,-0.010549191,-0.055755682,-0.012471514,0.053979177,0.0271214,-0.028196484,-0.06555674,0.07268315,0.010568713,-0.011575034,0.06011457,0.022404844,0.04257572,0.051728223,0.034782004,0.059913825,0.046652745,0.018680789,0.025666213,-0.06888012,0.044294354,-0.06541769,-0.008969233,-0.007179691,-0.05609288,-0.016837507,0.025841849,0.03940344,-0.05788276,-0.022300689,0.042216573,-0.07052905,0.065991096,0.10795249,-0.02990256,0.0337168,-0.03416071,0.028529696,-0.06907253,0.088415176,0.014186585,-0.042121086,0.009448731,-0.04801815,-0.13776211,0.03340048,0.002455819,0.02629733,-0.052079696,-0.03226186,0.059096396,-0.059578504,0.043928012,0.0036295403,-0.029832097,-0.05933847,0.052962873,0.019710729,-0.04310328,0.0096559245,0.0176591,-0.0016793394,0.069670364,-0.030672513,-0.06476609,0.10707434,-7.0994466e-33,-0.08600943,0.061037116,-0.08992862,-0.008909204,-0.021962706,-0.01020058,0.0031026662,-0.0049077086,0.016413951,-0.027528355,-0.093640074,0.01612208,-0.01883158,0.0070325504,-0.09230711,-0.011924056,-0.14938742,0.046832576,-0.008182672,0.030730657,-0.034607083,0.09404314,-0.03007593,0.023797393,-0.027003443,0.09230305,-0.03429208,-0.03102308,0.08440918,-0.00076010777,-0.014081593,-0.0902767,-0.0038860973,0.025337378,0.012119973,0.005463836,0.03565352,-0.05116724,-0.049525544,0.05440362,0.05651308,-0.034062963,-0.09055571,0.018893339,0.02316165,0.0003768777,-0.044810735,-0.030952267,0.047289506,0.03437008,0.059221443,-0.052618526,-0.062075607,-0.01991607,0.017631296,-0.033344507,-0.070118405,-0.046218134,-0.02664026,0.08198527,-0.019129813,-0.029961456,0.07204817,0.020167688,-0.05475879,-0.039791413,-0.019648585,-0.079118535,-0.0079641435,-0.04799475,-0.0012843215,0.051693592,-0.039571192,-0.01306401,0.016942399,-0.012785295,-0.08194741,0.006473584,-0.027078277,0.018932413,0.012969495,-0.02276275,0.037329968,0.14240286,-0.009809191,-0.01275277,0.11230864,0.11742345,0.063936196,-0.10588421,0.06578053,0.104905434,-0.05987587,0.09956467,-0.013983064,-6.8281004e-08,-0.033443097,-0.075821444,-0.012860222,-0.025861224,-0.012489605,-0.003852375,-0.058186796,0.0059327516,0.0053304713,0.10576144,0.0190527,0.03294869,-0.073065765,-0.032437574,-0.00044482757,0.029914314,0.0134483315,-0.0014577272,0.04887305,0.050407022,0.015889961,-0.05726107,-0.058619477,-0.03563236,-0.059011802,-0.06501536,0.0018836985,0.08983534,0.025186146,0.02124627,-0.03200353,0.022202503,-0.031000782,-0.0056394413,0.058522556,-0.037942447,0.01750652,-0.05700398,0.015693413,-0.038041603,-0.04227198,0.0011702195,-0.1051723,-0.0041043186,0.09578976,0.023945928,-0.031788155,-0.020873746,-0.007552238,0.12650712,-0.08468911,0.013922553,-0.046047263,0.05490935,0.0077605904,-0.068509996,-0.025716634,-0.06659274,0.07080495,0.030196453,0.023980068,0.075293645,-0.014645306,-0.05863726,12,35.11607,-4.4120884,14
