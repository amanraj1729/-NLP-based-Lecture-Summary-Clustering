kmeans_cluster,Session_Summary,rank_in_cluster
0,"we started by talking about population v/s sample. we consider a sample and determine the various attributes associated with it, like- mean mode, median and based on the values of these attributes, we predict those for our population. so, it is important to consider a sample that is good and represents the population well. the attributes estimated for population are known as parameters, while those calculated using the sample data are called as estimates/ statistics. we talked about the different kinds of attributes that can be associated with the population data. some of these include- mean, median, mode, standard deviation. our goal is to estimate these parameters of the population using the calculated statistics. all levels of measurement can be assigned different attributes of the data, depending on the type of data they account for. for example, nominal level can have the attribute of mode or count(frequency) but assigning the attribute of mean would be completely senseless, as nominal data is qualitative.
then we started discussing simple linear regression. in this model, we fit the data using a line (a simple linear equation with just one independent variable).
we can always use a point to represent any data, but this choice is very naive. so we instead use a simple equation like: y=ax+b where a and b are the estimates of population parameters and x is the independent variable. the y intercept, b here represents the sum total of the effects of all â€˜unknownâ€™ independent variables, which are not considered in the equation. so, as we include more and more independent variables, the value of b would start decreasing. also, depending upon the chosen sample, we can have various values of a and b. so, every sample will have a unique value for a and b. these values- a and b are known as the point estimates of the actual parameters. we can say with 100% confidence that the values of these point estimates do not coincide with the actual parameter values. so, in order to establish a certain level of certainty about the parameter values, we convert the point estimates to interval estimates, thereby giving a confidence interval, within which the population parameters would lie, with a given level of certainty. as the length of confidence interval increases, we become more and more confident that the estimated values resemble the parameter values. now, since different samples would give us different values of the estimates, we need to determine the values, which minimize the error. error for an observation is defined as the difference between the actual value and that obtained by estimating the data set with a line. so, to find the values of a and b, we need to minimize the sum of squares of individual errors. we do this by partially differentiating the expression with respect to a and b separately and by solving the two equations for two unknowns, we get the values of a and b in terms of the means and the mean of squares of the data set values. after getting the values of a and b, we put them in the equation to obtain the best model for the given data set. we consider the squares while minimizing errors instead of simply minimizing their sum as- it magnifies the errors, also it prevents cancellation of the errors, which can result when one point is above the line and other is below. since, we are getting the exact values of a and b, these are known as â€˜closed-form solutionsâ€™.
we do observe that the means of x and y (all observed values) satisfy the equation of the regression line. so, the predicted/ approximated line for the data values, pass through their mean.

",1
0,"today's class start with discussion of population and sample. we want sample to estimate/predict population. for better results sample should be good and representative.
we can calculate count (frequency), mode, mean, median, standard deviation, variance.
we can also do addition, subtraction, multiplication and division. if we calculate attributes from sample then it is known as statistics and when we calculate attributes from population then it is known as parameters. the second important thing what we have learnt today is simple linear regression. we took a example of sales v/s advertisements data and created a scatter plot and draws the best fit line having equation y=b0 + b1x.  here y represents dependent variable/response variable/label and x represents independent variable/feature and the b0 represents the bias which arises due to unknown variables which influencing the model. it has only one predictor. one amazing information is that we can also represents this line as a point but it is not correct model to represent the whole data. even a point can be considered as a model although a very naive model. we do not know all variables. in the equation y=b0 + b1x, b0 and b1 are the estimates of the population parameter. also, we have different model based on different sample. the difference between predicated y^ and y is error and we take sum of all squared error to minimize the error. we can't take only sum of errors as it nullify each other and there is of no use. a confidence interval (ci) is an interval which is expected to typically contain the parameter being estimated. important reasons behind taking sum of squared error is that it doesn't magnify the error and also doesn't differentiate between directions. we have 'closed form' solution to calculate the value of a, b and b0 and b1 are known as point estimates. one last thing is that we need to arrive at the possible interval with which these values lies such that there is a very high chances that b0p and b1p will lies within those intervals respectively.",2
0,"in today's lecture, we started the discussion with population vs sample that the sample must be good and representative of the entire population. we use the sample to predict/estimate various attributes of the population.

attributes of population:
1. count (frequency)
2. mode
3. mean
4. median
5. standard deviation
6. variance

operations:
1. count
2. add
3. subtract
4. multiply
5. divide

if these attributes are calculated from the population they are known as parameters and if they are calculated from a sample they are known as statistic.
we want to estimate the parameters based on the statistics.

slr -> simple linear regression:
(it has only one predictor) 

we find the best fit line for the given data in the slr model.
y = b0 + b1x
where, y -> dependent variable, response variable, label
             x -> independent variable, feature, predictor

even a point can be considered as a model - although a very naive model.

here, b0 is called as bias as it accounts for all the other features which we didn't account for in the model.
b0 and b1 are the estimates of the population parameters (i.e. statistics).
since these are calculated from the sample their confidence is very low. so, we need to find a confidence interval containing these estimates in which we can confidently say that the real population parameters would lie in.
on increasing the interval width, the confidence also increases.

we have y(hat) = ax+b, for each data value we define error ei = yi - yi(hat).
now to get the best fit line we need to minimize the errors.
for that taking the sum of all the individual errors won't work as the positive and negative errors might get cancelled leading to zero net error even if the individual error values are large.
to address this issue we can minimize the sum of either |ei| or (ei)^2. 
we prefer the sum of (ei)^2 as:
1. it magnifies the error for a better fit.
2. it doesn't differentiate between different directions.

summation(ei)^2 = summation(yi - yi(hat))^2 
                               = summation (yi - ax -b)^2
here, we can minimize the error by making the derivative of error zero with respect to both a and b to get their values.
we have 'closed form' solution to calculate the values of a and b.
we can also observe from the equations that the point with average values of features as x-coordinate and average values of labels as y-coordinate lies on the best fit line.

b0 and b1 are 'point estimates' so we need to arrive at the possible interval within which these values lie such that there is a very high chance that b0p and b1p (i.e. the population parameters) will lie within these intervals respectively. 
",3
0,"analyzing the entire population to understand its behavior/characteristics is not feasible both â€“ time wise and money wise. so, we make use of (representative) samples.

attributes
attributes associated with samples as well as population are:
1.	count (frequency)
2.	mode
3.	mean
4.	standard deviation
5.	variance, and many others
operations
operations associated with samples as well as population are:
1.	count
2.	add
3.	subtract
4.	multiply
5.	divide and many others

using a sample, we can estimate the mean of the population.
attributes associated with the population are known as parameters while those associated with the sample as statistics.
we want to estimate the parameters based on the statistics.

simple linear regression has only one predictor.

y is the response variable/dependent variable/label
x is the predictor variable/independent variable/feature

on a scatter plot in which it looked like the points were scattered on the surface of a circular disc. then he pointed out a few things, which are:
1.	simple linear regression is not required in such cases.
2.	it would not be the best possible method in such cases.
3.	but if someone wants to apply it, then it can be applied.
4.	in such a case a point is a better approximation than a line, this can be understood by taking any value of the feature (x) and then by looking at the difference in its actual value at that x and the predicted value.
5.	each point can be considered as a model â€“ although a very naã¯ve one.

bias - value of the y â€“ intercept in the equation obtained by simple linear regression.

as the size of the sample which we are using to estimate the population parameters increases the estimates become better.

estimation interval or confidence interval, finding î²0 and î²1 was just the start of machine learning. we need to take into account the error that would be there in our estimation. we have no basis to say that î²0 = î²0p.

confidence interval which is that with a% probability we can say that the parameter value will lie in an interval, that interval is known as confidence interval. typically, confidence intervals are of 90% or 95% and sometimes even 99%. 100% confidence interval is the interval from -infinity to +infinity.

options : 
1.	minimize sum of all ei
2.	minimize the sum of squares of all ei.
3.	minimize the sum of perpendicular distance between all pairs of {(xi,,yi) and (xi,yi-bar)}.
4.	minimize the sum of absolute value of all ei.
here ei = yi â€“ (yi-bar)    
(1) is rejected (4) is rejected because we donâ€™t want our solutions to be biased towards any direction so we choose (2).
a professor sir/maâ€™am has done (3).

",4
0,"today's lecture started off with a discussion about the difference between population and sample. sample is a small subset representing the entire population because gathering data for the entire population can be very difficult. so the sample should be a good representative of the population, and we should be able to predict the population's behaviour from the sample. there are various statistical attributes of data which can be calculated. some of them are frequency, mean, median, mode, std. deviation, variance, etc. if these attributes are calculated for the sample, they are called as statistics, whereas if they are estimated for the population, they are called the parameters. so our main focus with data science and ml is to estimate the parameters from the statistics. 
then we moved on to simple linear regression, where we have only one influencing or independent variable steering our predictions. our model is basically of the form y = ax + b, where x is the independent variable (predictor, feature), y is the dependent variable (response variable, label) and b is the bias term. the bias term accounts for all the unknown variables influencing our predictions. so as and when we bring more and more influencing variables into our model, the bias term keeps on reducing. 
one special thing that sir mentioned today was that a model can be as simple as a point. even a point predictor can be a model, however it may not be relevant for any practical usage and may not represent our data well. 
so a and b are the statistics of the population parameters. we also defined a confidence interval, which is the interval of values within which the parameters lie. the larger the confidence interval, the more confident we can be of the parameter value lying in that interval. then we also talked about minimising the error between our predictions and the actual data value at a given point. we said that the error can be formulated in various ways, but the most optimum way is to consider the sum of the squared errors, as that is something which is least influenced by the sign and the direction of fluctuation, and hence should be the best choice for minimisation. upon doing minimisation, we calculated the values of the statistics a and b. however these were closed form solution which were basically point estimates of the parameters. these estimates might not give us any confidence about the actual parameter values, hence we also need to find a possible interval within which the value of the parameters can exist with a very high probability.",5
1,"in today's class, we began with summarizing the learnings from the previous class. for y = b0 + b1x; we saw that b1 is more important than b0 because with b0 being zero, we at least have b1 which gives a slope for the regression. we discussed what statistically same and statistically significant means. we studied what happens when zero is the interval depending on which we can say if two values a and b are distinct or obtained by chance. if zero lies in the interval, then the values obtained are just by chance. as the models are similar to one another in the interval; the values obtained for a and b could have been zero. later, sir showed the class his interesting plot from class data. he used nlp and heatmaps to find the similarity in student's submissions. later, the class proceeded with multiple linear regression. in multiple linear regression, we have more than one independent variable. to extract information from text, we need to create feature out of it. this technique that deals with features is feature engineering. we need to create additional features from the data, because sometimes we may be given data about 'x', but we need information of its derivative. so, we create a feature of its derivative. 
we then studied the mlr gradient descent method and how it correlates the beta values, y-values, x-values and errors. from the data analysis toolpack, we studied the impact of different independent variables, on the f-value, p-value and r-sqaure terms. we notice that some independent variables do not contribute significantly to the regression process and hence we can omit them.",1
1,"we started the lecture by having a quick recap of the concepts from the previous class. we discussed what we exactly mean by saying â€˜a given number is statistically significant/ a number is not statistically different from 0â€™. we then started with a new topic- multiple linear regression. before starting, we discussed that before performing mlr, we need to convert the file/ data available to us in a vector form - [x1,x2,x3â€¦]. this vector contains various features of the data. this process is called as â€˜embedding vectorâ€™. sometimes, we also need to derive some new features based on the given ones, which matter more/ are more relevant in the context. so, this process of transforming the already existing features or performing operations on the existing features, so as to get new features, is known as â€˜feature engineeringâ€™.
we saw that we donâ€™t get a closed form solution for the coefficient values in mlr, like that in slr. however, the procedure needed to perform to obtain their values remains the same. like slr, in mlr also, we try to minimize the sum of squares of errors. so, if we have â€˜kâ€™ such independent variables/ features, we get â€˜kâ€™ such equations, on which we perform numerical methods to get the solutions. we also learnt about a statistic called the â€˜f-statisticâ€™. it is the ratio of average variance explained by our regression model to the variance explained by errors. so, we want most of our variations to be explained by the regression model. hence, we want msr to be greater than mse, which means f-statistic should be as large as possible. we learnt that the error metrics that we use to assess the validity of a model, are better interpreted when used to compare different models, rather than using it within the same model. also, since rmse and mae are in the same dimensions as the data, they are easier to interpret or relate as compared to other metrics, like sse, mse. 
in any ml model we first start by assuming that the errors in the predicted and actual data values are random. also, for any ml model, if more independent variables are available then the value of r2 increases, since more variables are available to explain the variability in the data. at last, we saw how we can use the p values for each independent variable to assess whether it has any effect on the data. if we get the p-value >0.025 (for 95% interval) then we can say that this particular coefficient is not statistically different from â€˜0â€™. hence, we can ignore it and reduce the number of independent variables in the regression model. we can continue this until we get only those variables whose p -values are <0.025. this implies that only these coefficients are significant and rest can be neglected. this gives us the true/ actual model. 

",2
1,"in our last session, we had continued to follow up on our statistical concepts based on the background we had developed earlier. we explored terms from our data analysis toolpack, discussed their meanings, interconnections, how to represent them graphically, and the potential sources of uncertainty or errors in applying them.
the significant portion of the class was assigned to understanding cases involving beta and beta 0 under specific conditions, with emphasis on how they feature in different statistical models. we also went ahead and introduced discussion on the concept of the p-value, bringing into light its role on critical decisions especially regarding feature selection.
at the end of the class, we started to touch on anova, or analysis of variance, and focused on the f-statistic. we discussed why it was important for this value to be large, also relating to how it relates to evaluating model performance.",3
1,"first of all we recap on the previous part like confidence interval and saw that if we have a sample which has an element called b from lower probability region then that sample is of importance, but in the case of broader 95% region 2 values are not really distinct values and if come distinct then it is by chance. then we saw that how summary is checked if it's similar to other session summary or not. we may get the data in future. we saw around 9 lines which shows that about 9 are pretty similar summaries. we convert text into features of x (vector). we talked about feature engineering and why rate is important and significant. we also saw about f which is equal to msr/mse and it shows behaviour of model itself and in comparision to other. we also dropped different features for our model for multiple linear regression and learnt about how to remove feature which are not important depending on p values. f will be large everytime and r2 remains around constant. multiple linear regression became clear in the class and also feature selection. we talked about a little about topic name like embedding for text to vector conversion also , will discuss in detail later. we saw there are n dimensional dataset in multiple linear regression. also to reduce error we saw we can use matrix to solve and get our matrices. we also saw error matrices like sse, mse , rmse and mae.",4
1,"in class, we picked up from where we left off, focusing on some key statistical concepts. we talked about terms from the data analysis toolpack, what they mean, how theyâ€™re connected, and how to interpret them visually, including the errors and uncertainties that can come with them.
we went over cases involving beta and beta 0 in specific scenarios, and we talked about the p-value and how it helps with feature selection. we also explored multiple linear regression and discussed how certain table values are specifically tied to it. toward the end, we introduced anova, talked about the f-statistic, and why itâ€™s important for it to be largeâ€”weâ€™ll dive deeper into that next class.",5
2,"in this session, we covered methods of analyzing and condensing high-dimensional data without sacrificing interpretability. we started by talking about heatmaps, which, while perhaps not showing all the minute variations, are well worth it to compare several parameters at once. heatmaps facilitate rapid pairwise comparisons that would be difficult to view individually, thus serving as an effective tool when performing exploratory data analysis (eda).

the variance inflation factor (vif) was subsequently developed as a multicollinearity diagnostic. vif measures the extent to which a particular parameter is accounted for by the other parameters in the data. for any feature, a large vif means that the feature is strongly redundant with others. if a feature's vif is greater than some chosen threshold, it should be eliminated to prevent multicollinearityâ€”a problem that is frequently more important than the requirement for dimensionality reduction through pca.

principal component analysis (pca) was explained as a technique that employs singular value decomposition to project data onto new, mutually orthogonal components. the principal components cover the original dataset's complete dimensionality, yet you can opt to keep only the most important ones in order to bring down the dimensionality. the kept components are weighted sums of the original features, whose weights are referred to as loadings. although pca is ideal for prediction, visualization in the context of eda, and data compression capturing the largest amount of variance, it is less ideal for ""what if"" or sensitivity analysis because the transformed components do not map directly onto the original parameters. in addition, pca is sensitive to scaling of the data, so normalization is necessary.

by contrast, t-sne employs a t-distribution and stochastic gradient descent to create a low-dimensional lossy representation of high-dimensional data. while giving up on precise distances, t-sne performs very well on preserving the relative proximity of the data points, and thus is very effective for visualizing clusters and intricate patterns.",1
2,"in todayâ€™s lecture we saw some more methods to reduce the dimensionality of data, inorder to solve data problems associated with large dimensions. 
in the last class, we had talked about vif analysis. we continued the discussion on it. 
in vif analysis, we start eliminating the features one by one, with the ones having large vif values. we continue the process until we get a set of features which have vif values, smaller than the desired threshold, which can be 10/5. we eliminate the features based on our domain knowledge.
depending on the threshold set, we can have different combination and numbers of features selected for the regression model. in such a case, we should evaluate the metrics for each of the model and then by comparing these, we can decide which one to use.
the next method we saw was that of pca (principal component analysis). it helps to reduce the dimensionality of the problem by creating orthogonal principal components. each of the principal component is a combination of the original features, with the weights called as â€˜loadingsâ€™. the loadings tell us the importance of each feature. so, instead of using too many features, we are combining them into a principal component and using this to predict the response variable. the number of principal components is equal to that of the original dimensions. however, we can decide to take up two/three of these pcs suitable to our model. the first pc explains the maximum variance in the data, the next explains some amount of the remaining variance. this continues until all the principal components explain the complete variance in the data.
the cumulative variance explained by all the pcs is given by the elbow diagram, which is asymptotic to 1. the elbow diagram tells us how many pcs would be needed to explain a certain amount of variance in the data.
among the two methods, vif is generally preferred first because of interpretability. x1,x2,x3 etc.  represent the actual physical features, whereas the pcs, which are a combination of these are just mathematical parameters. 
so, pcs are most suitable for prediction analysis but they cannot be used for â€˜what-if analysisâ€™ or â€˜delta analysisâ€™. hence, through pcs we achieve the goal of reducing the dimensions, however we cannot do any sensitivity analysis.
so, normally vif is done first and if still there are many features left, we perform pca.
the advantages of using pca is that it helps in dimension reduction (data reduction), predictive analysis, visualization (understanding of the structure of the data). so, this is a part of eda and can be used to determine the number of clusters (k) in k-means clustering.
another disadvantage of pca is that it is sensitive to data scale. hence, normalization (if required) must be done before doing pca.
next, we talked about the t-sne plots. 
the t-sne plots also reduce the large number of dimensions in the data into 2-3 dimensions which can be easily visualized. this is done on the basis of the distance between the points in the n-dimensional space. through this method, we lose the exactness of the data, but instead we get the information about the relative closeness of the points. 
t-sne creates a probability distribution for each of the point, which contains the probability of closeness of that point from every other point. 
gaussian normal distribution is used in higher dimensional space, but in t-sne, as the name suggests, t-distribution is used. t-distribution increases the distance between dissimilar points. so, in n-dimensional space, if the probability of two points being close is very high, they are clustered together in the t-sne plots.
all these methods help in reducing the number of dimensions in the data set.
",2
2,"today's session discussed variance inflation factor (vif) and principal component analysis (pca) in-depth.

we first discussed vif, a statistical factor to check for multicollinearity among independent variables in regression models. we understood that high vif suggests high correlation between independent variables, which tends to bias the coefficient estimates and decrease model interpretability. the session discussed vif, its interpretation, and how to deal with multicollinearity. the method consists of removing features individually, beginning from the largest vif, so that all remaining features have a vif value less than an arbitrarily selected value. this both reduces the curse of dimensionality and enhances the performance of the model. we also saw that vif scores would tend to form an elbow-like pattern upon plotting, from which a threshold can be conveniently determined.

then, we discussed pca, which is an unsupervised method of reducing dimension with retaining the maximum variance in the data. pca is dependent on singular value decomposition (svd) and assists in transforming data to a new set of uncorrelated principal components. principal components are orthogonal to one another, thus ensuring that multicollinearity is removed. we also observed how pca has the capability of dimension reduction from higher dimensions to lower dimensions, as in the example where data was reduced from two dimensions to one.

a major difference between vif and pca was explored. vif is usually performed first to provide better interpretability because it maintains the original features, which would make prediction simpler to interpret. pca, however, converts features into principal components, so it is better used for ""what-if"" analysis or delta analysis than for explicit prediction.

the session also touched on various applications of pca, such as dimensionality reduction, predictive modeling, and visualization (particularly in exploratory data analysis - eda). but we observed that pca is scale-dependent, and thus normalization becomes an essential preprocessing step.

near the conclusion, t-sne (t-distributed stochastic neighbor embedding) was introduced as yet another dimension reduction method. it was briefly mentioned how a gaussian and t-distribution comparison on the same dataset highlighted differences between them in picking up data variance.

the session, in summary, illustrated how vif and pca assist with multicollinearity and dimension reduction, ensuring machine learning models are interpretable and effective.",3
2,"in this session, we explored the variance inflation factor (vif) and principal component analysis (pca), focusing on their applications, differences, and use cases.

variance inflation factor (vif):

used to detect multicollinearity among features.
computed as 
vif=1/1-r^2, where r^2 is obtained by regressing a feature against all others.
a vif threshold of 10 (corresponding to r^2 = 0.9) is commonly used for feature elimination.
preserves real-world features, making it suitable for sensitivity analysis and interpretability in models.

principal component analysis (pca):

transforms features into new uncorrelated components (pcs) ranked by variance.
pc1 > pc2 > pc3 in terms of variance captured, with a decreasing trend in the explained variance graph.
common applications include dimensionality reduction, data visualization (e.g., pc1 vs. pc2 plots), and prediction modeling.
requires normalization to ensure fair variance contribution from all features.
unlike vif, pca does not retain original features, making sensitivity analysis difficult.

comparison:

vif works in feature space, removing redundant variables while keeping real-world interpretability.
pca creates mathematical features, making it more suitable for dimensionality reduction rather than feature selection.
vif is often preferred over pca when feature interpretability is essential.

additionally, the emnist dataset was mentioned as an example for handwritten digit classification using pca.

t-sne:  t-sne is a non-linear dimensionality reduction technique primarily used for visualizing high-dimensional data in a lower-dimensional space (typically 2d or 3d). 

how t-sne works:
measures similarity in high dimensions:
computes the probability that two points are neighbors in the original high-dimensional space using a gaussian distribution.
maps data to lower dimensions:
assigns similar points in a lower-dimensional space using a studentâ€™s t-distribution, ensuring that points that were close in high-dimensional space remain close.
optimizes for local structure:
unlike pca (which captures global variance), t-sne preserves local neighborhoods, making it great for cluster visualization.",4
2,"we started the lecture where we left off last time, discussing about the vif values and the iterative procedure of finding the most independent features. we understood that the threshold value chosen should have some justification. we then moved on to pca i.e. principal component analysis. we discussed that pca helps us to reduce the dimensionality of our problem, by somewhat changing the axes of our data such that each axis helps to describe the maximum variance in the data. thus pc1 axes captures maximum variance, pc2 captures the next maximum variance and so on. a simple example could be a two dimensional linear data, which could be reduced to one dimension by taking the axis along the line. we need to check how many principal components we need in order to explain majority of the variance of the data. each principal component is a linear combination of each of the original features. the coefficient of each feature in the linear combination, is known as loadings. now in between pca and vif, the vif procedure should be done first as it eliminates multicolinearity which is a bigger issue. also, pca hinders with our interpretability as the features are not what they actually meant, but a combination of multiple features. however, pca does help us in dimension reduction, for making good predictions and also in visualisation and understanding the structure of the data. when used for visualisation, pca becomes a part of eda. pca requires normalisation of the data. if pc1 and pc2 cover 70 to 80 percent of the variance of the data, then the multidimensional data can be visualised in two dimensions by plotting the data on a pc1 vs pc2 plot. now for better visualisation, we have another tool called tsne. tsne is a lossy transformation which helps us to visualise our data in two dimensions. it is based on the t-distribution and also uses gradient descent, and is hence slow and not consistent. tsne calculates a probability distribution of all the other points being close to one certain point. it does this for each and every point. this data is taken into a matrix and then the dimension is reduced. tsne means t-distributed stochastic neighbour encoding. t-distribution helps us to magnify the difference between the points if there exists any, and thus helps in classification and clustering. ",5
3,"in this lecture we started by looking at how results were used to be predicted earlier, without any ml models. people used to collect data, through experiments and observations and then by using those x and corresponding y values they used to manually fit the data into some curve/relation which could then be used to predict future values.
but with the advent of ml algorithms, the need to manually determine relations has vanished and we can use various available ml models to fit relations among collected data and the model can predict any future values fed to it. some of these algorithms include- linear and multiple regression, random forests, etc.
next, we studied about the 4 levels/scales of measurement- nominal, ordinal, interval and ratio.
the nominal level categorizes the labels qualitatively into groups, which donâ€™t have any specific order. we can use frequency distributions to analyze this class of labels. also, since in nominal level, we do not consider ordering, it is incorrect to represent the values in terms of single numerical values. instead, we should use a vector which will have only one value as â€˜1â€™ and others â€˜0â€™ depending on which category the label y belongs to.
next, we talked about ordinal level which is associated with a specific sequence. the example we considered that of grades. grades have a specific order of importance but the interval between these is not fixed/defined.
the interval level consists of values that have equal intervals but the absolute zero for the measurements is not defined. in this level also, order matters. we discussed the example of temperature in celsius or fahrenheit scales. in these scales the 0 value is not absolute. so, we can just compare the difference between two values of temperature and comment on which one is hotter, but we cannot comment on the ratio of temperatures.
ratio class has an absolute zero defined and at this value it means that there is complete absence of that measurable. we considered example of height and weight here. so, heights of two persons can be compared and we can comment about the ratio of their heights. (eg. we can say a person with height of 5 ft is twice as tall as the one with height of 2.5ft.)
nominal and ordinal levels consist of discrete data, whereas interval and ratio levels contain continuous data.
next, we talked about supervised and unsupervised learning models:
supervised leaning models are the ones in which we feed the complete data, consisting of both the features and labels, based on which the ml model determines the future values. it uses classification or regression techniques for the same.
in unsupervised learning, only the x values are given based on the various x values (features) the machine learns by itself and makes clusters of values with similar features and assigns labels to each, after which it can predict futures values, based on the determined relations.
we use k-means clustering and hierarchical clustering algorithms for the same.
so, the ml models try to determine the function â€˜fâ€™ which relates y and x as y=f(x), based on available data.
we can always collect a sample of the entire population of data to develop these models. 

",1
3,"so today's discussion start with two types of machine learning model which is supervised and unsupervised. for example supervised ml includes - simple linear regression, multiple linear regression, random forest and unsupervised ml includes k-means clustering and hierarchal clustering. the most fascinating things is that whatever type of ml we will use always get a generic equation - y(x)=b0 + b1x + b2x2 .....
further ahead we have learnt about 4 levels of measurement -
1) nominal - it have discrete values and we can only categorize them. also there is no ordering between them. example - gender, color
2) ordinal - it also have discrete values and work same as nominal level of measurement. example - grades
3) interval - it have continuous value. the concept of 0 is arbitrary in this case. example  temperature
4) ratio - it also have continuous value. in this measurement 0 has a meaning. example - height, weight, salary
 in case of nominal and ordinal we use one-hot encoding to change words into numbers by making vector.
we learned that in y =f(x) where y is the label and x is called features.
when we have both labels and features then we use supervised learning and when we have only features then we use unsupervised learning method. nominal and ordinal are use for classification purpose whereas interval and ratio are use for regression.
second thing what we learnt today about data. in ml we use sample instead of population.
sample is a small chunk of population.
that's what we have learnt today.",2
3,"the lecture covered some basic ideas on machine learning and statistics. machine learning is essentially finding a relationship between features-inputs and labels-outputs. to achieve this, methods like linear regression, logistic regression, random forests, k-means clustering, and hierarchical clustering are used. these methods help solve problems like classification-grouping things-and regression-predicting values.
the lecture also introduced levels of measurement, which describe how data is categorized:
nominal data is applied to name things without any kind of order, such as color or gender.
ordinal data is ordered, but the difference between them is not consistent, like grades: a, b, c. encoding the ordinal data using numbers can lead to problems since it may suggest a difference that does not exist. using vectors will help to avoid this.
interval data is used for measurement, like temperature, where zero does not represent nothing.
ratio data includes things like height or weight, where zero means nothing, and you can compare values meaningfully (e.g., 10 kg is twice as heavy as 5 kg).
in machine learning, if the label is nominal, the problem is classification. if the label is interval or ratio, it is regression. machine learning can be supervised or unsupervised. in supervised learning, labels are already given, while in unsupervised learning, there are no labels. clustering methods are used in unsupervised learning to group data by similarity, and labels can be assigned later.
the last discussion about the difference between population and sample occurred. population refers to the entire dataset, but there can be a size that may be computationally expensive to analyze; thus, we often use a sample which is actually a smaller, manageable part of the data.",3
3,"in today's session, we saw that the equation y=f(x) shows that we can find the function by identifying the data pattern of y and x. then we move onto 4 levels of measurement which are as follows:
nominal: discrete data like gender or color, which we can just classify not measure.
ordinal: discrete data like grades, which have their inherent order, we can also classify them.
interval: continuous data like temperature, here the definition of 0 is arbitrary.
ratio: continuous data like height, in which you can state 10m is twice of 5m.
we saw that we should not associate numbers directly with nominal and ordinal data types, instead we can use vectors to represent them. in y=f(x), y is called the label and the x's are features. if we know both, then to predict f we use supervised learning methods like slr, mlr, etc. if we don't know the labels, we can predict f with unsupervised learning methods like hierarchical clustering, etc. in which we form clusters of x's, analyze each, and then convert them into labeled data.
if the label data is nominal or ordinal, we use classification and regression for interval and ratio data types. then we see that whatever the size of the data is, it is always a sample that we need to analyze to get y=f(x), and further that you should back-test your model before presenting.",4
3,"we learnt about different ml techniques such as simple linear regression, multiple linear regression, logistic regression, k-means clustering. we learned about the levels of measurement. they are of 4 types: 1)nominal : in this categorization without a specific order is done and it is discrete. for eg.: gender, colour. 2) ordinal : in this categorization is done in ordered way without equal intervals and it is discrete. for eg.: grades, rankings 3) interval : in this categorization, it is ordered with equal intervals but no true zero. it is continuous in nature. for eg.: temperature. 4) ratio: this categorization is similar to the interval, just here the the zero is true zero. for eg.: height, weight. next, we learned about the general type of equal in ml : y =f(x), where y is label and x are features. if both of them are known then it is known as supervised learning. if labels are unknown, it is called unsupervised learning and then using k-means clustering and hierarchical clustering, we create labels for it.",5
4,"the class started off with the realisation that till now, we were working with good tailored data without any issues. however in real life, we almost never get any such data without any problems. much of the real life data has a lot of problems which need to be understood in order to solve the problems in the data and get some meaningful results. for this, we perform eda or exploratory data analysis on our data. our data could come in various formats like text, some files or some databases with large amounts of data present, and we might be exposed to non-tabular and non-numeric data. so we started off with a process for data mining, known as crisp - dm i.e. cross industry standard process for data mining. it has 6 steps which run cyclically, which includes business understanding, data understanding, data preparation, modelling, evaluation and deployment of the model. business understanding involves defining business objectives, assess situations based on domain knowledge and understand the goals of the firm, while also understanding the constraints. also understand the success criteria and create a project plan as to how much time to allocate to different procedures. 
the major steps which we were focusing on were data understanding and data preparation. data understanding involves collection of data, describe and explore the data and also verify the quality of data which you have collected as to whether it has some useful insights to give or not. data preparation involves selection and cleaning of the data and construct the data into that which is good to work on. we further moved on to eda, which is a part of the data understanding step. 
eda involves performing some initial investigations on the data, and to gain some basic insights from it, and to spot anomalies, create and test hypothesis and check our assumptions. eda basically uses mathematical and visual statistical tools. we then went through a mind map pertaining to data problems. problems can be in the dependent or independent variable or both. we could have maybe no data about the dependent variable, or maybe insufficient or incorrect data, or maybe we could have too much data to handle. each of these problems have different solutions which involve sampling or generating data based on some theory. problems with independent variables involve having too many independent variables i.e. having multiple columns and having problems within the columns and also between columns. within columns we could have missing data, some distribution issues, heteroscadasticity i.e. varying variance across the data. between columns, we could have too few or too many features. 
eda is the first step in the data analysis pipeline, where we can get some insights like how features are distributed, what are the outliers, etc. a common eda technique involves plotting histograms and checking the distributions of each variable. then we plot box plots, which show us the amount of variability in the data, where the central line shows us the median value and the box around it shows the variations. anything outside the main boxes may be considered as outliers. we also plot some correlation heat maps to understand correlations between different variables. matrix plot is a 2-feature-each plot plotted in a matrix formation, which are used to find valuable insights, but are limited to two features at a time. some line plots can show some trends in the data, which can be useful to make certain predictions. 
we then moved on to handling missing values. there are multiple types of missing data. mcar - missing completely at random, mar - missing at random or mnar - missing not at random. mcar has completely random data points missing, mar has some relationship between the missing data points which could be due to resource, method of measurement, etc. mnar says that unobserved values are itself responsible for the data being missing. now when we have identified missing data, we could maybe ignore these missing points. but many algorithms donâ€™t know how to handle missing data, and information may be lost. we could also delete all the missing values, which could lead to data loss, but it is the easiest to do. another option could be to replace the missing values with some data statistic like the mean or the median. mode could be used for categorical data. sometimes instead of the data mean, we could use some of the values close to the missing data point in order to fill the place. we also have multi variate approaches, where we observe the other columns as well in order to fill our missing value. knn takes the mean of the nearest neighbour and fits the mean of the neighbouring values, while mice fits a linear predictor in order to fill in values based on a trend. time series data is special because we can use temporal judgement to fill the data. we can use interpolation methods to fill in points or use simple moving averages as well. 
we then talked about outliers which are data points that are signifcantly different from the rest of the observations. some algorithms are not very sensitive to outliers, while some are quite sensitive. we discussed about quartiles and the inter quartile range, which can be used to detect outliers. standard deviation can also be used for normally distributed data, where any data point beyond 3 standard deviations from the mean, can be classified as an outlier. for multi variate data, we have dbscan which is density based spatial clustering of applications with noise, where outliers are points which are not classified into any broad clusters of high density points. outliers also can be dealt with in many ways like removing them from the dataset, etc. true outliers are data points which are extreme values but they are not erreneous. for such outliers, we could make a separate set out of these and deal with the normal observations and the outliers separately.  for outliers, we use the median and not the mean as the mean is influenced by the outliers, while the median is not. also, for calculating the median, we sort the data first so that we get the correct metric. 
not all techniques are useful for all kinds of data. hence domain knowledge becomes important so as to know what techniques are best for us. ",1
4,"todayâ€™s session focus mainly on exploratory data analysis but before that we learnt that how to differentiate between actual and predicted values in confusion matrices. crisp-dm which stands for cross industry process for data mining in these 6 steps run cyclically 
1) business understanding in which we do many steps including assessing situation (assumption and constraints), risk.
2)data understanding in these we collect initial data, explore data and also verify data quality. 
3) data preparation in these we select data, clean data, construct data, integrate and format data. after this we can infer which is dependent and independent variables. last three steps involved modelling, evaluation and deployment.
exploratory data analysis is an approach used in statistics and data science to analyze and investigate data sets. it involves statistical graphs and data visualization methods to visually represent the data. next we see that what are the problems associated with dependent and independent variables. in case of dependent variables problems such as not availability(remedy is that use unsupervised learning and create labels), incorrect, insufficient data( it includes very few observations or data imbalances) or too much data. and in  independent variables there can be problems within the column or between the column itself. heteroskedastic is that different variance exists throughout the entire datasets range. further ahead we explore types of missing data in the formats - 
1) missing completely at random 
2) missing at random 
3) missing not at random. so what do we do - 1) let them be n/a 
2) delete all instances with missing values 
3) or replace n/a values with a data statistics like mean or mode. mode value helps in categorical data. and multivariate data imputation can also be done. further we explore outliers which are the data points that differ significantly from the rest of data points. most of the models does not know how to handles this, but there are some models which can tackle this problem. how to detect these outliers we can do these either by univariate or multivariate methods. we calculate quartiles which divide a dataset into four equal parts, providing insights into data distribution. 
 1. first quartile (q1) â€“ 25% (lower quartile) - it represents the 25th percentile.
 2. second quartile (q2) â€“ it represents 50th percentile and median of the entire dataset.
 3. third quartile (q3) â€“ 75% (upper quartile)
 it represents the 75th percentile, meaning 75% of the data falls below this value.
standard deviation methods is similar. in case of multivariate we have dbscan which stands for density-based spatial clustering of applications with noise. next we see that how to handles the outliers we can do these by data trimming and data capping. values that lies in the extremes arenâ€™t erroneous but they are called true outliers. example - stock prices can drastically shoot up or down, extreme weather events. one important note is that means are influenced by the outliers, median are not influenced by outliers. to get median data should be sorted first. combination of univariate columns many results in error.",2
4,"in this lecture we learnt about exploratory data analysis. this is the first step towards creating any machine learning model. eda involves cleaning, transforming, understanding and analyzing the raw data through basic and primary techniques. data can be analyzed to reveal patterns in it, if any. this can be done using either mathematical formulae or by making visual charts. also, many times we donâ€™t get a single data file, instead we have to merge a lot of files to get a compact, single data file on which we perform further analysis. 
there is crisp-dm: cross industry standard process for data mining, in which there are 6 steps to be followed for building a good ml model. 
the steps are:
1- business understanding- this involves acquiring domain knowledge, clearly defining our goal and objectives, and understanding the problems that are to be tackled by the model. 
2- data understanding- this step involves, collection, analysis, and assessment of the relevancy and quality of data.
3- data preparation- it involves cleaning and transforming data into suitable form for analysis. feature engineering is done to select the most relevant/ appropriate features.
4- modelling- after performing the data analysis and cleaning steps, we move towards fitting a basic model to our data. the model is trained based on the available data. if y-values are not available, we used unsupervised learning algorithms, to cluster the data and assign labels to it.
5- evaluation - assessment of the model performance using various metrics and implementing changes in it, if any.
6- deployment- implement the final model and monitor its performance to make improvements with time. also, effectively present the insights and results of our model.
the entire process is iterative, we need to repeat the steps again and again, as and when required.
we need to establish a certain acceptance criterion for our model. for many algorithms, we assume that the distribution of the features is normal, however this may not be the case for every x. heteroscedasticity is another problem, in which the variance of the residuals is not constant and keeps varying. 
in eda, we have to clean the data, account for the missing values and also detect the outliers and take necessary actions to handle these. 
the example we discussed in class was that of clustering patients into diabetic/ non-diabetic, by considering various features including, insulin levels, glucose levels, age, bmi, etc. these features, in the very basic step, can be analyzed using histograms, which may give us some (if not complete) idea about the distribution. we can also use correlation maps (heat maps) to find out the relation between these features. however, we cannot completely rely on these, and we should take into consideration numerous other factors as well. we looked upon matrix plot, box plots, quartile plots and some others to find the relationship in the data. the box plots, histograms can be used to detect outliers. 
it is important to know/ find out what we should do with the missing values and outliers. in some cases, it is reasonable to ignore the entire row/ observation and in some others, it is better to fill up the missing values. all this depends on the count of missing values. if there are a large number of such missing values, then completely deleting those observations, would leave us with merely 10-15 rows which is not at all sufficient to build up a model. 
for filling up the missing values we can use either mean, mode or median, depending on the level of measurement. for example, when we have nominal data, we use mode because the concept of mean is not defined for nominal data. 
the next step is to detect the outliers. there are several methods to do so. we can plot histograms, box plots and find out the points that lie far beyond the region where maximum of the points lie. dbscan- density based spatial clustering of applications with noise is yet another method used for detecting outliers. it considers a point and finds out other points around it which are in close proximity to it. likewise, it clusters all other points, thereby forming groups. the outliers remain ungrouped. 
median is not affected by the outliers, but the mean is. hence, median is preferred over mean for detecting outliers. however, in some cases, use of mean is not justifiable as it deviates largely from the values nearby the missing values. considering this as the approximate missing value would lead to larger errors. 
instead of using mean or median for filling the missing values, we can also use a regression model in that region and determine the missing values. so, even for filling the missing data and for data cleaning (eda), we use various models. this suggests that all the steps involved in building a model from the basic/ raw data are iterative and are performed in loops. 
by carrying out these repetitive operations we keep refining our model, so as to get the best possible model for our data set.
",3
4,"in today's class, we discussed exploratory data analysis. crisp-dm is a widely used framework for data analysis. it has six steps that run cyclically: business understanding, which means knowing the domain, understanding the data, preparing data that is either transforming it, removing outliers, or adding missing data. then, we make the model using the data. then, we evaluate the model by applying it to the test data and checking its effectiveness. afterward, we think about what to do with the model and how to deploy it. eda uses statistical graphics and data visualization methods to represent the data. then we discussed doing the task during a particular duration is important, and having deadlines helps people to work accordingly. then we saw a mind map stating the methods when we know different situations about the dependent variable. then we applied eda to pima indians diabetes data, in which the outcome is either the person is diabetic(1) or not(0), we saw the histogram of each factor considered to get the outcome in which we see some were nearly normal, then we see the boxplot of these factors stating the variability in the data and outlier instances present in them, afterward we see their correlation coefficient with the outcome and we see a high correlation with glucose and age as more aged and people with more glucose level are prone to diabetes. we also see the scatter plots and matrix plots which shows more relation between the features, clusters are seen more visibly, but it can show the correlation between two factors only at a time. then we see the problem of class imbalance in which one class occurs more frequently than the other. we also see the india temperature data analysis, which majorly states that the temperatures are increasing as years pass for a particular time in a year, and also in a year, it first increases and then decreases. then we see about handling missing data values which are classified as: mcar- completely random data points missing, mar- some relationship between the missing data point and values in different columns, and mnar- the unobserved values themselves are responsible for the data being missed. we analyze the data and decide whether we need to add them or not, then we see that we have univariable data if we have just one column as independent variables and multivariable data if we have the combinations of such columns. then we handle the outliers by the median method and the standard deviation method, also we need to consider outliers sometimes as they have taken place and have an impact. among the quantile for outliers which are median and mean, the median is good as it is not influenced by outliers but the mean is impacted due to the outliers. at last, we saw that we just don't need to apply every technique to the data, we should get the domain knowledge, understand and analyze the data, and then perform the process of eda through the required and significant techniques.",4
4,"so in today's session we learnt about eda and how can we gain insights about our data along with the techniques used to solve the problems that exist in our raw data. till now, we've been working on only one excel file containing all the data but in reality we have tons of files with data to work on when we're deailing with real-life problems. then we learnt about how crisp(cross industry standard process for data mining) dm outlines the steps when we're working with real-life projects which includes business and data understanding, preparation, modeling, evaluation and deployment. then we delved into how we actually do eda on our datasets. we use boxplots to check the variability in our data, feature correlation matrix can be used to understand the data, inter-feature correlation can be checked via matrix plots two at a time. class imbalance is also an important aspect of a data as it can severely affect the performance of our model. then, we jumped on how to deal with missing values in our data. trends can be useful in filling up missing values. some ml algorithms can on their own, identify the missing values and deal with it whereas simple models such as linear regression cannot. mar and mnar are two types of missing values in which mnar is tough to deal with. we can either drop the row but doing this for several rows could impact the quantity of data we are working on so we fix it by filling up the missing values by various methods which include replacing the missing values with the columns with data statistics(mean, median or mode) , using knn or mice to solve for missing values or we could use value of last neighbours when we're dealing with a time series data. when dealing with outliers, we use median value (q2) , q1 and q3 in boxplots and define a tolerance value outside of which a data value will be considered an outlier, we could also use column std. deviation and anything outside of  mean +-3 x std.dev will be considered an outlier. dbscan can also be used. while dealing with outliers, median is a good method and mean is not because mean is affected by the outliers and median is not.",5
5,"to improve the quality of results we can improve the sample by either increasing quality of sample or size of sample. we can improve the method by using multiple methods and select best one. also we can fine tune and properly choose parameters. linear regression- outcome is expressed as a linear combination of independent variables. taylor seriers expansion- any function can be written as a sum of powers of x. so we can fit any curve by defining x2=x^2,x3=x^3,...... and use multiple linear regression. if we keep on increasing number of feature, adjusted r squared value decreases after a certain point. linear regression of non linear independent variables. the resulting regression method is known as polynomial regression. the technique of starting with all features and remove features one by one until the model performance reaches a peak is known as backward feature engineering. sir gave two exercises to solve based on feature selection and polynomial regression. for different datasets linear regression does not always give proper fit. there are many models under supervised machine learning. based on data and exploratory data analysis we choose which models to try on this data. each model can handle a different type of dataset better. we can use multiple models at a time also. we need to fit a single good model rather than fitting multiple models to predict output. we have to think in long term. if we fit multiple models, total cost of ownership increases. linear regression and similar methods are parametric methods. random forest is a non parametric model. with parametric model we can do delta analysis(if there is certain change in feature what would be the change in output). xg boost is also a nice non parametric model it gives residuals in better normal distribution as compared to random forest. neural networks are examples of parametric models, most of the present day ml models are based on this. if there are more than one layer it is a deep learning network. but deep learning requires a lot of data. when y is nominal or ordinal we use classification techniques. when it is internal and ratio, it becomes a regression problem. we then went on to logistic regression which is a classification method. regress- to go back to the mean. in logistic regression we try to find boundaries between groups. we need to find the boundary so that the misclassification is minimised.  the output in this case is a categorical variable which denotes to which group the given point belongs. we need a function that will convert any variable in between 0 and 1. we use the sigmoid function for this purpose. ",1
5,"we began by addressing a doubt from the previous lecture summaries. it was about the ways in which we can improve our results from the model apart from just increasing the sample size. so, we can also consider and try out different models before fixing one. we can compare the various metrics of these models and figure out the best one. if we want to stick at our original model, we can fine tune/ use it more appropriately to improve the quality of our results. one of the solutions suggested was that of grid search.
after having a brief discussion about the statistics from the summary submissions of the previous classes, we then moved on to understand the significance of feature engineering in improving our results from the model. by using feature engineering, we can create more features or destroy the already existing ones, to arrive at a set of most relevant and appropriate features that significantly describe the data. we can use something like polynomial regression. we already have one feature, x1 with us. we can raise it to higher and higher powers and introduce these as additional features in the mlr model. we can also have a combination of the polynomial and trigonometric functions like sin(x1). we observe that as we start adding more and more of these features to our model through feature engineering, the r2 value increases, however the adjusted r2 decreases. this is because we are adding more and more features to the model which do not significantly improve the results. hence, by using feature engineering and analyzing the scatter plots and histograms of errors, we can arrive at the best set of features for our model, which can explain much of the variation in the data.
all this is a part of exploratory data analysis (eda).
there are two types of feature engineering techniques - forward feature engineering and backward feature engineering. in forward feature engineering, we keep on adding more and more features in our model, by transforming the existing features. in backward feature engineering, we start eliminating these features one by one from the model, already having many features to arrive at the best set of features. the elimination is done on the basis of the p-values of the corresponding coefficients.
after this, we talked about multiple model creation. so, in real life scenarios we first have to get the data, then perform eda and preprocess it to get â€˜good dataâ€™. this good data can now be fed to multiple models. we can simultaneously compare the metrics of all the models and decide the best one from that. we can use multiple or single models to fit our data, depending on the variations in it. however, if a single model describes the data well, then it is always preferred over handling multiple models.
so, mainly there are two types of models- parametric and non-parametric. in parametric models, we can control the various parameters (regression coefficients) and we can also perform â€˜delta analysisâ€™ on it. this means we can find out how much the y value would change if the x value changes by an amount, say delta. slr and mlr are examples of such a model. however, in non-parametric models, like random forest, we cannot perform delta analysis, instead they give us better predictions than the parametric models. these models are also more flexible, i.e. they can adapt to different types of data well.
one of the models which we used on the data was the artificial neural networks (ann). ann consists of â€˜hidden layer(s)â€™ which have nodes. these nodes map/ link to each and every other node in the network to form â€˜linksâ€™. these links have certain weights associated with them. the y value is a function of these weights and the x values. these weights keep changing and are recalculated as more and more features are introduced. when there are more than 1 hidden layer in the network, we call it the â€˜deep learningâ€™ network.
more the layers, more capable the model becomes but at the same time we need to feed more data in the network. data (x values) is converted into information, which is represented by the weights.
we fitted many models on the same data set, and compared various metrics like r2, mse, etc. of these to determine the best model for the data. apart from comparing between models, we can also use the metrics, like mse within the model, to assess its validity.
we compared the relative difference between the metrics of the test and train data for a single model, to arrive at meaningful conclusions.
lastly, we started with classification models and discussed logistic regression in that. these classification models are used on discrete data like nominal and ordinal data to classify it into various classes. the y values are called â€˜labelsâ€™ and they determine a particular class. using logistic regression, we are trying to find out the line/ curve which separates or segregates the data into different clusters, such that misclassification despite of overlapping is minimized.
so, every point will have feature values x1,x2,... associated with it based on which it is given a y value, i.e. a label.
we concluded by discussing the sigmoid function and how it can be used to identify the equation of the line. further discussions to be continued in the next class.
",2
5,"to improve the results you can either improve the sample or improve the method used. improving the sample could mean increasing the quality of the sample or the quantity. when it comes to improving the method, you can either fine-tune or better understand the method or use multiple and select the best one based on your knowledge about the metrics. one technique used to improve the method is call grid search where you form an n-dimensional grid with all the possible value combinations of the n parameters of the method, and then evaluate which one is the best.
polynomial regression is a type of linear regression where we add additional features derived from the given features, and they are respectively, the features raised to an exponent, eg.- x1 -> x1^2, x1^3, etc. more generally, we can use domain knowledge and insights gained from exploratory data analysis to engineer even better features.
there are two extremes to the process of feature selection. in forward feature selection, we start with an empty set of selected features, and then based on our knowledge, keep on adding features to the dataset in the hopes of improving the results. in backward feature selection, we start with all the features we can think of and then start reducing them by some metric (eg. p-value).
in parametric methods (such as linear regression), we can perform delta analysis, which is answering questions like how much does the output change with a slight change in a particular feature. this can be somehow achieved in non-parametric models also, but is much easier in parametric ones.
in real life after spending 80% of the time doing something with the data that does not include 'fitting a model', we actually need to fit many models and then decide which amongst them is the best based on various factors, some of which are: interpretability (whether we can make sense of what the model is doing), maintainability (the model will have to be recreated when 'data drift' {some change(s) to the trend in the data that occurs with time} is observed, hence if one model can fit the whole data, it is better), and of course the understanding and feel for numbers and error metrics.
a neural network is a model where we have hidden layers that are connected to input and output and amongst themselves using links that have an associated weights. the output function can still be expressed as w_i*x_i. more layers (called deep neural networks) add more flexibility / provide more degrees of freedom to the neural network, but in turn it becomes extremely 'data hungry'. the latest neural network models such as chatgpt have billions of parameters and are trained on internet-scale data, but the task that it performs is just: given a few characters / words, what is the most likely next character / word.
logistic regression is classification period.
need to predict the boundaries that separate the different classes. in case of overlap, we look for minimizing the number of mis-classifications. define an equation for the boundary and then based on the output, assign a class label. one such function that can do this is the sigmoid function (s(a) = 1/(1+e^(-a))).",3
5,"in today's lecture first we started by answering the question how to improve the quality of results? there are three ways to do so:
1-  to increase the quality  and size of sample|
2- improve the method like by using multiple methods and selecting the best one
3- fine tuning or properly using the methods
then we learnt that in linear regression outcome is expressed as linear combination of independent variables and there is nothing else linear in that.
when we have four features which are like x , x^2,x^3,x^4 and sin x , then
eventually your model will try to minimise coefficients of all the x1 to x4 and the coefficient of sin x will resemble the most that is will be the maximum so many of the features have to be eliminated otherwise the model will become unfavourable.
then we learn the basic difference between forward feature engineering and backward feature engineering. forward feature engineering is that we start with one feature and then keep adding until the performance becomes better and backward feature engineering is just the reverse of it that is we start with all the features and eliminate 1 by 1 based on the performance.
so naturally what we do is that we get the data we perform exploratory data analysis then we preprocesses and then we see multiple methods and then we compare them using the matrix and then select the best one.
after that we learnt about parametric like neural networks like which has the weights involved and if there is more than one layer in a neural network it is called as deep learning network.
in artificial neural network the data should be large otherwise overfitting will happen.
if we have to compare just the model within itself then we compare mean squared error with the value of y bar to get percentage and check whether the model is good within itself or not.
can we started with classification which is supervised learning in which why denotes the class and we have x and y both are available it is used for distinguishing between the discrete values.
then we learnt about regression and in which we learnt about the basic of logistic regression in which outcome is a classifier and our main purpose is to draw the boundaries between different types of classes.",4
5,"today's session basically started off with some review of the previous lectures, where we discussed that how if the error plot is not random, and shows some non linear relation i.e. if there is a non linear relation between the independent and dependent variable, then we need to introduce polynomials of the independent variable also as features to our model. this is part of feature engineering, and using polynomial functions of the independent variable as a feature for the mlr model, is known as polynomial regression. we also discussed how any non-linear and non-polynomial relation (eg. sin, cos, log) can also be converted to a polynomial relation using taylor series expansion. we also learnt that even complex neural networks employed some kind of polynomial regression itself, in order to make good predictions.
then we moved on to data which displays different kinds of relation in different ranges. we saw that we could use a variety of models for such data, and we could even use multiple models for a single dataset in case there was a completely different relation. however using one single model for the entire dataset is always more beneficial and easier than using multiple models, as it will create complications when implementing for the test dataset. also in such cases, we concluded that the best method of finding out the best model for our data was to try out all possible models and then choose the one with the best prediction accuracy according to us. we saw the responses of various models over a dataset and we observed that even though one model (random forest) was fitting our data highly accurately, we observed that it might be too good a fit for our model to be able to make any useful predictions on the test data. however, some other models, although not fitting the data that accurately, were doing quite well at prediction as compared to random forest. hence, choosing them would be a better choice than choosing a model which accurately fits our data but is not able to make accurate predictions. 
we then moved on to logistic regression and classification. we noted that this scheme applies to nominal and ordinal levels of measurement and not to ratios as classes are discrete and need to be dealt with in that manner itself. we also said that unlike linear regression, logistic regression was not meant to predict the data values, but rather the class in which an observation belonged. the output of such a model is the class, and we are basically trying to predict the boundaries between the classes. the expression for calculation is somewhat similar to that of linear regression, however in this case, we use weights instead of coefficients (beta) as per the nomenclature. the idea behind both the regression algorithms was similar; to reach a mean or mediocre value. in linear regression we were trying to predict the value of data points based on the mean of observations, whereas here, we are trying to find the mean boundary between classes so that we can accurately classify our observation points. ",5
6,"in this lecture, we continued our discussion on eda. we learnt how to create pivot tables in excel, which summarize the entire data into a table, whose entries can be chosen by us. this will help us create a summary of the data and understand the various trends and patterns in the data. using the pivot table entries, one can then create plots to analyze the distribution/ pattern in the data. this can be used to predict any desired value in future. the example which we discussed in class was that of the average number of characters in the summary on any particular day. this can be useful to predict the approximate number of characters in the summary of any day in the future.
we can use various plots including- box plots, scatter plots, histograms, as well as descriptive statistics function in excel to determine the outliers in the data set. any of these can be used to reach out to a conclusion regarding the outliers. also, it is not always correct to completely disregard the outliers, as they may also reveal certain important problems, patterns in the data, which cannot be ignored.
eda includes all these tasks of cleaning, comprehending, analyzing the data and extracting valuable insights from it, which can be further used to build a model. 
apart from this, we also looked at two case studies. one was regarding the measurements of numerous different parameters related to a process in chemical plant, with the measurements collected on daily basis. we created a pivot table which included the various parameters in the columns and the dates in the rows. then we added the values of- count of the total measurements made in that year, the average, max, min of these measurements for each of the parameter. we observed in some cases, the min value of the parameter dropped to 0, which cannot be practically possible, as we cannot have 0 pressure or temperature. the problem occurred due to the missing values of these parameters on some days.
the next case study was about the creation of energy using solar radiation. the oil and water temperatures were recorded at different time intervals, and the data was available. various line plots were created to analyze the variation in these temperatures at different times of the day and further on different days.
correlation maps were also made to understand the correlation between the various independent variables. there were certain patterns in the temperatures, from birdâ€™s eye point of view. however, more distinct patterns were observed within these large variations, when looked through a zoomed in perspective, suggesting hour-to-hour as well as daily variations in the temperatures. all these plots could be analyzed, to help us understand how we can separate the noise from the signals, identify the outliers, and use them to predict the values of the output variable in the future.

",1
6,"class started by the introduction to pivot table. we can find various parameters of a data like average of total characters, sum, min, max and std of the total characters. we analyzed the data of the submissions of summaries. we checked the histogram  for the average number of characters. we noticed that the majority of the portion lies in the lesser number of characters. we further created a box plot to find that there were some outliers in the data. 
we calculated the skewness and kurtosis and plotted a scatter plot to support the conclusion for outliers. 
we get shape of distribution by the histogram. other methods can help us know about the outliers. 
for the data, we explored the maximum submissions, the minimum submission, the number of times they have submitted. then, we went to ""who on which date submitted what length of submission?"". there were several missing values in the overall sheet. 
sir moved to the chemical plant data. we understood how data can be calculated for every 5 min or every hour. but it will just explode the data. to find the inner fluctuations, we need to dub up data. on using pivot table, we found that a lot of data is missing. we went to the min, max and average value of the data. we found an anomaly: min value for zero for a parameter. to check the place in which the error occurred we cut down the observations into small chunks until we found it. on plotting this data, these anomalies were clearly visible. we also created histogram and box plots. it is a good practice to create all the images and view them at once. we noticed how the plots with and without outliers look different from one another.  later, we proceeded to see the documentation of the data. in the anomaly and questions were highlighted. we do not want to exclude feature that may be useful. we went on to check how many of the variables are independent. we noticed that out of these hundreds of data rows. we only need 17 principle component or 17 independent process to describe the entire process. also, we can reorganize data and bring similar observation together. this makes the heatmap look better. 
we also checked for the data obtained from a transformer operations. the incomplete can be from sensor failure or device failure. we noticed how there can be points where hypothesis can be generated and we need to decide if to accept it or reject it. quality of input depends on the quality of our exploratory data analysis. ",2
6,"in todayâ€™s session, the instructor provided a detailed tutorial on using pivot tables to conduct exploratory data analysis (eda). the session began with a hands-on demonstration of pivot table functions applied to our summary analysis, where we explored plots of averages, maximums, and minimums, such as analyzing the number of words in summaries. this practical approach enabled us to see firsthand how pivot tables can reveal underlying data trends and discrepancies.

building on that, the instructor delved into broader aspects of eda, discussing various techniques and strategies for handling different types of challenges encountered during data exploration. he emphasized that understanding the nuances of eda is critical for diagnosing data issues, cleaning datasets, and ensuring the accuracy of the analysis. key points included methods for managing missing values, outliers, and other common data irregularities that can skew results.

the session then transitioned to a real-world demonstration, where the instructor showcased eda on datasets from a chemical plant and a solar plant project he had worked on. through this demonstration, he illustrated how comprehensive data analysis, when well-documented and carefully presented, can drive insightful decision-making and operational improvements. the importance of creating clear, detailed reports was highlighted, ensuring that the insights derived from eda are accessible and actionable for stakeholders.

finally, the teaching assistant supplemented the session with a presentation on exercise e2, further reinforcing the practical application of the techniques discussed.",3
6,"sir started with exploratory data analysis. sir explained them with the example of the session summary we have been uploading. in there data there were length of character in response submitted by each, average length of summary submitted. pivot table- a tool which helps us analyse and summarise data of a column. we get different values computed for a given column of data such as mean max min std dev and other such statistics. this initially gives us some inferences about data which we can use to further process the data.  linear regression is not valid beyond training data. doing exploratory data analysis gives us ideas on what to focus on. statistical summaries, varies kinds of plots can be created using plots. sir then showed how to analyse data of a chemical plant. we have 250 columns and we have to understand nature of these columns. the data has certain parameters at which the plant operates. edge computing- analysis data when we acquire data. sensors getting more advanced that the they acquire and analyse data and give us kind of summary. the pivot table automatically identifies columns like date and time. when we see min and max of data we will find blank or outliers or erroneous data, anomalies. by using this we can remove such data. we can use box plot or scatter plot to identify outliers. in the report of this data analysis, we will give some basic definitions of some metrics, plots and inferences. iqr=q3-q1(75 percentile-25 percentile). outliers will be =q1-1.5*iqr and =q3+1.5*iqr. the factor of 1.5 can be changed as per our requirement. some times we can identify a boundary between outliers and other data based on plots. in some cases the mathematical boundary may not be suitable. so we have to choose accordingly. while plotting we selectively pick some parameters and them analyse them from plots. based on the plots or data we need to ask questions about possible how the data is generated or what process behind that parameter. we can identify relationship between variables, and we can combine them together. we have to ask questions about data and add those into our data. when we get data we will get incorrect incomplete data. we have to attribute why such an error has arises. then tas gave us feedback on e2 assignment. sir then said the importance of documentation.",4
6,"in this lecture, we explored how to perform exploratory data analysis (eda) on multiple datasets to extract useful information.

we first worked with a dataset containing summaries submitted by students. using an excel pivot table, we calculated important statistics like the mean, maximum, minimum, and standard deviation of the number of characters in each summary. to gain more insight into the distribution, we plotted histograms and scatter plots, which helped us understand how the data was spread out. additionally, we analyzed the minimum number of characters in the summaries for each day and plotted it against the dates. we did the same for the maximum and average values and also tracked the number of students submitting summaries each day. moreover, we examined each student's submission patterns, such as the number of characters they submitted and whether they submitted summaries on specific days.

next, we analyzed a sensor dataset with observations for each date. using pivot tables, we counted the number of data points for each year, which helped us select which data to use for training. we also visualized the input and output columns using line graphs, which allowed us to observe how the output changed with different inputs. by applying principal component analysis (pca), we discovered that only 17 out of 240 columns were needed to capture most of the variance in the data, meaning these 17 columns would be the independent features to use for further analysis.

finally, we worked with transformer data to create a time-series dataset. we focused on understanding the patterns over time and identified anomalies in the data. for example, we noticed unexpected fluctuations during the night when no output was expected, indicating a problem with the system.",5
7,"we first revised a few concepts from the previous class. to assess the quality of any classification model, we use the confusion matrix. the confusion matrix gives us the number of points that are correctly identified and those that are incorrectly identified, in a tabulated manner. sir showed us some code for fitting a given data set with a logistic regression model and obtaining the various metrics associated with it. the â€˜scoreâ€™ represents the accuracy in a classification model, just like it represents r in regression. we also found out the true positive rate and the false positive rate. these are key metrics used to evaluate the performance of a classification model. true positive rate measures the fraction of the actual positives that are correctly determined by the model. similarly, false positives represent the proportion of actual negatives that are incorrectly classified as positive.
the receiver operating curve (roc) plots tpr vs fpr. if the curve is nearly vertical, then it means before classifying any points as â€˜false positivesâ€™, all the â€˜true positivesâ€™ have already been identified. on the other hand, if the line is exactly 45 degrees, it means the classification is completely random, or we can say that we are not using any classifier at all. so ideally, we want our plot to be as steep as possible. also, the area under the curve for a good classifier should be as close to 1 as possible. various points along the curve represent different threshold values. as you increase the threshold, the model becomes more stricter and hence, there are lesser chances of obtaining true positives, thereby increasing the false negatives. 
a good classifier is fine until there arenâ€™t any significant overlaps in the data sets. however, if the two sets have significantly high overlap, then no classifier will work well for it.
so, we can improve the quality of our classifier by either transforming the data or creating new and relevant features using feature engineering.
we also looked at yet another metric- â€˜supportâ€™. this represents the total number of observations that are in â€˜supportâ€™ of the class, i.e. the number of observations, that have been classified under that class by the model.
next, we started with â€˜clusteringâ€™. this is another unsupervised learning algorithm. in this, the points in a given data set are clustered and different groups are made among these. this is usually a part of eda and is used to assign labels to unlabeled data. there are two types of models in it- k means clustering and hierarchical clustering. 
in k- means clustering, we have to specify the number of â€˜meansâ€™ or in other words the number of clusters that we want. this is not the case for hierarchical clustering. 
in k- means clustering, all the points are randomly classified first and based on these, a mean value is obtained. using this mean value, the distance between a point and the means of various clusters is calculated and this is done for every point. the point is reassigned to a cluster, whose mean value is closest to the point. the new mean for a cluster is re- calculated based on the new points. then again, the points are reassigned into different clusters. this is repeatedly done, until we reach a step wherein there is no change in the arrangement of the points. this is our final clustering. 
if the initial assignment of the points to various clusters differs, we can get different clusters every time for the same data set. so, we run multiple such algorithms and the final cluster for a point is chosen to be that cluster in which it falls for the maximum number of times. 
hierarchical clustering algorithm considers every point as a cluster in itself, and starts clubbing all the clusters one by one, until it reaches a final single cluster with all the points. this is represented as a dendrogram. the height of the leg of the diagram represents the distance between the two points/clusters. so, hierarchical clustering gives many different clusters, which are combined and recombined to form larger and larger clusters. we can decide upto which point we want the clusters. thatâ€™s it for this class. 
",1
7,"the lecture continued on the lines of logistic regression and classification ideas, from where we left off. we started off by discussing the error metrics and the confusion matrix. then we were given a demonstration on some sample data on â€˜playground.tensorflow.orgâ€™, which allowed us to play around with data and neural networks. it allowed us to add features to our classification problem. then we moved on to some code for logistic regression. we understood that the score function for a logistic regression model library gives us the accuracy, which we should not believe. we also plotted the confusion matrix for the data, and understood the true positives and true negatives. we then studied the reciever operating characteristics (roc) curve. the roc curve rose vertically at the start and then became horizontal. this should us that for the given dataset, our model first detected all the true positives before starting to detect false positives. ideally, the roc curve is usually shaped in a crescent shape. the roc curve indicates the quality of the classifier, where a sharper roc curve indicates a better model. the logistic_regression function returns the predicted class for an observation, or the probability value associated with it. hence, by changing the threshold between the classes, we can change the quality of the model and check it using the roc curve. the 45 degree line on the roc curve tells us that there is no classification between the points. hence, the flatter the roc curve for our classifier, the worse is our classification model. we also measure the area under the roc curve and call it as auc. for a good classifier, this value is close to 1. the worst classifier is the one with the roc curve the same as the 45 degrees line. hence the worst case auc value is 0.5. the points on the roc curve correspond to different threshold values for the classification. 
we then moved on to data having more than 2 classes, and we realised that sometimes, due to lack of anomalous data, we might not be able to use our classifier. in the entire system of the classifier, some classes may have very low f1-score, which means that they are under-represented and the classifier canâ€™t be relied on for classification of data into that class. each class has itâ€™s own roc curve, and hence a classifier can be good for one class, while being equally bad for another, which can be seen using the roc curve of that class for our classifier. 
then we moved on to clustering, which was a part of unsupervised learning. in this case, our data does not have any labels, and we need to assign labels to the data ourselves based on some characteristics. this assignment becomes a part of eda. we then studied about some clustering algorithms like hierarchical clustering and k-means clustering. for k-means clustering, we need to give the number of clusters as the input, which is difficult to find out just based on visualisation of data. however, in hierarchical clustering, we plot a dendogram and then based on that, we can decide the number of clusters we need. 
in k-means clustering, we first specify the number of desired clusters. then the algorithm starts randomly assigning points to any cluster, and then we find the centroid of each cluster. then we find the distance of each point in a cluster to the centroid of all the clusters. then we reassign each point based on the centroid they are closest to. this process repeats until each point is closest to the centroid of its cluster than any other cluster. it is an o(n^2) algorithm. in this algorithm, the initial assignment also changes our final results. hence, we run the algorithm multiple times and check that for each point, which set was it clustered into multiple times, and hence we decide the final cluster it belongs to. 
hierarchical clustering does not require any initial cluster number. it results in a dendogram, which can help us decide our degree of clustering. it starts by assigning each point as a different cluster. it then gets grouped with other clusters and we get the dendogram. the number of clusters can be decided by where we observe the dendogram. the pair wise distance between each pair of clusters is calculated, and those clusters are grouped, whose distance is less than all the other pairwise distances of those clusters. this goes on till we have just one cluster, giving us the dendogram, and the number of clusters is decided by where we cut the graph. ",2
7,"during this session, we explored key concepts in model evaluation and clustering techniques. we began with understanding the roc curve, a graphical tool for assessing classifier performance by plotting the true positive rate (tpr) against the false positive rate (fpr). tpr (or sensitivity/recall) is defined as the ratio of correctly classified positives to the total actual positives, while fpr is the ratio of negatives incorrectly classified as positive to the total actual negatives. by varying the decision threshold, different tpr and fpr values are obtained, which form the roc curve. a perfect classifier achieves an immediate vertical jump to (0,1) with an auc of 1.0, whereas a poor modelâ€™s roc curve follows the diagonal with an auc near 0.5.
additionally, the auc, which ranges from 0 to 1, summarizes model performance and assists in comparing models. however, limitations exist, especially with imbalanced datasets and the need for selecting a proper decision threshold.

we got to know about a tool ('neural network playground') , where we can vary the parameters and visualize the classification and regression with neural networks.

we also discussed converting regression problems into classification tasks by discretizing the continuous target variable. this method simplifies the problem by grouping continuous values into categories but involves trade-offs, including loss of detail. domain knowledge is essential for choosing proper bin thresholds.

then shifted focus to clustering methods. k-means clustering organizes data into groups based on similarity by assigning points to centroids and iteratively updating these centroids. hierarchical clustering, in contrast, builds a dendrogramâ€”a tree-like diagram that shows how clusters merge over various dissimilarity levels. the vertical height in the dendrogram represents the distance at which clusters merge. in particular, complete linkage defines cluster distance by the maximum distance between any pair of points, yielding compact, well-separated clusters.

",3
7,"we started our discussion with the classification evaluation metrics. then, we visited a website to visualize different data distributions and their classification boundaries by adjusting nodes and features for a neural network. after that, we moved to the notebook for logistic regression, where we loaded the data and converted it into a dataframe. we analyzed the classification performance using different evaluation metrics, as accuracy alone is not sufficient to validate our model's performance. we also looked at other evaluation metrics such as the confusion matrix, f1 score, recall, and accuracy. 
next, we examined the roc curve, which shows how many false positives our classifier detects before identifying true positives. the area under the roc curve (auc-roc) indicates the quality of our classifier. if the auc-roc value is less than or equal to 0.5, the classifier performs no better than a random guess.
this was for binary classification. we then moved on to multiclass classifiers, where we examined the classification evaluation scores and the confusion matrix for each class. we also looked at the auc-roc curve for each class and compared the performance of our classifier across different classes.
we started discussing clustering methods, which are unsupervised machine learning techniques. first, we explored the hierarchical clustering method. we plotted a dendrogram to analyze the closeness of data points and determine how to divide them into clusters. then, we moved on to the k-means clustering method, where we discussed how k-means assigns data points to clusters.",4
7,today we continued discussing about logistic regression the class began with the  demonstration of artificial neural networks and we learnt that selecting appropriate and relevant number of features is very important. we then revisited the logistic regression metrices and a new metric was introduced today and that was roc graph which tells us that how many true positives did the model predict before predicting the false positives and for a good model which can make a boundary between two clusters then the area under the curve must be close to 1 if the data is not good and if there is too much overlap of the data then in such cases the area under the curve will be close to 0.5. we then saw two examples of logistic regression one showing how to interpret the metrices like accuracy. precision and confusion matrix. we looked at another example where there were four clusters in the data but one had class imbalance and we could clearly see in the roc curve that specific class had very low area under the curve. we looked at clustering methods mainly k-means and hierarchical clustering. the main difference between these two is that we have to specify  number of cluster we want in k-means but the hierarchical clustering considers every data point as a cluster and then starts making broader groups. k-means works on taking the n-means and then starts calculating the euclidean distance between other points which ever is close to the nth mean it is classified into that cluster. hierarchical clustering uses indivdual points as a cluster and then finds which element is close to it forms a bigger cluster with that now distance between this cluster and other cluster is calculated and which ever is smaller is taken in to form a bigger cluster,5
8,"
the lecture began with an recap of multiple linear regression did in last lecture , where we have multiple independent variables (x_1, x_2, x_3, ... x_n). although multicollinearity is an important consideration, it was noted that this would be discussed later.  

next, we covered the concept of training and test data, emphasizing that the entire sample should not be used to train the machine learning model. a typical 80-20% split (80% training, 20% testing) was recommended, as a 50-50% split might lead to an unrepresentative training sample.  

we then discussed two outcome sets: training matrix and test matrix. for the training matrix, we introduced the concept of the confidence interval. this led to a discussion on overfitting, which occurs when training accuracy is much higher than test accuracy. a graphical representation was used to illustrate this issue.  

after this, we performed regression statistics in excel. several key terms were defined:  
- multiple r: the square root of r^2 (coefficient of determination), which provides a measure of the nonlinear correlation between y and the independent variables.  
- adjusted r^2 the formula for adjusted r^2 was explained.  
- the denominator in variance formulas: we examined why it is n-1 instead of (n)due to degrees of freedom being reduced when using the mean of x.  

we clarified that linear regression does not necessarily imply a straight line, but rather a linear relationship between yand x 

towards the end, we transitioned to python programming for linear regression. several statistical tests were covered, including:  
- omnibus test and its p-value criteria
- skewness and kurtosis statistics
- durbin-watson test for autocorrelation  

additionally, we discussed quantile-quantile (q-q) plots, which help assess whether a dataset follows a particular distribution by dividing the sample distribution curve into sections.  ",1
8,"in today's session we discussed briefly about closed form solutions for mlr where we got to know that these solutions exist but we don't use them because of the large size of the matrices and computing the inverses of those matrices would consume too much time and computational power and also because multicollinearity exists between two or more variables. when we want to generalise our model for the entire population but we dont have access to entire population's data so the sample we have is divided into train and test data. depending upon the size of the sample we can choose different proportions of train and test data. some parameters used for training data might not be used for the test data like the ci and other error metrics like sse and tse. we use mostly r squared value and mae or rmse to decide if our model is working good enough on the unseen(test data). an overfit model fails to generalise for unseen data and hence lacks the ability to perform good on the test data. adjusted r^2 value is used for mlr to see if our r^2 value is increasing significantly when we add a new variable and that helps us to decide how many independent variables we would want to have in our data. also the term linear regression does not always result in a straight line, it just means that the dependent variable would be a linear combination of all the independent variables. also we later jumped into python and see how to code for a lr problem using different libraries like pandas, matplotlib, sklearn etc and their functions. for a problem we have we should always try to solve the problem with different models and decide which one's better in terms of the less error values and more accurate results.",2
8,"after exploratory data analysis on our dataset, we split the data into two classes: 80% for training and 20% for testing. training data was used to create a linear regression model, and hence, we had two key sets of output metricsâ€”training results and testing results.

overfitting was discussed as a potential issue in model performance. this arises when the model performs well with the training data but poorly on the testing data, which gives an indication that the model learns specific patterns specific to the training data rather than generalizable trends.

we also discussed the idea of adjusted râ². while regular râ² increases with each additional predictor, adjusted râ² adjusts for the number of predictors in a model and provides a better measure of the fit of the model. this value gives us the amount of variance explained per degree of freedom, giving a more realistic view of model fit.

linear regression models: simple linear regression (slr) and multiple linear regression (mlr) were covered. these are parametric models that depend upon the use of p-values, coefficients like î²â‚ and other statistical measures in order to understand the relationships between the variables.

we then looked at how to determine if errors are normally distributed. a q-q plot, or quantile-quantile plot, is used for this purpose. if the error values fall along a straight line in the q-q plot, it suggests that the errors are normally distributed, which is an assumption of linear regression.

the session explained the basic setup of scikit-learn, or simply sklearn, the python library for building and evaluating ml models. in this session, we learned how to use this tool to successfully implement linear regression.

hypothesis testing: the session concluded with an introduction to hypothesis testing, including an overview of p-values. we touched on some common statistical tests, including the omnibus test, jarque-bera test, and durbin-watson test. these tests help assess the underlying assumptions and validity of the model, such as checking for autocorrelation, normality of residuals, and other model fit aspects.",3
8,"this lecture focuses on multiple linear regression (mlr) and its implementation in python.  while mlr has a closed-form solution, it's computationally expensive for large datasets. therefore, gradient descent is the preferred optimization method.

a crucial practice in machine learning is splitting the available data into training and testing sets.  a common split is 80% for training and 20% for testing, performed randomly.  this allows for evaluating the model's performance on unseen data.  two sets of evaluation metrics are generated: one for the training data and one for the test data. a good model exhibits strong performance on both sets.  if the model performs well on the training data but poorly on the test data, it's a sign of overfitting. conversely, poor performance on both sets indicates underfitting.

the ""multiple r"" metric is simply the square root of r-squared.  it represents the correlation between the multiple independent variables (x) and the dependent variable (y).  adjusted r-squared is a modified version of r-squared, calculated using the residual sum of squares (rss) and the total sum of squares (tss).  the formula incorporates  n-1 (degrees of freedom) because we lose a degree of freedom for each estimated parameter. adjusted r-squared penalizes the model if the rss doesn't decrease sufficiently relative to the increase in the number of predictors.  it helps to prevent overfitting by considering model complexity.  it's important to remember that mlr doesn't always result in a straight line; it can model more complex relationships.

the lecture then transitions to implementing mlr in python using the sklearn library.  sklearn is excellent for model building, predictions, and handling large datasets.  however, it can lack fine-grained control for researchers needing detailed model analysis.  to assess the model's fit, the distribution of residuals (errors) is examined using a histogram and a q-q plot.  ideally, the residuals should be normally distributed.

for more in-depth analysis, the statsmodels library is introduced.  statsmodels provides access to a wider range of statistical metrics.  one such metric is the ""omnibus"" test, which combines skewness and kurtosis to assess how closely the residuals approximate a normal distribution.  the jarque-bera test is another normality test for residuals.  a desirable outcome is a non-significant p-value (typically greater than 0.05) and a test statistic below a certain threshold (e.g., 2), indicating that the residuals are likely normally distributed.",4
8,"for multiple linear regression, the closed form solution for b exists but it might be impractical to calculate these as matrix inversions might not exist and etc. if we have a sample of data, we should not use the entire data for creating the ml model. we need to split the data into two parts in the ratio 80%-training data and 20%-testing data. this splitting has to be done randomly. two sets of outcomes that we have to derive and measure: training metrics and test metrics. some of metrics will only be relevant to the training data but may not have any meaning for test data. as we are building model using the training data there are some metrics which are suitable for this only. overfit  situation: when r square value of training data is much greater(generally a difference grater than 0.2) than r squared value of test data. the training errors maybe less but test errors will be too much in case of overfitting. this is a practical tradeoff how much error we are allowing for test data and training data. both these errors are important. as we add more variables, r square value increases. this doesnot mean that the data is better fit. so we introduce a adjusted r square value which is the part of variance captured by each independent variable. n-1 comes in the denominator of calculations involving variance and standard deviation. the calculation of variance involves mean of x. this is already calculated using all n variables. this reduces one degree of freedom and it becomes n-1. the outcome of a linear regression need not always be straight line. slr and mlr are called parametric methods of model creation. there are non parametric methods of model creation. data drift- if we create a model now, the data coming after one month maybe far away from this model. so we have to change our models also frequently according to the data. then we shifted on to python. sir explained how to do multiple linear regression in python. q-q plot tells us how much our data is similar to normal distribution. regression model can be imported from scipy or stats model libraries. sm.ols: sm-statsmodel, ols-optimised least square. stats model gives more statistical measures and parameter than scipy. low values of aic and bic are better. aic and bic are to be discussed later. omnibus statistics, omnibus p value, skewness. we cant use the old p value everywhere. we need to use different type of p-value. omnibus- normality of distribuition. jarque - bera test- check the normality of residuals. durbin watson test tries to asses based on value of error can we predict next error or kind of autocorrelation in the residuals of the models. ",5
9,"during exploratory data analysis we get insights on type of data and understand problems present in the data. we can remove outliers based on trend followed by data: if all data follows a particular trend and some doesn't follow we can remove this data. we can use data smoothening to reduce noise in data. real data has a lot of fluctuations. this can make it difficult to find trends ir pattern in such data. simple moving average sma consider a window around every data points and average the values. window width can be varied to adjust the level of smoothening. based on application we can take window width. we can take moving average at a point as average of all past points or average of some past and some future points. there will be problems at the end of data as there won't be any points to the past of starting of data. so we have to define accordingly at end points. the moving average reduce the noise and maintains the characteristics data. another way is exponential moving average that weight nearby samples more while averaging. this is used in time series forecasting.
in data analysis, it is crucial to handle missing values and outliers before proceeding with further steps like calculating moving averages. ignoring these aspects can lead to distorted results, such as sudden changes or incorrect trend patterns. handling missing values can be done by either ignoring, discarding, or using methods like regression to fill in the gaps. outliers should be removed or adjusted to prevent them from skewing the analysis. after addressing these issues, the data is ready for further analysis, where both visual and mathematical methods are used to ensure accuracy. normalization transforms data to a 0-1 range. in this case, normalization had little effect on the target values and coefficients, with minimal changes except for one variable (x2). this shows that scaling didn't significantly impact the model. for the next messy dataset, a clustering algorithm is suggested. in the process of data analysis, independent normalization of variables (x and y) can impact the results, especially when using algorithms like k-means clustering, which rely on euclidean distance. when the data is normalized, it alters the shape and relationships between variables, leading to more refined results. algorithms sensitive to scaling, such as k-means, are particularly affected, and normalization helps produce more accurate clusters by balancing the influence of different features.


a standard normal distribution has a mean of 0 and variance of 1. to standardize data, the formula is used, where is an observation, is the mean, and is the standard deviation. standardization does not change the shape of the data's distribution, meaning the transformed data retains the original distribution's shape. this method is often used in statistical tests, especially hypothesis testing, where normal distribution is required. data transformations like box-cox are applied to achieve normality before performing such tests. descriptive statistics of the transformations should be reviewed to understand their impact.
the box-cox transformation is a technique used to make data more normally distributed. it involves raising each observation to a power (lambda), with the optimal lambda value determined through a process called maximum likelihood estimation. in this case, the lambda value is 0.17, which is chosen to best transform the data into a normal distribution. this method is useful when algorithms assume normality in the data, ensuring that the data fits those assumptions.
the box-cox transformation and similar techniques, like square root or cube root transformations, pull data points closer together, particularly those that are far from the mean. this helps in dealing with skewed distributions. for a lambda value close to 0 in box-cox, the transformation approximates ; if lambda is 0, it uses the log transformation. the result is a more normally distributed dataset, which is essential for certain algorithms. when applying transformations like box-cox or logarithmic transformations to features (e.g., x1), these same transformations must be applied consistently to future or test data to ensure accurate predictions. additionally, sometimes transformations must be reversed to interpret results in their original context. various scaling methods, such as standardization and normalization, are commonly used to handle such transformations.
the condition where data variance changes across different levels of the data is known as heteroscedasticity.
the process of scaling data, such as using log transformations, reduces the emphasis on large values and minimizes the impact of variations in data. this is crucial because many algorithms, especially those that rely on euclidean distance or hierarchical clustering, are sensitive to data scaling. various scaling methods are used to bring the data to a common scale while preserving the variation within the dataset. however, in some cases, transformations like the box-cox transformation are applied to also change the shape of the data for better analysis.
when preparing data for analysis, steps include identifying and fixing missing values, exploring correlations between features, and creating visual representations like scatter plots or matrix plots. scaling transformations are often applied before conducting these analyses to ensure accurate and consistent results.
the issue of data imbalance arises when certain classes are underrepresented in a dataset compared to other classes. this is particularly problematic in classification tasks. in the example given, there are four classes: red, green, blue, and white, where the white class is underrepresented and often merged with the blue class. when subjecting these classes to classification, boundaries are created by the algorithm based on the dominant class, leading to misclassification of the underrepresented class.
as a result, false negatives occur, where observations belonging to the underrepresented class are wrongly classified as another class. this is evident in confusion matrices and the performance metrics like precision and recall, where the underrepresented class shows poor classification. the algorithm focuses on correctly classifying the majority class, but the minority class suffers from poor performance.
data imbalance occurs when one class significantly dominates over others in a dataset, causing challenges for learning algorithms. this imbalance can negatively affect the algorithmâ€™s ability to learn, particularly in cases like medical diagnosis (e.g., diabetes detection) or fraud detection, where underrepresented classes are crucial. algorithms may become biased toward the majority class, leading to misclassification and poor performance for minority classes.
an example of this is in the detection of exoplanets based on flux values from distant star systems. most stars do not have planets, meaning the dataset is heavily skewed, with 99.3% representing stars without planets and only 0.7% representing stars with planets. in this case, the algorithm may focus on the majority class (stars without planets) and miss important patterns in the minority class.
to address this, data needs to be balanced or projected into a lower-dimensional space for better visualization and understanding. algorithms require different performance metrics (e.g., precision, recall) to detect the presence of data imbalance and ensure that the model learns effectively from both the majority and minority classes. neglecting this can lead to biased models that perform poorly on critical underrepresented classes.

in cases of data imbalance, particularly when the disparity between majority and minority classes is not extreme, dropping a small number of values from the majority class can be acceptable without significantly affecting the sample's representativeness. however, if the difference is significant, dropping too many values from the majority class may lead to a sample that is no longer representative of the population.
one approach to address data imbalance is oversampling the minority class by duplicating its data points. however, this method does not introduce new information; it merely replicates existing data, which can water down the dataset without solving the core issue of imbalance.
a more effective method is using synthetic minority over-sampling technique (smote). instead of duplicating data points, smote generates synthetic samples by selecting a minority class data point and creating new points as linear interpolations between it and its nearest neighbors. this method enhances the representation of the minority class while maintaining diversity in the dataset. by creating synthetic samples, smote helps improve the balance without simply duplicating existing data, providing a better way to handle data imbalance in machine learning tasks.
some other data balancing are adasyn. tomek links are used to to undersample data. majority class with their nearest neighbor being a minority class sample are removed. smote and tomek links are done one after other to get nice data. 
",1
9,"we started off the class by discussing about outliers and how they can be found out using scatter plots and box plots. they can also be found out by descriptive statistics and line charts. however whether to actually ignore and drop the outliers or to perform some processing on them, depends upon the domain and thus required domain knowledge. sometimes, outliers can be hidden in the data, which can be observed by maybe rescaling the data or by dropping the true outliers and then observing the remaining data again. now we need to smoothen out our data and remove all the noise, in order to observe some trends in the actual data. thus, we perform data smothering, which could be done by various methods. we discussed about simple moving averages, where we consider a window around every data point and average the values in that window. the window width can be adjusted according to the level of smoothening. but in doing so, we need to take care about missing values as well, or else our smoothening algorithm might not work correctly. the higher the window size, the smoother our data becomes. we also have methods called exponential moving average or weighted moving average, which weighs the nearer points more as compared to the farther points. 
we then moved on to handling data where each column has a different scale. suppose we have a data where one column has very large values as compared to another column. this becomes a problem when we try to run a model like multiple regression, as the larger value data creates a very large maxima, which leads to all the other minimas getting overshadowed. hence gradient descent is not able to find the most optimum minima and the algorithm fails. hence we need to scale our data appropriately to prevent this. the most common method of scaling is to scale each column data to lie between 0 and 1. we then discussed that any algorithm which depends upon euclidean distances, like the k-means clustering algorithm, will be affected by the normalisation of the data. we could also perform an operation known as standardisation, which comes from the term â€˜standard normal distributionâ€™ i.e. n (0, 1). standardisation and normalisation do not change the shape of the distribution of the data.
the difference between the mean and the median of the data also gives us an idea about the skewness of the data, where a larger difference indicates more skewness. 
we then discussed about various other transformations like logarithmic transformation, where we de-emphasise the higher values. this transformation actually changes the shape of the distribution of the data. 
we then moved on to data imbalances, which is mainly used in the context of classification. it means that data from one class is highly under-represented as compared to the other classes. in such cases, we can either under-sample the majority class, or generate data belonging to the minority class. we could use smote algorithm, which creates synthetic samples based on linear interpolation between the existing samples. the hyper parameters for this algorithm are the number of neighbours we want and the smote percentage. we also have tomek links, which under sample the majority class data. ",2
9,"we continued our discussions on eda. eda is the first step before performing any complex algorithms on the available data. it involves discovering problems associated with the data as well as few possibilities and insights from the data. in todayâ€™s class we looked at some specific problems associated with the raw data and the methods/ algorithms used to tackle these.
the first problem which we discussed was that of presence of lots of noise in the data, which causes hindrance in detecting the true signal. we discussed few methods to solve this problem. we specifically looked at /moving averages for data smootheningâ€™, in which we first learnt about the simple moving average. in this method, we take nearly about 50 data points surrounding a particular data point and evaluate their average inorder to get an average value associated with that particular point. we do the same with all the other points and finally get a smoother curve with the values as averages of some 50-60 points in the neighborhood. the smoothness of the curve depends on the number of data points we are choosing to evaluate the average. as we increase this number, the curve becomes smoother and smoother and eventually becomes a flat horizontal line, when all the points in the set are included in the window. this creates an â€˜artificial signalâ€™ and the original variations in the data vanish completely.  there can also be different ways in which we consider these points. for example, we may consider only those points which lie behind the selected point. this causes a problem at the end point, particularly the one on the left. similarly, we can also consider that window which includes some points behind the selected point and some ahead of it. this also causes problem at the end points. we can choose any of this, however our choice should be justifiable, according to the particular data set. 
next, we can also use exponential/ weighted averages, in which we assign higher weights to the points in the proximity. we must select an optimal size of window or the moving average method, such that we are able to extract the signal from the highly fluctuating values. 
before creating moving averages, we should address the problems of missing values and outliers, else they would cause trouble while creating the ma. 
the next problem is that some values in a column have significantly higher values than some others. this makes the gradient decent algorithm, more biased towards the larger values, thereby causing data imbalance and giving false results. hence, it is important to normalize the values in the columns so that every value lies between 0 and 1. standardization is another process wherein we convert the existing data into standard normal distributions, with mean 0 and standard deviation 1. both of these do not change the shape of the data. if we transform or scale the data, then any algorithm based on calculating euclidean distances would be largely affected. it is important to transform the data first because some algorithms assume that the data is already normally distributed. 
there is another kind of transformation- box cox transformation in which we evaluate the transformed value of each x by using a parameter lambda, which is optimized such that we get the closest approximation to the normally distributed data set. 
apart from transforming the features, we also have to reverse transform the transform the response variables, to get the values in the original form back. 
the third problem which we discussed was regarding the data imbalance, which occurs whenever we have a class whose number of samples are very small compared to the other classes. hence, the class is eventually suppressed by the others. we need to fix this problem, as we may get incorrect results/ predictions on using the imbalanced data. also, the algorithm may not learn well using the imbalanced data. 
to fix this data imbalance, we can do the following:
1)	under sample the majority class.
2)	over sample the minority class. the most naive method is to duplicate the values of the existing points.
3)	ideally, we should sample more points in the surrounding of these points by linearly interpolating between any two points. this is known as smote.
4)	next, instead of just randomly dropping any sample value from the majority class, we can drop those values which have the nearest sample belonging from the minor class. this has two advantages- first is that it creates a distinct boundary between the classes and also in this way we can get rid of some possibly misclassified points. this is known as tomek links.
5)	so, first we can apply smote then use tomek links to improve the quality of the classification model.
",3
9,"we began by looking at how outliers affect the data plots, and how much of a difference it can make (we saw a graphical representation after removing outliers and rescaling to better understand the data). we also saw how noise can change the signal data and in order to fix it we started learning about data smoothing. fluctuations due to noise can make it difficult to identify trends and patterns, and can even lead to wrong analysis, hence the need for cleaning. sma stands for simple moving average, it a moving window average, it's a form of low effort filter, to filter and check data. we start with let's say 50 data points and underlying trend is visible after that, then we have a higher window size to do more smoothing. when we come to the end points we can choose to stop the moving average according to the type of method we're choosing (if we say that moving average of a window is at the left most point of the window, then we'll have to ignore the right most end point). there are other methods like exponential moving averages that weigh nearby samples more, also used in time series forecasting. then we observed the clustering of data, and again we went through the data of session summary and saw the graphs of plotted values of number of submissions and the length of summaries. we also saw standardized and normalized transformations. we saw how a skewed distribution can turn into normal distribution with transformation, we should be aware when to apply the transformation and how it'll be useful. then we to got to see kepler exoplanet data, plotting the flux values for stars belonging to two classes. data imbalance, and we saw techniques to see data imbalance, either by under-sampling the majority class or over-sampling the minority. or, a more interesting approach is to generate synthetic data for minority, which is a type of oversampling. we understood how smote oversamples the minority class, minority classes with samples with nearest neighbor being minority class are removed.",4
9,"todayâ€™s class start with a discussion on how do we eliminate noise from dataset. noise makes it difficult to find a trend in data therefore it is becomes a necessary part of exploratory data analysis. we learn one method named â€œsimple moving averageâ€ in which we consider a window around every data point and average the values. window width can be varied to adjust the level of smoothing. higher window size makes more smoothing. this method is also used for filling up missing values, replacing outliers.
another method is â€œexponential moving averageâ€ which works better on time series data. one good practice is that first removes the outliers then apply moving averages method. next we learnt about the standardization and normalization of datasets. linear regression model is immune to the data scaling but many models such as gradient descent method is not. clustering algorithm based on euclidean distance will greatly influence by scaling the data. the normalization makes value lie between [0,1] using formula such as x=[x-x(min)/ x(max)-x(min)] and the standardization creates standard normal distribution. normalization does not change the shape of distribution of the data. we use log transformation when data is heteroscadascity. next we discuss about data imbalance which happens when certain instances of a class might show up more frequency than others. example rare disease diagnostic, forgery. we can overcome this by either under sampling the majority class or over sampling the minority class. we can do this with the help of smote tool.",5
10,"in today's class we went deep into the simple linear regression techniques by using some data in excel. in the data we have only x and y column from which we calculated other terms such as x_bar, y_bar, xbar_sq, ybar_sq, error and many more terms. we also create the scatter plot between x and y and realized that it follows linear model. we have also plot the histogram to check what is the nature of distribution of data. if it's a bell shaped curve then it is good. the scatter plot of errors values display a distinct pattern showing that model has failed to pick-up the inherent pattern in the data. next by using the data analysis tool in excel we created the summary output of an linear regression model giving many values related to regression statistics. another interesting thing is that - one that explains most of the variations in the data is 'good model'. further we learnt about some regression statistics  short forms like  1)sst = measure of total variation in the given dataset.
2)ssr => total variation explained by the regression model and 3) sse => variation not explained by the model, attributed to random errors. coefficient of determination(r^2) which is the square of the correlation coefficient 'r' between x and y. if the x is increasing and y also increases then it have (+ve) correlation. if the x is increasing and y is decreasing then it have (-ve) correlation. we conduct some 'thought' experiments, related to estimating the population mean from the sample mean: assume that from a population we can take multiple good, representative samples, let's say k samples, each of size n. let's call each sample as s_i. using each s_i, we calculate its mean and call it m_i. for samples are good, representative samples of the population, they will result in means m_i that are close to each other. if we collect all the m_i and create a frequency table and a histogram, it's shape will be bell curved.",1
10,"in today's class, we revisited the formula for a and b in the simple linear regression equation:
y = ax + b.

using a csv file in excel, we created columns for y_predicted and error. before that, we calculated the averages (xì„ and yì„), as well as xx_bar, xy_bar, and xì„â². from these values, we calculated a and b, which were used to compute y_predicted. the error was calculated as the difference between the actual y and the predicted y.

we plotted the best-fit line for our data and then visualized the errors. on a 2-d scatter plot, the errors looked fine, but when plotted as a histogram, they did not follow a gaussian (normal) distribution. this indicated that the model couldn't fully capture some trends in the data.

next, we used the regression feature in excelâ€™s data analysis toolpack to get statistical details about the regression model. we learned that the standard error measures how much the sample mean varies from the population mean. additionally, we discussed that for many samples, the distribution of sample means tends to follow a normal distribution.

lastly, we introduced three key terms:

sst (total sum of squares): measures the total variation in the data.
ssr (regression sum of squares): the variation explained by the model.
sse (error sum of squares): the variation not explained by the model.
these terms are related by the formula:
sst = ssr + sse

the râ² value, calculated as râ² = ssr / sst, represents the proportion of the total variation explained by the model.

",2
10,"in today's class we took a sample data with 100 data points and found out the best fit line using simple linear regression in ms excel. we calculated the values of 'a' and 'b' using the data to get the regression line y(hat) = a*x + b. we plotted a scatter plot between x and y and also added the y(hat) points on the same graph. then we calculated the error values
ei = yi - yi(hat) and plotted the error values on a scatter plot and a histogram as well. for a perfectly random data the error values should be a normal distribution (bell curve) on the histogram, which is not the case as we observed so we say that the model failed to pick up the pattern in the data.
then we used the data analysis tools for linear regression to get various information about the sample data.

then we moved on to discuss ""what is a good model?"" 
the model that explains most of the variations in the data.
sst = summation(yi - ybar)^2
sst = sse + ssr 
1 = sse/sst + ssr/sst
1 = r^2 + sse/sst
r^2 = 1 - sse/sst
r^2 : coefficient of determination
in case of simple linear regression coefficient of determination is same as the square of correlation coefficient 'r'.  
r^2 = r^2


when drawing k representative samples (si) of size n from a population, the means of these samples (mi) are expected to be similar due to their representativeness. if we plot the frequency distribution of mi, we typically get a histogram that resembles a normal distribution, as suggested by the central limit theorem. this histogram illustrates the sampling distribution of the sample mean. important characteristics of this distribution include that its mean is close to the population mean, and its standard deviation, known as the standard error (sx), is connected to the population standard deviation (sigma) and the sample size (n). these characteristics highlight the advantage of using multiple smaller samples rather than relying on a single large one, as the sampling distribution offers valuable insights into the population's traits.",3
10,"today's lecture was mainly aimed at giving us a first hands on experience with data. we worked with a sample dataset on excel, where we created scatter plots and tried to implement simple linear regression. we used a tool in excel called the data analysis toolpak, which gives us a lot of information about our data and the linear regression statistics. next we discussed about histograms, which is a frequency chart showing the frequency of data distributed into various bins. 
then we studied that in an ideal case, our model should be able to harness all the predictable patterns in the data, leaving the noise or errors to be random. however, if we are able to predict the errors, that means that our model has not captured the trend between the errors. we also studied that if our outcome is dependent on a large number of unknown causes, then the distribution observed is known as a gaussian normal distribution. 
we went on to discuss that a good model is one which can explain most of the variations in our data. we defined 3 terms:
sst = measure of the total variance of the data
sse = sum of squares of the errors / noise variance
ssr = sum of squares of the total variance captured by the regression model 
then we derived a relation between these three terms to be that sst = ssr + sse. now when we divide both the sides by sst, we get a term ssr / sst on the rhs. this term is defined as the coefficient of determination, or r^2. this term tells us how close our model is in measuring the actual variance in the data. this term has a maximum value of 1, and a minimum value of 0. this term should be as close to 1 as possible, which indicates a good model. 
the term is defined as r^2, because for simple linear regression, the coefficient of determination is equal to the square of the correlation coefficient. since correlation coefficient is termed as r, the cod is termed as r^2. however this result doesn't hold true for multiple linear regression. the correlation coefficient is defined as the measure of how y changes with respect to its mean as x changes with respect to its mean. 
we moved on to define a special histogram, which captures the frequency of means of various samples of a given data. such a histogram is called the sampling distribution of the sample mean. it tells us that if we have a good representative sample, then its mean will lie very close to the mean of the population. ",4
10,"### summary of todayâ€™s class: excel tools & simple linear regression (slr)

### **working with excel**

we spent about an hour exploring excel's functionalities for plotting, statistical calculations, and visualizing regression.

1. **statistical calculations:**
    - calculated metrics such as mean (xë‰,yë‰), standard deviation (xstd, ystd), and extra means (xy_).
    - used these values with the slr formula (derived in the previous class) to estimate the parameters of the regression line.
2. **visualization:**
    - plotted the regression line and visualized the errors (differences between actual and predicted values).

### **simple linear regression (slr) in depth:**

1. **prediction range for slr:**
    - predictions are reliable only within the range of the training data. slr does not extrapolate well beyond this range.
    - for predictions outside this range, advanced techniques like time series analysis are recommended.
2. **understanding plots:**
    - **histogram:** shows the frequency of values within bins.
    - **scatter plot:** visualizes data points on a 2d plane using two coordinates.
3. **error analysis:**
    - after fitting the regression line, we calculated the error for each data point and plotted its histogram.
    - **key insight:** if the error histogram follows a normal distribution, it suggests the model captures the overall relationship between input and output well.
    - in cases where errors show a pattern (e.g., parabolic input-output relationships), the regression line is not a good fit.
4. **excelâ€™s data analysis toolkit:**
    - provides key regression metrics like standard error, r2, adjusted r2, multiple r, confidence intervals, p-values, and t-values for the parameters.
5. **evaluating model performance:**
    - r square(coefficient of determination): represents the proportion of variance explained by the regression model relative to the total variance.
    - **why r square is called r square?** in slr, r2 equals the square of the correlation coefficient between input x and output y (denoted by r). however, this equivalence does not hold for multiple regression.
6. **what is standard error?**
    - the standard error measures how much the sample mean deviates, on average, from the population mean. as the sample size increases, uncertainty in predictions decreases, leading to a more accurate model. this is why a lower standard error indicates more precise predictions and a better-fitting model.
    - **given 100 observations, should we treat it as 1 sample of size 100 or 10 samples of size 10?**itâ€™s better to treat it as 1 sample of size 100. the standard error, which is equal to sigma^2 / n, will be smaller with a larger sample size n. this results in higher precision and less variability in predictions.",5
11,"the notes focus on estimating the population mean using a single sample and the principles of linear and multiple linear regression. to estimate the population mean, the first step is to calculate the sample mean and assume it is close to the true population mean. the sampling distribution of the mean is used, which follows a normal distribution for large samples. the standard error, which measures the variability of the sample mean, is calculated by dividing the sample standard deviation by the square root of the sample size. confidence intervals are then used to estimate the range within which the population mean is likely to lie. for smaller sample sizes, the t-distribution is used instead of the normal distribution to account for additional uncertainty.

the notes also delve into linear regression, which models the relationship between a dependent variable and an independent variable. the regression equation is represented as ( y = î²_0 + î²_1x ), where ( î²_0 ) is the intercept and ( î²_1) is the slope. to determine if the regression model is valid, the significance of ( î²_1) is checked. if the confidence interval for ( î²_1 ) includes zero, the regression is not statistically significant. a low p-value, typically less than 0.05, suggests that ( î²_1 ) is significantly different from zero, validating the regression model.

in multiple linear regression, the model expands to include multiple independent variables. the equation is ( y = î²_0 + î²_1x_1 + î²_2x_2 + ... + î²_kx_k ). anova, or analysis of variance, is used to assess the significance of the model by comparing the variability explained by the regression to the variability due to error. this is done using the f-statistic, which is the ratio of the mean square regression to the mean square error. a significant f-statistic indicates that the model is a good fit for the data.",1
11,"in this session, the instructor discussed several key statistical concepts, including error distributions, standard error, sample means, and the central limit theorem (clt). a significant takeaway was that for regression models to accurately reflect the true trend in the data, the errors must be normally distributed. if the errors can be predicted, it suggests that the model has not fully captured the underlying trend of the sample, which may lead to biased results.

the session outlined how to estimate the population mean from a sample, which involves three main steps:

1.) first, calculate the sample mean, assuming it is close to 0. the standard error (sxbar) is found by dividing the population's standard deviation (sigma) by the square root of the sample size (n).
2.) next, compute the sampleâ€™s standard deviation, assuming it is a good estimate of the population's standard deviation.
3.) finally, calculate the standard deviation of the sample distribution, which is essential for determining the confidence interval (ci) that indicates where the population mean is likely to be. for example, a 95% ci means that 95 out of 100 sample means will fall within that range.

the session also delved into the concept of standard error, which reflects how much the sample mean might differ from the actual population mean. for a normal distribution, the standard error is calculated as
ð‘ /âˆšð‘›
, where ð‘  represents the sample standard deviation and ð‘› is the sample size.

following this, the instructor explained the differences between normal and t distributions. when the population standard deviation is known, the errors follow a normal distribution. conversely, if the population standard deviation is unknown and the sample size is less than 30, the data adheres to a t distribution. the z-statistic and t-statistic serve as test statistics for normal and t distributions, respectively.

the concept of the p-value was introduced, with a focus on its connection to the confidence interval. a low p-value indicates strong evidence against the null hypothesis, while a high p-value suggests weak evidence. the session also pointed out that if ð›½1 in the regression equation ð‘¦=ð›½0+ð›½1(ð‘¥) is statistically equivalent to 0, then the regression model lacks significance.

additionally, the session delved into multiple linear regression and anova (analysis of variance) as methods for comparing statistical equivalence among multiple averages. the f-statistic, calculated as ð‘€ð‘†ð‘…/ð‘€ð‘†ð¸, is utilized to evaluate the overall fit of a model in anova, determining whether the group means differ significantly.

in summary, the session offered a thorough understanding of statistical inferences, emphasizing essential concepts like standard error, p-values, confidence intervals, and the importance of regression models in data analysis. these concepts are crucial for effective data analysis and informed decision-making in statistics.",2
11,"today's class start with a question that we have only 1 sample (eg 30 observations) so, how to estimate population mean based only on 1 sample ? is it possible. one interesting thing that when errors become predictive then it is not a regression model. whatever the distribution of sample could be -> when we take their multiple sample then we always get normal distribution. let's suppose we have sample from a population then first step would be calculate the mean and(assume this mean is close to the population mean). if it is close to the population mean then sampling distribution of the mean is close normal distribution with mean and variance. the formula for calculating standard deviation of the sampling distribution of means is standard deviation of the population /root under number of observations in sample. further ahead we have learnt about the confidence interval which is basically the area under curve. for 95% confidence, the area under curve is 0.95 and for the observations that doesn't lie in this region is 0.025 each of side. one more important thing what we learnt that if number of observations is less than 30 then we do not get normal distribution and we use t distribution which is basically a continuous probability distribution that generalizes the standard normal distribution. the 95% confidence interval practically means that if we take 100 samples the mean of 95 of those samples will lies between that area. in the equation y=b0 + b1x if b1 is statistically equal to zero then we don't get any regression. if p- value < 0.05 then value of b1 is accepted and p-value also helps in the selection of the features. in the last minutes we also talked about the multiple linear regression which is a statistical technique that uses multiple independent variables to predict the value of a dependent variable and anova -> which is used to compare statistically equivalence of ""multiple averages"" simultaneously. the equation for mlr is y=b0 + b1x1 + b2x2 + b3x3 ....and if atleast on of coefficient is not zero then regression is possible. f-statistic = msr/mse.",3
11,"we started the lecture with a quick recap of the concepts from previous class- including regression coefficients and discussed about the sampling distribution of mean (histogram plots of the sample). we learnt that whatever be the distribution of the population, the sampling distribution of the sample mean will always be normally distributed. we donâ€™t have the opportunity to take multiple samples. we can practically take a single sample, which can have multiple observations. so, using these â€˜statisticsâ€™ values, we have to estimate the population parameters. so, while taking up a sample, we assume that it is a â€˜goodâ€™ representative of the population.
so, our first step towards estimating the population parameters, is to calculate the mean of sample, by assuming that it is close to population mean.
then we find out the sample standard distribution, assuming that it is close to population standard distribution. so, our ultimate goal is to find out an interval around the calculated value of sample mean, within which we can say with certain confidence level that our population mean would lie. for our sample observations, we make different categories/â€™binsâ€™ in which we divide the data values. then we plot a histogram (frequency distribution graph) for these bins. we observe that as we start decreasing the width of the class, the curve becomes smoother and smoother. this curve has a gaussian normal distribution.
if we divide the frequency values for various bins, by the total frequency, we get the probability that a certain data point will lie within that bin. the area under the curve within that interval, gives us the value of probability. 
so, when we say there is a 95% confidence interval, by it we mean that- if you take 100 samples, then 95 out of these would lie in that interval. also, the area under the curve within this interval would be 0.95. these plots are called â€˜probability density function(pdf)â€™.
if the number of observations is less than 30, then the distribution becomes a â€˜tâ€™ distribution, rather than a normal distribution.
the normal(or t) distribution is centered around the mean, and its tails extend to infinity on both the sides. for an interval, say (a,b), any value within this is â€˜not statistically differentâ€™ from the mean. but any value out of this interval can be said to be â€˜statistically differentâ€™ from the mean values. it may also come from an entirely different population. 
so, if we want to check whether a model is truly a slr model, we can check whether the coefficient of x, i.e. 'a' is statistically different from 0 or not. if it is, then we may conclude that the model is appropriate.
we consider that 0 is the at the centre of the distribution and then define an interval for 95% confidence, we want the value of the regression coefficient to lie outside this, so as to make our regression model valid.
twice the area under the curve from the point, which corresponds to the regression coefficient, to infinity is termed as the â€˜p-valueâ€™. so, for a 95% confidence interval, we want our p-value to be less than 0.05.
we can use this for multiple linear regression as well. those independent variables that have corresponding p-values, greater than 0.05(95% confidence) can be neglected in the regression expression.
lastly, we talked about anova- analysis of variance, which is a tool that tells you what is the statistic probability that at least one of the coefficients in mlr is non zero.
",4
11,"today's class dealt with the statistical principles underlying linear regression as well as the evaluation of the reliability of the regression coefficients. regression coefficients are estimates obtained from sample data and are not necessarily the population values. thus, how would one assess whether such estimates are reliable or not is using statistical concepts such as sampling distributions and confidence intervals, among others.the question explained that taking several representative samples from a population leads to a normal distribution formed by the means of the samples. that knowledge can then be used to obtain a standard error in expressing variability in regression coefficients. a topic covered is using confidence intervals in order to give some limits of expectation to how a true coefficient's value will be, as far as one is allowed to make estimations of those. if the confidence interval for a coefficient does not include zero, the coefficient is statistically significant and represents a valid model.
the p-value was defined as the probability that a regression coefficient is simply zero due to chance alone. a p-value less than 0.05 indicates that the coefficient is statistically significant and represents an important relationship between variables. sample size was discussed as being important. larger samples decrease variability, giving narrower confidence intervals and more dependable conclusions, whereas smaller samples increase uncertainty and make the intervals wider. the session also brought to the fore the fact that such statistical tools must be used to ensure that linear regression models are reliable and generalizable, representing true population relationships.",5
12,"feature encoding techniques

during this session, we reviewed several techniques used to transform categorical data into numeric representations, which is an essential step in getting data ready for machine learning models. the discussion began with the introduction to feature encoding techniques such as vectorization techniques and utilization of one-hot encoding, label encoding, integer encoding, binary encoding, frequency encoding, and target encoding.

one-hot encoding:
one-hot encoding converts categorical features into vector where each category has a separate column. one-hot encoding is particularly helpful when encoding the input features (x) in the case of multiclass or multilabel problems. one-hot encoding is less ideal for encoding target variables (y). one of the primary disadvantages of one-hot encoding is that it can cause the curse of dimensionality. for example, if you have a variable such as pincode with thousands of distinct values, applying one-hot encoding will lead to a dimensionality explosion, making the dataset sparse and computationally intensive.

label encoding vs. integer encoding:
label encoding gives each category a specific integer. the method is well-suited to encoding target variables (y), especially for classification tasks, but may not be suitable for input features where the model may mistake the numerical ordering as an ordinal relationship. on the other hand, integer encoding is used when the target variable is naturally ordinal instead of nominal, such that the order of the categories holds important meaning.

binary encoding:-
binary encoding, or pseudo one-hot encoding, offers a more compact representation compared to one-hot encoding. by converting categorical values into binary code, a few columns (for example, three columns) can represent multiple classes (up to eight classes in this example). this method helps mitigate the issue of high dimensionality while preserving the distinctiveness of each category.

frequency encoding:
frequency encoding substitutes each class with its frequency in the dataset. while it reduces the representation, it is not necessarily good for target variables because two classes can have the same frequencies, causing possible loss of useful information.

target encoding:
target encoding allocates to each category the target variable's mean for that category. target encoding can be especially useful when there is high correlation between the target variable and the categorical feature. however, caution needs to be exercised to prevent data leakage while training the model.

simplification strategies:
tackling complicated regression questions by converting them into classification questions using methods such as feature binning is common. also introduced briefly was converting text into numeric vectors through vectorization methods as a key step in handling unstructured data.

in general, the suitable encoding method is determined by the type of data, if the variable is utilized as a feature or target, and possibly the effect it may have on dimensionality. every approach has its strengths and weaknesses, and the proper choice is critical to constructing efficient and effective machine learning models.",1
12,"todayâ€™s lecture started off with the birdâ€™s eye view of the next one month, and we briefly discussed about the project. we then started off by discussing about feature encoding. both the independent or dependent variables might need to be encoded in order to convert them into numerical forms for training the ml models. basic feature encoding includes one-hot encoding, which involves vectorisation i.e. creating vectors of the size equal to the number of classes and then assigning 1 to one of the vector components based on the class. we then talked about different types of encoding like label encoding, which involves taking all possible values of the categorical variable and assigning integers to them sequentially. this kind of encoding works for the dependent variable, but not for the independent variable as the independent variable defines the model and hence any kind of ordering or hierarchy in its values will lead to errors. integer encoding has an inherent sense of order, where the assigned values may not be sequential and may have some meaning. one hot encoding has an issue that it increases the number of columns, and thus invites the curse of dimensionality. hence, one hot encoding should be used carefully and with data having lesser number of categories. we also discussed binary encoding, which uses bits for encoding. frequency encoding involves assigning the frequency of a class as its encoded value. this method is not very useful for the dependent variable as we need unique values for categorising y, whereas frequency encoding may lead to assigning same values if the frequency of two classes is the same.  we also have target encoding, where the average of the y values for a given class, are assigned to all the class values in the data columns. we then moved on to problems where we want to convert continuous data into discrete data such as height, weight, etc. in such cases, we might be moving on from regression to classification, which could lead to improvement in our metrics. we then talked about text processing, where we discussed large language models, which are examples of the manner in which statistical processing can generate a deterministic output. text processing basically involves converting text to numbers so that processing is efficient. usually the methodology includes dropping some very common words called stop words, which do not add to the meaning of the sentence. we then create a dictionary of words and then express the document using the dictionary.  ",2
12,"feature encoding. when either the dependent variable or some of the independent variables are categorical then, they have to be appropriately encoded prior to being used for training ml models. the project will be related to assessment of exercises which we have done. label encoding, one hot encoding, binary encoding, integer encoding, frequency encoding, target encoding. multiclass problem- mnist dataset- there are 10 classes(0-9), multilabel problem- there are multiple labels associated with a single object, a image with both cat and dog. how we encode a variable depends on domain knowledge. label encoding- take categorical variables and assign numerical values to these values-this for nominal variable. we can use label encoding to encode output variable, but we should avoid using it on input variable. integer encoding is for ordinal variable- the numbers carry a value and have a meaning. one hot encoding- converting the output into a vector of dimension of number of classes. this increases the number of columns in data and introduces the curse of dimensionality. they choice of encoding depends on number of classes. one hot encoding can be used for nominal variables and classes are not too many. binary encoding- it is just a different notation of one hot encoding. we are converting vectors in one hot encoding into binary values(pseudo one hot encoding).  frequency encoding- the category values are replaced with its frequency in the column. class takes on value of occurance we have to check whether two classes have same frequency. target encoding- we collect all y values corresponding to a particular class take their average and use this to represnt x in input. this is used to encode the input variables. for encoding input variables, we can use one hot encoding generally. for output variable encoding we can use other techniques. eda is important as if we are not able to capture what data is properly we will not be able to perform the further processes correctly.  we need to think multiple times before applying a particular encoding. feature binning- many times we want to convert a continous problem to a discrete problem. we will divide the continous variable into bins and then assign categories to this. this now becomes a classification problem. it can be used for several reasons including problem simplification, reducing the impact of outliers and noise in the data, handling non linear relationships. how to process text data- natural language processing(nlp)- examples of the manner in which statistical processing can generate deterministic output- code generation. how to convert text into numbers so that analysis is useful. method1- drop the common words(stop words); convert to lower case. - create a dictionary. -express the document using the dictionary. in a sentence we cant know exact meaning until we know the context in which it is being used as each word might have multiple uses. ",3
12,"this lecture was mostly based on feature encoding. feature encoding is done when the dependent or independent variables in your data is categorical. you need to convert this categorical data type to numerical so that it can be processed by the computer. there are different ways by which this can be done. we can expect two types of output through this. one is that there are multiple classes, out of which there is only one class which the y represents at a time. this is known as â€˜multiclass problemâ€™. another is the one in which two or more labels can be present in a single y value. this is known as â€˜multilabel problemâ€™. for example of multiclass problems, we can consider a situation in which the y value is some color. so, the color can be any of the decided number of color options. we can use label encoding for this type of problem. it just assigns labels to each of the possible options (in this case, the colors). these labels donâ€™t have any inherent value, they just represent the different classes. 
for multilabel problems, we can consider the situation in which the y value is an image which consists of both dog and cat. so, here for single y we can associate it with two different labels, cat and the dog.
for such data sets where y values are nominal, we use label encoding. 
but, if the y values have ordinal level of measurement, then we can use integer encoding. it assigns integer values to each of the class and these integer values also have inherent value associated with them, by which we can compare between two distinct values. 
the other types of encoding methods include:
1)	one- hot encoding: in this, we assign values of 1s and 0s to all the available classes. we basically create vectors, whose components can be either 1 or 0, depending on the actual class. so, for one feature, we create n different columns. â€˜nâ€™ depends on the number of classes associated with that feature. this increases the dimensionality. so, it shouldnâ€™t be used when there are large number of classes associated with a feature.
2)	binary encoding: it is also known as â€˜pseudo one-hot encodingâ€™. instead of using separate column for each class and unnecessarily increasing the dimensionality, we can reduce the number of columns used by using a combination of 1s and 0s to represent the different classes, instead of using just one column with 1 for each class.
3)	frequency encoding: in this type of encoding, the value associated with each class is just equal to its frequency in the data. this is generally not used for y variable as it may happen that two different classes in y have the same frequency. so, in this case, we wonâ€™t get â€˜distinctâ€™ label for each class.
4)	target encoding: in this, the value for each class is just the average of the corresponding y values associated with all the occurrences of that class.
5)	feature binning: in this, the numerical values are categorized (divided into categories) to turn the problem into a classification problem.
next, we just discussed a bit on how we can process text data. all the above techniques were used when the features were already known to us. but in case of text data, we donâ€™t have the features, we need to create them.
also, we need to convert a word to some numerical value, so that the computer can deal with it. a problem which can be encountered here is that one word can have multiple meanings in english language. so, we should be very careful while determining the meaning of a statement. the context of the statement should be clear before making any kind of predictions.
",4
12,"today we started the discussion with feature encoding methods. when either of the dependent or independent variables are categorical we have to use feature encoding techniques to convert them into numerical forms with which we can work with. 
one hot encoding method creates columns which are equal to the number of unique categorical labels in the original data and fills each of these columns with respective true(1) and false(0)  values. this method should be used very cautiously as it increases the dimensionality of the data significantly and becomes cumbersome to work with. for one hot encoding treated data, the classification models should be tree based or neural network based as logistic regression cannot handle such data. to overcome the curse of dimensionality of one hot encoding we can follow another method which is similar to onh but uses bits instead of true or false values to encode the categories for example for categories of weather related data like hot, very hot the encodings can be 001 and 011 respectively. other most widely used data encoding method is label encoding in this integers are allotted to different categories present in the columns this is best suited for nominal scale entries of the columns as in this method there is no ordering of the integer values that is for example red:1 and blue:2 does not mean that blue is greater than red and its also important to note that this method works well for y and using this method for encoding x features must be avoided. integer encoding is another type of encoding which is very similar to label encoding but the only difference is the integer values have a sense of ordering and thus is suitable for ordinal scale valued columns. frequency encoding uses the number of occurrences of  particular category in a column and sets as the label for that particular category should only be used for features and not target variables, because there might be possible that two different categories might have same frequency but this issue some how doesn't affect the feature variables and thus can be used. target encoding this also used for encoding the feature variables where for a particular category all the corresponding y values are noted and then average of these noted values is set as the category label. we then discussed feature binning where the features with continuous values in discretised by making bins or categories so that the regression problem can be converted into classification problem , such conversion might be very useful when the actual data variance is very high and the r squared value of the regression model is very low. began discussion about processing text and how the text data is converted into numerical data which can be used to do some useful analysis. the common stop words are removed and dictionary of all the other words is made and a vector is associated to each sentence of the document based on whether the given word from the dictionary made is present in that particular sentence or not.",5
13,"sir started the class by taking in questions submitted via tha form futher logistic units were discussed. starting with conditional probability as it is the outcome of a logistic unit which is in turn linked with a class. probability of the predicted outcomes and the known outcomes which are know from the dataset are compared further sir linked the probability of y given x with a binomal distribution. here we are interested in maximizing probability of the predicted if the label predicted matches that from the dataset for which we define likelyhood which is the product of probabilities. a log likelyhood is considered because dealing with product is difficult for optimization and as minima are more stable and easier to compute so now we consider negative of the log likelyhood which is to be minimized with respect to the weights. the optimisation is carried out by gradient descent. also a case were the boundary may be non linear was considered where we were encouraged to use polynomials and other non linear functions with emphasis on preventing overfitting while maintaining generalization which is done by understanding the nature of the population. also it was explained how every regression method can be used for classification. futher confusion matrix were introduced here the difference between false negatives and false positives was discussed. here the concept of data imbalance was discussed and its interpretation with accuracy, precision which is defined as how many of the event you have detected how many are correct and recall which is defined as how many of the class events how many of those have you detected. class ended with a discussion on submissions in exercise 1",1
13,"in the class first we started by taking some doubts in which we learned about the standard deviation of mean of samples which is equal to sigma divided by root x and its square is the variance of mean whereas sigma square is the variance of sample. can we learn about different methods like sturges theorem to calculate the number of bins in the histogram.
then we started with logistic regression . there are two different classifications then it is easy to classify them when the clusters are far away and when the clusters are not far away then we use probability like if probability is greater than 0.5 then it is x1 otherwise it is x2. our goal is to minimise the error between the predicted value and the actual value of all the samples. then we learnt thatp(y/x) = sigma(wtx+b). we want to maximise the likelihood of predicted outcomes being closed to targets if we get 100 percent correct classification of training data then there can be higher error in the test data we need to generalise the boundary of classification. then we learnt about confusion matrix and how to calculate it then we learn some definitions like accuracy which is how often is the classifier correct precision of the events you have detected like how many have you detected correctly recall is for the class events like how many of a class 1 events have you correctly detected. and f1 value is just the harmonic mean of precision and recall so that we get a performance mean of the precision and recall. then then our assignments were being assessed and the reviews were given about them in the later part of the class. ",2
13,"in the beginning of the class we discussed a few questions raised by students about the previous topics discussed in class: learnt about expectation algebra, histogram showing a particular distribution and how many bins to consider, when to consider a division between two clusters and what boundary to consider, model which changes regression expression according to ranges of x either by going for two different forests or some other method. 
after that we continued our discussion from previous class about logistic unit and logistic regression. soft max function redistributes numbers around 0 and 1 so that they can be used. if p>0.5 then we push it into one class and otherwise, we push it to another class. then we saw the notations of logistic regression with 3 model examples. y1 and y2 are known outcomes, which can also be referred to as 't'. then we learnt about how to calculate the weights wi. we do this such that the likelihood of getting desired targets is maximized (another way of saying minimize the differences). y is known as the target and denoted by t.  we find likelihood using the function, and maximising l, we get the desired weights wi. the expression for l should satisfy the two requirements, that when t=1 we would be getting maximum p and when t=0 we should be getting maximum (1-p). likelihood is a product based function, but it's easier to deal when in summation form, so we take log of likelihood. it's better to do this way, because maxima are always unstable, while minima are stable points. use gradient descent method to do so. also, we defined something like n ( learning rate). then we saw the confusion matrix (cases of tn, fn, fp and tp, and the examples that are associated with them). false negatives are more disastrous in cases of scenarios like being sick but told that you're not. definition of accuracy: being able to correctly identify the situation, how close the measurement is to the actual value. definition of precision: of the events that you detected, how many did you do correctly, how close the measurements of the same observations are to each other. definition of recall: of a specific class, how many could you correctly identify. f1 value is defined as the harmonic mean value of precision and recall. 
in the end of the class we discussed the topics of assignment e1, and the common mistakes most people made, like directly using kurtosis function from excel without checking if we need the positive kurtosis or negative kurtosis, and in q1 which involved comparison between the two models, the slope terms weren't same which most people just subtracted intercept term from predictions and used it for comparison. general: excel file should be submitted along with calculations and formulas, not just values. and put in more effort during documentation and organize well.",3
13,"at the beginning, sir explained what are the ways to determine number of bins in order to get an idea of histogram (there are some formulas to calculate bin number or bin width, but mostly it depends upon how closely we want to get insights from the data(bird-eye view or precise view)). clustering is also useful in cases, even if there is an overlap between two clusters. then topic resumed to logistic regression and sigmoid function. if p(x/y)>0.5, push the outcome to class 1; else, push the outcome to class 2.  learnt about the notations (p, t, w, etc.) in logistic regression. in order to classify (or to come upon with a prediction/outcome), the given things are 1. number of observations(n) and 2. corresponding targets; and our goal is to 1. calculate the weights (w_i) and 2. maximizing l (likelihood) such as the desired weights outcome expression for l satisfies requirements. then we discussed about log likelihood, gradient descent method to solve it (objective function is to minimize log likelihood). then we discussed about confusion matrix (how false positive and false negative affect the outcomes; false negative is disastrous some times). then we saw some quality metrics to assess the logistic regression (like accuracy, precision, recall, f1-metric, etc.); f1 is better quality metric as compared to accuracy( accuracy may fail in case of imbalanced clusters). at the end, tas presented the assessment and insights from of exercise-1. ",4
13,"we started with logistic regression and found out how we could find the weights that will drive our predictions. our aim is to bring our predicted results as close to the real targets as possible. when the actual value is 1, we want our predicted probability to be high; if it is 0, then low. this requires taking that into consideration for all of the observations in our training data.

since dealing with product terms can grow complicated, we take the log of the likelihood function, and turn multiplication into addition, so turning things over is significantly easier. maximizing this gives us the minimum amount of error possible, and hence finds us the best fit for our model.

for analysing whether or not our model was a good fit, we use a confusion matrix, which categorizes predictions into four groups:

- true negatives (tn):correctly predicted negatives 

- false positives (fp):positives wrongly predicted (false alarms) 

- false negatives (fn):negatives wrongly predicted (missing actual positives) 

- true positives (tp):correctly classified as positive 

by using all of this, we get a number of valuable performance metrics: 
- the ratio of the correctness of outputs from the model for majority of the cases. 
accuracy= (tp + tn)/total count of observations

- precision:when the model indicates something is positive, how frequently is it truly accurate? 

- recall:among all the true positive instances, how many did the model accurately recognize? 

- f1-score:a compromise between precision and recall. unlike accuracy, which may be deceptive in the presence of imbalanced data, the f1-score accounts for both precision and recall, avoiding misplaced confidence in the model's effectiveness.

in summary, precision indicates the trustworthiness of our positive predictions, recall measures our effectiveness in identifying positives, and the f1-score guarantees we remain realistic about our accuracy.",5
14,"at the start of class, sir discussed the midsem question paper. sir explained the question paper and shared the approach of solving the question and problems associated with it. 
first step is exploratory data analysis. we first try to find the missing values in the data. there were four rows with missing values. one way to solve this is to drop the four rows. we can justify it because we have 2371 rows. another way to solve it is to fill in the missing values. plotting the parameters, we noticed that there is randomness in the data. the histogram would show a normal distribution. this suggests that even if we replace the missing data with the mean it would be ok. before filling the value, we check the range of value of parameters to check if we need to do standardization or normalization. we find that for some parameters the range is very small while for some the range is really high. we still don't have visualization for outliers. for this we use the box-plots. we first use box plot on all the columns to check if any column is different. then we check for individual outliers (surprisingly there are no outliers). for the column with ailments, we use a bar-chart or pie-chart. we notice that heart-diseases is under-represented. one way is oversampling or under sampling. as the difference is very large for the under sampled data, we can conclude that the model would not be able to heart-disease properly. 
heatmap is the first step in finding the correlation between the columns. we notice that there is no correlation between the columns. the point is that doing pair-wise correlation is suicidal. we need additional steps to decide if there is any correlation between columns. the problem here is of multi-collinearity. we can use variance inflation factor analysis. 

when we have two entirely different samples, we try going for kde plots. after creating the kde plots, we notice that the distribution of each column is different for both the samples. this suggests that the model fitted on one sample cannot be used on the other sample. this means that both the samples come from different populations. 

by the end of the class we started with a new topic: the curse of dimensionality. too less data chasing too much features. we don't have enough data for representation of the columns. this is a problem when dealing with high dimensional data. this problem increases sparsity. this results in loss in the discriminative power of the model. the consequences are: overfitting (model stick to the given data and are not much reliable). for this we can try to reduce the number of features. we can do feature selection, regularization or increase the amount of data. these solutions are easier said than done. we checked for variance inflation factor. if a feature has a vif more than 10, then it can be expressed as a linear combination of other features. we can hence eliminate the features with vif greater than 10. 
",1
14,"in this class, we mainly discussed the midsem paper. sir showed us how we could have attempted the questions. the first part involved performing eda on the given data set to find out the missing values and outliers. the early exploratory stage involves creating scatter plots and histograms of distribution of the parameters to understand how they are distributed. next, to handle the missing values, we can either drop them completely or we can approximate them to some value. it depends on the trend in the data (which can be seen through the scatter plots), whether to use simple mean or some moving average. in this case, the data was randomly spread in the entire region, hence we could use the mean value of each column to fill the missing values. 
there can be mainly two types of problems- problem within the column, which involves missing values and the problem across the columns, which involves significant differences in the values of the parameters in the two columns. in such a normalization is required, to ensure that a particular class is not under-expressed/ suppressed by the others.
box plots give us idea about the outliers. in this case, there werenâ€™t any outliers.
we can visualize the categorical data using bar charts/ pie charts.
 correlation heat maps can also be used to check for the relations between the different parameters. the problem with this is that it only checks for the correlation between a feature and only one other feature at a time. however, it is possible that a feature depends on multiple other features and is a linear combination of those. correlation maps do not provide this information. hence, they are sufficient but not necessary check of correlation.
for solving this problem, we use â€˜variance inflation factorâ€™(vif). it compares each feature with all other features taken together. to find out the vif, we express each feature as a linear combination of the others and fit a regression model on it. we then find the r2 values for each of the feature. vif is then calculated as 1/(1-r2). so, as r2 increases, vif also increases. hence, if r2 values is high, it indicates strong correlation among the features.
if vif of a feature is >10 then that feature can be expressed as a linear combination of the others, hence we remove that feature column from our data.
in this way, we reduce the dimensionality of the data set, as high dimensionality is a curse. it spreads out the data more, thereby making it difficult to find out important patterns from the data.
the next part in the question was to check whether the derived regression model worked well for another different data set as well. by plotting kde plots and analyzing the descriptive statistics of the new data set, we observed that it had completely different distribution than the original data set, which suggested that it belonged to a completely different population. this was also evident by the looking at the values of accuracy, precision, recall and f1 scores of the model, fitted to the new data. hence, the original model did not work well for the new data, which belonged to a different population.
",2
14,"midsem discussion:
we discussed about the questions that came up in midsems. the first question was all about exploratory data analysis (eda), we donâ€™t have to create an entire model as such, but we needed to analyze all the data that was given to understand what itâ€™s all about. stuff like, how many rows, how many columns, basics. and then we moved on to missing rows, in this case, since the number is very small, we could drop those rows. else, we could replace those with the mean of the data. this can only be done when data has no particular trend, and it has missing values. we observed a scatter plot to see that. then we saw the histogram as per each column. some are bimodal, some are unimodal, but those didnâ€™t give many insights. then we plotted a pie chart. also, we saw the heat correlation map, it shows a very ideal case, no correlation between the columns so we donâ€™t have to drop anything. however, this analysis is necessary, but not sufficient to say that the columns are not related. sometimes there might be a more serious problem of multi collinearity. 
for the second part of the question we learnt that the main reason why the model created wasnâ€™t working for the new data, was because it was for a different population. performing descriptive statistics on both the data side by side revealed this to us.
important points learnt:
â€¢	on any raw data, perform eda to get a complete understanding of data. 
â€¢	if the data is random, if there is no particular trend, only then you can replace the missing values with mean of the data. 
â€¢	if heat correlation map indicates correlation, it has correlation. but if it doesnâ€™t, then thatâ€™s not a good enough conclusion to say that the columns are not related. sometimes there might be a more serious problem of multi collinearity. so itâ€™s necessary, but not sufficient. ",3
14,"in today's lecture, sir explained how we were supposed to approach midsem problem. we were required to essentially perform eda and then try to fit a classification model (note clustering was not required at all). in order to perform eda, we had first find missing value and then drop or replace it(dropping values was not a big deal as only 4 rows had missing values in 2000+ rows). we then had to plot box plot to detect outliers but there was no outlier. check if there was a need to normalise the data by checking the range of parameters.(large difference in range proposes a need for normalisation), plot scatter plots and histogram, and state descriptive statistics, generate heatmap of all the parameters (there was not much linear correlation between them, but there was possibility of multi-collinearity) count the number of cases of each ailment (heart diseasewas under represented); after all this, fitting a random forest classification model was required. then we had to plot kde plots for both the data set (q1 and q2) to see whether the distribution is same in both cases or not. the kde plots were not same and classification was not predicting the disease properly which suggests that the dataset were essentially of two different populations.
then the tas discussed e3 evaluation with us. at last, sir gave an introduction and explained 
 about the topic the curse of dimensionality. what are the reasons for its occurance and how to tackle it? dropping features, features selection, dimensionality reduction, regularisation, etc.",4
14,"sir started with discussion of midsem question paper. solution of midsem problem: 1. perform exploratory data analysis: we need to find problems with data set. find missing values in the dataset. we can drop rows with missing values if there are very less such rows and we have more data. or we can replace the missing values with mean or median based on distribution of data. understand the distribution of data within the columns and understand the distribution across the columns. normalise the data as required. if we have categorical data we can plot a histogram or pie chart. if there are very less data points of a particular category, just create a model that cannot detect that particular category. it is better not predict such categories with very less points. we have advantage if we have independent columns. in exam, the validation dataset were derived from a different dataset. we need to fit random forest to this kind of data. then sir discussed about solution to both parts. then one of the teaching assistant gave presentation about e3 review. then sir explained about curse of dimensionality. it describes the problems that occur when the number of features(dimensions) in a dataset increases significantly. we can address this by dimensionality reduction, feature selection, regularisation, increase in the amount of data. ",5
