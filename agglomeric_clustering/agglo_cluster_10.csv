SerialNo,Session_Summary,BERT_Feature_0,BERT_Feature_1,BERT_Feature_2,BERT_Feature_3,BERT_Feature_4,BERT_Feature_5,BERT_Feature_6,BERT_Feature_7,BERT_Feature_8,BERT_Feature_9,BERT_Feature_10,BERT_Feature_11,BERT_Feature_12,BERT_Feature_13,BERT_Feature_14,BERT_Feature_15,BERT_Feature_16,BERT_Feature_17,BERT_Feature_18,BERT_Feature_19,BERT_Feature_20,BERT_Feature_21,BERT_Feature_22,BERT_Feature_23,BERT_Feature_24,BERT_Feature_25,BERT_Feature_26,BERT_Feature_27,BERT_Feature_28,BERT_Feature_29,BERT_Feature_30,BERT_Feature_31,BERT_Feature_32,BERT_Feature_33,BERT_Feature_34,BERT_Feature_35,BERT_Feature_36,BERT_Feature_37,BERT_Feature_38,BERT_Feature_39,BERT_Feature_40,BERT_Feature_41,BERT_Feature_42,BERT_Feature_43,BERT_Feature_44,BERT_Feature_45,BERT_Feature_46,BERT_Feature_47,BERT_Feature_48,BERT_Feature_49,BERT_Feature_50,BERT_Feature_51,BERT_Feature_52,BERT_Feature_53,BERT_Feature_54,BERT_Feature_55,BERT_Feature_56,BERT_Feature_57,BERT_Feature_58,BERT_Feature_59,BERT_Feature_60,BERT_Feature_61,BERT_Feature_62,BERT_Feature_63,BERT_Feature_64,BERT_Feature_65,BERT_Feature_66,BERT_Feature_67,BERT_Feature_68,BERT_Feature_69,BERT_Feature_70,BERT_Feature_71,BERT_Feature_72,BERT_Feature_73,BERT_Feature_74,BERT_Feature_75,BERT_Feature_76,BERT_Feature_77,BERT_Feature_78,BERT_Feature_79,BERT_Feature_80,BERT_Feature_81,BERT_Feature_82,BERT_Feature_83,BERT_Feature_84,BERT_Feature_85,BERT_Feature_86,BERT_Feature_87,BERT_Feature_88,BERT_Feature_89,BERT_Feature_90,BERT_Feature_91,BERT_Feature_92,BERT_Feature_93,BERT_Feature_94,BERT_Feature_95,BERT_Feature_96,BERT_Feature_97,BERT_Feature_98,BERT_Feature_99,BERT_Feature_100,BERT_Feature_101,BERT_Feature_102,BERT_Feature_103,BERT_Feature_104,BERT_Feature_105,BERT_Feature_106,BERT_Feature_107,BERT_Feature_108,BERT_Feature_109,BERT_Feature_110,BERT_Feature_111,BERT_Feature_112,BERT_Feature_113,BERT_Feature_114,BERT_Feature_115,BERT_Feature_116,BERT_Feature_117,BERT_Feature_118,BERT_Feature_119,BERT_Feature_120,BERT_Feature_121,BERT_Feature_122,BERT_Feature_123,BERT_Feature_124,BERT_Feature_125,BERT_Feature_126,BERT_Feature_127,BERT_Feature_128,BERT_Feature_129,BERT_Feature_130,BERT_Feature_131,BERT_Feature_132,BERT_Feature_133,BERT_Feature_134,BERT_Feature_135,BERT_Feature_136,BERT_Feature_137,BERT_Feature_138,BERT_Feature_139,BERT_Feature_140,BERT_Feature_141,BERT_Feature_142,BERT_Feature_143,BERT_Feature_144,BERT_Feature_145,BERT_Feature_146,BERT_Feature_147,BERT_Feature_148,BERT_Feature_149,BERT_Feature_150,BERT_Feature_151,BERT_Feature_152,BERT_Feature_153,BERT_Feature_154,BERT_Feature_155,BERT_Feature_156,BERT_Feature_157,BERT_Feature_158,BERT_Feature_159,BERT_Feature_160,BERT_Feature_161,BERT_Feature_162,BERT_Feature_163,BERT_Feature_164,BERT_Feature_165,BERT_Feature_166,BERT_Feature_167,BERT_Feature_168,BERT_Feature_169,BERT_Feature_170,BERT_Feature_171,BERT_Feature_172,BERT_Feature_173,BERT_Feature_174,BERT_Feature_175,BERT_Feature_176,BERT_Feature_177,BERT_Feature_178,BERT_Feature_179,BERT_Feature_180,BERT_Feature_181,BERT_Feature_182,BERT_Feature_183,BERT_Feature_184,BERT_Feature_185,BERT_Feature_186,BERT_Feature_187,BERT_Feature_188,BERT_Feature_189,BERT_Feature_190,BERT_Feature_191,BERT_Feature_192,BERT_Feature_193,BERT_Feature_194,BERT_Feature_195,BERT_Feature_196,BERT_Feature_197,BERT_Feature_198,BERT_Feature_199,BERT_Feature_200,BERT_Feature_201,BERT_Feature_202,BERT_Feature_203,BERT_Feature_204,BERT_Feature_205,BERT_Feature_206,BERT_Feature_207,BERT_Feature_208,BERT_Feature_209,BERT_Feature_210,BERT_Feature_211,BERT_Feature_212,BERT_Feature_213,BERT_Feature_214,BERT_Feature_215,BERT_Feature_216,BERT_Feature_217,BERT_Feature_218,BERT_Feature_219,BERT_Feature_220,BERT_Feature_221,BERT_Feature_222,BERT_Feature_223,BERT_Feature_224,BERT_Feature_225,BERT_Feature_226,BERT_Feature_227,BERT_Feature_228,BERT_Feature_229,BERT_Feature_230,BERT_Feature_231,BERT_Feature_232,BERT_Feature_233,BERT_Feature_234,BERT_Feature_235,BERT_Feature_236,BERT_Feature_237,BERT_Feature_238,BERT_Feature_239,BERT_Feature_240,BERT_Feature_241,BERT_Feature_242,BERT_Feature_243,BERT_Feature_244,BERT_Feature_245,BERT_Feature_246,BERT_Feature_247,BERT_Feature_248,BERT_Feature_249,BERT_Feature_250,BERT_Feature_251,BERT_Feature_252,BERT_Feature_253,BERT_Feature_254,BERT_Feature_255,BERT_Feature_256,BERT_Feature_257,BERT_Feature_258,BERT_Feature_259,BERT_Feature_260,BERT_Feature_261,BERT_Feature_262,BERT_Feature_263,BERT_Feature_264,BERT_Feature_265,BERT_Feature_266,BERT_Feature_267,BERT_Feature_268,BERT_Feature_269,BERT_Feature_270,BERT_Feature_271,BERT_Feature_272,BERT_Feature_273,BERT_Feature_274,BERT_Feature_275,BERT_Feature_276,BERT_Feature_277,BERT_Feature_278,BERT_Feature_279,BERT_Feature_280,BERT_Feature_281,BERT_Feature_282,BERT_Feature_283,BERT_Feature_284,BERT_Feature_285,BERT_Feature_286,BERT_Feature_287,BERT_Feature_288,BERT_Feature_289,BERT_Feature_290,BERT_Feature_291,BERT_Feature_292,BERT_Feature_293,BERT_Feature_294,BERT_Feature_295,BERT_Feature_296,BERT_Feature_297,BERT_Feature_298,BERT_Feature_299,BERT_Feature_300,BERT_Feature_301,BERT_Feature_302,BERT_Feature_303,BERT_Feature_304,BERT_Feature_305,BERT_Feature_306,BERT_Feature_307,BERT_Feature_308,BERT_Feature_309,BERT_Feature_310,BERT_Feature_311,BERT_Feature_312,BERT_Feature_313,BERT_Feature_314,BERT_Feature_315,BERT_Feature_316,BERT_Feature_317,BERT_Feature_318,BERT_Feature_319,BERT_Feature_320,BERT_Feature_321,BERT_Feature_322,BERT_Feature_323,BERT_Feature_324,BERT_Feature_325,BERT_Feature_326,BERT_Feature_327,BERT_Feature_328,BERT_Feature_329,BERT_Feature_330,BERT_Feature_331,BERT_Feature_332,BERT_Feature_333,BERT_Feature_334,BERT_Feature_335,BERT_Feature_336,BERT_Feature_337,BERT_Feature_338,BERT_Feature_339,BERT_Feature_340,BERT_Feature_341,BERT_Feature_342,BERT_Feature_343,BERT_Feature_344,BERT_Feature_345,BERT_Feature_346,BERT_Feature_347,BERT_Feature_348,BERT_Feature_349,BERT_Feature_350,BERT_Feature_351,BERT_Feature_352,BERT_Feature_353,BERT_Feature_354,BERT_Feature_355,BERT_Feature_356,BERT_Feature_357,BERT_Feature_358,BERT_Feature_359,BERT_Feature_360,BERT_Feature_361,BERT_Feature_362,BERT_Feature_363,BERT_Feature_364,BERT_Feature_365,BERT_Feature_366,BERT_Feature_367,BERT_Feature_368,BERT_Feature_369,BERT_Feature_370,BERT_Feature_371,BERT_Feature_372,BERT_Feature_373,BERT_Feature_374,BERT_Feature_375,BERT_Feature_376,BERT_Feature_377,BERT_Feature_378,BERT_Feature_379,BERT_Feature_380,BERT_Feature_381,BERT_Feature_382,BERT_Feature_383,kmeans_cluster,TSNE_1,TSNE_2,agglo_cluster
8,"explained logistic regression, which is used for binary classification. we discussed clustering, a method for grouping similar data points. true positive and true negative concepts were highlighted for model evaluation. outlier detection techniques were introduced to identify rare occurrences in data. the lecture also covered loss functions, which guide model training. finally, we explored different approaches to solving logistic regression effectively. ",0.009866362,-0.03688608,-0.0041121827,0.043050233,0.053560887,-0.03780793,0.087067604,-0.023458127,-0.048390746,-0.025301697,0.059110194,-0.01979867,0.09548435,0.012106358,-0.030739823,-0.04849238,0.01920925,-0.017735109,-0.076339304,-0.05015123,-0.050496712,0.07610511,-0.022192344,0.089748435,0.026644703,0.0003597672,0.039739244,0.03802989,-0.066697314,-0.059342507,-0.07413062,0.058044966,0.040046606,0.025711678,-0.015273428,0.009545746,-0.014251442,0.051625196,-0.036390428,-0.015684187,-0.028751086,-0.073796704,0.036611363,-0.026160063,0.0159563,0.017508294,-0.053893518,0.0057435464,0.04195259,-0.029242393,-0.010217176,0.021317158,0.004590876,0.023459481,-0.10010228,-0.038244586,-0.0045513283,-0.064142324,0.016359335,-0.047517028,0.07458815,-0.019012423,0.0037707274,-0.018456819,0.013832521,0.018485913,-0.015879188,0.052776534,0.069879696,0.016726447,0.048891988,-0.009237458,-0.029766537,0.045080382,0.009808464,0.012918767,0.08553422,0.020684412,-0.027563816,0.051630422,-0.017500523,0.029926518,0.004292156,0.055167396,0.105855905,-0.071661234,0.0383958,0.049589258,-0.0543704,0.026600106,-0.058235273,0.061378535,0.07558221,-0.037789285,0.0046513365,-0.03976791,-0.0092014,-0.046006225,0.10172923,0.070474625,-0.08562676,0.06954508,0.027807418,-0.031846337,0.049378484,-0.045516234,0.06744866,-0.0032474613,0.07646932,-0.048754692,-0.009173054,0.054710478,-0.06599554,-0.037477046,0.055951104,-0.022065427,0.11458594,0.054148376,-0.10082154,0.05888671,-0.007908411,0.0338227,0.049743958,0.0045165885,0.06522419,-0.0695151,-0.08500659,1.6073157e-33,-0.0033216707,-0.015364357,0.028546924,-0.093766056,0.046518542,-0.021202564,-0.1293469,0.036896776,0.034701783,0.0017933479,0.0055455673,-0.019169644,0.059757378,0.08436371,0.06516955,0.016788334,-0.00927048,0.09616842,-0.029727932,-0.0727084,-0.092413284,-0.0844443,0.033532757,-0.072137624,-0.061446257,0.044166584,0.065751776,-0.04317753,0.019050902,-0.013544646,0.06937704,0.025987105,-0.02168862,-0.033742268,0.07577538,0.046054825,-0.03174135,0.01872462,0.044580232,0.008862281,-0.13081212,-0.0003426588,-0.02405093,-0.11102537,0.057137657,0.11341623,0.07149365,-0.0937007,-0.017114198,-0.07065811,-0.03559313,-0.09455438,-0.010524987,0.07598985,-0.10754127,0.041917033,0.050533026,-0.032599214,-0.014646659,0.04354856,0.04133724,-0.030569673,-0.0016424973,-0.056717,-0.050096054,-0.06811768,0.06572573,0.036043532,0.0334421,0.060510486,0.009750046,0.026172007,0.046347145,0.0063026473,-0.014291177,0.070694245,0.084147304,-0.015304894,0.0055119763,-0.079569675,-0.026473714,-0.10390723,-0.024951177,-0.0630912,-0.051322944,0.09324908,0.034360483,-0.04900898,-0.113434225,-0.0075544533,-0.09772523,0.045223664,-0.08286897,0.023917189,0.040436074,-2.6216145e-33,-0.027064132,0.058929544,-0.026855258,-0.06341937,-0.0033357448,-0.032966826,-0.09280081,0.034641128,0.032304674,0.0036580407,-0.0011262608,-0.01102359,0.05032974,0.005170089,-0.05235134,0.036777794,-0.026559617,0.023615494,0.028239999,0.04159288,0.05048167,0.019876925,-0.06555425,-0.05766548,-0.034740333,0.028345838,-0.028658213,0.04590488,-0.013401474,-0.05467399,-0.036405675,0.0839372,0.0010516418,-0.063178875,0.03906898,0.008162144,-0.04989738,-0.03389129,0.03304661,0.014179153,-0.006944238,0.07753888,-0.056731433,-0.016712109,0.005115609,-0.06690419,0.053761527,0.017041013,0.013637125,0.07607061,-0.052462514,-0.03430402,0.015184917,0.054904275,-0.011425666,0.045343876,-0.038232066,-0.048716392,-0.03867064,0.04912,-0.04348892,0.00130145,0.045659687,0.0970097,-0.044903245,-0.092006296,-0.008737581,-0.006780947,-0.03032449,0.027803771,-0.033988547,0.04948209,0.033669516,-0.0080036,-0.035154343,-0.065577894,-0.0615223,-0.047112253,-0.047670487,0.008057196,0.04133898,-0.055315714,0.020532416,0.077777795,-0.0037448704,0.06831417,0.07298282,0.013014608,0.043064557,-0.042478763,-0.09704723,0.06381073,0.013295884,-0.0029421065,-0.09852515,-3.474261e-08,-0.055702727,-0.03618964,0.01067231,-0.0018910294,0.06914623,0.050491717,-0.10114253,0.110072516,-0.084846005,-0.0034888962,0.052238792,-0.05853248,-0.14109536,0.0027528354,0.024814263,0.042904884,0.060425665,0.018899988,-0.02737357,0.05073761,0.02644463,0.016790159,0.07188106,0.008609891,-0.0074186637,-0.054753818,0.013345537,0.09549963,0.028214034,-0.03063899,0.01057912,0.018138941,0.012253415,0.03983618,0.03791896,0.12811516,-0.03504919,-0.025649149,-0.119005,-0.00354554,0.0026961197,-0.006743417,-0.0121264495,0.005049113,0.0048493166,0.041541673,0.021373574,-0.00853591,0.025587492,-0.06604722,0.01067316,0.0036847496,0.0026744783,0.090918295,0.013618008,0.06867604,-0.03653631,-0.040704887,0.040051725,0.03556275,-0.010689529,-0.044606503,-0.0047872877,0.003392941,13,2.4487174,-29.884356,10
14,"we started off by discussing that with nominal and ordinal data, our observation or data points have an additional characteristic property called as a label. labels basically help us to identify which class that observation belongs to. 
then we took a small detour to understand standard error in detail. we understood that standard error is the variance of the means of different samples of a population. also we discussed about the number of bins which should be considered while plotting histograms. it depends on the data and how deep do we need to analyse the data. if we need to understand broad trends, we can use lesser number of bins, while if we want to find some minute trends, we need to use more bins. 
we came back to logistic regression and discussed about the logistic unit. a logistic unit gives us the probability of an observation being in a particular class. we use the sigmoid function in order to get the exact probabilities, and then map the probability to a particular class. so p(y|x) is calculated as the sigmoid function evaluated at a = w1x1 + w2x2 + w3x3. now there will always be a difference between the probability value and the actual class label. the probability value is denoted by â€™pâ€™ while the known class outcomes are denoted using â€˜tâ€™. our aim with logistic regression is to maximise the likelihood of our predicted outcomes being close to the targets, or to reduce the difference between the probability value and the target value. so â€˜tâ€™ is the observed outcome i.e. 0 or 1, â€˜pâ€™ is the probability of t=1 and thus, 1-p gives the probability of t=0. our aim is to maximise p or 1-p based on the value of t. now to ease out our calculations and maximisation, we take the log of the probability products, so that they can convert to a summation, and that gives us our loss function which we need to minimise.
we then moved on to some quality metrics which we need to assess our model. we discussed about the confusion matrix and the various terms, true positives, true negatives, false positives and false negatives. we discussed about accuracy, and how it is not a very good measure in case there is a class imbalance i.e. one class is under-represented and has very few observations. we understood precision, which is defined as of the events that we have detected, how many have we detected correctly. recall is defined as of a specific class, how many have we detected correctly. f1 value is defined is the harmonic mean of precision and recall. these terms give us a better picture of the data and prevent us from being misled by the accuracy. 
lastly, the tas of the course gave us insights about our assignment submissions. ",0.055021796,0.005631537,-0.043832798,-0.06877261,0.08123766,0.013323395,-0.047515962,0.059022624,-0.0037949889,0.057903424,0.017490162,-0.029743856,0.014133144,0.010875363,0.069438085,-0.058779232,0.004010441,-0.025866881,-0.0088624535,-0.0050731576,0.0076873954,0.017826587,0.03309993,-0.023717131,0.0013425216,-0.0030671568,-0.028148694,0.0069759996,-0.028668387,-0.019849569,-0.051024493,0.109090485,0.12319994,0.022425678,-0.057491418,-0.011065908,0.05638884,0.07479423,-0.0039106193,0.07470975,-0.016414182,-0.02685094,0.038053174,0.104408495,0.0037864696,0.011873979,-0.02237704,-0.037998665,-0.043212138,0.07406203,-0.035605356,0.07909911,0.06781463,0.010922869,-0.062317092,-0.011110327,0.001264302,-0.0742947,0.04178021,0.014276663,-0.060731307,-0.031109259,-0.013883973,-0.05072287,0.046814628,-0.07864305,0.034966636,0.009977164,0.076088406,0.08548656,-0.014398227,0.0016169564,-0.037409972,0.04862815,-0.028186979,-0.044271257,0.032573517,-0.0113292,0.02411178,-0.09711566,-0.055339668,0.07571865,0.058816306,0.019760052,0.08032212,-0.0066621755,-0.012746032,0.06178828,-0.085917726,0.00244803,-0.026362494,-0.013935034,-0.0033146003,0.017197637,0.09506739,0.049689215,-0.01663955,-0.06271869,0.12187162,0.018939048,0.0075749317,0.012190618,0.010289111,0.0076862373,0.030537622,-0.0713004,0.051574063,0.0064275707,0.035177264,-0.0064097177,-0.01440683,0.037219346,-0.13217208,-0.034067143,0.07492012,-0.07690575,-0.026761362,0.023506261,-0.06352414,0.024493096,-0.026851136,0.028314337,-0.014499045,0.03227529,0.027944745,0.011780715,-0.023462355,3.947239e-33,0.05838251,0.0066301133,-0.07962189,-0.0045219027,-0.052290138,0.075498655,-0.117889516,0.002440978,0.08150228,0.053639475,-0.00020311396,0.073966295,0.016255967,0.100300714,0.06811936,-0.014491301,-0.011345867,0.08779784,-0.047877017,-0.06872737,-0.11165526,-0.045376826,0.04583823,-0.037953526,0.05639073,0.10498518,0.011757737,0.05602989,0.03538349,-0.00024124826,0.06299056,0.01928567,0.008856938,-0.049519673,0.039288532,-0.035234023,0.00014108903,0.03090932,0.029474055,-0.0209948,-0.05729059,0.043862358,0.026854731,-0.029802375,-0.035110217,0.015224898,0.04300588,0.024412211,-0.0266544,-0.035223674,-0.028566655,-0.04551608,-0.01626192,0.012884359,0.0040078503,0.061086386,0.04824675,-0.04208858,-0.08134522,0.029855372,0.027858507,0.0036926,0.09042452,-0.010558503,0.025113922,0.09619478,-0.024113331,0.019201001,0.012522196,0.029072503,-0.038141817,0.04674386,-0.035197586,-0.040791042,0.016215123,0.041906137,0.056983028,0.020500751,-0.021751778,-0.00315802,0.006184879,-0.0699236,-0.011977274,-0.07356282,-0.065089956,0.06580687,0.054906655,0.001374577,-0.05658662,-0.013268218,-0.0494882,0.011127633,-0.081142746,0.035602625,-0.050231084,-6.494788e-33,-0.11133877,0.11487375,0.023805786,0.06600167,-0.04870156,-0.017208394,0.0033008032,-0.016858546,-0.017287936,-0.046325266,-0.061318967,-0.009466913,0.0009740771,0.004675621,-0.021670396,0.04373985,-0.07749085,-0.048505947,-0.099970065,-0.013691755,0.037899073,0.029966248,-0.13817173,-0.09606318,-0.08102001,0.034328107,-0.094145015,-0.02335606,-0.061039224,-0.09869036,-0.035094213,-0.10452005,-0.054572746,-0.091866255,0.10733409,0.023230786,-0.000455916,-0.060350604,-0.050831012,-0.024577897,-0.049640942,0.029567866,-0.019088347,-0.10293088,-0.020300072,0.03785168,0.0007431624,0.0525147,0.023960777,0.047076654,0.01978519,0.07959764,-0.016560817,0.0067527904,0.029776117,0.006025861,-0.10861322,-0.033863094,-0.12546137,0.023138799,-0.0749913,-0.011695703,-0.007319641,0.07377857,-0.10703026,-0.024535181,-0.031076724,-0.038937047,0.015000737,-0.026577434,-0.02250223,0.014974068,0.02460187,0.0029254316,-0.10251075,-0.02860099,-0.0077302256,-0.039797183,0.0143628735,0.084189616,-0.009088392,-0.05001255,0.01122051,0.07463772,-0.028389119,0.023540614,0.090973936,0.0014671066,0.028554242,-0.028563222,-0.030814266,-0.004653355,-0.10326651,0.055342097,-0.0025129982,-7.410493e-08,-0.064017534,0.0020068171,0.107511744,-0.048995007,0.13372204,0.06636049,-0.073421024,0.030544125,-0.008791951,0.015651235,0.05225519,0.049849063,-0.13751823,-0.07584333,0.023770219,0.020608626,0.07861502,0.004770559,0.031030942,0.0715968,-0.015360892,0.0005416083,-0.011028957,-0.08254289,0.015417367,0.004439356,0.01718949,0.103592694,-0.011292556,0.040485278,0.037861686,0.024026548,-0.021125106,0.01696323,0.026151793,0.04495848,-0.023012187,0.0010280961,0.035398945,-0.014632975,-0.007153249,-0.035300337,-0.031383652,0.037960898,0.007888402,0.051120725,-0.037944987,0.025659926,-0.04488374,-0.0039423923,-0.052742757,0.018296754,-0.008872867,0.03500657,0.015127396,0.041467458,-0.05734121,-0.10476708,-0.06281205,0.05199208,-0.040502023,-0.0079637775,-0.08144658,-0.0550493,13,-1.5628266,-31.23141,10
19,"in the beginning of the class we discussed a few questions raised by students about the previous topics discussed in class: learnt about expectation algebra, histogram showing a particular distribution and how many bins to consider, when to consider a division between two clusters and what boundary to consider, model which changes regression expression according to ranges of x either by going for two different forests or some other method. 
after that we continued our discussion from previous class about logistic unit and logistic regression. soft max function redistributes numbers around 0 and 1 so that they can be used. if p>0.5 then we push it into one class and otherwise, we push it to another class. then we saw the notations of logistic regression with 3 model examples. y1 and y2 are known outcomes, which can also be referred to as 't'. then we learnt about how to calculate the weights wi. we do this such that the likelihood of getting desired targets is maximized (another way of saying minimize the differences). y is known as the target and denoted by t.  we find likelihood using the function, and maximising l, we get the desired weights wi. the expression for l should satisfy the two requirements, that when t=1 we would be getting maximum p and when t=0 we should be getting maximum (1-p). likelihood is a product based function, but it's easier to deal when in summation form, so we take log of likelihood. it's better to do this way, because maxima are always unstable, while minima are stable points. use gradient descent method to do so. also, we defined something like n ( learning rate). then we saw the confusion matrix (cases of tn, fn, fp and tp, and the examples that are associated with them). false negatives are more disastrous in cases of scenarios like being sick but told that you're not. definition of accuracy: being able to correctly identify the situation, how close the measurement is to the actual value. definition of precision: of the events that you detected, how many did you do correctly, how close the measurements of the same observations are to each other. definition of recall: of a specific class, how many could you correctly identify. f1 value is defined as the harmonic mean value of precision and recall. 
in the end of the class we discussed the topics of assignment e1, and the common mistakes most people made, like directly using kurtosis function from excel without checking if we need the positive kurtosis or negative kurtosis, and in q1 which involved comparison between the two models, the slope terms weren't same which most people just subtracted intercept term from predictions and used it for comparison. general: excel file should be submitted along with calculations and formulas, not just values. and put in more effort during documentation and organize well.",-0.0036197358,-0.09929187,-0.03601914,-0.013848282,0.06746462,-0.08509105,0.033134993,0.05070116,-0.023672452,0.07806772,0.043162733,0.020370375,0.046108525,0.0399236,0.020100223,0.0550262,0.015515335,-0.002609405,-0.19538032,0.069789216,0.06450442,0.0067287474,0.023396196,0.015644124,0.0073772906,-0.124433234,-0.008195795,0.030559313,-0.030048423,-0.062028106,-0.076224566,-0.001688747,0.07262016,0.031385183,-0.029853165,-0.012168853,0.011455455,-0.016572287,0.01706916,0.025096396,-0.08757721,-0.081978396,0.01406693,-0.015836444,0.029127501,0.019976001,-0.036282364,-0.004956573,-0.026374912,-0.020914916,0.021688875,0.034375466,-0.10987566,-0.022106543,-0.10228983,-0.0534668,0.054035038,-0.04899141,0.007412844,-0.017001364,-0.06862337,0.02500499,0.013126886,0.022585701,0.016549869,-0.14772154,-0.062803514,0.05494479,0.015133556,0.11796425,-0.012905881,-0.06500073,-0.054436747,-0.039872315,0.11794928,-0.009223847,0.08629664,0.078813426,-0.057273295,-0.013548606,-0.016457418,0.004944078,0.10345692,-0.033300355,0.024651134,-0.032334916,-0.07270834,0.043373816,0.09113311,-0.0052901036,-0.040893886,-0.015257014,-0.05852608,0.08528568,0.0369469,-0.023803543,-0.048049383,-0.052400723,0.056035277,0.06950724,-0.11712387,0.04307614,0.094892025,-0.09372314,-0.008858335,-0.13676497,-0.06180963,0.012626513,0.032421038,-0.10832228,-0.015062564,-0.0024187095,-0.07590634,0.032666065,0.007138208,-0.04868947,0.09025451,0.068505086,-0.1366178,0.040107097,0.038351674,0.03852416,-0.02882008,-0.0426774,0.038340315,-0.041924097,-0.112736166,4.3072335e-33,0.07853815,-0.0037850325,-0.049547385,0.0038742812,0.04057909,0.016125923,-0.038741197,0.028432332,0.03200204,0.029603997,-0.035793956,0.021151653,0.08725252,0.08991614,0.07142346,-0.006545876,-0.06238826,0.05634396,-0.0074171238,-0.025418045,-0.048476204,0.027006743,0.025103025,-0.089647435,-0.0019540093,0.070152275,0.054193486,0.012186305,-0.02172025,0.019527575,-0.0009486526,-0.006582212,-0.06670926,-0.03786953,0.006468042,0.054034386,0.012642813,-0.0076588504,0.018573267,-0.0640301,-0.02317415,0.07084999,0.037343167,-0.011801257,-0.009544317,0.018138837,0.034847878,0.035461448,-0.024993522,-0.035417188,-0.02429786,-0.0762729,0.041543785,-0.018753057,-0.018363345,0.0472454,0.02242594,0.047643315,-0.053272713,-0.00026787483,0.0036672768,-0.0043345345,0.08145307,0.0251086,-0.011439668,0.02202149,-0.04797694,-0.013559695,0.10022761,-0.054514937,0.009529572,0.0063478337,0.027229153,-0.030481536,0.06392187,-0.0022735393,0.106465995,-0.02129538,-0.018926417,-0.043518033,-0.004765342,0.02243462,-0.019171162,-0.070854776,-0.061663643,0.056824386,0.0024893663,-0.048127204,-0.07975846,-0.009281749,-0.060369413,-0.013572824,-0.09176387,0.0040850006,0.04761756,-7.872893e-33,-0.042354874,0.0847821,-0.04880084,-0.031054392,-0.0014166784,-0.03396385,-0.030491808,-0.071415775,-0.016082928,-0.053079218,0.018987283,0.020920359,-0.014009438,0.04368328,-0.023292078,-0.0608123,0.0059864437,0.004952093,-0.06064872,-0.023387194,0.06709556,0.04508697,-0.107803315,-0.035822056,-0.0393823,-0.018066112,-0.06712058,0.05206987,-0.007877803,-0.03413207,-0.039040346,-0.07582037,-0.06639018,-0.020732922,-0.0037482968,0.020460803,0.010587017,0.0062812762,-0.01777987,0.031897817,0.03397882,-0.031455383,-0.0047323825,-0.039108858,0.038337104,-0.03725018,0.10268815,-0.04101168,0.07996983,-0.057514008,0.018907718,-0.00962676,0.014831934,0.06498206,-0.016952746,-0.0044722618,-0.014887763,-0.0651705,-0.07126483,0.024606947,-0.02353729,0.00538882,-0.025829397,0.095440775,-0.11862532,-0.05988579,-0.023696454,-0.028409,0.03699886,0.006718107,-0.05311612,0.03068024,0.0780916,0.047542475,-0.05457478,0.007490909,-0.016765025,-0.044680886,-0.027159626,0.025219906,-0.01889625,-0.060889002,-0.01077973,0.066071905,0.06454368,-0.09036454,0.08700391,0.0373957,0.083733745,-0.07663861,-0.024348004,0.06414646,0.027238073,0.010835626,-0.09046412,-7.528403e-08,-0.0032926074,0.022379013,0.037003744,0.010656575,0.09426241,0.09306943,-0.049628556,0.0018970981,-0.080194995,0.056181118,-0.007112283,0.096668445,-0.02783847,-0.06835555,0.022930602,0.082577854,0.030523682,0.0029007103,-0.0021254353,0.043966956,-0.017669756,-0.052598994,-0.027617248,-0.025882443,0.044109713,-0.01863693,-0.013118071,0.022516374,0.009101159,0.03431332,0.03898029,0.071201675,-0.02724944,-0.0072489996,-0.015618972,0.011920029,-0.042463403,-0.013123843,-0.03842987,0.04912251,0.0473471,-0.05313544,-0.0063915993,0.016269159,0.100280024,0.06966556,-0.06589869,0.0036201482,0.009283248,0.0062436843,-0.002051151,0.034127217,-0.033645824,0.012821225,0.062197592,0.029374998,-0.026301417,-0.046350077,-0.035001636,0.04900701,-0.017719988,0.09485877,-0.00015484379,0.015948076,13,-1.9275827,-27.528463,10
22,"in todayâ€™s class we learnt about logistic regression. we began by understanding the classification problem which is about determining a decision boundary between two classes .the logistic unit takes multiple input features and produces an outcome (0 or 1). to achieve this, we need a function that maps inputs to probabilities. the sigmoid function ensures the output is always between 0 and 1, which gives probabilities. we then learnt about maximum likelihood estimation, which minimizes negative log-likelihood to find optimal weights. gradient descent minimizes the error function. then we got better understanding through some examples. we also have several metrics to decide how good is our classifier.",-0.029764352,-0.06686287,-0.0443784,-0.050577503,0.027889486,-0.05089505,0.04540215,0.009507288,-0.032306317,0.045684002,-0.0063330685,0.0035125772,0.04310901,0.0026095011,-0.020157147,-0.028776413,0.002901456,0.05862775,-0.08880467,0.01543423,0.0005559771,0.032562573,0.006728551,-0.004641572,-0.033919413,-0.09132195,0.007854018,0.026407728,-0.0647464,-0.05397308,-0.039619178,-0.0202853,0.065083705,0.02499728,-0.09472607,-0.0053338637,-0.007609782,-0.03775598,-0.025738675,-0.002451197,-0.035380855,-0.078861676,0.020014687,-0.016071046,0.039172165,0.024483375,-0.011245529,-0.02647552,0.008018432,-0.007428613,-0.05448282,0.03219258,-0.023342256,-0.020348618,-0.11521248,-0.009583026,0.04732154,0.00630507,-0.029794715,-0.0034102942,-0.011501257,-0.08810573,0.01100756,-0.023791755,0.021672955,-0.11778582,-0.041347254,0.0022426199,0.06961343,0.07729593,-0.017652495,-0.026723562,-0.042708118,0.022923358,0.028656742,0.00326956,0.10093,0.03265909,0.021958975,0.02689046,0.015156991,0.08050163,0.055300757,0.033900082,0.12458713,-0.031446368,-0.018349856,0.047561552,0.00946149,-0.016137423,-0.026719667,-0.01514709,-0.052873492,-0.012514626,-0.047715444,-0.03127735,-0.05527087,-0.06326283,-0.004804868,0.019938538,-0.13532044,0.04163853,0.065716684,-0.02464386,0.07542133,-0.04494917,0.031825364,0.01845243,0.069431596,-0.094460845,0.010401946,0.03767282,-0.07329682,-0.004578772,0.05915564,-0.037153117,0.052336503,0.019097539,-0.09616922,0.10411378,-0.05897596,0.03834162,-0.03800084,-0.046953134,0.014439815,0.009031201,-0.061988927,1.9447562e-33,0.010004335,0.036608096,-0.023059066,-0.040904097,-0.033378903,0.043204192,-0.07746664,0.02343372,0.016250089,0.034467164,-0.0435525,-0.018082384,0.05859023,0.1597657,0.08878116,0.002374259,-0.06999167,0.028620837,0.025505234,-0.033683684,-0.03551026,-0.021334805,0.07286629,-0.14447592,0.015915178,0.084285416,0.05549441,0.013285882,-0.040655177,0.013382675,0.021355089,0.015490295,-0.03194745,-0.023495749,0.07642355,0.06695127,-0.058080554,0.050611094,-0.02480877,-0.013233671,-0.120400086,0.046389688,0.026202269,0.012735222,-0.034156263,-0.017044408,0.017980205,-0.030624032,-0.020377012,-0.11060898,-0.026369045,-0.069015585,-0.01965368,0.014497998,-0.023818078,0.06940632,0.027326671,0.00076201034,-0.074943796,-0.031977054,0.010006199,0.0010363014,0.057953168,-0.02181553,-0.028218435,0.034696437,0.021060502,0.0035940984,0.025042813,-0.065164216,0.0522894,-0.014838665,0.07001312,-0.0461829,0.030825093,0.10803661,0.06499888,-0.07837428,0.061287347,0.022838822,-0.009349422,0.02221519,-0.03435598,-0.096887805,-0.015336833,0.05157008,0.017384393,-0.017379781,-0.037339605,0.019019507,-0.11547881,0.044912893,-0.030537276,0.04184919,0.0010172662,-4.495606e-33,-0.07932251,0.060916,-0.017572867,0.016443754,-0.044112947,-0.0070464397,-0.07753718,-0.008637551,-0.06376573,0.058116622,0.05546745,0.018557876,0.05117606,0.013744753,-0.03659922,0.06053607,-0.10386588,0.006938603,0.009768206,-0.01804064,0.08821418,0.06757111,-0.11289541,-0.012366881,-0.049927678,-0.0343969,-0.032770716,0.022444632,-0.009516692,-0.06856054,-0.026596589,-0.043552887,-0.078957975,-0.02127519,0.065360285,0.046576705,0.0020927254,-0.019693427,0.011901437,0.049267177,-0.03346728,0.035132132,0.04068122,-0.040387735,0.029756982,-0.10531458,0.0785854,0.037288625,0.06296362,0.005001737,0.0023378134,-0.0223152,-0.011264403,0.03646401,-0.009397237,0.029233154,-0.02249941,-0.03705084,-0.016987821,0.05892706,-0.057713483,0.050186176,0.016061906,0.14431444,-0.092051566,-0.038027693,-0.02557191,0.087564245,0.009602073,0.003562327,0.00019125278,0.047195435,0.06513057,0.04507528,-0.08437182,0.009634713,-0.038150035,-0.038117595,-0.04357555,0.0006291477,0.029846227,-0.0781445,-0.027407406,0.07885617,0.07712106,-0.04188678,0.11809254,-0.0031141494,0.07541932,-0.08058821,-0.025465174,0.054026444,-0.02585681,-0.008429468,-0.101669475,-5.3156814e-08,-0.018892853,-0.022668574,0.10734459,-0.013689009,0.028481873,0.06841903,-0.04524193,0.07548329,-0.05370562,0.010304668,-0.001439274,0.124609455,-0.07175762,-0.05063676,0.02975907,0.06426488,0.058052372,0.07163258,0.027058035,0.061084237,0.067654036,-0.05789055,-0.0031724002,-0.054924875,0.014720335,-0.06868064,-0.0018938597,0.04886208,-0.06684227,0.009448161,-0.014595144,0.10717739,-0.02893889,0.003267197,0.06847487,0.094604686,0.021672575,-0.06551899,-0.076370515,-0.028787289,0.008006161,0.014999366,4.462464e-06,-0.014176497,0.023694646,0.06377203,0.02356474,-0.044413384,-0.042835873,0.07175916,0.02018187,0.04696291,-0.0060074986,0.050430696,0.0682095,0.027167613,0.0065625454,-0.096352845,-0.018079557,0.061650485,-0.05463555,0.0779337,0.03918566,-0.03331582,13,-0.59705067,-26.888552,10
32,"we discussed the idea that using one model is often better than combining two models, as combining them might lead to discontinuities at the boundary. then, we moved on to logistic regression, introducing the concept of a logistic unit, which has two possible states: 0 and 1. we considered a function in the form of a linear combination of input variables: w1x1 + w2x2 + w3x3 + b. since the output needs to be either 0 or 1, we introduced the sigmoid function, which converts this linear function into a probability value.

the probability function p(y|x) was then defined as sigma(w^t x + b). if a point belongs to class 1, the predicted value should be close to 1; otherwise, it should be close to 0. the goal was to maximize the likelihood of the predicted outcome matching the actual target. when t = 1, we maximize p, and when t = 0, we maximize (1 - p). this led to the definition of the likelihood function as a product over all data points: p^t (1 - p)^(1 - t). to make optimization easier, we took the log of this function, leading to the log-likelihood expression. since finding a minimum is often more stable than finding a maximum, we defined j as -log l and minimized it using gradient descent. the weight parameters w1, w2, ..., wn were updated iteratively with a learning rate (eta).

we also discussed that the decision boundary does not necessarily have to be linear. then, we introduced the confusion matrix, which contains true positives, false positives, true negatives, and false negatives. the accuracy metric was defined as (tp + tn) / total. however, it was noted that accuracy is not always a good measure, especially when data is imbalanced. if one class has significantly more data points than the other, accuracy might still appear high despite poor performance on the minority class. to address this, we introduced precision and recall as alternative evaluation metrics.

towards the end of the session, the tas discussed assignment 1, pointing out common mistakes and providing suggestions for improvement.",-0.04017694,-0.09728816,-0.026924124,0.0032855957,0.11958209,-0.08367076,0.043812815,0.017364357,0.022870101,0.0053418395,-0.0062002027,2.3794768e-05,0.08729667,0.022616437,0.028924052,0.07400317,0.0064886604,-0.031913586,-0.12178288,0.08216002,0.055835217,0.036678802,0.030589473,0.011397041,0.037109766,-0.07766866,0.008134847,0.04764928,-0.057361934,-0.03635108,-0.01682351,-0.06184298,0.054301858,0.011580544,-0.013803138,-0.061709665,-0.024919113,-0.016494336,-0.0069176364,-0.0026790362,-0.013075332,-0.08337049,0.013036684,0.014552738,0.010144623,0.01835075,-0.022386735,0.029551996,0.021910006,-0.0014950096,-0.032388218,0.009293485,-0.036353428,-0.022907278,-0.11640512,-0.047465876,0.032235105,-0.003206839,-0.0051084626,-0.048925217,-0.061077923,0.0052939467,0.02760665,-0.0015955591,0.040696595,-0.12668996,-0.08235197,0.03260905,0.023204567,0.09682922,-0.028098939,-0.048163123,-0.06774983,-0.031712167,0.07462887,-0.0027619114,0.13053381,0.04834418,-0.009305872,0.032645002,-0.014160785,0.013089653,0.025372073,0.008382768,0.04459344,-0.045323774,-0.05118561,0.01622984,0.09256733,-0.06700957,-0.09981516,-0.0024620013,-0.064416766,-0.016056392,0.026878897,-0.03604211,-0.039612856,-0.079444095,0.05827033,0.05955434,-0.12895,0.0624896,0.08081742,-0.061844073,0.032720428,-0.07728683,-0.004582686,0.02338962,0.04701026,-0.10029018,0.013977237,2.1658447e-05,-0.019477222,0.013073196,0.0097578345,-0.019218942,0.028775373,0.01693703,-0.07193762,0.03270942,-0.012213715,0.0038131613,0.02737348,0.005872188,-0.03327553,-0.03746744,-0.049442977,7.0828905e-33,-0.006910084,0.00435878,-0.06309466,-0.0060125166,0.057780962,0.0756747,-0.06296975,-3.7603993e-05,-0.03521008,0.043849614,-0.091278486,-0.06428702,0.04092045,0.11590646,0.09090911,-0.021401502,-0.0224007,0.04332732,0.019655662,0.0068332595,-0.012552798,0.02189037,0.011617484,-0.07742385,0.006059884,0.054349065,0.06325405,0.056795936,-0.06696057,0.00516558,-0.019418104,0.04125609,-0.058734648,-0.0756989,0.02183604,0.0811063,-0.031714916,-0.031002186,0.040270317,-0.04186,-0.08979123,0.026558422,0.042833228,-0.037786953,-0.0784155,-0.06616278,0.0033802432,0.03652407,-0.04721417,-0.06340996,0.0042542038,-0.022351697,-0.011523867,0.0064038755,-0.07760524,0.045848984,0.084925935,-0.026948081,-0.071228184,-0.02615588,-0.020347748,-0.00060054444,0.04200399,-0.034501918,0.0071439347,0.033036016,0.04537978,0.02244816,0.08653905,-0.035354607,0.04275816,0.0043672468,0.037626673,-0.066400506,0.049971998,0.015451164,0.062311426,-0.021771258,0.066561595,0.016904727,-0.0004812707,0.016154326,-0.016751653,-0.06983879,-0.018068863,0.049165644,-0.0031782486,-0.019550953,-0.14050788,-0.043956313,-0.060000073,0.0796473,-0.025467321,0.059267335,-0.0009696521,-8.783557e-33,-0.07548102,0.06261256,0.046111412,-0.030190913,-0.034636468,-0.033398777,-0.08336852,-0.123255834,0.0036423574,-0.024206912,0.04944591,0.00040167794,0.040691305,0.015745193,-0.04444374,0.013949542,-0.001342218,-0.003343541,-0.022341184,-0.015787154,0.13502638,0.060049966,-0.10629833,0.0228448,-0.10212431,0.0012513737,-0.019886117,0.07119262,-0.0029682273,-0.0057643377,-0.06279284,-0.0197634,-0.010288432,-0.076554194,0.03823806,0.07172131,-0.019141512,-0.026899537,0.017192446,0.02815935,0.0061463253,0.01617076,0.0071434597,-0.04077782,0.054768924,-0.03304872,0.098099135,0.006125985,0.058849484,-0.0090185385,-0.036056284,0.10875741,-0.033134397,0.048032474,-0.027934639,-0.0278851,-0.08616454,-0.07168276,-0.04824655,0.0073148496,-0.052897245,0.056465622,0.016658468,0.11989935,-0.042148557,0.006430757,-0.04471559,0.030569814,0.036757674,0.027723987,-0.018100709,0.10837745,0.14524323,0.051217422,-0.021354256,-0.01333695,-0.058347654,-0.029996041,0.0071971384,0.03852923,-0.039787218,-0.0025817526,-0.024807706,0.020056412,0.077922314,-0.058486544,0.037707217,0.08817713,0.08537687,-0.11647875,-0.030359423,0.061458692,0.0053297225,0.012092689,-0.10036605,-7.630449e-08,-0.04300883,0.011007426,0.043021288,0.016628215,0.018141912,0.060688715,-0.04175604,-0.05834543,-0.023968685,-0.026218927,-0.035879653,0.1187987,-0.08686796,-0.0606573,0.01766025,0.057296265,0.023480026,0.015162173,0.030563591,0.017910954,0.05915097,-0.053008042,-0.064227484,-0.066131,0.049841944,-0.0142776845,-0.08553135,0.07310036,-0.0070494753,0.0071613807,0.050183058,0.05078504,-0.037333254,0.053927343,0.037835978,0.08188636,0.0108899195,-0.0053222994,-0.030658878,0.00025913827,0.022410545,0.042297397,-0.04664314,0.00929058,0.070575655,0.104971975,-0.00961328,-0.0844369,-0.020933667,0.02286212,0.013338545,0.031844642,-0.005136397,0.042201474,0.027846381,0.03320933,-0.037195932,-0.0086822165,-0.02790097,0.079135634,-0.05412181,0.0068277614,0.0097118905,-0.0030728325,13,-1.9197681,-26.86246,10
40,"to improve the likelihood of accurate predictions, logistic regression is used to generate weighted values. we maximize p if t=1 and if t=0 we maximize 1-p. because working with product terms can be complex, it is necessary to utilize a logarithm to simplify computations by converting them into summations. we evaluate model performance using a confusion matrix consisting of true negatives, false positives, false negatives, and true positives. important measures include accuracy , precision and recall (the model's capacity to identify all actual positive examples).the f1 score is a harmonic mean of precision and recall, a balanced statistic that doesnt burden off accuracy alone.",-0.033721548,-0.030477662,-0.08085453,-0.022733707,-0.003078685,-0.04158615,0.022545159,0.072027735,-0.0032038398,0.011812089,0.0038281856,-0.054339048,0.048359767,0.029328834,-0.056801002,-0.0028746992,0.04673878,0.10463006,-0.037336912,-0.034770153,0.0026924538,0.035797548,0.06548923,0.03350523,0.0026834,-0.096075565,-0.046093814,0.0136796925,-0.028181791,-0.061833814,-0.012564933,0.047974024,0.057414457,-0.007265288,-0.10828282,-0.022478426,-0.061382927,-0.029002083,0.06789582,-0.015963005,-0.033316284,-0.06539556,0.004693656,0.0032608868,0.0077451975,0.014097989,-0.026906086,0.038689584,0.04066727,0.055117104,-0.107632,0.08595687,-0.053381752,-0.04072192,-0.10917915,-0.006278488,0.04300498,-0.03859228,-0.004792305,-0.026310055,-0.011123606,-0.07130294,-0.05713287,0.00877422,0.048651237,-0.0014299841,-0.099906,-0.031723183,-0.034383163,0.07993741,0.0054483735,0.048714172,-0.085600704,-0.0075530955,-0.032453813,0.09355952,0.033251457,0.02185488,0.016995773,0.13905163,-0.021213327,0.02284108,-0.05421592,-0.015323415,0.13887222,-0.078673534,0.043709297,0.08461107,0.042043,0.032241832,-0.03764448,-0.05596347,0.07643067,-0.023717135,-0.05545745,0.08194478,-0.010968765,-0.063405566,-0.02932098,0.04772974,-0.052179813,0.09756765,0.01190212,-0.097646005,-0.028277019,-0.05454496,0.03977488,0.03317155,0.065890625,-0.07447591,0.010370445,0.0069100116,-0.09267162,0.042156868,0.022673687,0.024195882,0.040677875,0.065789655,-0.042478207,0.11998742,-0.00026101747,0.052259017,0.09185478,0.038277134,0.016846796,-0.029572753,-0.12423846,4.58568e-33,-0.070196435,0.050624657,0.09540443,0.019875284,-0.027373385,0.013731,-0.11968505,0.029632637,-0.009598407,0.019937752,-0.032438356,0.057859,-0.035349324,0.0783997,0.044805758,0.008023054,-0.073797055,0.051565234,-0.014217594,-0.06689472,0.01597003,-0.07992378,0.051208217,-0.08875285,0.04776252,0.0075123715,0.0025589622,0.06545086,-0.040581435,0.00670847,-0.0067867674,-0.00395842,-0.0050807167,-0.06468668,0.033065647,0.020234521,-0.07574254,-0.020518675,0.040624548,0.04568004,-0.099731006,0.018410807,-0.026566124,-0.056399506,-0.07151905,-0.04168791,-0.0122395735,-0.0054672067,-0.034412533,0.06355901,-0.036772225,-0.045358893,-0.058438692,0.045098804,-0.08229622,0.020896582,0.03764668,-0.012711244,-0.01697176,0.037084118,-0.0341277,0.033988144,-0.008268077,-0.03904289,-0.06893604,0.099477336,0.06708716,0.04610066,0.051796246,0.04615825,0.039106645,-0.028442064,0.044326182,0.019510454,-0.01463382,0.068257175,0.08344173,-0.028381487,0.039534677,0.029582273,0.09192083,0.015836375,-0.0027137515,-0.032055017,-0.04241143,0.012370379,-0.003922135,-0.001341056,-0.098450534,0.019778207,-0.0819214,0.046370924,-0.032233115,0.025561415,-0.078365035,-6.212811e-33,-0.12173016,-0.017601611,0.01555782,0.037885047,0.018887008,-0.066048205,-0.026968256,-0.03219939,-0.035376165,-0.034148686,0.04158149,-0.03696618,-0.043074284,0.013307281,-0.022945803,0.002022088,-0.025278477,0.024746904,-0.03311067,-2.170243e-05,0.050176654,0.081684634,-0.07220261,0.03671579,-0.0655642,0.03458735,-0.03253263,0.0093048625,-0.026519677,-0.069912486,-0.043961056,-0.032818284,0.0038283227,-0.00881704,0.027530871,-0.00735061,0.072933465,-0.024010163,0.046351675,0.079764105,0.05931245,0.058215592,-0.042562157,-0.017130427,-0.03172067,-0.08323582,0.035970166,-0.049233735,0.10199533,0.030417833,-0.024995139,0.030688222,-0.088606335,0.099343054,-0.037932463,-0.006439213,-0.04336604,-0.0563287,-0.06973922,0.022414805,-0.12903231,0.07550835,-0.020227391,0.03599771,-0.003824888,-0.06382417,-0.002570246,-0.0048783896,-0.018090013,0.020736216,0.023618653,0.03278586,0.088109955,-0.0034946508,-0.042904582,-0.078653686,0.00025298444,0.034508895,0.013249999,-0.0013838045,-0.0017392145,-0.038244613,-0.0056330035,0.019554025,-0.03595654,-0.0023804707,0.0526688,0.016541695,-0.0067277495,0.01920561,-0.04275761,0.108728796,0.019891923,0.0058500143,-0.054682333,-5.461709e-08,0.020723475,0.0067152204,0.07248431,0.05166027,0.019721668,0.01323348,-0.07008547,0.013497448,-0.055735122,-0.047167126,0.06553024,0.008948574,-0.13646634,-0.03223701,-0.006155972,0.057433557,0.004433861,0.14034665,0.022784514,-0.052796215,0.06888071,0.0042077582,0.040892065,0.007682483,0.041127,-0.044803333,-0.027538238,0.1317784,0.014880565,0.028251905,0.037226077,0.011689131,0.0010929236,-0.02192555,0.023986071,0.04865985,0.054097865,-0.03813115,-0.024025755,-0.0060346415,-0.020900682,0.07143149,-0.05972224,0.03483286,0.050519064,0.008999729,-0.020470051,-0.015151961,0.0020098274,-0.03389599,0.045368668,0.08256542,-0.03815723,0.06098674,0.05287979,0.07732978,0.00995999,-0.077290066,-0.06402649,-0.011490797,0.07034703,-0.05083723,-0.008040091,0.005697775,13,2.9830835,-25.08447,10
41,"logistic regression and clustering.
we want to maximize the likelihood of our predicted outcomes being close to the targets. 
maximizing likelihood is same as minimizing the error function.
we create a confusion matrix:-true negative,false positive,false negative,true positive.
terms we defined :
precision:of the events we have detected how many have we detected correctly 
f1 value-harmonic mean of precision and accuracy",0.060161937,-0.035773855,-0.06813931,-0.039401315,0.030432295,-0.043920804,0.063377135,0.019651834,-0.016550528,0.03800628,0.081545375,-0.046367764,0.07387115,0.038182203,-0.05543997,-0.020063585,0.049291838,0.047698386,-0.09332328,-0.0136710685,-0.044572532,0.038328566,0.04693442,0.06932363,-0.04102093,-0.05547623,0.015580475,-0.0055754916,-0.02410957,-0.052697312,-0.03350766,0.061889447,0.05915347,0.03719729,-0.06406324,0.020566812,-0.0049383272,0.005319611,0.053143244,0.03745618,-0.03283339,-0.04659025,0.036505755,-0.040190626,-0.038558163,0.04040731,-0.10917142,0.055445436,-0.029878808,0.026976958,-0.082533605,0.06433064,-0.012972831,-0.051024973,-0.06945767,-0.0474805,0.036602095,-0.0675036,0.023765408,-0.024048492,0.03752641,-0.037761163,-0.009068426,-0.026429828,0.054419175,0.0067020147,-0.073540635,-0.033195667,0.044928603,0.04826367,0.016833171,-0.013409739,-0.048412856,-0.0033399994,-0.0036477887,0.037632838,0.011709028,0.04727257,0.0018481277,0.09884252,0.022233304,0.0181083,-0.010841189,-0.016811775,0.122512795,-0.06687601,-0.005551154,0.061264485,0.047496643,-0.020370279,-0.085650966,-0.036513656,0.026791317,0.019161744,-0.051805712,-0.015515235,0.016938819,-0.0653241,0.07046854,0.04595233,-0.047379952,-0.008089231,-0.029696329,-0.026278308,-0.0046003954,-0.073032714,0.0049618576,0.025695143,0.060612265,-0.053176273,0.00015866467,0.014923253,-0.10026644,-0.020352112,-0.020073859,0.03301243,0.06990296,0.106331356,-0.026578309,0.024037598,0.014720998,0.00819184,0.051920425,0.027006416,0.07949341,-0.066856846,-0.12828605,-2.9344928e-34,-0.051214818,-0.018790385,0.040221814,-0.046004273,-0.026097322,0.0061842734,-0.14525285,-0.0072877253,0.055558313,-0.026181651,0.0064945705,0.03585749,0.03639942,0.06823309,0.044397548,-0.0051457062,-0.009615472,0.1168312,-0.10864253,-0.11984165,0.0005124249,-0.04997778,0.027191767,-0.09998946,0.0011846621,0.024839794,0.013903612,-0.007424007,-0.015119251,-0.008296884,0.03498244,0.026614964,0.032247357,-0.0070121847,0.10219583,0.09299916,-0.04141717,-5.1614683e-05,0.03883824,0.012262271,-0.106047004,0.015138997,0.0038750083,-0.1041607,0.021608938,-0.011630169,0.02053997,-0.018532194,-0.04393138,-0.04751721,-0.03605826,-0.10824072,-0.014173048,0.07640443,-0.037352223,0.022223286,0.018185942,-0.017590081,8.4492436e-05,-0.03222447,-0.017823052,-0.09161341,-0.022362253,-0.080334276,-0.032597452,0.019373007,0.026525963,0.013310614,0.13614605,0.061627086,0.002750406,0.045161497,0.10935919,0.004505099,-0.06562869,0.07236427,0.07813751,0.06348526,0.02395209,-0.011064106,0.038032416,0.024098147,-0.13516062,-0.03519892,-0.06534194,0.03212724,-0.041258395,-0.013074681,-0.15967128,-0.002277164,-0.057927523,0.04363895,-0.023647372,0.048820306,-0.029036948,-2.649816e-33,-0.064751975,0.03489856,0.019579928,0.0059275776,0.0043073785,-0.019896401,0.03409464,-0.02573656,-0.06971855,0.00254235,0.026435135,-0.025375048,0.019426726,0.04613388,-0.096713096,0.05144876,0.042941954,0.047577832,0.044528123,-0.0034369861,0.05450222,0.004243458,-0.057193663,-0.014157228,0.018460093,0.055355784,-0.02030564,-0.032597546,-0.027652299,-0.054360695,-0.06453468,0.036564566,-0.02488733,-0.00728857,0.06313436,0.024217032,0.02941829,-0.045645922,0.013068583,-0.014031445,-0.013462592,0.05525138,-0.03355774,-0.012376186,0.031178135,-0.091959566,0.083437026,0.018617714,0.024840374,0.0089483,-0.07120212,-0.038106244,-0.041379508,0.087469615,-0.029944045,0.08185822,-0.06786609,-0.037639752,-0.057362854,0.03856351,-0.06670913,0.0056622117,-0.02425708,0.10393139,-0.063712545,-0.03487401,-0.027191805,0.006692367,-0.01422031,0.0061261407,-0.029241268,0.059603117,0.088148855,0.018855702,-0.046880256,-0.11050362,-0.060512383,-0.015091719,-0.02075934,0.032917727,0.010904618,-0.049379166,-0.015913533,0.0956466,0.01840489,-0.0004838791,0.02825714,0.053124715,0.018286752,-0.017078755,-0.053476375,0.046374705,0.03876858,-0.050446052,-0.111728124,-4.6062457e-08,-0.023022085,-0.012557653,0.010569889,-0.051229607,0.04806557,0.01763662,-0.05856386,0.029801693,-0.06951396,-0.053894643,0.058414903,0.023999963,-0.14422633,-0.03182778,-0.01046823,0.013732143,-0.034145582,0.09629531,0.025423123,0.0015477662,0.05262132,-0.023889348,-0.011217167,0.02062682,0.070840836,-0.013958942,-0.011749962,0.16170923,-0.04262393,-0.0343138,-0.021561557,-0.010127537,0.05260767,0.044351432,0.011583436,0.09950368,-0.019571355,-0.0040829526,-0.063792445,-0.032634646,-0.018802019,0.06291033,-0.054392908,0.013444839,0.043917283,0.096843906,0.034219965,-0.0017673228,-0.008127686,-0.0071244207,0.00579305,0.069811255,-0.02675286,0.08945458,0.037370138,0.078041285,-0.030524539,-0.055957183,-0.0008559295,0.014111938,0.061487112,0.07674356,0.0028576464,-0.032388143,13,1.9696025,-26.309166,10
49,"the following were discussed in today's class
1. a short discussion about a website where one can play with multiple aspects of neural networks by interacting with it directly.
2. most of the discussion revolved around this section where various classification model performance model analysis tools/numbers/metrics were discussed. some of them are
	a) confusion matrix: overall glimpse of the model performance, showing a matrix of predictions vs outcomes
	b) precision: this metric is defined for every class. precision of a class shows how precise is the model when in classifying data as this class(tp/tp+fp)
	c) recall: this metric is defined for every class. this shows how well the model is able to recall this particular class(tp/tp+fn)
	d) f1-score: harmonic mean of precision and recall. signifies how good the model is. accuracy metric is not a very good metric for model performance analysis as it doesn't capture 	misclassification of under-represented class: example of fraudulent transactions which have very less representation(npci example)
	e) roc(receiver operating characteristic) curve: explains the trade off of increasing tpr with fpr of the given model. auc(area under the curve) signifies how much one has to trade 	off tpr to decrease fpr.
3. clustering problem(unsupervised learning problem). example of customer segmentation was discussed. then went on to clustering algorithms like k-means, hierarchical clustering etc.",-0.043618936,-0.07428308,-0.07633201,-0.03325984,-0.016549332,0.057944205,0.033471663,0.07738696,0.046800584,-0.05808664,-0.053039145,0.019297922,0.037529744,0.02399085,-0.13178127,-0.10633737,0.061418016,0.06750017,-0.053599477,0.012227855,0.041371603,-0.0026485317,0.09463698,0.03569241,-0.051413227,-0.055726804,-0.05116791,0.04495206,-0.07219327,-0.028061252,-0.024412576,0.0015578358,-0.017830156,0.07474595,-0.10452302,0.027118424,0.015460894,0.024568405,0.05095589,-0.07707753,0.017372964,-0.056799747,0.03504568,0.0026452425,0.11788766,-0.039929323,-0.020576764,-0.021475282,-0.07315516,-0.018393597,-0.08221446,0.0691905,-0.020352196,0.0029786073,-0.0546542,0.0644724,0.018392103,-0.0016295707,-0.07681972,-0.013335482,0.027388236,-0.080676034,-0.067252785,-0.018312136,0.03936614,0.011543921,-0.05840708,-0.019956581,0.031548753,0.026979847,-0.0017912262,0.029073682,-0.0070167575,0.015347087,-0.022640591,-0.0025449945,0.042176887,0.09497011,-0.0101941265,-0.02867653,0.035380978,0.065107256,0.008425389,-0.0340286,0.12068221,-0.024508463,0.0076674484,0.0020340497,0.01053859,-0.05609715,0.035672132,0.039815135,0.015753351,-0.10851739,0.06916905,0.041415595,0.042700242,-0.045875564,-0.008858322,0.022359358,0.014029838,0.08322876,-0.05656421,-0.089634165,0.07287006,0.038183983,0.008280432,0.054728433,0.09469301,-0.05653003,-0.0038272408,-0.031159222,-0.12679951,0.0074681966,0.0745452,0.03021709,-0.041236702,-0.032106765,0.069784746,0.14042968,-0.15843095,-0.012479865,0.053234167,-0.008153384,0.0547283,-0.008824603,-0.1306654,8.300415e-33,-0.054086536,0.011576218,-0.04780717,0.015029032,-0.020631403,-0.011530339,-0.0805242,0.009847088,0.0729405,0.031058664,-0.059621233,0.058339894,-0.014573828,0.06717913,0.052248377,0.054587204,-0.08283927,0.0030751345,0.012122279,-0.022810671,0.11421659,0.023061587,0.1051078,0.035600945,0.03965238,0.042240374,-0.035698894,0.079251274,-0.078228176,0.026334895,0.006695426,-0.044944476,0.021635968,-0.011062427,0.08976763,0.010215471,-0.032220695,0.021372268,0.007809867,-0.050115675,-0.078592,-0.023959642,-0.013107011,-0.0053055603,-0.10254013,-0.009165337,-0.0240672,-0.020613698,0.024890536,0.0121338125,-0.005575508,-0.048307724,0.00894533,-0.023354404,-0.03415635,0.018477514,0.02095762,0.011644486,-0.043078188,0.07416248,-0.020535773,0.0019555998,-0.024591861,-0.044150997,-0.065632746,0.005373086,-0.034182213,0.1351637,0.023531621,-0.017099686,-0.040324114,0.008016212,-0.03458029,-0.01841309,0.04372309,0.007397514,0.08139834,-0.06678875,-0.039294466,0.074085765,-0.0720194,0.023015909,0.0020249903,-0.044335738,-0.104875274,0.032185517,0.096554816,0.057667587,0.044812884,0.0141120795,-0.039375447,0.034007378,0.03556863,0.035155617,-0.09941133,-9.40309e-33,-0.044964064,0.029551873,-0.04727092,0.085409574,-0.0108309295,-0.004989895,-0.07384242,0.0008600957,-0.048524164,-0.024564823,0.02116399,-0.010967178,0.02912106,0.00068507,-0.014621661,-0.039475605,-0.083984606,-0.052818332,0.050655413,0.020565381,0.0994969,0.03618423,-0.0630631,-0.0055471994,-0.07571253,0.059024487,-0.044168826,-0.014856621,-0.022567559,-0.07113279,0.030693052,0.0057458184,0.048888158,0.045259174,0.020158712,-0.009997357,0.078218706,-0.050483994,0.032936946,0.09076953,0.0043657403,0.036280356,-0.12441725,-0.016392002,0.008407417,0.01546372,-0.047675304,0.053070925,-0.022162737,0.0075734407,0.03872807,-0.014184138,-0.032168634,0.01336223,-0.042314317,0.009595651,0.0032686286,-0.013463274,-0.033208884,0.12491974,-0.01719767,0.013052446,-0.043965265,0.058807444,0.02237255,0.0029321448,-0.011686019,0.017100845,0.010627956,0.08540446,-0.033593524,0.029396292,-0.015870115,-0.030190054,-0.04864939,-0.031564616,-0.019778145,0.023629215,-0.0076407515,0.01986459,0.0007161418,0.03245636,-0.00018771739,0.008400369,-0.06228333,0.02191204,0.088704295,0.06765889,0.012862603,-0.06257722,0.02577959,0.049810767,-0.03101566,0.006252567,-0.04762034,-7.0864075e-08,-0.027005073,0.04797062,0.050023593,0.00750039,-0.005133218,-0.04299351,-0.07324151,0.038594462,-0.025172833,0.057467997,0.037740685,-0.0059665744,-0.15986268,-0.09281811,0.0072037885,-0.05094975,0.012299075,0.116115876,-0.020891847,-0.015608689,0.06852091,-0.019288328,0.028793054,-0.018240113,0.056043513,-0.051507194,-0.046449438,0.101732545,-0.035614166,0.037175495,-0.026854906,0.016493095,0.07012891,-0.05043247,0.012196258,0.08477737,0.023809265,-0.057440493,-0.019841172,0.05980151,-0.035645153,0.00051804,-0.063960284,-0.0008669528,-0.0065584388,-0.029663807,0.0029286824,-0.020000149,-0.02878924,0.029932752,-0.027718008,0.05297846,-0.0956657,0.050056085,-0.04678111,0.008526706,-0.023048474,-0.12917104,-0.029276283,0.001932615,0.0272378,0.029999947,-0.0461409,-0.0013810204,7,6.232601,-30.644234,10
110,"discussion on likelihood maximization, and weight calculation using gradient descent. the confusion matrix was introduced, explaining accuracy, precision, recall, and the f1-score. we also addressed common student questions on clustering, regression models, and histograms. lastly, we reviewed assignment e1, highlighting mistakes such as misusing the kurtosis function and incorrect model comparisons. emphasis was placed on submitting well-organized excel files with proper documentation and formulas.",0.008413245,-0.053615306,0.0058876737,-0.011661084,-0.036144603,-0.014932222,0.0054045697,0.07352507,-0.044477545,0.007236805,0.08843054,-0.019290624,0.056010492,-0.019096479,-0.08325776,-0.0051641315,0.042548023,0.08189749,-0.072073296,-0.08700051,0.0017614317,0.04493531,0.0074404795,0.008200671,0.05867037,0.009990701,-0.007264502,0.015425572,-0.042413574,-0.031805106,-0.008296468,0.0042123008,-0.004894198,0.030178975,-0.0889346,-0.0054440997,-0.035416793,0.0025206024,0.024740605,0.059878245,-0.10250454,-0.073090844,0.005157443,-0.0067986296,0.012785651,0.03126182,-0.12855354,0.024281029,0.038802687,0.026309475,-0.038352314,0.021701993,-0.060107097,0.014631057,-0.008066226,-0.047096703,0.025241924,-0.052029513,0.01822099,-0.0024728975,-0.009885712,-0.04638541,-0.027016856,0.06426041,0.019981595,0.005663569,-0.07784414,-0.06326286,0.0075331163,0.02706233,0.0056279884,0.04240513,-0.033959094,0.04865418,-0.052597597,0.07661835,0.03411603,0.01115284,-0.0036170478,0.04767501,-0.027391953,0.066681996,0.035848286,-0.04822915,0.14064348,-0.05753671,0.012415963,0.04842249,0.0479104,0.00081844954,0.019286525,0.0047548693,-0.038266934,0.06814991,-0.037324306,0.04209725,0.011124323,-0.066312484,0.027137157,0.052653752,-0.06095219,0.09140589,-0.012781277,-0.0260097,-0.093689315,-0.08914301,0.06752374,0.04784883,0.06312838,-0.12665014,0.025174953,-0.031063015,-0.09271804,0.0057464684,-0.06008882,-0.034549132,0.15792295,0.0358445,-0.040933255,0.10569224,-0.031342477,0.0023036192,0.061570644,0.0098669,0.010007326,-0.092263825,-0.12927285,8.383244e-35,-0.104647435,-0.02668082,-0.003032755,0.008202789,-0.017841227,-0.022560496,-0.09775122,0.015865954,0.0014033213,-0.02906029,0.0011131136,-0.00669094,-0.009128288,0.053460725,0.0061245672,0.05832469,-0.014320337,0.07413735,-0.082675524,-0.05669719,0.06610289,-0.10472703,0.104391456,-0.060820002,-0.047983777,0.07473145,0.03977578,-0.010342849,0.04154647,0.000992778,-0.019271236,-0.08929455,-0.034117598,-0.07837233,0.0066803033,0.06570144,0.046514906,-0.0118199065,0.04526649,-0.0011767442,-0.0115807885,0.030533783,0.07741721,-0.076549634,-0.05612245,0.034606356,0.05886183,-0.0044957367,0.10153135,-0.03257951,-0.076451056,-0.09938054,0.0060767913,0.016309699,-0.028652702,0.0941052,0.011036804,0.029211877,0.0044703246,-0.016921863,0.02997183,0.02585095,-0.02690851,-0.033381872,-0.03908293,-0.023493271,0.0077610663,0.036815472,0.042646796,0.04503827,0.0013447894,0.004615182,0.08998387,-0.0020521497,0.015010015,0.07406664,0.04417351,-0.0021475093,-0.013246573,-0.07832632,0.058544926,0.014401982,-0.07453622,-0.03656179,-0.09530777,-0.0027882818,0.030190637,-0.046670176,-0.038558032,0.060540054,-0.03102873,0.03126961,-0.03483217,0.02604705,-0.02191152,-2.077332e-33,-0.10625059,0.06104962,-0.005993566,-0.010640927,0.066186756,0.0004010958,-0.009033308,-0.014680966,0.007047168,0.006810776,0.034286562,-0.014272038,-0.12196992,0.0032945944,-0.007914767,0.042422306,-0.0019293167,0.0066916435,-0.03341621,-0.060754605,-0.0004988452,0.06638365,-0.054497223,0.0110175535,-0.018088536,0.06526062,-0.0105832545,0.01879416,-0.061037395,-0.018194057,-0.033396028,-0.017805405,0.027940612,0.06498153,-0.008033855,0.050022904,0.0006851035,-0.03181607,0.013393612,0.032595452,0.06537863,0.08609463,-0.07273148,-0.07460696,0.06928681,-0.035853732,0.041246176,-0.054589402,0.076191194,-0.0036020116,-0.0830541,0.012674967,-0.072787054,0.08123112,-0.028489243,0.08986615,-0.012719836,-0.05188832,0.0032676742,-1.5101368e-05,-0.037766818,-0.028807392,-0.036527127,0.033970427,-0.035955768,-0.03629271,-0.015018933,-0.028662449,-0.0015339999,0.10165218,-0.10264394,0.024581427,0.08286422,-0.0745057,0.014470694,-0.06408244,0.0038764395,0.0285197,-0.058537703,-0.040562868,0.062759146,-0.047383904,0.025843648,0.09474001,0.040216196,0.04454729,0.027265484,0.046164583,0.0072727203,0.0027969663,-0.053869728,0.06830086,0.11659576,-0.06826366,-0.09210565,-4.1861533e-08,-0.030841634,0.039028388,0.015643118,0.046000663,0.00993926,0.012497156,-0.03574135,0.07946755,-0.04491942,0.02205455,0.0379581,-0.055986438,-0.07843726,-0.025597554,-0.03469325,0.09703217,-0.038148765,0.09841789,-0.004321489,-0.062370963,0.039999995,0.014501995,0.032254774,0.044712953,0.05042423,-0.06913929,0.013774222,0.14617145,0.006670308,-0.015995393,-0.06247399,0.0024510059,0.0023799702,0.015488709,0.006143812,0.039402917,0.006931297,0.008907884,-0.009786649,0.07223544,0.018700436,-0.0014807055,-0.029175045,0.010054984,0.12145457,0.09313187,-0.020582987,-0.040588453,0.030003354,-0.028670613,0.045254435,0.008723306,-0.03825639,0.1203149,0.0426247,0.007214876,-0.040184136,-0.037358426,-0.014110945,-0.049800437,0.07343104,0.0054401094,-0.049655266,-0.03259942,13,6.2391677,-25.522535,10
121,"we started with doubts in the google form and then explored logistic regression, focusing on how different variables influence outcomes and how to adjust weights to prevent overfitting. then we discussed evaluation metrics, including confusion matrix and accuracy. lastly, the tas gave feedback on assignment 1.",-0.04212684,-0.068234965,-0.016776528,-0.018990088,-0.001618492,-0.03679044,0.083190635,0.01854808,-0.07652005,0.0009938185,0.009875798,-0.03256366,0.0098059075,-0.019231293,-0.040444683,-0.045344952,0.027372094,0.014866076,-0.070147514,-0.04942446,0.019595385,0.043535527,0.08587046,0.0056347805,-0.04952646,-0.08595393,-0.055377405,-0.03332778,-0.0042994563,-0.038008917,-0.060880277,0.022085916,0.012042915,0.012744342,-0.08767997,-0.012517508,-0.047051392,0.03188508,0.0103344275,0.0315472,-0.10288887,-0.120619394,-0.016228352,-0.026798328,0.0505758,0.010229123,-0.09421596,0.0046440302,-0.03059828,0.06166841,-0.11227662,-0.020819968,0.011554376,-0.09034287,-0.11105212,-0.046825595,-0.04600813,-0.041709825,0.006832786,-0.060862564,-0.07260583,-0.03312545,-0.043454427,0.028374719,0.06178484,0.0008925857,-0.07325833,-0.051421057,-0.01375651,-0.026299752,-0.022769282,0.025967589,-0.04848658,0.02449119,-0.006092168,0.04734043,0.015909908,0.033399496,-0.018681992,0.09366153,0.015317042,0.06544752,0.006758998,0.05739456,0.069533594,-0.12554114,0.0465425,0.05778462,0.014739914,0.0080073,0.07249228,-0.00682923,0.06240738,0.015890379,0.025086226,0.067648605,-0.036481567,-0.05608972,-0.010394481,0.0054225726,-0.044551704,0.089219786,-0.030305106,-0.054857187,-0.008829681,-0.032306716,0.024029009,0.0096395975,0.10156421,-0.08389158,-0.0032164643,0.06950258,-0.081175104,-0.0036768068,0.010788751,0.021858303,0.02837029,0.080353305,0.0012650365,0.08574772,-0.033713646,0.020103732,0.07815341,-0.054916106,-0.0063316138,-0.039784808,-0.085211575,1.7655372e-33,0.018874504,0.05681314,0.021732438,0.05715652,-0.038031235,-0.002436242,-0.11676142,0.048556726,-0.033083655,-0.04937637,-0.033216678,0.06622821,-0.011196082,0.10354463,0.052333843,0.042339843,-0.07496167,0.12209376,-0.028956432,-0.013676018,0.03325026,-0.0034740325,0.054755498,-0.082830004,0.060763035,0.025902258,-0.037782036,0.027205642,-0.03572318,-0.014104054,0.02135851,-0.07931717,-0.038723975,-0.0072073527,0.00691065,0.05373951,-0.061108038,-0.0037890172,0.021941736,-0.0042885123,-0.09097869,0.024789963,0.02121939,0.013795987,-0.052083265,0.026804727,0.04403055,-0.09203515,0.037120942,0.03405003,-0.040282637,-0.02461968,-0.0658821,0.052812412,-0.05213113,0.08904759,0.047052354,-0.056914944,-0.08049899,-0.021764114,0.045969684,0.034217134,-0.017803626,-0.085087806,-0.12687986,-0.03386902,-0.010524716,-0.045914467,0.096761994,0.009691391,-0.08549582,-0.005669443,0.04205936,0.006799344,-0.04049713,0.080744274,-0.028771257,0.01913123,0.021188246,-0.029111885,0.07054814,0.026131783,-0.011403044,-0.025153844,-0.033617217,-0.020875001,0.039183896,-0.0134199485,-0.10374998,0.032462504,-0.05235215,0.024399515,-0.035981815,0.022782821,-0.011187025,-3.9962914e-33,-0.14790747,-0.0040276307,0.019644143,0.08300668,0.016305106,-0.020232977,0.0421821,-0.072973825,0.06336603,0.04118534,0.0021054775,0.03655508,-0.0025144168,0.051242687,-0.04578091,0.030407544,-0.054918878,-0.012515763,-0.035577234,-0.015612318,0.08854796,0.130586,-0.055069312,-0.035880607,0.0073504266,0.04632738,0.015935052,0.010751331,0.027161565,-0.0372496,-0.021955624,-0.018627012,-0.012510665,0.0652712,0.09897158,0.027717276,0.024518723,-0.034480978,0.03989055,0.093790755,0.008008774,0.03356506,-0.025187872,-0.058588404,-0.011058446,0.026688818,0.015734166,0.016379882,0.03267015,0.0055686547,-0.010699555,0.027554879,-0.09037582,0.065317325,0.019918028,0.020946352,-0.0114435805,-0.0820944,-0.013053774,0.036538146,-0.053731482,0.07466192,0.032441426,0.053408634,-0.027575051,-0.06018573,-0.003838144,0.037770312,-0.0063768714,-0.025543112,-0.038242914,-0.045202035,0.09049097,-0.0366767,-0.0068084677,0.0057054944,-0.0013961833,0.020626513,-0.060527056,-0.042496882,0.030452514,-0.047033858,0.00118999,0.10783477,-0.015232383,0.09142494,0.08532706,0.018055923,0.02158699,0.023088088,-0.08541246,0.039573886,-0.004739267,-0.0044992967,-0.123440206,-4.331884e-08,0.015418956,0.0489503,0.064795926,0.06567492,-0.025242895,-0.02241969,-0.037532058,0.05106051,-0.08757794,0.055730544,0.053105384,0.039050736,-0.088784955,0.00693129,0.0015568575,0.00610466,-0.029721025,0.11394576,-0.0001692933,0.019012103,0.08477673,0.008701619,-0.038648862,-0.0007308893,0.043159384,-0.018693369,-0.04241182,0.0927982,-0.07118284,-0.0494184,0.054571323,0.034181774,0.018669872,0.0154278865,0.046263378,0.0031057103,0.0009409908,-0.059102677,0.03348631,0.10963786,-0.0053288955,0.07295696,0.018717043,0.049232665,0.076414324,0.036125727,-0.02573858,-0.0013041993,-0.06023568,0.021033427,0.01768629,-0.020106792,-0.04661971,0.07258099,0.14333034,-0.025656383,-0.0046231304,-0.026885107,-0.03500718,0.09326873,0.0133076925,0.08952286,-0.02848205,0.016050331,13,4.658779,-23.414137,10
126,"we started with logistic regression and found out how we could find the weights that will drive our predictions. our aim is to bring our predicted results as close to the real targets as possible. when the actual value is 1, we want our predicted probability to be high; if it is 0, then low. this requires taking that into consideration for all of the observations in our training data.

since dealing with product terms can grow complicated, we take the log of the likelihood function, and turn multiplication into addition, so turning things over is significantly easier. maximizing this gives us the minimum amount of error possible, and hence finds us the best fit for our model.

for analysing whether or not our model was a good fit, we use a confusion matrix, which categorizes predictions into four groups:

- true negatives (tn):correctly predicted negatives 

- false positives (fp):positives wrongly predicted (false alarms) 

- false negatives (fn):negatives wrongly predicted (missing actual positives) 

- true positives (tp):correctly classified as positive 

by using all of this, we get a number of valuable performance metrics: 
- the ratio of the correctness of outputs from the model for majority of the cases. 
accuracy= (tp + tn)/total count of observations

- precision:when the model indicates something is positive, how frequently is it truly accurate? 

- recall:among all the true positive instances, how many did the model accurately recognize? 

- f1-score:a compromise between precision and recall. unlike accuracy, which may be deceptive in the presence of imbalanced data, the f1-score accounts for both precision and recall, avoiding misplaced confidence in the model's effectiveness.

in summary, precision indicates the trustworthiness of our positive predictions, recall measures our effectiveness in identifying positives, and the f1-score guarantees we remain realistic about our accuracy.",-0.004578165,-0.069503516,-0.050516427,-0.041678105,0.05224077,-0.07400832,0.056231946,0.07179716,0.027227454,-0.018889213,-0.007852742,-0.032052495,0.053060636,0.034111556,-0.037722293,-0.03240148,0.091523856,0.040307313,-0.10169493,-0.018461267,-0.016811693,0.001183228,0.060414925,0.017089747,-0.05832919,-0.014553022,-0.062412493,-0.026041333,-0.080089346,-0.071099356,-0.02680569,-0.0013705468,0.07481891,0.011724417,-0.04768742,-0.0015017026,-0.0444666,-0.057838164,0.01771299,-0.001488303,-0.019907234,-0.026047038,0.013893365,0.0002525148,-0.019645883,-0.05789462,0.02516337,0.03957942,0.003551304,0.01062921,-0.13265912,0.0721042,-0.021334339,-0.048174795,-0.10596405,-0.013816914,0.031328835,0.0040282467,-0.005823726,-0.03915498,-0.010460255,-0.045128826,-0.021190628,-0.046009105,0.0064311074,-0.04458597,-0.070908695,0.0063341614,-0.02433497,0.14577988,-0.0007944295,-0.0075187716,-0.055247344,-0.020407492,-0.019633042,0.112645954,0.10514169,0.024525251,-0.023754325,0.122252926,0.020962855,0.043709163,-0.0046054847,-0.0016085018,0.13679928,-0.07567613,-0.0012297237,0.021505872,0.08582657,0.011950101,-0.025552707,-0.03655282,-0.02044081,-0.01381656,0.0748206,0.07531543,-0.0065730363,-0.062378254,-0.01911808,0.04894667,-0.050864134,0.10111499,0.06330493,-0.11287267,0.041670848,-0.046491086,-0.006808542,0.07351204,0.07502105,-0.06446047,0.004933522,0.03609024,-0.059042756,0.01694156,0.06997752,0.013390206,-0.018775208,0.014640197,-0.11402079,0.12354004,-0.029485464,0.06806556,0.07472209,-0.008989308,0.039776113,0.06695768,-0.13600421,8.111909e-33,-0.075997986,0.08656004,0.027848428,0.028446566,-0.06281818,-0.004396924,-0.09749683,0.03159723,0.05257292,0.044538792,-0.05493174,0.045016825,-0.0032400745,0.099028625,0.085238285,0.03059843,-0.10256127,0.018186895,-0.028189963,-0.029436009,-0.03285674,-0.08721616,0.01821592,-0.07137368,0.032720145,-0.008084184,0.02796225,0.07679141,-0.07409182,-0.02705482,0.014865376,0.002977691,0.07228083,-0.03984022,0.061459847,-0.01833335,-0.095280565,-0.0049882317,0.058663007,0.015688412,-0.09296923,0.029657096,-0.023121368,0.0272081,-0.018398296,0.0024242767,-0.051957108,-0.055632252,-0.0712688,-0.032307398,0.026776643,-0.050518706,-0.0016219866,0.07099236,-0.12814194,0.014968663,0.017292738,-0.055236496,0.010260455,0.013503853,-0.028792698,-0.046105128,0.015712695,-0.030472592,-0.062017325,0.023783159,0.09583622,0.019920703,0.0026743708,0.011593934,0.021497266,-0.013136033,0.07215336,0.002719074,0.105352454,0.041256484,0.07273141,-0.025037177,0.029096106,-0.0046888757,0.061635304,0.089194074,0.02024943,-0.053421076,-0.045020018,-0.033082493,-0.021891642,-0.0138618965,-0.13083664,0.06876025,-0.060659647,0.04406599,-0.012474584,0.015395184,-0.022944631,-7.959186e-33,-0.07789106,-0.046754826,0.06534552,-0.00049640116,0.007620046,-0.060008097,-0.023685299,-0.060754575,-0.01724656,0.011285796,0.024961641,-0.015363489,0.043050792,0.05217361,-0.061168034,0.021248072,0.0028517453,0.043112293,0.0681616,-0.022707831,0.11623221,0.09556915,-0.1446417,0.014277633,-0.029516082,0.04516047,-0.04613156,0.030132309,0.00774143,-0.07010816,-0.04068725,0.014120894,-0.018824585,0.010400056,0.085958704,0.026848443,0.018408157,-0.032675833,0.03808918,-0.0048036547,-0.01332984,0.051525705,-0.07537277,-0.023153434,-0.00996673,-0.07077319,0.06936569,-0.058296874,0.061655384,0.04510577,-0.027026433,0.008873873,-0.045449425,0.087055035,-0.029181307,0.035281304,-0.070595995,-0.088637866,-0.068119735,0.024740549,-0.110778384,0.028678078,0.03421815,0.005528847,-0.048924774,-0.0023145906,0.018625215,0.055784464,-0.0070731323,-0.032615,-0.032874797,0.0046835816,0.039366934,0.05536936,-0.04915221,-0.018390855,-0.023457328,0.011282324,-0.029147329,-0.005623838,-0.032403704,-0.047862303,-0.04440746,0.011371828,0.026233863,0.037212465,0.048977736,0.044037584,0.009217638,-0.036933407,-0.02901906,0.0030493247,0.0214026,0.06676579,-0.04350063,-7.7892906e-08,-0.022200918,-0.018484868,0.06666359,0.0076667,0.023832072,0.00421493,-0.028245403,0.060174417,-0.040703647,-0.01885433,0.024316885,0.009950494,-0.17264976,-0.021140723,0.037334993,0.03088264,-0.05736751,0.08414435,-0.007437343,0.030745434,0.058298036,0.01940244,0.004738775,0.01383229,0.047631286,-0.09120317,0.005203306,0.039021492,-0.040049467,0.013249378,0.011201085,-0.016387744,0.0100803245,0.05775979,-0.024297874,0.04976013,0.021317393,-0.06524975,-0.032898318,0.019347353,0.005846406,0.0767055,0.015648978,-0.005850545,0.004800378,5.0847943e-06,-0.025353128,0.045979135,-0.0028952274,-0.023359474,0.024159012,0.049910825,-0.069099784,0.062946156,0.05985598,0.01075607,0.009307413,-0.0007494924,-0.028226838,0.08363666,0.009422178,-0.028583784,0.022999454,0.020481879,13,2.4381762,-24.273611,10
127,"today we started with discussing more upon logistic regression. we talked about logistic unit, which is basically the probability of an observed data to be in the labelled class. so this is basically a conditional probability, that is p(y when x occurred). a convention we used in class is that the probabilities are denoted by p and the labels are denoted by t. to calculate the loss function here we usually use log to ease calculations.
the metrics in this model is mainly the confusion matrix. it is a 2 v 2 matrix with true positive, false positive, true negative and false negative. then there is precession and recall, precession is defined by the number of observations we classified correctly. and recall is for a particular class, that is how many have we classified correctly under that class. and another term if defined as f1 which is the harmonic mean of the above two.
and then tas gave us their findings from the exercise, from which the main point was the confusion in kurtosis and excess kurtosis. excel reports excess kurtosis. ",-0.0312348,0.0004711683,-0.063852794,-0.0109019615,0.0143167125,-0.0043572136,0.05880596,0.077995904,0.10221675,0.07533943,0.090002544,0.034480628,0.03336515,-0.04807414,-0.018890029,-0.092593774,-0.013320765,-0.0021349946,-0.106611244,0.062271595,0.124439016,0.05289483,0.023904888,0.05534086,-0.0095419595,-0.09423955,-0.02275111,0.040443923,-0.003688701,-0.026926877,-0.06482912,0.059537128,0.018323401,0.060496617,-0.08178716,0.051458694,-0.000799895,-0.03560087,0.050639264,-0.0004554469,-0.07338009,-0.08663848,0.0029771184,0.06730114,-0.0024248438,0.019471219,-0.061128438,-0.017536419,-0.021934485,0.0028977026,-0.029173177,0.08679032,-0.00843974,0.004446843,-0.026121916,0.039829493,0.034310333,-0.04476538,-0.0390234,-0.0039295754,-0.073513314,-0.09621577,-0.08306771,-0.01670415,0.027736837,-0.01229529,-0.039382257,0.0018553077,0.039634135,0.07203306,-0.059525564,-0.05449068,0.01508048,0.0021820178,-0.013277181,0.022582809,0.0750641,0.06534327,-0.049773086,0.02276786,0.04389571,0.12558259,0.0613158,-0.012169245,0.10024546,-0.024870845,0.047694594,0.025060253,0.05057382,-0.02924456,-0.0042996868,-0.03215378,-0.0067507545,-0.0012824531,-0.028418425,0.049614698,-0.04684874,-0.05880121,0.06550875,0.007919118,0.029263537,0.058133375,0.00095851085,-0.004153277,-0.0114230225,-0.089495525,-0.014496023,0.044340335,0.06349936,-0.016712226,0.013526252,0.042841982,-0.07602135,0.053361505,0.011978172,0.033331685,-0.014835118,0.026112493,-0.015494692,0.022671288,-0.058785953,0.057932824,0.051885515,-0.0075214873,0.012066943,-0.070039794,-0.047547527,-1.945629e-34,0.020912917,0.021735704,-0.01758733,-0.04536772,-0.05353311,0.02571416,-0.132366,-0.005014002,0.04125911,0.041281678,-0.026264155,0.043234475,0.010187491,0.07456217,-0.0041216346,0.004156913,-0.13002953,0.03249883,0.036511403,-0.031672236,-0.08221359,0.01784623,0.09959918,-0.06806024,0.0065114144,0.061241906,-0.040536493,0.008727159,-0.0426326,0.036426775,0.0012743935,-0.020494327,-0.008022789,-0.10995538,0.01556398,0.083422884,-0.03410195,0.030011851,0.0045086243,-0.05421015,-0.07657959,0.012134366,-0.0011027505,-0.061389163,-0.116619565,-0.1235236,0.0359595,-0.04996665,0.012245734,-0.00965499,0.008352197,-0.12498198,0.03387913,0.012785956,-0.04969372,0.09498271,0.0702059,0.014519288,-0.05685144,0.058143824,0.004381313,-0.0123503925,0.051460158,0.011143157,-0.07556486,0.0030500519,-0.0016817901,0.016170971,0.06810658,-0.0014466088,-0.003592464,-0.06290098,0.030273067,0.039076347,0.0681806,0.091696925,0.0109410705,-0.0117957,0.03303897,0.010347553,-0.025833962,0.022119459,-0.06219045,-0.018134885,-0.054220296,-0.011515127,0.068025626,-0.054874334,-0.094402276,0.02417555,-0.03626882,0.050213948,0.0015743104,0.076215416,-0.035160176,-4.8612848e-33,-0.026177263,0.10621996,-0.044363815,0.004833089,-0.030056063,-0.06361409,0.034490958,-0.010330434,-0.040309496,-0.06612851,0.05564367,-0.045428794,-0.091986544,0.06448407,-0.015080794,0.016227629,-0.07635753,-0.029175002,-0.07215397,-0.07253403,0.09228746,0.05779785,-0.030596653,-0.048235014,-0.01211095,-0.026300525,0.039209936,0.0010523489,-0.030190613,-0.06863727,0.00086217717,-0.022048594,0.011611645,0.071626276,0.043573383,0.013516803,0.038464215,-0.031468365,-0.028927786,0.023029616,0.029706871,0.056030627,-0.040934373,-0.014566907,-0.023619886,-0.04560197,0.054616835,0.0075086975,0.116942324,0.011227878,0.012850264,-0.031627353,0.007005641,0.07583782,-0.038689367,0.026327442,-0.062921315,-0.12057057,-0.12239423,0.07247132,-0.05745043,-0.012011905,0.0023085107,0.02548285,-0.0071808477,-0.044702742,-0.034956787,-0.009656219,0.004961502,0.011207268,0.01117946,0.062538035,-0.07863398,-0.05680168,-0.0071367356,-0.05541201,-0.056276716,0.0024240143,-0.019677881,-0.021660412,-0.0641435,-0.030482149,-0.045885444,0.11150116,-0.07370691,-0.008718645,0.087349445,-0.00027494767,0.0004133182,-0.013598464,-0.034991782,0.06523278,-0.007811688,0.046139155,-0.07967789,-5.5890272e-08,-0.05455332,0.0020470058,0.07744505,0.017778391,0.09377519,0.014329419,-0.04591863,0.014470295,-0.03862672,0.02772648,-0.00731682,0.030055517,-0.061074764,-0.10082199,-0.062630154,0.00053122157,0.02802776,0.10272192,-0.016332947,-0.007970887,0.07323046,-0.062407102,-0.008138023,-0.040791266,0.022882307,-0.102021724,0.040797953,0.13699728,-0.052821487,0.013107634,0.019968325,0.029209398,-0.010436574,-0.018131709,-0.0264408,0.04474976,0.064243004,-0.095274314,0.06041373,0.061357528,-0.06991213,-0.06722367,-0.009270548,0.0057536657,0.009228866,0.09518984,-0.027732946,0.04436655,-0.025947258,-0.022456363,0.009861165,0.06410097,-0.041855846,0.079167366,0.04013888,0.03907352,0.027847722,-0.027986644,-0.022599328,0.009868549,0.020495707,0.060222264,-0.015079169,-0.059772886,13,3.1917,-27.169556,10
131,"in today's class, we first saw that the mean can be the expected value from the x sample. then we saw that different bin widths can give us different patterns in the histogram. in excel, there is a certain mechanism through which it sets the bin size initially but we need to choose the number of bins or bin width based on what we are looking for in the histogram. then we saw that to calculate the standard error, if we have just one sample, we take the standard deviation of the population to be equal to the standard deviation of that sample. then we continue with logistic regression where we name the clusters as class label 1, label 2, etc., and make boundaries to separate them. suppose we are getting a=w1.x1+w2x2+w3.x3+b then we need a function that will convert a into either 0 or 1. we use the sigmoid function p(y|x)=sigma(a)=(1/(1+(1/e^a))), the prediction is 1 if p(y/x) is greater than 0.5 and else it is zero. then we saw the metrics associated with the classification like true positive is when it is 1 or 'yes' and prediction is also 1, and false positive is when it is 0 or 'no' and predicted is 'yes'. accuracy tells us about how often the classifier is correct. suppose in a case there are 100 points of one class and 5 of another, then still if we do not consider the 5 points, the accuracy is high though, this is the case of data imbalance. then we saw precision means off the events that are detected, how many have we detected correctly and recall is off the specific class, how many did we classify correctly. at last, we got the feedback from tas regarding e1, it points out the point that we should calculate the error kurtosis using its formula and not through excel's inbuilt function, and we also that we should document our observations properly, adding all the relevant things so that the reader can understand your conclusions by reading the document.",0.038916755,-0.0063055926,-0.080261536,0.028505927,0.08568246,-0.031784147,-0.008815434,0.03025798,-0.0055972463,0.041098434,-0.003879321,-0.04617944,0.056229003,-0.04022723,0.04437584,-0.054662865,-0.016455153,0.013992754,-0.092321,-0.02495899,0.0306704,0.030946756,0.02989267,0.018658405,-0.0007565163,0.005841309,-0.03424672,0.028843874,-0.0321211,-0.02889968,0.0074069146,0.08173006,0.13542847,0.032016747,-0.021903792,-0.011171397,-0.0012780203,0.06561172,-0.014962426,0.0065704505,-0.011546798,-0.0044222684,0.045602385,0.033178847,-0.056851916,0.00032954232,-0.066399045,0.014458153,-0.03228686,-0.012630705,0.014834018,0.07389047,-0.005996912,0.010787518,-0.073609166,-0.06861679,0.037323795,-0.017935056,0.0630031,-0.007381094,0.021080872,-0.05773276,0.031249708,-0.016261937,0.056368418,-0.028005946,-0.0012856125,-0.03094982,0.056014802,0.010171311,-0.019922053,-0.033490233,-0.03963178,0.032746058,0.06286917,0.0073420615,0.043302614,0.052630603,0.006296592,0.023708127,-0.108252056,0.10302227,0.12029287,0.023322782,0.07806682,0.033558898,-0.025827121,0.011624719,0.054669514,-0.011951915,-0.07907978,-0.009983953,-0.066842265,-0.041696083,-0.0059577473,-0.09928565,-0.054349996,-0.05368975,0.16059713,0.034244318,-0.0002629945,0.03994879,0.087049454,-0.022733327,0.06555995,-0.05806044,0.05270872,0.0040409486,0.046092585,-0.025106983,0.020403428,-6.434867e-05,-0.11511232,-0.0017096681,0.02103199,-0.025132928,0.062392604,0.043822367,-0.098412715,0.068602696,-0.002120719,0.016186284,-0.0010052079,-0.010645717,0.013090063,-0.054366264,-0.10355634,3.7848806e-33,-0.02695126,-0.09211559,-0.029775826,0.00032593872,0.023947,0.07729459,-0.12329188,0.011924812,0.06767385,-0.0010188941,-0.034689505,0.060324598,0.08550534,0.08749808,0.09191923,0.025083974,-0.04825291,0.084389046,-0.0061970116,-0.10039511,-0.054409593,-0.03566248,0.08372706,-0.037803616,-0.04558342,0.039986607,0.016718304,0.04158491,0.011778492,0.0045604017,0.010326306,0.0040884106,0.0010380655,-0.0057288264,0.07497904,0.06794189,-0.00028374154,0.012843346,0.044646867,0.021924594,-0.036709648,0.056595515,0.07907776,-0.021013705,-0.055544466,-0.008852247,0.011610974,0.01342103,0.018620458,-0.064207666,-0.05247938,-0.030806068,0.046471618,0.016626686,-0.0055970005,0.09030539,0.05809207,-0.050448623,-0.047452044,0.012213951,-0.009282399,-0.008534738,0.0008975002,0.055853713,0.031726044,0.06119471,0.06814217,0.010248511,0.0016485836,-0.008740054,-0.0037679912,0.025847316,-0.047961365,0.050901767,-0.031127432,0.02262611,0.04633307,0.034137674,-0.06714568,-0.057440188,-0.024944318,-0.00029032887,-0.10150566,-0.09155162,-0.092374586,0.05425071,0.044400476,-0.0031217444,-0.1016916,-0.06507753,-0.049654704,0.0011391034,-0.04147293,0.023270374,-0.058673806,-6.60271e-33,-0.09318404,0.08616899,-0.0027789033,-0.018562956,-0.032009754,0.032416627,-0.027186237,-0.050901383,-0.024208855,-0.016560072,0.006959023,-0.045719538,0.07015692,0.02014924,-0.07607663,0.038429335,-0.0030427754,0.0081467815,-0.06607977,-0.041467633,0.10749786,0.018931242,-0.0626404,-0.06779467,-0.11920724,-0.022945117,-0.10080088,-0.009391257,-0.06996311,-0.019320423,-0.029969553,-0.07693685,-0.099738196,-0.03319796,0.11558037,-0.03221031,-0.0916626,0.011236512,-0.008883915,-0.07302891,-0.04695206,0.0039519924,-0.057239167,-0.042700358,0.08613899,0.009190398,0.042944882,0.00450508,0.021715358,-0.028866269,-0.016529666,0.066032544,-0.021410894,0.046534885,-0.004894474,0.08422131,-0.08134782,-0.00031881576,-0.11733841,-0.015442686,-0.050747275,-0.047094207,0.0045146057,0.06747157,-0.08717271,-0.04457541,-0.01863097,-0.050806735,0.017999146,0.008724783,-0.047314707,0.029746382,0.093504995,0.0039315005,-0.10085055,0.0015212882,-0.016314235,-0.077330686,-0.011343866,0.04664372,0.009356117,-0.010936825,0.050604023,0.0866726,0.05397269,-0.06465929,0.104031414,0.03595645,0.042781904,0.061166227,-0.01448102,0.03748582,-0.042005844,0.020382853,-0.08855806,-7.7835615e-08,-0.018112551,-0.0068764794,0.05045568,-0.015204447,0.015911326,0.08051294,-0.10598054,-0.0054818937,-0.02239957,0.020556401,-0.0052203885,0.048729107,-0.14104088,-0.048564367,-0.020954197,0.04770435,0.048155174,0.062333588,0.03241013,0.03060718,-0.06631622,-0.06697303,0.031169554,-0.062060647,0.02942094,-0.03438495,-0.049903717,0.02653095,-0.043127175,0.0065405876,0.08521855,0.03616921,0.012211183,0.046447385,0.0015838763,0.032815393,-0.051940743,0.048309818,0.03676775,-0.0070721093,0.0328428,-0.0392719,-0.011170898,0.013763756,0.047022417,0.09353345,0.0019677274,0.064977705,-0.055109784,0.016134841,0.019322943,0.02368675,0.002168763,0.040555432,0.025779577,-0.02065729,-0.067727275,-0.09802503,0.0207986,0.055013854,-0.05133824,0.025023699,-0.007411516,-0.052841816,13,-1.2550293,-30.295977,10
145,"in today's session we initially discussed expectation algebra and how each data point in a sample has an expected value equal to the mean and the variance of each observation is equal to the population variance which can be replaced with sample variance if the number of observations is large. after that we moved on to formulating the boundary conditions and how can we use mathematics to deal with logistic regression. the boundary separating any two classes can be either linear or non-linear. we then saw how using matrices to compute the weights is a lot easier. closed form solutions exist for logistic regression which looks similar to the one we had for mlr. in logistic regression, in order to increase our classification accuracy we maximise likehlihood (or log likelihood) which just means that the probability of those data points which belong to class 1 should be closer to 1 and the ones belonging to zero should be closer to zero. also disussed various terms which are used to say about how our model is performing and its ability to classify the data into respective classes. we use accuracy, precision and recall and various other terms such as f1-value which is the harmonic mean of precision and recall which is indicative of how our model is performing.",-0.035979167,-0.031893186,-0.044162706,-0.034931842,0.017634341,-0.0759309,-0.005889296,-0.004123986,-0.018713472,0.030712921,-0.013802968,0.012586962,0.026308194,0.06006303,0.06376567,-0.0075053424,0.015991358,0.087082446,-0.12521753,0.053285524,0.0060421727,0.038972907,-0.008017747,0.058135785,-0.026471375,-0.10451783,-0.007897994,0.020539392,-0.01577613,-0.03213112,-0.01767039,-0.017467441,0.0106557375,-0.0141019635,-0.1440822,-0.03769793,-0.033727415,0.029649496,-0.0044719633,-0.012890049,-0.06452095,-0.049031064,0.054076996,0.044122294,0.054229356,0.016741926,0.01329179,-0.03631539,-0.0027129643,-0.001108265,-0.040682737,0.04764993,-0.033280812,-0.050785206,-0.119155884,-0.06282233,0.08466398,-0.029572451,-0.005417089,0.0018026783,-0.051682986,-0.051115226,0.01996945,-0.039956346,-0.046706744,-0.100855455,-0.009633475,0.058079507,0.038662367,0.11518688,-0.01506068,-0.026545977,-0.06604245,-0.005198156,0.047234178,0.025437618,0.024628075,0.05200096,0.0071198293,0.11916833,0.01419875,0.05993894,-0.0076003843,0.04091118,0.044688795,-0.05704867,0.030841863,0.07003409,0.005779952,-0.043449488,-0.011443294,-0.0013067304,-0.056273315,0.018374184,0.0075439885,-0.025334872,-0.041002143,-0.002001951,0.04110326,0.04864772,-0.03756999,0.010702949,0.0410372,-0.014562634,0.010097435,-0.038785677,0.036604006,0.03026192,0.034357063,-0.10951671,0.027195064,-0.03926369,-0.13953814,0.025202494,0.011823152,-0.050770625,0.086557366,0.028088378,-0.010690838,0.10913887,0.0017922764,0.038511287,0.018924898,-0.032308124,0.10817573,-0.0069258446,-0.11239725,2.9265697e-33,-0.051479656,0.042617474,-0.011277725,-0.0034230852,-0.013029787,0.010518424,-0.04656747,-9.182557e-05,0.07847416,0.069442905,0.0069592623,0.01239908,0.06356157,0.097377695,0.07353348,-0.025545089,-0.06977033,0.037943985,0.0052012736,-0.056367654,-0.019140486,-0.016454645,0.09518395,-0.09016171,-0.017463066,0.0027752155,0.0383236,0.0019134996,-0.038946185,0.014743162,0.07954663,-0.017167054,-0.033912454,-0.01500137,0.04401824,0.05770656,-0.011995529,0.037667565,0.06984225,-0.0546954,-0.06590161,0.06075958,0.03595382,0.05132645,-0.055048957,-0.05009462,-0.029691875,0.03309403,-0.047082726,-0.03802324,0.007206762,-0.075001724,-0.08472885,0.020534175,-0.027900243,0.04755951,-0.021835832,0.037492353,-0.043839663,-0.02149327,-0.010646209,0.010055121,0.01899494,-0.0060498975,0.0015205273,-0.029037956,0.041608267,0.03305617,0.019696023,-0.07722729,0.018988222,-0.04283021,0.048041977,0.002824854,-0.00953788,0.02804016,0.070086665,-0.042788003,0.03068763,-0.00760007,0.046293713,0.072773606,-0.037324306,-0.05408802,-0.07192484,-0.002183332,-0.020994741,-0.0014226581,-0.08628914,0.021349771,-0.08169762,0.015421368,-0.040334884,0.038542263,-0.010791231,-5.9097964e-33,-0.04473495,-0.00042862596,0.04606015,0.0043553603,-0.0053290757,-0.065727845,-0.022062657,-0.06586019,-0.0009863639,-0.077137,0.025577746,0.043925796,0.0016180049,0.07551829,-0.004249785,0.01533808,-0.071306184,0.055897713,-0.039284512,-0.027127743,0.05114249,0.020879008,-0.13046679,-0.0096004475,-0.015241212,-0.049551453,-0.076219484,0.030565843,0.023004318,-0.033028733,-0.06574304,-0.122782685,-0.010587354,-0.07182481,0.027050128,0.07926846,0.09568007,-0.003318242,0.030369127,0.020324187,-0.048464917,0.005792613,0.034137424,-0.07705409,0.021548688,-0.03694712,0.03435862,0.03805147,0.072209366,-0.034753412,0.015807046,0.020018937,-0.0781224,0.07722458,0.029777247,0.018147064,-0.021008486,-0.06334841,-0.10286923,0.03179259,-0.08042421,0.010674212,0.012776083,0.09309659,-0.07199797,-0.06306298,-0.019487608,-0.006462405,-0.030932782,0.022312852,-0.13437942,-0.01611744,0.071016476,0.09440392,0.0029188944,-0.058968488,-0.0040418063,-0.05891995,-0.047656942,-0.00050766475,0.0030079782,-0.047598533,-0.027937107,0.0452898,0.090002194,0.00419756,0.117242254,-0.045205552,0.04817552,-0.09101572,0.0065928707,0.056849625,0.020396564,-0.0056659975,-0.092790194,-6.809165e-08,-0.016302826,-0.042621214,0.11045028,-0.025355713,0.031236276,0.060775116,-0.045081515,0.015378176,-0.054573577,0.02634202,0.02149384,0.08342342,-0.08174859,-0.038347326,-0.022231026,0.08982677,0.034528792,0.07087692,0.019985935,-0.01191327,-0.06517563,-0.059102707,-0.011051484,-0.073192075,0.10890698,-0.08360754,-0.02395673,0.027733466,0.04091818,0.024882786,0.017730318,0.052358855,-0.0019012366,-0.0048948433,0.077442765,0.05715064,0.030476201,-0.06614976,-0.08489602,-0.040685177,0.060551662,0.05885814,0.020159004,-0.0079790205,0.17875811,0.061061617,0.0117642,0.028818993,-0.05841295,0.0066149626,0.013708186,0.020215282,-0.028728584,0.08514539,0.059893705,0.05506755,-0.012364382,-0.07551545,0.030168924,0.048479814,0.005466049,0.072974965,-0.0030149198,-0.022227611,13,-0.9562997,-27.801392,10
159,"we began by using logistic regression and learned to compute the weights. the intention is to predict outcomes such that our predicted ones match the true targets as much as possible. if the actual outcome is 1, we want to maximize the probability of predicting 1, and if it's 0, we want to maximize the probability of predicting 0. we look at the overall objective across all training data. since working with products in probability calculations can be tricky, we take the logarithm to simplify things, turning them into sums instead. we evaluate how well our model performs by using a confusion matrix, which breaks down predictions into four categories: true negatives, false positives, false negatives, and true positives. from this, we define key performance metrics. accuracy tells us how many predictions we got right overall. precision measures how many of our predicted positives were actually correct, while recall checks how many actual positive cases we successfully identified. finally, the f1 score, which is the harmonic mean of precision and recall, gives a more balanced assessment, avoiding the misleading optimism that accuracy alone can sometimes create.",-0.010831982,-0.00033196394,-0.125336,-0.006498427,0.005199392,-0.041189898,0.042460226,0.07958931,0.043417554,-0.009748846,-0.050477773,-0.035835784,0.042113163,0.037407305,-0.059067067,-0.0098156985,0.03458551,0.06681054,-0.05182864,-0.020355863,0.020523785,-0.011265128,0.10542149,0.0073614162,-0.01257412,-0.059186377,-0.06479244,-0.039288245,-0.04542318,-0.07952066,0.033242032,-0.0071917386,0.05473718,-0.004700606,-0.13798665,-0.01888032,-0.016552007,-0.046368554,0.045769393,0.000809754,-0.035752095,-0.042816073,0.004307072,0.033550482,0.05600208,-0.0015474408,0.011199859,0.031016879,0.03652888,0.06967527,-0.10590291,0.08024579,-0.038002245,-0.04556705,-0.09707487,0.03740922,0.040099964,-0.011661118,-0.010107585,-0.05043237,-0.010437493,-0.1241121,-0.044585954,-0.032656904,0.02198615,-0.063242696,-0.054535944,-0.013133405,-0.010058793,0.0793022,0.017479053,0.040070105,-0.04970918,-0.0042734784,0.011337274,0.08938268,0.0068703834,0.020987518,-0.0050919675,0.1334329,0.021085402,0.031189235,0.0050684903,-0.0107003935,0.12737392,-0.041200235,0.04369028,0.06642343,0.036802176,0.029309243,-0.0398136,-0.04412545,0.019185862,-0.046810243,0.034738384,0.09254415,-0.036375333,-0.0806719,-0.018130528,0.031326927,-0.011274177,0.06551789,0.013045411,-0.13051888,0.020168103,-0.022566546,0.006075917,0.0619722,0.0722539,-0.08677184,0.05850775,0.039786343,-0.06353545,0.053287137,0.03298416,0.06316574,-0.027003061,0.07001031,-0.05064205,0.13492575,-0.05241414,0.037042726,0.08553165,0.032719497,0.0024315335,0.046149183,-0.0835036,4.2835503e-33,-0.057860345,0.05752539,0.05338346,0.0026533247,-0.029137598,-0.021164676,-0.06450249,0.02458501,-0.0061723073,0.011428122,-0.029828155,0.06433339,-0.0025856933,0.066073336,0.073900186,0.027552467,-0.11081782,0.020079998,-0.0041808295,-0.035976704,0.015547072,-0.05441015,0.03488378,-0.06989675,0.06491778,0.02274436,-0.00043000266,0.07402989,-0.08132295,-0.014386494,-0.018032217,0.015893688,0.005628667,-0.04340355,0.058251094,-0.00039839264,-0.066313066,-0.025500735,0.036441334,0.03042059,-0.13228449,0.007384553,-0.025649684,-0.034585364,-0.053909842,-0.04047702,-0.057871982,-0.034518678,-0.052083313,0.030736629,0.0006524612,-0.030795105,-0.008155067,0.039811462,-0.07259107,0.015798902,0.029619098,-0.03567169,-0.01125112,0.036773227,-0.024684584,0.0054651885,-0.020423563,-0.0038942823,-0.10821345,0.090223834,0.056250162,0.050348625,0.03356113,0.0034526377,0.03976029,-0.01325199,0.04372781,-0.0154242115,0.044763815,0.049044605,0.07247126,-0.024632975,0.0324834,0.022052394,0.086649545,0.0934194,-0.032642905,-0.029736603,-0.05181697,0.021045571,-0.018164081,0.0072216615,-0.08953514,0.06760459,-0.06415016,0.040971808,-0.0031534983,0.026972488,-0.079069644,-3.9193362e-33,-0.0869723,-0.025786107,0.028181432,0.045143243,0.0076031163,-0.041794326,-0.052334394,-0.09483053,0.008636995,-0.0300197,0.03164107,0.0071792104,0.011912585,0.049283378,-0.046134073,-0.029978102,-0.06606175,0.03658127,-0.025886694,-0.042382713,0.1171991,0.11013873,-0.13263881,0.03787815,-0.03205156,0.045243803,0.006041119,0.0059286114,0.016026698,-0.0695408,0.004198188,-0.05238188,0.008584066,-0.027438391,0.070963085,-0.022667438,0.07598458,-0.033243705,0.06030204,0.06996866,0.030015765,0.047695015,-0.061263148,-0.02007222,-0.015298718,-0.051927824,0.04243563,-0.019210652,0.07370247,0.024158144,0.013548394,0.025018001,-0.10775516,0.07992141,-0.04107551,-0.009573286,-0.05213112,-0.04721449,-0.03198111,0.077624805,-0.12255736,0.059090197,0.03696035,0.077678226,-0.031589657,-0.042776547,-0.02830516,0.041679453,-0.019138394,0.04008708,-0.03017479,0.05081787,0.07246061,0.020624163,-0.022607518,-0.020352267,-0.017290846,0.007834023,-0.042043462,0.0055063674,-0.028114619,-0.045755006,-0.022693826,0.02718372,0.011504852,0.07425792,0.07998476,0.018443389,-0.0046409117,-0.025778811,-0.02490048,0.037920084,-0.014712455,0.0053566657,-0.085639544,-6.165221e-08,0.013163428,-0.0062420024,0.051329806,0.027263787,0.004922125,0.05002016,-0.05290703,0.03980109,-0.079467274,-0.057592124,0.08261363,0.016140664,-0.1636045,-0.051453356,0.053245325,0.054612707,0.002851667,0.11884332,-0.00041743123,0.01966552,0.0800858,-0.019072874,0.021221377,-0.0053368653,0.012193646,-0.0814141,-0.005020509,0.13428083,0.003403834,0.025399815,0.00950074,-0.0069649373,0.016070025,0.020416034,-0.008268693,0.013315023,0.07522771,-0.025286645,0.010112778,0.0014762942,-0.03347122,0.052488137,-0.006401229,0.019984348,-0.032858916,-0.0033464734,-0.02527304,0.073054776,-0.02157342,-0.011031503,0.017254816,0.016671319,-0.0515474,0.06267923,0.06565177,0.05009759,-0.020358507,-0.09512079,-0.056229487,0.070036575,0.040065814,-0.02132615,-0.012787455,0.006447778,13,2.8793416,-24.68984,10
179,"we began with logistic regression, which aims to maximize the likelihood of predicted outcomes to match targets. for t = 1, we maximize p, and for t = 0, we maximize 1 - p. because likelihood involves products, we take the logarithm to simplify calculations.  the confusion matrix contains:
- true negative (tn), false positive (fp), false negative (fn), and true positive (tp).

important performance metrics: 

- accuracy:  (tp + tn)/total events
- precision: correctly detected events among all detected
- recall: correctly identified instances of a class
- f1 score: harmonic mean of precision and recall for balanced evaluation",-0.015184835,-0.017481586,-0.053419236,-0.061117172,0.023325775,-0.038241733,0.04727852,0.08084334,0.0189041,0.04319454,0.030382255,-0.03593028,0.020251721,0.036156386,-0.043017823,0.0028622902,0.02945292,0.08744946,-0.06393237,0.0061620404,-0.0035008627,0.042021614,0.08831462,0.03939432,-0.065754294,-0.075897805,-0.029522121,-0.019246737,-0.037461415,-0.07556589,-0.019622266,0.025049388,0.04145801,0.016622258,-0.08487317,0.009315269,-0.056696046,-0.034974415,0.059293125,-0.003687474,-0.035659995,-0.06903533,0.008432664,-0.008513759,0.005824439,0.016150692,-0.048839852,0.057264235,-0.010256706,0.033998515,-0.117178574,0.06749352,-0.021131018,-0.058063906,-0.103936,0.017031634,0.042689405,-0.03368139,-0.022951135,-0.022811657,-0.062915415,-0.07200886,-0.084892355,-0.015161305,0.039621513,-0.019132117,-0.09918306,-0.03171806,0.006906613,0.093098864,-0.027597204,0.005242263,-0.061458178,-0.0058162077,-0.016000072,0.1021282,0.02749009,0.020746365,-0.011862894,0.12287861,0.010976964,-0.0040244753,-0.037459627,-0.016034603,0.16396795,-0.122630425,0.017227754,0.10282302,0.053662702,0.01945228,-0.021420885,-0.035800155,0.04889986,-0.0072782864,-0.018758817,0.055217527,-0.008389864,-0.04836519,-0.0048338226,0.022440407,-0.05254569,0.08474482,-0.032914694,-0.0609678,-0.026977783,-0.057401367,0.007196507,0.02913142,0.10112668,-0.0844011,0.018143836,0.012161507,-0.05901858,0.042843968,0.009342656,0.04652205,-0.01278539,0.09028939,-0.012950559,0.08217506,-0.03449264,0.01627273,0.06097277,0.017109152,-0.0044605,-0.01381477,-0.095107354,3.541464e-33,-0.024600342,0.058512356,0.024655057,0.014268171,-0.03758864,0.02472784,-0.14599863,0.005790423,-0.008334723,0.034656413,-0.03287534,0.011327881,-0.0130318515,0.08771728,0.016062919,0.0021302016,-0.11911597,0.10323596,-0.06723832,-0.01807436,-0.027950209,-0.018639231,0.013485174,-0.10744939,0.04572623,0.015533082,-0.0030467808,0.056295734,-0.028470704,-0.015271373,0.010659996,0.020899558,0.01569616,-0.04230047,0.055452608,0.07227645,-0.094406344,-0.015880724,0.043260027,0.022024289,-0.09647634,0.024853494,0.0037187785,-0.08372642,-0.06723581,-0.075134836,-0.020392071,-0.019934367,-0.03920713,0.0035582536,-0.035945874,-0.05139659,-0.03662667,0.01509545,-0.059619654,0.013464114,0.04950868,0.005348335,-0.011286027,0.046255317,0.014958536,0.009709515,-0.009140752,-0.065287225,-0.0864134,0.051730894,0.016503457,-0.0076621873,0.0965523,0.019100055,0.02411697,0.00490372,0.073391855,-0.0013096832,-0.011703535,0.05862999,0.072898075,0.019089239,0.04365251,-0.0013778966,0.09305813,0.011729643,-0.044976603,-0.030223196,-0.06406141,0.0057909093,-0.030184066,-0.032629386,-0.12826316,0.04705619,-0.022039942,0.04685789,-0.0067080557,0.043908536,-0.05218523,-5.7581668e-33,-0.08939324,0.024765484,0.031124553,0.02953221,0.02723399,-0.05053709,0.014245354,-0.08931181,0.0034019544,0.0063261017,0.042477727,-0.018496916,-0.035350412,-0.0049677836,-0.05805378,0.0031449278,0.0060298396,0.038647834,-0.0013697369,-0.03789548,0.119178206,0.08576937,-0.059755936,0.017244205,-0.042138774,0.051160254,0.038499046,-0.012424974,-0.008329923,-0.09280687,-0.035786334,0.0025412093,-0.024122205,0.018575443,0.06110689,0.0055930754,0.055850692,-0.045675777,0.03568017,0.0697649,0.04986092,0.064684786,-0.050093815,0.0012816134,-0.03457202,-0.079737656,0.06656042,0.013093431,0.08736671,0.0016735715,-0.06546998,0.029662283,-0.07063195,0.073782526,-0.022787143,0.017397229,-0.025725313,-0.08631844,-0.07363176,0.05946873,-0.095047034,0.09908031,-0.04100739,0.060629193,-0.02380332,-0.011752532,-0.011868032,0.049364876,-0.04076534,0.0052870954,-0.0042918418,0.04835304,0.06277002,-0.02668647,-0.020729588,-0.06580209,-0.060850043,0.03865054,-0.016781963,0.019945346,-0.041172627,-0.069023296,-0.0425986,0.05626715,-0.036066797,0.043273874,0.06081169,0.025930297,-0.009337659,-0.022798562,-0.072491005,0.0611771,0.054748617,-0.007826502,-0.050772507,-5.884565e-08,0.013501727,0.007852314,0.028747646,0.050872494,0.008175704,0.017893843,-0.09097124,0.040585138,-0.079501025,-0.014139381,0.04967484,0.02813491,-0.153917,-0.062269952,0.043418944,-0.0069043343,-0.019736601,0.15444504,0.007865337,0.02280794,0.082293205,-0.014038174,0.019000484,-0.028092485,0.0130312955,-0.04171324,-0.004154126,0.1624985,-0.015058151,-0.004884651,0.0208485,0.004154645,0.034216817,-0.017546525,0.019548679,0.04013343,0.028144281,-0.019046808,0.007954366,0.03154284,-0.048072807,0.02519573,-0.043291718,0.042767853,0.0088525,0.05562299,-0.009084554,-0.018640844,-0.0365393,-0.0026664003,0.026241327,0.07264449,-0.049357526,0.045819648,0.09938753,0.083831795,0.02957217,-0.07138892,-0.053240694,0.01702294,0.081433274,0.03080204,-0.025203437,-0.014862718,13,3.018388,-25.532728,10
210,"today we discussed about logistic regression. it is used to solve classification problems basically to distinguish between two or more classes. the sigmoid function gives value between 0 and 1 which is essentially the probability.
so basically we intend to get the probabilities of all the classes and the class with highest probability is the the class to which our test case belong to. we also talked about softmax function and clusters.
we also discussed the mathematical interpretations of logit function.",-0.056258194,-0.04777586,-0.052868318,-0.048648376,0.042274073,-0.076991655,0.0797698,0.033341836,-0.0397333,0.032275263,0.020462573,0.0141901905,0.019999074,0.014218999,0.0118193915,-0.018208673,0.014269975,0.05507187,-0.082940675,0.037618972,-0.0050085327,-0.029844014,0.03744149,0.02107652,0.015016084,-0.0895346,0.043171298,0.014839573,-0.058162954,-0.046004016,-0.07428869,0.04117121,0.06470278,0.06887748,-0.06731086,-0.0022493196,-0.01752884,-0.02276148,0.0071679037,0.023147067,-0.02638192,-0.0689365,0.042809356,-0.020713648,0.053506624,0.031188983,-0.030172596,0.0015444023,-0.060603213,0.013354981,-0.031184688,0.041156553,-0.00754916,2.085648e-05,-0.08024431,-0.030746082,0.05676854,-0.060730726,-0.01710135,-0.0015140584,-0.0031443369,-0.051833447,0.066394396,-0.010343885,0.015762525,-0.065996565,-0.06835719,0.03822331,0.05528385,0.023425905,-0.05349873,-0.023208153,-0.053033695,0.049777053,0.08531177,-0.013637957,0.051404215,0.017248526,-0.002341826,0.041676767,0.0035089941,0.011620416,0.027945114,0.010523831,0.10991539,-0.009882852,0.013027762,0.056010008,-0.05280902,-0.004935331,-0.09296473,0.022855205,-0.004146397,0.009084034,-0.047315408,-0.015817251,-0.034678735,-0.11391007,0.055299543,0.0012592089,-0.1323548,0.04920841,0.08748212,-0.07187378,0.023781113,-0.07419084,0.03514849,-0.060455468,0.06361334,-0.119560234,-0.025876796,0.029596731,-0.053551506,0.020038849,-0.013625179,-0.03635627,0.044106845,0.04536737,-0.060847893,0.069281876,-0.0016283337,0.019320985,-0.027956216,-0.039400805,0.051736858,0.045769975,-0.0522706,-2.6642887e-35,0.023018882,-0.02752777,-0.07139598,-0.034503855,0.016586749,0.011943186,-0.053701308,0.039414536,0.010634035,0.009756607,-0.06803274,0.03923757,0.064936765,0.11491289,0.074854374,-0.0012445004,-0.04113074,0.049348846,0.01378837,-0.03567637,-0.03981592,0.038656224,0.036017668,-0.108942814,0.013451999,0.08334928,0.037595328,-0.006932613,-0.0034894366,0.010263673,0.0059726634,0.056175202,-0.04898291,-0.025447456,0.04472272,0.092302956,-0.06031137,0.032458257,-0.020768013,-0.003891272,-0.15439779,-0.0020152675,0.016164664,-0.0020311147,-0.02496793,-0.00525325,0.011515205,-0.0047760247,0.014179945,-0.10771196,-0.0736525,-0.0881474,0.021833781,-0.013102887,-0.051766608,0.03965715,0.05470004,-0.017863946,-0.10124368,0.033243343,-0.036171764,-0.014949697,0.04690126,-0.02045786,-0.017708126,0.010705643,-0.012898672,-0.052828487,0.08958919,0.0013498003,0.09007571,0.03993457,0.041893337,-0.01093045,0.017947456,0.123993374,0.09212751,-0.013844564,-0.016552985,-0.010160911,0.022202032,-0.03323744,-0.024557821,-0.09364817,-0.024034044,0.14025354,-0.016231162,-0.021361342,-0.08679926,-0.011553037,-0.08780414,0.02752542,-0.022483537,0.018881993,0.030771721,-2.23371e-33,-0.06586344,0.061611805,-0.025279485,0.016819887,-0.0064618876,0.00089459,-0.08894362,-0.030880459,-0.042666167,0.062058903,0.048709694,-0.006686927,0.075348675,0.022312753,-0.038194682,0.02539714,-0.08071549,-0.0076543107,-0.012495428,-6.665046e-05,0.08569221,0.06672924,-0.06914937,-0.0402946,-0.048027057,-0.016447904,-0.0482069,-0.019142872,-0.0047440995,-0.0651656,-0.026575446,-0.032592338,-0.06604397,-0.035138056,0.087622695,-0.0055529308,0.044429448,-0.0046203113,-0.010841073,-0.00726792,0.02475309,-0.0015633598,0.017053708,-0.045159634,0.028558131,-0.045094058,0.069673344,0.0071700285,0.04660837,0.06297244,-0.009330892,-0.02123786,0.06496023,0.050357074,-0.026141256,0.058854107,-0.086877905,-0.033215404,-0.057185158,0.06332053,-0.03262848,0.03924285,0.040019646,0.16324656,-0.10418723,-0.07471953,-0.041261554,0.05324253,-0.012203103,0.03031873,0.0328642,0.045501158,0.0902436,0.10312553,-0.07618115,-0.0011730904,-0.043239553,-0.024680568,-0.0303064,0.041776534,0.025519578,-0.064246744,-0.040693186,0.06310483,0.024932845,-0.05066845,0.09769202,-0.002095372,0.09911708,-0.109252304,-0.06139294,0.05785702,-0.03933974,0.0013629048,-0.13067086,-4.554647e-08,0.043940134,-0.047085337,0.027138032,-0.010096657,0.06768815,0.053655077,-0.07747909,0.038185414,-0.10356179,0.004404758,-0.0034704471,0.11094515,-0.044734623,-0.07617104,0.013169817,0.04419377,0.08293414,0.06510276,0.039779164,0.039844256,0.048605695,-0.033112407,0.016210118,0.015059396,-0.00020759871,-0.0051636645,-0.007922055,0.020418685,-0.056951117,0.009397738,0.0072096484,0.0785948,-0.02248026,-0.014655847,0.050329965,0.12967095,-0.0060273395,-0.03321573,-0.067004606,0.0075686155,0.03132219,-0.062800325,-0.00052024814,0.029237915,0.0009990273,-0.015894402,0.012483473,-0.0871601,0.045076665,0.025057783,-0.019416554,0.06268607,-0.054613627,0.025518265,0.020721197,0.06044843,-0.0109405285,-0.06444486,-0.029483406,0.024030892,0.0007587109,0.078163765,0.05250831,0.0014033974,13,-0.50664973,-26.88378,10
215,"the lecture started with the ""expectation algebra"" concept of how, for one sample of points, the expectation of any point is the sample's mean. we had seen the derivation of standard deviation of the sample mean which is expressed in terms of population standard deviation.
in classification, we have seen that using two different models causes more problems at the intersection points.
then we started with ""logistic unit and logistic regression"". the logistic unit predicts the probability that whether data belongs to class 1 or class 0. it doesn't predict the class. the probability function used here is the sigmoid function, which gives the probability that given data point x belongs to class y. if p(y/x) is greater than 0.5 then  x belongs to class y. then we see how the weight matrix is calculated in logistic regression. we formulated a likelihood function so that on maximizing likelihood we get the desired weight matrix. however, maximizing is not a stable method so we introduced a negative sign in likelihood and now minimize the negated likelihood to get weight matrix.
at last, we saw the ""confusion matrix"" and ""quality metrics"" like accuracy = (true positive + true negative)/total, precision = of all events you detected, how many are detected correctly, recall = of the specific class, how many you able to detect correctly, f1 score which is the harmonic mean of precision and recall.
at the end, we have a ta session discussing errors in exercise 1.",-0.00017227983,-0.05055238,-0.018113649,-0.0020272946,0.06922081,-0.10493694,0.06580768,0.031993642,0.03396243,0.028897312,0.050769147,0.0004408194,0.0476661,-0.038405664,-0.019760733,-0.04224015,0.012974702,-0.014017466,-0.11413306,0.10764486,0.06996144,0.020988762,0.032005977,0.040385615,-0.036813002,-0.11857452,0.031574234,0.07335691,-0.05063795,-0.05591748,-0.020959906,-0.0016539685,0.091118984,0.0026626205,-0.0672724,-0.055502072,0.026846172,0.007018336,-0.029863942,0.03754687,-0.054484617,-0.05124589,0.0064076795,0.05714111,0.008331472,0.02287415,-0.0006472018,-0.060622275,-0.03628779,-0.0029301369,-0.023688644,0.06429496,-0.07636588,-0.115322456,-0.129066,-0.03656454,0.04307618,-0.019932294,0.003805443,-0.0006519861,-0.035964336,-0.050048657,0.024236836,0.00676616,0.014162369,-0.13891332,-0.028398637,0.014364219,0.054048315,0.041014746,-0.040569466,-0.024400095,-0.04698745,0.031824104,0.0454159,0.0083308155,0.036982536,0.07788759,-0.034515474,0.08192378,-0.014751037,0.05267415,0.065758325,0.01007248,0.087418675,0.006597887,-0.014486211,0.08367069,-0.007509544,-0.022469062,-0.039630674,-0.01729505,-0.10213997,0.029842813,0.056336053,0.044624902,-0.031444687,-0.06237474,0.039089773,0.046202116,-0.059144203,0.0851538,0.06739152,-0.038927037,0.033130784,-0.08612739,0.0213305,0.011457012,-0.0056485697,-0.079167835,-0.0039875056,-0.053736884,-0.14034216,0.005702766,-0.033956382,0.022923771,0.06607935,0.05273425,-0.12869942,0.026225924,-0.06487039,0.023106184,0.014675871,-0.024289986,0.012474372,-0.01422682,-0.0998945,4.3996252e-33,-0.006393604,-0.06751891,-0.014424014,-0.034363646,0.020295326,0.030243848,-0.099835135,0.015648132,0.036347102,0.05970001,0.019499347,-0.03782738,0.12950924,0.07341644,0.09093114,0.058177345,-0.08158324,0.0038912154,0.00089170225,-0.025150755,0.028491385,0.015946388,0.04612925,-0.09368496,-0.023498,0.11310705,-0.0017215066,0.041038174,-0.017310178,-0.014627883,-0.032769628,0.031345706,0.02903319,-0.106635876,0.029771395,0.039818626,0.013999275,0.032226518,0.004541919,-0.07285423,-0.02366586,0.04494759,0.06319753,0.0077488823,-0.048976284,-0.007270118,0.02662522,0.008291526,-0.045711026,-0.0289254,0.0057736486,-0.04792458,-0.028161686,0.028743831,-0.024945755,0.059078507,0.061349485,0.048401017,-0.082803056,-0.0023597183,-0.004824878,0.02582786,0.018529246,0.00045155192,0.018225942,0.034656774,0.027302615,-0.007619098,0.058543146,-0.0073016323,0.063620396,-0.026771156,-0.038659,0.008793395,-0.021423029,0.031457987,0.009339465,0.021633178,0.07347956,0.008579025,0.022431852,-0.024684086,-0.08390983,-0.11689142,-0.11030614,0.030883776,0.01876625,-0.031615283,-0.019866025,0.058690734,-0.022213506,0.022141092,-0.0658686,0.09349379,-0.0160844,-7.7596065e-33,-0.08539285,0.040066313,-0.059753787,-0.0103146555,-0.011175167,-0.025234258,-0.033941545,-0.0676606,-0.034954228,-0.019797007,0.00510053,0.017188849,0.010295497,0.076707214,0.026687281,0.030332355,0.0147382775,0.025827369,-0.051995464,-0.05784119,0.11829004,0.088095516,-0.082258716,-0.07674312,-0.033711653,0.0011793402,-0.032306284,0.07720579,-0.049962025,-0.08030939,-0.069420695,-0.04982818,-0.02729624,0.01916179,0.06293561,0.0426361,0.003502571,0.0118939085,-0.0116065955,-0.003656183,-0.028121693,-0.011073697,0.0018976469,-0.037727814,0.052122902,-0.056057576,0.0965752,0.0634275,0.09598263,-0.07131803,-0.038547695,-0.011637743,-0.0254571,0.07292235,-0.021290323,0.05448988,-0.038446695,-0.07379523,-0.0846634,0.064532354,-0.0009777849,0.038715854,-0.052301012,0.12681578,-0.08126299,-0.0619353,-0.0031234033,-0.008301131,0.02030982,-0.005758803,-0.055152643,-0.0072016213,0.07764833,-0.0024872276,-0.046527646,0.009100384,-0.019499756,0.041227926,-0.008355623,-0.0520023,-0.0382244,-0.023216963,-0.005106853,0.07899447,0.057176646,-0.016282761,0.04096701,0.013616462,0.06723711,-0.05881728,0.024754101,0.05461733,-0.017101731,0.060585093,-0.064519286,-6.933197e-08,-0.048699193,0.004217234,0.04486258,0.0068913093,0.03086507,0.059923936,-0.097195975,-0.022883797,-0.05235993,0.06273795,-0.010788743,0.064057335,-0.111392066,-0.051166564,0.0052247075,0.094073236,0.018095734,0.09050518,0.027225593,0.02286011,-0.026857521,-0.15832862,0.00812249,-0.060701106,-0.0004640837,-0.024283975,-0.015734082,0.06859505,-0.0027274946,0.011595547,0.010461099,0.0051691476,-0.054092605,0.012049048,0.07344879,0.017036114,-0.0021890656,-0.060222935,-0.0039471765,-0.0050435457,0.046514068,-0.04901274,0.024271013,-0.017056078,0.08613425,0.08246628,-0.008470861,0.0061069527,0.024447372,0.06684667,-0.03446809,0.025155665,-0.06616237,0.020796185,-0.006015335,0.043282244,-0.00026554012,-0.040910766,-0.021917673,0.066828065,0.019353082,0.058022365,-0.013789298,-0.0060847113,13,-1.5079068,-28.081472,10
257,"sir started the class by taking in questions submitted via tha form futher logistic units were discussed. starting with conditional probability as it is the outcome of a logistic unit which is in turn linked with a class. probability of the predicted outcomes and the known outcomes which are know from the dataset are compared further sir linked the probability of y given x with a binomal distribution. here we are interested in maximizing probability of the predicted if the label predicted matches that from the dataset for which we define likelyhood which is the product of probabilities. a log likelyhood is considered because dealing with product is difficult for optimization and as minima are more stable and easier to compute so now we consider negative of the log likelyhood which is to be minimized with respect to the weights. the optimisation is carried out by gradient descent. also a case were the boundary may be non linear was considered where we were encouraged to use polynomials and other non linear functions with emphasis on preventing overfitting while maintaining generalization which is done by understanding the nature of the population. also it was explained how every regression method can be used for classification. futher confusion matrix were introduced here the difference between false negatives and false positives was discussed. here the concept of data imbalance was discussed and its interpretation with accuracy, precision which is defined as how many of the event you have detected how many are correct and recall which is defined as how many of the class events how many of those have you detected. class ended with a discussion on submissions in exercise 1",-0.09028485,-0.0093934415,-0.083452515,-0.043435853,0.005752077,-0.0583576,0.11737488,0.05555743,0.015177332,0.02955933,0.006734813,-0.018144945,0.10463299,0.009547924,-0.034403432,-0.020354392,0.04085348,0.055424713,-0.15491123,0.030993517,0.04198661,0.035284456,-0.012656456,0.04790713,-0.01986857,-0.07017516,0.022566687,0.012192348,-0.043729916,-0.026235133,-0.025941605,0.029682009,0.046557084,-0.0016884462,-0.09572243,0.0225907,-0.083961874,0.014732545,0.00521206,0.020891339,-0.085808694,-0.047726486,-0.025082566,0.019978547,0.043070566,0.023755578,-0.051524714,-0.02757516,-0.019490685,0.0146434065,-0.06372088,0.007596445,-0.015714003,-0.02989082,-0.10789212,-0.09975655,0.07606228,-0.051629078,-0.019073898,0.008007929,-0.06822182,-0.06412958,-0.022956317,-0.004870084,0.045689184,-0.07223025,-0.055934936,0.023444384,0.023669481,0.069631875,-0.026310626,-0.012695991,-0.053639345,0.041514866,0.0061871926,0.032829374,0.11858199,0.062205695,-0.048858676,0.06285587,0.036240634,0.09690867,0.06029551,0.0015167483,0.071299314,-0.0021063262,-0.028663892,0.07322322,0.0036191721,-0.012915282,-0.05045258,0.004151806,-0.008774042,0.028939174,-0.00096285687,0.023887036,-0.045654725,-0.08417472,0.08423049,0.057500064,-0.06387209,0.01267808,0.015984226,-0.07271493,-0.029671552,-0.07739224,-0.023076864,0.021200294,0.09550464,-0.11629648,-0.033047397,0.018383538,-0.06834084,0.003267261,-0.012916461,0.0055005443,0.012825246,0.05303611,-0.10214282,0.043588445,-0.050326984,0.06131245,0.026861772,-0.021101728,0.042027164,0.014137519,-0.06906693,7.619716e-33,-0.00093390956,-0.045626342,-0.013878239,-0.0765979,0.012589368,0.008609779,-0.07727117,-0.0022772448,0.08018015,-0.0054461895,0.006849261,-0.0018151739,0.005965186,0.0280933,0.05617837,0.008325802,-0.043406487,0.031063208,-0.048151277,-0.028018191,-0.070768304,0.01871054,0.10556824,-0.114520505,0.034310225,0.02820913,0.077841304,-0.0006502514,-0.008194322,0.02437811,-0.0037788835,-0.0061196988,-0.017204123,-0.07919617,0.010107701,0.038656346,-0.05364985,-0.029189935,-0.03915662,0.00010854511,-0.080566004,0.011464815,0.049819227,-0.017818142,-0.027958792,-0.002332375,0.006958788,0.026710214,-0.03743715,-0.059477888,-0.033744242,-0.060699824,-0.028705977,-0.035472117,-0.011720506,0.021854827,0.0064077633,0.051515505,-0.061273094,0.06197791,-0.039679125,-0.016566884,0.025027495,-0.04457421,-0.05902893,-0.0034441312,0.022396939,-0.018954553,0.050630827,-0.004096023,0.06842214,0.008869658,0.07746543,-0.022636486,0.059288576,0.115369014,0.03437435,0.051038228,0.005904174,0.014714234,0.024171527,0.04217939,-0.069514506,-0.053840257,-0.077054836,0.014226619,0.037436415,0.014339263,-0.08127178,0.08869009,-0.14029606,0.021017123,-0.045531493,0.046290204,0.004888335,-9.296566e-33,-0.11691616,0.040518176,-0.032654136,-0.017860306,-0.013557294,0.0024284935,-0.015323903,-0.057494894,-0.039860893,0.0035002222,0.012638765,0.005868086,0.04386398,0.026921686,-0.09897704,0.057626717,-0.062133927,0.025204834,-0.020953732,-0.0050942376,0.076797076,0.14083236,-0.12777919,0.025837908,-0.008757757,-0.019728534,-0.03578965,0.10691043,0.013388969,-0.03220293,-0.0145731345,-0.013276488,-0.064123034,-0.0086828,-0.0064756568,0.019009842,0.037236907,-0.03407513,0.004450393,0.05338635,-0.028756972,0.058791857,-0.03972804,-0.11283001,-0.01668636,-0.11324413,-0.0055658245,-0.03097708,0.12373819,0.011643341,-0.030326732,0.028862339,-0.013864669,0.104517005,-0.023546275,-0.008159621,-0.07358715,-0.08270522,-0.013056744,0.07010468,-0.07635415,0.0422566,0.049467113,0.07367925,-0.04193053,-0.043815367,-0.04879132,0.069315664,0.050471555,0.03370736,0.035468202,0.049106453,0.06467759,0.019487549,-0.031083725,-0.020321988,-0.009407306,0.011675386,-0.04651464,0.014606089,0.035472717,-0.050120577,0.012070736,0.10086458,0.008573176,-0.018657139,0.11535777,-0.001340979,0.08808663,-0.057708744,-0.046822973,0.027168423,0.007291551,0.064297676,-0.058582116,-6.488609e-08,-0.028036809,-0.060840283,0.058702067,0.0056756767,0.072962426,0.051802516,0.0024187404,0.08073829,-0.058968145,0.033867933,0.004183248,0.048011746,-0.042526055,-0.042470247,-0.035955533,0.07785425,-0.0043220785,0.07876521,0.050041508,-0.004053981,0.11100403,-0.07645884,-0.046989094,0.023970028,0.025049243,-0.08855082,0.034622688,0.06723856,-0.010772668,0.04758395,-0.0864491,0.03320648,-0.02913526,-0.0073506176,0.037355524,0.08637832,0.01563267,-0.064473376,-0.04137844,0.06673204,0.011229871,-0.032349724,-0.0069761905,0.01629901,0.0842748,0.013297654,0.026210485,-0.03490662,0.025920633,0.0043341927,-0.009830794,0.0074007385,-0.025092594,0.042128827,0.046471067,0.04196881,-0.023587342,-0.06276062,-0.05198932,0.061345667,0.0315632,0.017330598,0.05805875,-0.044370234,13,0.62926185,-27.295668,10
263,"we started the lecture by discussing a few questions from the previous class. sir showed us how we can find the standard error of the sample means just by using a single sample and then we discussed a few questions related to frequency distribution charts. the number of bins that need to be considered for correctly interpreting the â€˜goodnessâ€™ of the model depends on how we want to view the distribution. if we want a rough idea then we can use lesser number of bins. also we can use various formulae to find out the number of bins, depending on the size of the data. 
we then discussed some problems with fitting two different models to the same data. if we use two different models then it becomes difficult to find out which model we should use at the boundary. also, there maybe some discontinuity at the boundary and it may be difficult, in some cases to clearly define the boundary.
next we started with the topic of classification. we continued with the logistic regression model and studied about the sigmoid function. the sigmoid function returns a probability value, p which is the probability that the point belongs to a certain class. if the point is far away from the inflection point then it is clear which class it will belong to. but if it is close to the inflection point, then we can find the probabilities and depending on the probability values we classify the objects in different classes.
if p represents the probability of the object belonging to class 1 then if p > 0.5, we can say that the object belongs to class 1 otherwise it belongs to class 0.
our goal is to find the weights such that the difference between the predicted values and the true values is minimized.
if t is the actual value and p is the probability of t=1 then if t is actually 1 then we want p to be as close to 1 as possible. on the other hand, if t=0 then we want p to be as close to 0 as possible.
rather than maximizing any function we want to minimise it because maxima is unstable.
after fitting a model to the data, we can use a confusion matrix to assess the quality of the model.
we then discussed a few metrics related to classification. accuracy is one of the metrics that we discussed.
precision is yet another metric which is defined as the ratio of number of events correctly detected to the total number of events detected.
however accuracy can give us â€˜ false hopesâ€™. in situations wherein we have a large number of data points belonging to one class and a very few from the other then accuracy can be very high in such a situation because it is biased towards the data set with large number of points.
recall is defined as- how many events of a particular class you were able to detect correctly.
f value is the harmonic mean of recall and precision and it is better than accuracy and it doesn't give false hopes like accuracy. so it is a better metric to judge to goodness of the model.

",0.025388207,-0.038903456,0.009685816,-0.051374152,0.02528549,-0.0023697305,-0.05305215,0.068557866,-0.0044402876,0.01455944,-0.029723244,-0.05691064,0.049821045,0.012556681,0.10378763,-0.10753462,0.017239813,-0.059009388,-0.07356329,0.014849075,0.04157001,0.04083719,-0.00035881315,-0.040148027,-0.04674778,-0.019797187,0.016217148,0.03629277,-0.0083001815,-0.053154144,0.0005136874,0.06743101,0.045707088,0.041420534,0.008249376,-0.030422606,-0.0030036122,0.07508304,0.059146576,0.043380067,-0.040671926,0.0011493433,0.091742866,0.03187624,0.09162655,-0.023891991,-0.031164972,0.025973849,0.0021857654,0.02267107,0.04400468,0.030153621,-0.014811123,0.052804984,-0.11009064,-0.059424784,0.04650721,-0.10078249,0.01507234,0.023471864,-0.024400217,-0.061222415,-0.034766093,-0.0027355058,0.039636325,-0.05304427,0.0039074416,0.024127424,0.085156776,0.09677316,-0.041951638,-0.0150220385,-0.09878183,0.054942526,0.037977386,-0.043107755,0.015158993,0.05293225,-0.036337398,-0.07903714,-0.05373245,0.00042319423,0.029652454,-0.07135998,0.027681163,0.018791301,-0.05476858,0.030917479,-0.050423905,-0.029221058,-0.020173905,0.049092192,-0.03011362,0.029749716,0.051332165,0.05627235,-0.012392559,-0.009254891,0.12482116,0.08793053,-0.008164377,-0.03399898,0.03288237,-0.017311193,0.035927866,-0.061563138,0.013275233,-0.012670097,0.020421768,-0.07751961,0.0046653417,-0.026184585,-0.084082544,0.02112139,0.07384529,-0.09558018,0.07828161,0.016116627,-0.06032821,0.048852064,-0.035887077,0.018299785,-0.038506933,-0.025218926,0.023458203,-0.017872114,-0.073479764,1.612746e-33,0.034440357,-0.053561267,-0.013477686,0.061296273,0.07751049,0.1108353,-0.054865908,0.05192797,0.05128678,0.08682604,-0.008804099,-0.0035200752,0.01367323,0.040823042,0.068953045,0.029359192,-0.05330452,0.02334781,-0.062446386,-0.030738674,-0.025270477,0.012101375,0.10699548,-0.0014756909,-0.017456185,0.10686529,0.008025265,-0.024760826,0.0021557873,0.04644025,-0.039547358,-0.015924742,-0.03357003,-0.03378352,0.031643502,0.039703764,0.0184235,0.03211727,-0.04322922,-0.1508154,-0.07661444,0.03647475,0.042547837,0.04587586,-0.0496395,-0.028495058,0.08276041,-0.032628767,0.03793319,-0.028365383,-0.023174275,-0.028343586,-0.02427976,-0.017469676,-0.0022124469,0.059984066,-0.00028498014,-0.0179615,-0.060056534,0.044610925,0.074033916,0.0014773796,-0.016214876,0.05278404,0.11797851,0.02881321,-0.08842673,-0.017344495,0.0061871237,-0.063570336,-0.003903846,-0.00095797726,-0.032128192,0.006641889,0.012222674,0.011321261,0.032565318,0.030401826,-0.019516919,-0.03389277,-0.032504126,-0.03621261,-0.06589811,-0.0865691,-0.12557708,0.028043885,0.0923089,0.006521302,-0.001327133,-0.06123321,-0.06524656,0.051919512,-0.051040005,0.0311339,-0.00086327136,-4.832174e-33,-0.07847096,0.1179231,0.023118576,0.022132978,-0.0466876,0.022718841,-0.038264096,0.024426902,-0.034215197,-0.079222925,-0.042664267,0.033548683,0.058433704,0.008522564,-0.07695734,0.021439131,-0.029490244,-0.07704499,-0.018056322,0.029391458,0.021574657,0.0039441437,-0.13248244,-0.03947643,-0.11395301,0.007130709,-0.14910953,-0.052585218,-0.03990256,-0.031219201,-0.02621918,-0.07794007,-0.021166861,-0.0782205,0.01966136,0.009057971,0.033190295,0.011661015,-0.009705074,0.030355277,-0.002439836,0.0077053965,-0.05052252,-0.09269069,0.04019544,-0.00042169285,-0.038781296,0.02454162,0.0074057505,-0.012017123,0.012723085,0.07813537,-0.026295839,0.011580909,0.057352055,0.032668382,-0.04134384,-0.02070831,-0.05169396,0.01316315,-0.084740855,0.024806846,-0.032373097,0.029611602,-0.04562064,-0.044970077,-0.06712774,-0.009711405,0.06315796,0.056448184,-0.1508062,0.0026338112,0.036030766,-0.019814426,-0.014246862,-0.012603213,-0.005032796,-0.095855154,-0.037615363,0.03669807,-0.03817991,-0.04645623,0.002653105,0.11129613,0.035122942,-0.016010182,0.12134904,-0.07182382,0.08158352,-0.014583518,-0.009651029,0.07112556,-0.068270326,0.012553753,0.0024248254,-7.433776e-08,-0.045767926,-0.04412745,0.06912007,-0.07102147,0.054943174,0.08978354,-0.07366171,-0.019484775,-0.015666286,0.0060842684,0.03161949,0.046562746,-0.12706584,-0.009401704,-0.049947977,0.05370029,0.030628614,0.11637479,0.017468913,0.049266234,-0.054343246,0.026825795,0.03103209,-0.063118316,0.06497317,-0.0199648,-0.011098634,0.028817814,-0.05178847,0.05732609,0.03321127,0.057581212,-0.03205645,0.00866996,0.03143927,0.01602285,-0.058409296,-0.027613768,0.0009787577,0.051515397,0.019433059,0.023344198,-0.06519431,0.04895121,0.116000004,0.04571888,0.016579697,-0.006606214,-0.0030984369,0.03185978,-0.064881064,0.032614816,-0.0118252495,0.05583343,0.01465146,0.0829664,-0.05383815,-0.07601363,-0.04317359,0.02079995,0.026601812,0.05043066,-0.06906124,-0.010211166,13,-2.0263968,-31.017195,10
269,"we discussed doubts submitted in the google form, like how is the standard deviation of the sample  related with the variance and the number of data points in our sample (n) which came out to be s=sigma/root n
we moved to our theory and looked at logistic regression, how multiple variables can affect the result of our logistic regression, and how to set the weights of these multiple parameters in the training data set, so that the training data set best represents the actual sample, without much overfitting. 
later we looked at the quality metrics which can be used to assess our results of our model. here we looked at the confusion matrix and the four types of entries which can be made which are false positive, false negative, true negative and true positive. we then looked at the 
different calculations which can be made on these entry values to gauge and arrive at a certain conclusion, like accuracy, and so on
at the end, the tas of our course had come and they discussed the exercise 1, what was missing in the spreadsheet and report, and how it should be organised and presented well. one important thing was that read the documentation of the spreadsheet functions and how they actually act, for example the kurt (kurtosis) function.

",-0.074999444,-0.036292374,-0.033738144,-0.010092024,-0.0015590636,-0.05918617,0.015140045,0.07100786,0.0043874085,0.0162008,0.02011214,0.00011073984,0.05475475,-0.10867376,-0.03973435,-0.07153476,0.024249343,0.00028804925,-0.11271353,0.002333048,0.059336107,-0.002229976,0.025461594,0.027184188,-0.016181074,-0.06775591,-0.06753824,-0.05433213,-0.011510394,-0.04582362,-0.042987704,0.041351844,0.031141946,0.029461667,-0.026315264,-0.026168367,0.024757005,0.020176524,0.038002975,0.022358341,-0.060666602,-0.13864529,0.0031974816,-0.009589579,0.02893879,-0.020277672,-0.06165167,-0.025626168,0.014446602,0.03225247,-0.13642527,0.018581811,-0.033100676,-0.08517494,-0.11878437,-0.043786623,-0.04565367,-0.014435653,-0.00081366213,-0.00806564,-0.026535718,-0.032604873,-0.079830654,0.01773216,0.07167016,-0.013787922,-0.038467728,-0.059242792,0.036333565,0.042994704,-0.021570653,-0.018242855,-0.07537938,0.027858574,-0.001556109,0.03106536,-0.020848678,-0.006493049,-0.01457646,0.035242002,0.053717826,0.09047287,0.0646297,0.058454532,0.040811893,-0.0722929,0.106974185,0.056041747,0.037159953,-0.004804466,0.064317614,-0.06383841,-0.050316855,0.05980136,0.094629616,0.0762128,-0.027746154,0.012456489,0.08055999,0.04578922,0.014544681,0.086601935,0.02819433,-0.101434544,-0.0340895,-0.033389293,0.002045716,0.014444379,0.044544283,-0.07683732,0.007969178,0.047116525,-0.0959393,0.003368341,0.03552671,0.031776715,-0.0030374753,0.058948003,-0.03554663,0.065977536,-0.034499634,0.06475252,0.040858362,0.023395095,0.04518438,-0.044919863,-0.061370213,2.339796e-33,0.024577888,0.06816655,0.036323883,0.01922268,-0.049025346,0.017910704,-0.14206624,-0.0017380404,0.005596785,0.064784616,-0.011719217,0.10312457,0.0125077395,0.039416786,0.012380926,0.065578505,-0.12032529,0.01682352,0.02959207,-0.008242378,0.08505814,-0.056585018,0.07237995,-0.0984888,0.01542946,0.06312335,-0.012869504,0.062550195,-0.0829609,-0.0037201617,-0.039735,-0.08237499,-0.023348039,-0.0075888326,0.058472097,0.06368487,-0.0070765074,-0.027127834,0.019204699,-0.021519108,-0.061457206,0.0402681,0.029760635,0.026354484,-0.039255418,-0.008673558,0.03807416,-0.10477526,0.033086203,0.013124198,-0.025110194,-0.024201239,0.02936315,0.014118974,-0.07966592,0.119425505,0.04231921,-0.073036425,-0.06465354,-0.004801795,0.047603548,-0.023351086,-0.071653366,-0.030941783,-0.12053863,-0.009145071,-0.044327922,-0.023509186,0.08197834,-0.062094104,-0.079502486,-0.07077635,-0.016354656,-0.023271248,0.047342632,0.02923026,-0.004420748,0.07051955,-0.017777653,-0.07089863,0.07765694,-0.006247441,-0.023186589,-0.074633345,-0.059314735,0.00097201887,-0.001633322,-0.035865318,-0.055283174,-0.00029350806,-0.07200791,0.033540882,-0.06171764,-0.006078792,-0.0016464883,-5.2703642e-33,-0.12937622,0.010289922,-0.03588817,0.09289305,-0.0001061788,-0.04288112,0.03408867,-0.011408441,0.057334933,-0.011996454,0.043712262,-0.0073588383,0.0030315802,0.019231966,-0.062137797,-0.009584537,-0.02839698,0.025882406,-0.097875886,-0.054067254,0.059580583,0.14800926,-0.016708827,-0.046286695,0.011504938,0.07125733,0.0025874102,-0.033992458,0.02181719,-0.017325101,0.02074505,0.011730719,-0.005715745,0.06602376,0.09716914,-0.019754428,0.06023729,0.0354543,0.028063493,0.029694986,0.020919036,0.055894013,-0.001505551,-0.09155694,0.048382804,0.037954293,0.022613922,-0.017864807,0.055178285,-0.032603737,0.05357785,-0.0066705584,-0.05639026,0.07722951,0.007296723,0.03242146,-0.04987477,-0.09962503,-0.03565773,0.026844386,-0.072602525,0.053262614,-0.017651401,-0.028724756,-0.0081439195,-0.03988228,-0.065986246,-0.022810692,0.019176891,0.023600088,-0.114345044,-0.059316296,0.044628527,-0.07674242,0.029348323,0.017304784,-0.04621927,0.0073869987,-0.0497985,-0.008265281,0.04266975,-0.01751748,0.006577493,0.074275754,0.018024124,0.07774343,0.073951766,0.04450878,-0.018699197,0.049431954,-0.036561765,0.036808554,-0.0015516141,0.06734528,-0.07091172,-6.429214e-08,-0.017455747,0.06834783,0.012465454,0.022416685,0.019194344,-0.024082627,-0.016691647,0.013789342,-0.07912552,0.048106913,0.022847861,0.023932062,-0.108403906,-0.031132003,0.03894979,0.05299494,-0.04418219,0.05901733,-0.034953386,-0.019599115,0.08407814,-0.039260983,-0.057374116,0.026901552,0.0803704,0.018059226,-0.02250643,0.08726236,-0.05167182,0.03746066,0.05805801,0.017871434,0.017609227,0.000100175814,-0.029266855,0.00638316,0.0381644,-0.028677851,0.04392729,0.057152625,-0.043418135,0.07235385,0.018188965,0.030940764,0.070480436,0.038306884,-0.052346777,0.06383908,-0.080171,0.02665689,-0.0050182296,-0.0011942403,-0.09528179,0.10554806,0.053171754,-0.012382108,0.025371015,-0.011844664,-0.0686707,0.026927402,0.0019242195,0.06074208,0.0048810043,0.010843518,13,4.7468925,-23.217539,10
290,"discussed true/false positives and negatives
f1 value = balance of precision and recall, used confusion matrix too
recall (correct ones out of real class)
accuracy (how many right ones you got) and precision (correct ones out of predicted)
aim was setting weights to make predictions match actual results (reduce errors)
target can be either predicting class or chance (like coin flip probabilities)
we looked at p and 1-p for all examples
if chance > 0.5, it goes to one class, else other one
features can be more; started with 3 features, answer was 0 or 1
logistic doesn't say the exact class but tells the chance of belonging to one class or another
",0.0082166,0.042177353,-0.080152124,0.023014452,0.044821538,-0.048030652,0.026150472,0.07656328,-0.07032132,-0.0012421422,0.016056605,-0.05686292,0.028537758,0.0016605424,-0.04530759,-0.012662658,0.021997979,-0.005555593,-0.09025833,0.027262714,0.018993078,-0.027588557,0.048275672,0.06751648,-0.021440236,-0.04651609,-0.016492127,0.029746765,-0.05495733,-0.09184858,-0.01352738,0.09525497,-0.0011545969,-0.017285405,-0.096790716,0.009801867,-0.05749165,0.0017620097,0.00863641,-0.020972513,-0.042921115,-0.096953556,0.034087572,0.005641169,0.008371384,0.036318883,-0.04584013,0.03820841,0.01874995,-0.026222939,-0.030114152,0.09276283,-0.027399337,-0.065912366,-0.11568197,0.026479768,0.068327226,-0.008325031,-0.027115962,-0.0563106,-0.017082624,-0.056964282,-0.06961613,0.0087099895,0.03745061,-0.010956705,-0.09635166,-0.04056201,0.052494425,0.06452906,0.059869252,-0.032289438,-0.030998064,0.012096555,0.03241518,0.07314053,0.024043577,0.026637578,-0.0073008095,0.083641775,-0.015665485,-0.031090893,-0.022145718,0.023805387,0.13668884,-0.06706155,-0.021614322,0.0066882214,-0.040871993,0.063015856,-0.0037980124,-0.076500684,0.061023977,-0.00094981905,0.019357339,0.05784848,-0.024751782,-0.064120114,-0.035933454,0.020007422,-0.028083088,0.061918836,0.031226069,-0.046948176,-0.002066819,-0.045694385,-0.008082057,0.01405205,0.05043827,-0.077946916,0.01514385,-0.008642693,-0.022116005,-0.018296983,0.044514377,0.0815876,-0.026767123,0.09416649,-0.13368097,0.059872985,-0.08843829,0.017443718,0.031650893,-0.032144647,0.0066311443,0.0052569336,-0.12887849,9.8776915e-33,-0.028212072,0.0011956815,-0.010022552,-0.028760884,-0.018490836,0.0055054436,-0.12388091,0.026729902,0.0058990703,0.0051290337,0.027512193,0.027618635,0.033981334,0.088269606,0.0011171283,-0.021128418,-0.124369845,0.049829468,-0.051733103,-0.0075183655,0.013566666,0.0028941045,0.04274327,-0.12235921,0.04691612,0.06659798,-0.03431023,0.028082086,-0.031254094,-0.0024380349,-0.013049624,0.024589473,0.025661131,-0.063501306,0.088018104,0.038346335,-0.010062958,-0.0571522,0.0057701524,-0.017375564,-0.08378866,-0.0030673647,-0.023012858,-0.041585363,0.0011914403,-0.02776903,-0.03266624,-0.013146161,-0.08634702,0.089205936,0.0032169556,-0.06372504,-0.040870458,0.041909527,-0.027044121,-0.012820011,0.031097231,0.014664563,-0.03692082,0.053938776,0.10569999,0.035047915,-0.027382232,-0.059854317,-0.100155175,0.048153885,-0.0010449283,0.014111159,0.028867574,0.028369734,0.05515963,0.029869542,0.010837005,0.038063347,0.02660167,0.067691095,0.04247987,0.023137564,0.007286841,0.008641834,0.079397246,0.032096196,-0.008564185,-0.018282002,-0.078997485,0.001867879,0.01729864,-0.01575719,-0.082096614,0.06713224,0.0010420294,0.018871758,0.039404586,0.06060574,-0.032758042,-1.05478335e-32,-0.1471207,0.07547888,-0.002907768,0.0065429597,0.029060345,0.0077004833,-0.019172793,-0.05872577,0.0046871854,-0.0013460241,0.058051422,-0.02013908,-0.061015837,-0.004103484,-0.06295757,-0.029902156,-0.062317323,-0.025188826,-0.041691948,0.010400066,0.09329866,0.07744698,-0.0916817,0.07133056,-0.019399552,0.024736013,0.011069977,-0.053889442,-0.0029209375,-0.09045168,-0.0052800183,0.02580442,-0.024989609,0.016439237,0.060200624,-0.013627762,0.062613316,-0.018957423,0.023220442,0.04840886,0.028968254,0.023290196,-0.063969,-0.012597235,-0.05058158,-0.08642171,0.0230204,0.03665911,0.17205812,0.0018971784,-0.023612345,-0.0273603,-0.027908936,0.1009976,0.009532643,0.0013823746,-0.019501507,-0.05454148,-0.03542773,0.09489172,-0.12409041,0.027194342,0.033208977,0.01797713,0.0005243795,-0.005223172,-0.019185647,0.08730729,-0.04008035,0.020683762,-0.018027766,0.07168441,0.10251179,-0.026150255,-0.008722563,-0.05061551,-0.057764433,0.0505066,-0.0034742854,0.009989375,-0.028751656,-0.04902272,-0.017154707,0.10278495,0.0037708103,0.030089814,0.007161875,0.027874874,0.00015355165,-0.006297673,0.017690249,0.07221649,0.020002566,0.05645955,-0.032274228,-5.7684606e-08,-0.00035099496,-0.04293727,0.047563918,0.037220586,0.021925261,0.03258755,-0.059734616,-0.010318132,-0.076452844,0.010353159,-0.04052905,0.09781634,-0.122252926,-0.06831705,0.061976504,-0.0070226216,0.0037497533,0.12635759,0.0021605487,-0.044043906,0.11764744,-0.10783674,-0.029547028,0.025851283,0.013291632,-0.05853621,0.0145059135,0.14213257,0.015742827,0.06187119,-0.0035775637,-0.010351128,-0.034540765,-0.054771997,0.06762839,0.077442616,0.016429331,-0.026227409,-0.012181649,0.010477487,-0.020367166,0.00060698256,-0.0022417197,0.02134229,0.018428039,0.059143014,-0.06421948,-0.011798538,0.0017825675,-0.036874667,-0.03335364,0.08838419,-0.059262015,0.112642035,0.04734373,0.08161332,-0.034157492,-0.08634603,-0.061051648,-0.017561728,0.05549608,-0.013190006,-0.020316174,0.049912296,13,3.609731,-25.605818,10
320,"the lecture covered logistic regression, a classification technique using a sigmoid function to predict probabilities. we explored clustering methods to group data points and discussed true positives and negatives in model evaluation. outlier detection techniques were examined for identifying anomalies in datasets. loss functions, which measure model errors, were explained in the context of optimization. finally, we reviewed methods for solving logistic regression, including gradient descent.",-0.008167109,-0.028918443,0.00877796,0.049244232,0.082036436,-0.061639015,0.07872807,-0.009811318,-0.055362377,-0.01118669,0.054798763,-0.021193279,0.081038736,-0.034907892,-0.064643666,-0.08211098,0.025072342,0.023599403,-0.062419172,-0.060419034,-0.07203031,0.071124814,-0.0048692185,0.0333793,0.047375917,-0.0544527,0.0141406115,0.04960946,-0.088281326,-0.024700826,-0.047726583,0.044532046,0.027397405,0.030822676,-0.0029888747,0.023186732,-0.023039814,0.027481036,-0.006302614,-0.0064728,0.0074180895,-0.05390359,0.05871935,-0.017928198,0.015149779,-0.015273345,-0.05564898,-0.014695932,-0.03076639,0.01717739,-0.024050156,-0.0012536953,0.011790132,-0.013977228,-0.1381266,-0.013820738,0.02835929,-0.03342663,0.011019828,-0.017842732,0.06833198,-0.041529343,0.004263438,-0.005593601,-0.009765185,-0.011231685,0.0073336214,0.029834826,0.045665108,0.023850817,0.047401756,-0.0022721994,-0.025624415,0.028399473,0.037404697,-0.019002441,0.11831151,0.011517135,0.013521511,0.01964513,0.010366286,0.068595074,-0.00028549627,0.025677774,0.1425227,-0.040822327,0.043529898,0.020984441,-0.010995044,0.029317413,-0.06816435,0.05015761,0.0061185076,-0.018458977,-0.008504343,-0.030574616,-0.023148052,-0.08025481,0.09664564,0.05270179,-0.08658693,0.090086326,0.037717726,-0.06103281,0.065608956,-0.035743877,0.073444344,0.009459359,0.07330645,-0.076376684,0.0093763,0.059657685,0.006742188,-0.0363486,0.041551396,-0.029866531,0.116696335,0.03797419,-0.12810628,0.086363636,-0.027323667,0.032301992,0.052639004,-0.0043036514,0.02738876,-0.027681448,-0.10076522,2.0194765e-33,-0.004001172,-0.006998885,0.03177061,-0.10541937,0.00052004063,0.008785102,-0.11102402,0.017577674,0.0070677204,-0.020813437,-0.048242185,-0.047473717,0.014474612,0.082146116,0.10858391,0.008832342,0.018195074,0.0795172,-0.0056091244,-0.044658575,-0.05466242,-0.115254894,0.0050318902,-0.09872049,-0.043849085,0.096471585,0.05558337,0.0057142167,-0.025870593,0.008065999,0.04349983,0.039084278,-0.041810222,-0.037846576,0.073546365,0.060602058,-0.011868731,0.04151695,0.05833936,0.02349347,-0.15158944,0.0074728685,-0.00899705,-0.06481858,-0.002463162,0.037769523,0.07201743,-0.06952868,0.019379992,-0.11511214,-0.08039445,-0.088784054,0.031993836,0.06220439,-0.07729877,0.016435163,0.04979123,-0.026944198,-0.032637067,0.008573024,0.002382629,-0.051885568,0.0021689544,-0.05927372,-0.039324597,-0.03846562,0.042915907,0.014708312,0.033469547,0.06528205,0.05537052,0.013289262,0.038269643,0.025127513,-0.0063496702,0.10371178,0.05160651,-0.002372687,0.009751465,-0.058128085,-0.028897524,-0.025013078,-0.041905038,-0.046514206,-0.016433941,0.055658173,0.018233059,-0.0035519013,-0.08498671,0.014593941,-0.14981535,0.059992544,-0.04950462,0.02437492,-0.00033933576,-3.1974644e-33,-0.087826416,0.07115086,-0.063626945,-0.0692037,-0.01593432,-0.0399641,-0.095148034,0.015492857,-0.0146213705,0.033592533,0.00059460517,-0.036214795,0.035254695,-0.0317108,-0.014177495,0.07124994,-0.04068691,0.024634697,0.03425435,0.03897996,0.07063542,0.0015799757,-0.058630135,-0.01442692,-0.045122776,0.033293955,-0.042393308,0.05526134,-0.057770256,-0.06380028,-0.06123893,0.05454646,-0.03444783,0.0014327512,0.094263494,0.034180615,-0.029443325,-0.056360748,-0.0012501029,-0.006467971,-0.032744367,0.10006181,-0.031976555,0.010297534,0.055991415,-0.08967057,0.059240468,-0.015138384,0.008286197,0.079618305,-0.100699134,-0.027194558,0.03224445,0.07519153,-0.08690185,0.08359589,-0.0316828,-0.050491687,-0.031423103,0.06926852,-0.050295245,0.0042049238,0.051045947,0.0772244,-0.05233067,-0.07668025,-0.0021823598,0.01955114,-0.009134796,0.02814175,-0.015236024,0.03380832,0.05418739,0.016482452,-0.0581537,-0.06502557,-0.05775197,-0.025035854,-0.0059903883,0.003409101,0.08086576,-0.045336686,0.03259085,0.076165035,0.02060842,0.017727386,0.06303193,0.07885633,0.0652349,-0.06480233,-0.05457762,0.02435456,0.015264806,-0.020061504,-0.09472714,-3.8168224e-08,-0.050785057,-0.019365711,0.03306465,0.0106704375,0.015372664,0.024749102,-0.10883427,0.11408196,-0.038735047,-0.067806624,0.014875363,0.0027016872,-0.07087772,-0.035286345,0.022075847,0.03568185,0.058792952,0.058994193,-0.009881692,0.01687167,0.0077162865,0.009470827,-0.008769485,0.0014232951,-7.412504e-05,-0.059271168,-0.031445928,0.09461115,0.006786133,-0.026445063,0.002519366,0.022259928,0.0158446,0.048408262,0.045778785,0.1642729,0.001340796,-0.047054652,-0.0884205,0.005265369,0.036946304,0.023186846,-0.020783337,0.01919109,-0.012515191,0.013194303,0.039360467,-0.028599173,0.06993608,-0.040815726,0.008995098,0.019054705,-0.033371754,0.104713194,0.03248384,0.011969225,-0.033378117,-0.042388562,0.00010797538,0.026947007,-0.020222224,-0.0048769116,-0.021013053,-0.040808327,13,2.2896852,-29.589048,10
324,"today's class start with a discussion about  question on 1) how to calculate standard error of one sample 2) how to determine the width of bins of histograms.
starting with first question - we can calculate the standard error of one sample just by supposing that the standard deviation of that sample is equal to the standard deviation of population. in second question the width of bins depends on the problem what we are looking for or we can also done it by using some predefined rules like sturge's rule or freedman diaconis rule. in logistic regression we use sigmoid function in which we get two values either 0 or 1, it gives only probability. the boundary can be non-linear not necessarily to be linear. if we give more flexibility, then it works well on training data but not on test data.
there are many metrics that associated with classification just like  regression - 
1)precision -> of the events we have detected, how many we correctly detected.
2)accuracy-> overall how often is the classifier correct ? in case of data imbalance, it is not a accurate method.
3)recall -> of the specific class how many we have classified correctly 
4)f1 -> it is the harmonic mean of precision and recall.
we also learn about the confusion matrice in which there are 2 rows and 2 columns. 
1)true positive -> it is when a prediction is correct 
false positive -> it is when a prediction is 2)incorrect 
3)true negative -> it is an correct prediction that a sample is negative 
4)false negative -> it is an incorrect prediction that a sample is negative",0.05302441,0.019176517,-0.007830389,-0.041150317,0.012151608,-0.049547948,-0.02991769,0.045990802,-0.1199534,0.054728318,-0.073321365,-0.04847072,0.025675874,-0.033819593,0.0014905176,-0.060964767,-0.011533415,0.032108154,-0.084381394,0.037346493,0.06434455,0.011910272,0.0061351955,0.033253442,-0.04315788,-0.038765006,0.0066667325,-0.010734312,-0.08360186,-0.081504464,0.017547728,0.01716562,0.079611026,0.030728541,-0.03712477,-0.06479041,0.026304517,0.03684323,0.03972355,-0.0050576325,-0.007599306,-0.05170162,0.048042353,0.0661816,0.041734986,-0.009236166,-0.0044199135,0.0023674753,-0.019886013,0.0004401491,0.0023620692,0.060727857,0.087413736,0.061361518,-0.11399421,-0.07067648,0.036018703,0.0051447735,0.0446575,0.060055688,0.033772357,-0.036995426,0.013786994,-0.038025316,-0.059681267,-0.05937459,0.0150636,0.0072890557,0.077579945,0.052723095,-0.009243096,-0.0076739863,-0.06361672,0.035924282,0.010908271,-0.042191666,0.014565944,0.079078875,-0.022321183,0.016168844,-0.086647876,0.07458218,0.12406998,-0.05010293,0.10252086,0.03974446,-0.013759539,0.09456233,-0.027872255,0.0008954188,-0.014025997,0.001676555,-0.014524946,-0.020263374,0.040713247,-0.032102346,-0.044997804,-0.017257217,0.08492126,0.052812163,-0.016080108,0.012848278,-0.016591346,-0.009894235,0.09143603,0.04981631,0.061609197,-0.0034419512,0.0576349,-0.062336758,0.0114170825,-0.0029876623,-0.11424763,0.017117338,0.10114226,-0.05567035,-0.022235116,0.06401566,-0.07710485,0.07846131,-0.031207725,0.04340255,-0.0029534562,-0.00037297996,0.028008735,-0.018907806,-0.044263795,6.6786674e-33,-0.021425651,-0.032599196,-0.035179768,0.030324755,-0.06531051,0.090452686,-0.08822763,0.008765639,0.07019454,0.045788758,0.03162659,-0.0056398218,0.062326964,0.08425371,0.11460007,0.056586687,-0.07693897,0.01708563,0.016261604,-0.038040582,-0.07306408,-0.039087504,0.09116056,-0.06564838,0.023342118,-0.012550687,-0.020516243,0.062705286,-0.013056038,0.03784065,0.003474241,-0.011660163,0.019579444,-0.016768025,0.025024973,0.0069597275,-0.051500686,0.08616699,-0.0372862,0.011766676,-0.064405516,0.033020552,0.053941287,0.0336217,-0.057956945,-0.008774164,0.049164083,-0.000935434,0.024476273,-0.0029542036,-0.026757555,-0.007591988,-0.025188822,-0.0072220797,0.064063825,0.05989997,-0.0024066437,-0.011525315,-0.0036259112,0.04274897,0.067783386,-0.04207535,0.033776805,0.024119426,-0.009603493,0.03931102,-0.028263448,-0.007485641,-0.022998713,-0.06905217,0.010158181,-0.017022494,-0.045858677,0.01327809,-0.034014188,-0.0126541685,0.06571342,0.0489983,-0.039991498,-0.045861367,0.010661318,0.03009701,-0.035057887,-0.07201367,-0.13985883,0.019954933,0.04154429,0.061120432,-0.08464998,0.032429148,-0.07708252,0.0063050142,-0.03866313,0.00366755,-0.040897142,-9.040331e-33,-0.03952703,0.120883495,-0.013740452,0.030221341,-0.059969176,0.033534903,-0.04038303,0.009788412,-0.025512228,-0.026545297,-0.0057911435,0.0065117795,0.007680236,0.036731232,-0.06895177,0.038662784,-0.0750473,-0.03892966,-0.041040875,0.019936452,0.08521081,0.029264748,-0.099634685,-0.08662903,-0.09696518,-0.026991151,-0.14164817,-0.005297174,-0.016350023,-0.06936301,-0.024935998,-0.07571929,-0.06489544,-0.074162304,0.09037115,-0.02600753,-0.02605997,-0.01784134,0.08514871,0.013641873,-0.011859279,0.058833383,-0.084676266,-0.07348016,0.026519455,0.00014049945,0.0030652725,0.029077532,0.04387691,0.018614098,0.014229549,0.055141117,-0.04161118,0.061634794,0.050946593,0.010201968,-0.053319495,0.0043473598,-0.12380528,0.029687615,-0.13113254,-0.012829825,0.025618996,0.045643333,-0.053049296,-0.09141839,-0.035880156,0.018515158,0.02582236,0.022030672,-0.06573235,-0.0038378926,0.021023117,0.009673842,-0.043155286,0.02335603,0.02508412,-0.07236018,-0.017561369,0.039096273,0.023595223,-0.06003275,0.032480497,0.0804361,-0.015132444,0.017786983,0.1671863,-0.06056904,0.045527622,0.01322125,0.025648588,0.0066983006,-0.03791952,-0.02416986,0.008752886,-7.000394e-08,-0.027147813,-0.06297054,0.05695229,-0.006666089,0.00635326,0.08824145,-0.07948992,-0.0047476296,-0.026102416,-0.0073843817,0.037792332,0.0906562,-0.104864135,-0.07311991,-0.027004614,0.015214788,-0.027885944,0.09492063,0.021556165,0.09863334,-0.028139148,-0.082960874,-0.007775497,-0.08214304,0.016038822,-0.008939451,-0.06268131,0.04526553,-0.06506716,0.049717076,0.10291577,0.04707467,-0.02123268,0.019984119,0.058463056,-0.04996569,-0.012061819,-0.022410298,-0.030550756,0.045870163,0.0373183,0.017123593,0.0031055068,-0.019302487,0.053380765,0.027041463,0.048443872,0.029731303,-0.11021878,0.015084267,0.022932352,0.013129406,-0.021383764,0.05102694,0.036213428,0.028891519,-0.040950287,-0.10516746,-0.06722754,0.031476896,-0.036020044,0.05706721,-0.09650503,-0.05846092,13,-1.6749997,-30.751787,10
337,"logistic regression is a binary classification method that predicts outcomes as either 0 or 1. the model's performance is assessed using metrics like accuracy, precision, recall, and the confusion matrix.
it is trained by optimizing weights through gradient descent, minimizing errors, and using the likelihood function to improve predictions. the model processes input features by multiplying them with weights, adding a bias, and applying a sigmoid function to obtain a probability score. if this probability is greater than 0.5, the outcome is classified as 1; otherwise, it is 0.

",0.04730711,-0.039108343,-0.091116846,0.010234534,0.021031722,-0.073130004,0.04011021,-0.006111739,-0.058021665,0.045228362,0.0041261166,-0.03918462,0.020302916,0.0045146532,-0.054486256,-0.009293662,0.014596714,0.0014361127,-0.04692797,0.028252473,0.025220953,0.047356337,0.04286258,0.04057087,-0.011462676,-0.129435,-0.03361516,0.024865814,-0.065717794,-0.028331647,-0.014843246,0.010905161,0.04196672,0.04914933,-0.10514313,-0.0027992488,-0.019651368,-0.050886687,-0.020860502,0.0017618602,-0.03433907,-0.088913165,-0.0035681983,-0.022944517,0.062374666,0.039809063,-0.062687084,0.03912352,-0.023009328,0.008863176,-0.037196755,0.037526716,0.033820704,-0.039306555,-0.11158845,-0.029754395,0.0059393803,-0.015073145,0.022720225,-0.041520204,0.0062105223,-0.04750511,-0.015662327,0.017570885,0.019708311,-0.06739369,-0.09349464,-0.072627716,0.062365457,-0.019477984,-0.003988598,-0.078592144,-0.011915784,0.0010810874,0.01850177,-0.009407402,0.07136679,0.03422315,0.037183184,0.099281706,-0.052020803,0.010578196,0.03801334,0.02821569,0.08470688,-0.023603749,0.07409048,0.047855534,0.0003188545,0.025990708,-0.04825592,-0.01996209,0.016346855,-0.030859828,-0.050712943,-0.005819858,-0.031328563,-0.043409847,-0.020759005,0.024675354,-0.06991415,0.05522613,0.037253592,-0.014598861,0.03668073,-0.046187934,0.073917896,0.041735854,0.012352774,-0.07640463,0.01760769,0.063454665,-0.06450464,-0.007962282,0.018230235,0.0066959118,-0.025122156,0.047044393,-0.1037028,0.13796784,-0.018301766,0.036859863,0.0036538204,-0.05992084,-0.0171404,-0.06436848,-0.05170443,6.8821167e-34,0.013050841,0.016779587,-0.00037512282,-0.0682644,-0.039956823,0.025111955,-0.10714686,0.061834387,-0.02191474,0.010109225,-0.040537197,-0.01406526,0.041843552,0.17934616,0.07580517,0.027525498,-0.065199405,0.088519074,0.047955766,-0.006933583,-0.039381128,-0.02367872,0.013103494,-0.09190182,0.028067445,0.07208431,0.007505447,0.03048534,-0.05899141,0.007877675,0.03210086,0.031780235,-0.02688416,-0.071868494,0.067843005,0.034715652,-0.020045944,0.055720817,0.051617697,0.039053775,-0.12767093,-0.019977344,0.0013923716,-0.032792553,-0.070494115,-0.01585912,0.021462902,-0.024053851,-0.054476373,-0.002812632,0.0483532,-0.031602323,-0.028762469,0.038407348,-0.053829227,0.0635576,0.0996096,-0.028617118,-0.07668678,-0.022639884,0.043773204,-0.014821138,-0.023874206,-0.025037996,-0.021697586,0.005582317,0.042620335,-0.0087850485,0.07029035,-0.030254535,-0.0036950395,0.0010269929,0.09710534,0.0016435891,-0.048036564,0.0934591,0.08123582,-0.059112906,0.07201758,0.06198865,0.015015306,0.02550073,-0.05507555,-0.064164996,-0.017580055,0.09205961,-0.043118037,-0.0034490514,-0.09561463,-0.04189346,-0.06527941,0.033208884,-0.046497636,0.0075392853,0.01969005,-2.8968853e-33,-0.07310391,0.03898566,-0.0051363767,0.018021634,-0.016079769,-0.025945552,-0.0336261,-0.07931634,-0.0055517405,0.02280688,0.048006322,-0.0010825744,0.054310486,0.04549256,-0.00040718712,0.031916257,-0.13555668,0.037050784,-0.056322828,0.052810356,0.08448582,0.08613019,-0.07289716,-0.061072722,-0.040757533,-0.016234072,0.008828318,0.017328395,0.05899945,-0.04991138,0.016782831,-0.007558291,-0.02662368,-0.06440275,0.12860674,0.03606076,0.018788978,-0.038427442,0.02912171,0.02363282,-0.018645743,0.029896861,-0.048832994,-0.030058037,-0.015625484,-0.051961165,0.07300757,0.030846773,0.08014204,0.02024127,-0.036906067,0.031212846,-0.00707121,0.11483831,-0.0075962413,-0.01950829,-0.07915645,-0.032621775,-0.047162738,0.027093034,-0.040242374,0.08250366,0.07572508,0.0820552,-0.023338795,-0.04480488,-0.072760396,0.033042114,-0.032995123,0.0011534528,0.04969132,0.04289408,0.09406562,0.009007245,-0.040471066,-0.0744566,-0.061629903,-0.021383783,-0.04460924,0.0104063405,0.037923057,-0.033752404,-0.023787513,0.05800701,0.03126655,-0.056566484,0.08873257,0.03197919,0.020214656,-0.033176683,-0.023335975,0.0868027,0.005141505,0.049550347,-0.16190316,-5.3763053e-08,0.007151009,-0.036384918,0.093291864,0.015817236,-0.00334391,0.099114135,-0.02651242,-0.003692758,-0.05143843,-0.049010124,0.023140311,0.08645015,-0.11825027,-0.07681845,0.08060953,0.03748996,0.07242463,0.06891357,0.03749029,0.03751899,0.07959181,-0.08609197,-0.029223062,-0.0618758,0.013787429,-0.06615438,-0.01738464,0.09673291,-0.04055432,-0.025373844,0.035713665,0.064698815,0.02979577,0.019846037,0.031687465,0.09586108,0.054838136,-0.029432436,-0.05799977,-0.024891378,-0.0010235951,0.008746874,-0.027217498,0.005798469,-0.033325106,0.0356276,-0.0062955334,-0.027861949,-0.0012664176,0.024046825,-0.021876575,0.047573574,-0.043309998,0.05272543,0.03505133,0.07608127,0.018082025,-0.05401739,-0.026123568,0.08131689,-0.048577636,0.05247969,0.06316316,-0.033236373,13,-0.5483279,-25.563532,10
406,"today we studied logtistic regression. we have to maximise the likelihood function which is similiar to the error function in linear regression. since products are hard to work with we use the natural logarithm of the products instead. we studied about the condusion matrix.
accuracy is tp+tn/total. precession is number of detected correctly/total. ",0.026236607,-0.059002806,0.0023123003,-0.098833956,-0.00672491,-0.015900131,0.06310362,0.042366926,0.013143196,0.11298798,0.050302193,0.011035249,0.03357209,0.07019454,-0.01582129,0.025499165,0.0950469,0.06628282,-0.05738549,0.003541012,0.027504038,0.046564195,-0.047735192,-0.027555546,0.021580413,-0.023355372,0.040850904,-0.092387035,-0.009253036,-0.026584767,-0.063611776,-0.0051243682,0.09232613,0.017968824,-0.03170165,-0.029290946,-0.027207984,-0.08408781,0.060359277,0.067836635,0.014641358,0.012765257,0.030185737,-0.01642977,-0.0039742873,0.04430388,0.056855995,-0.024289621,-0.030266022,0.038244423,-0.054384515,0.04267601,0.04048623,-0.005176186,-0.066376686,-0.14132684,-0.0011646716,-0.013175615,0.033942882,0.04201371,0.024547398,-0.03186459,-0.022073364,-0.0572005,0.09212138,0.023783142,-0.11064183,-0.047667187,0.0068245423,0.08926681,-0.05385506,0.051240027,-0.053462654,-0.015730325,-0.0315495,-0.04696362,0.048072614,-0.018205732,0.071084164,0.009618829,0.034180056,0.049734406,-0.039669655,-0.05619746,0.093135945,-0.047398057,-0.050830774,0.15243639,0.036652964,-0.040142193,-0.00032497538,0.0036659725,-0.077897,0.014834646,-0.09626788,-0.04755908,0.014946418,-0.0294043,0.09576823,0.06405852,-0.091951504,0.012886156,-0.008307902,0.04724747,-0.085898295,-0.06809338,0.036902376,-0.03921054,0.047626708,0.009612529,0.0061052595,0.017223524,-0.022154506,0.079728596,-0.0010079056,-0.014240986,0.021443198,0.017744267,0.013715035,-0.0069884844,0.07258721,0.009948477,0.14785425,0.060801987,0.061495043,0.060671452,-0.11515146,3.634426e-33,-0.098380215,0.013911663,0.018368768,-0.034508873,-0.03430592,0.06753389,-0.09318858,-0.055537954,0.053070966,-0.002615911,-0.068889305,-0.0054581733,-0.04273747,0.022545096,0.0015224975,0.03133519,-0.032528765,0.049403057,-0.03018368,-0.004158854,-0.016748462,-0.024622506,0.045651417,-0.038025327,0.018556124,-0.04260125,0.078077644,-0.030599905,0.013619092,0.019272022,0.057274222,0.008495068,-0.057467125,-0.07092565,0.012176464,-0.009138206,-0.02254969,0.021666555,0.026972882,-0.042955857,-0.0201709,0.030815966,0.034227625,-0.05202096,-0.057584543,-0.03682695,-0.029274901,-0.009908804,0.021426631,-0.11130069,-0.021054497,-0.06813915,-0.06693214,-0.072239496,-0.040209237,-0.040366713,-0.024877798,-0.016286185,0.03779696,0.06328621,-0.025139771,-0.05463787,0.03480963,-0.10017292,0.020495936,0.048427135,0.09276963,-0.0015469114,0.11174997,0.0044247336,-0.010098135,-0.03628001,0.08526062,-0.031854633,-0.024835521,0.023376659,0.05479429,0.043729234,-0.04288822,0.026222512,0.0052448455,0.030041555,-0.0122462055,-0.06873424,-0.020510968,-0.08046659,-0.105866276,-0.051079813,-0.08226529,0.09131833,-0.01267598,0.023217028,-0.026383115,0.0803108,-0.030583238,-4.278296e-33,-0.08274611,-0.0045731696,0.02047804,0.018286815,-0.01368265,-0.08693361,0.004868376,-0.0022348822,-0.0027517977,0.021245997,0.10319415,-0.032107532,-0.0054878383,0.072173476,0.065336086,0.07267372,-0.026565542,0.073124915,0.058953118,-0.13412924,0.031213405,0.061059788,-0.010602472,-0.009102889,-0.049246687,0.021129338,-0.10844704,0.10373521,-0.05439684,-0.037877545,-0.018400684,-0.024463756,-0.045590036,-0.020290758,-0.007126327,0.022488983,0.0098361615,-0.030274015,0.046692498,-0.0076079257,0.06675234,0.04840182,-0.016962396,-0.061179258,-0.023226792,-0.055628743,0.081872374,-0.077875085,0.05696476,-0.004983981,0.043153506,0.045728434,-0.028934015,-0.005692859,-0.050027594,0.096030496,-0.054740604,-0.048372563,-0.008182483,-0.0038933828,-0.072438546,-0.018981002,-0.038502,0.08763094,0.008175693,-0.02162687,-0.048597883,0.054765508,0.122117564,0.014851768,-0.025557606,0.04330508,0.020531576,0.0941541,-0.018874649,0.031720776,-0.060126435,-0.038782235,0.016834497,0.029371615,-0.06728957,0.0066837063,0.0021109004,0.017727522,0.0060724667,-0.03610666,0.06800287,0.012306496,0.0264795,-0.062802516,-0.08003243,0.012974119,0.009020117,0.0069782916,-0.005985404,-4.4698155e-08,-0.078313716,-0.06964861,0.03623999,-0.016688026,0.06026648,0.037668914,-0.037304386,0.07032074,-0.06796673,-0.018598571,0.029612975,-0.002643364,-0.05925983,-0.072998606,-0.03169184,-0.0021396503,-0.047385443,0.04989461,-0.018851796,0.0019245401,0.12193167,0.04950635,0.015866224,0.008554332,-0.024391498,-0.069836505,-0.052352123,0.11811041,-0.040595137,0.053633466,0.0023972595,0.025735183,0.028425794,0.03114258,-0.0059574014,0.041174646,0.035288937,-0.038973924,-0.05554683,0.00065607415,0.01269347,-0.026886746,-0.07446433,0.014604355,0.018808331,0.059499733,0.0019273413,-0.09634289,0.0007536262,-0.01954476,0.061219633,0.056414865,-0.011913327,0.035962205,0.042978767,-0.034988448,-0.028799072,-0.04175943,-0.036448818,0.01032292,0.065291606,0.035260584,0.10386726,-0.02888714,13,1.1511412,-24.269583,10
416,today's lecture started with discussing about some doubts asked in the lecture summaries. here we discussed about how to change the the bin size of the histogram according to some rules which helps use to use the metrics more efficiently. how two models can be used to fit a single dataset but the problem with this is that the continuity is not maintained. some discussions on expectation algebra were also done. then we shifted our discussion on logistic regression where we discussed how it is used for classification by implementing probability and mapping this probability to classes. then we discussed about some metrics which can be used to gauge the precision of our classification. confusion matrix which can be used to depict the classified and misclassified data. precision is the correctly classified events out of all the events. recall is the correctly detected events of a particular class. f1 score is the harmonic mean of precision and recall.harmonic mean is taken so as to predict whether we have a balanced model or not. thank you.,-0.025597846,-0.0031941608,-0.0882885,-0.047454227,-0.021671489,0.027246714,0.0014215264,0.041147932,0.003961983,0.029288743,-0.001701603,0.026611751,0.015212671,0.011585954,-0.04719949,-0.07058061,0.014234048,0.064939804,-0.107717946,0.002047728,0.014190493,0.04331275,0.050680663,0.050313156,-0.03658645,-0.04085158,-0.023987189,0.026271319,-0.09100907,-0.06983754,-0.017094033,0.0543997,0.039545774,0.0075986907,-0.13962378,0.006371631,0.055794045,0.06734629,0.03518529,-0.028912429,-0.042829305,0.008215243,0.032717165,0.05341584,-0.0083421655,-0.020483844,-0.052278116,-0.04310667,-0.023442568,0.016327446,-0.103078656,0.07448701,-0.012222114,-0.030585062,-0.062440224,0.004732045,0.03819002,-0.028338872,4.541551e-05,-0.050238486,-0.053199258,-0.082255594,-0.01840838,0.015531806,0.029869456,-0.031011123,-0.022022376,0.016390562,0.07460003,0.05330898,-0.07615854,0.021311423,-0.04201554,0.06490577,-0.011314148,-0.02358789,-0.011525325,0.062195394,0.034427907,0.058019575,-0.04243511,0.042863347,0.0822919,-0.025323145,0.13903935,-0.027803248,0.007713397,0.064907625,-0.059400015,-0.072524145,-0.0052036694,-0.026136419,0.057400774,-0.043974023,-0.0009191247,0.041388568,0.0038056783,-0.028602563,0.07284263,0.061817694,0.021533433,0.06929673,-0.0473666,-0.049877863,0.027767055,-0.065262735,-0.00021541781,0.05321961,0.082666315,-0.088347845,-0.021091146,-0.020753592,-0.13759905,0.002174315,0.03839517,0.034642257,-0.0012054263,0.075947925,-0.045837443,0.097282924,-0.061939985,0.02563028,0.054246884,-0.023570938,0.09000636,-0.0057533225,-0.054829404,4.2529834e-33,-0.030194461,-0.056937568,-0.026709247,0.05414279,-0.04019348,0.03050752,-0.109999344,0.038310025,0.11963193,0.021551058,0.0453464,0.11900531,0.025098927,0.05812224,0.06745988,0.046760384,-0.10485258,0.07468993,-0.035558145,-0.079599254,-0.002198672,-0.011700979,0.1221004,-0.055276014,0.03605991,0.052800864,0.032832395,0.085297376,-0.02151519,0.01602516,-0.018819949,0.014097265,0.024752866,-0.030114112,0.046060123,-0.0058022696,-0.051618844,0.023470152,0.033205908,-0.08983743,-0.033893358,0.024183525,-0.046661787,-0.027138209,-0.05311236,-0.002402464,0.072774954,-0.017760588,-0.018293444,-0.016935702,0.010409219,-0.064008445,-0.015282216,0.00367039,-0.020207122,0.06509033,0.022764672,-0.024132073,-0.07260739,0.015708307,0.007927569,0.026707992,0.01695562,0.0098501295,-0.026774613,0.047807213,-0.024423517,0.047236837,0.045320183,0.0031834054,0.014997114,-0.024867881,-0.024211556,0.029984133,-0.009145268,0.04290798,0.05745866,0.016093886,-0.041812807,0.03438027,0.045527253,-0.056574244,-0.051920895,-0.08736908,-0.11348298,-0.001159929,0.08342189,-0.0036956575,-0.0782416,0.043707855,-0.10497672,0.049881354,-0.02144096,0.03772705,-0.025446245,-6.939935e-33,-0.08059398,0.073395796,-0.06098488,0.08373464,0.026171114,0.030820819,-0.007892423,-0.051324513,-0.030174134,-0.061151687,-0.019492002,-0.002360058,0.024314051,0.012421521,-0.09705809,0.018657515,-0.040198006,-0.0032120163,-0.08245945,0.025924355,0.08824653,0.052927587,-0.13963436,-0.054795563,-0.035416063,0.057607703,-0.045954388,-0.023328075,-0.043964434,-0.06366889,-0.018651009,-0.07329691,-0.0018416053,-0.043559793,-0.014500618,-0.06649403,0.062622726,-0.0077405744,-0.026993223,0.08907563,-0.003090355,0.06622117,-0.11826893,-0.06552605,0.031481937,-0.022996368,0.037410684,0.07320518,0.014301078,-0.030950215,-0.0069536692,0.010812314,-0.07584794,0.02136552,0.008830796,0.036432546,-0.043986887,-0.054899223,-0.123639025,0.09503896,-0.09178514,0.011618144,-0.011471029,0.09421613,-0.021573026,-0.0553808,0.018330444,-0.0054929755,-0.050984558,0.049011502,0.015674612,-0.01950821,0.03917465,-0.023713905,-0.0075671356,-0.010759729,-0.0117922425,-0.032213096,-0.034274578,-0.014982486,-0.011776198,-0.058977928,0.028933695,0.082280375,-0.015896752,0.054158542,0.10912292,-0.023304319,0.054734133,-0.025246447,-0.053379357,0.05275582,-0.01644923,0.028413171,-0.058354985,-7.2271945e-08,-0.0063247397,0.041547257,0.07627531,0.000102907215,0.0014928712,0.015062179,-0.0908606,0.049316876,-0.03313247,0.028445361,0.045473993,0.03589533,-0.13791971,-0.029813727,-0.008383234,-0.023254693,0.024727248,0.11489617,-0.021435129,0.037709914,0.050629444,-0.082819924,0.03297361,0.009347807,0.05391667,-0.036916256,0.021061985,0.11648848,-0.011415321,0.029506437,-0.019996334,0.043395486,0.06259402,-0.040082384,-0.0029624554,-0.028900085,0.0077225976,-0.03306477,-0.0043788133,0.006075787,-0.015210592,0.0019630142,-0.049154438,0.060627732,0.061088845,0.034649555,0.030847223,0.038741853,-0.043500263,0.057817943,-0.042210776,0.05480912,-0.05537317,0.022512097,0.07114813,0.06333389,0.011107357,-0.07567928,-0.06224074,0.010791136,0.056411065,-0.011314982,-0.09091803,-0.01835347,13,4.0825076,-27.734903,10
478,"we again started our discussion of clustering and logistic regression ( it doesn't directly predict the value of class but probability that value will be in this class or another ). also we were discussing expectation algebra in the starting as a doubt question. if probability > 0.5 (kind of set level) then we classify it to one class otherwise to another class . features can be many in the class we started with 3 features and output to 0 or 1 . predicted is p. then we want to calculate weights such that likelihood of getting desired output is maximised (minimise differences). now the output we wanted can be either predict the class or the probability ..  (it is kind of binomial distirbution) .. discussion on p and 1-p for all observations. we also discussed on log likelihood and error function was negative log likelihood for classification. we also saw false positive , true positive , false negative and true negative. accuracy and precision (of the events you detected how many were correct) and recall(of the specific class how many were correct) . combination of precision and recall makes f1 value (harmonic mean) and confusion matrix.",-0.008370654,-0.034267467,-0.054120127,0.0074854777,0.019235743,-0.037440844,0.0866857,0.036331274,-0.026393343,0.009662204,0.033149615,-0.04218746,0.08155838,0.017616475,-0.04162176,0.00015417804,0.036425598,-0.0001630402,-0.110450014,0.014275641,0.045319967,-0.009217072,0.07043142,0.0716482,-0.00635061,-0.0003830401,0.0477429,0.038284983,-0.06865465,-0.08353468,-0.036149275,0.023308754,0.029831786,0.00033058415,-0.060068574,0.035984784,-0.0594086,-0.022964096,-0.022256825,-0.0018891885,-0.030641751,-0.057587683,-0.0012777187,0.0038431385,-0.008543505,0.038397897,-0.09722959,0.060494512,-0.048431553,-0.036402717,0.030704591,0.08892035,-0.04517454,-0.09538829,-0.13591285,-0.022819858,0.12819536,-0.023261856,0.0018964467,-0.06519839,-0.03923746,-0.05342592,0.005195844,-0.014165234,0.027426057,-0.0054214215,-0.03072661,0.022327483,0.09828061,-0.001382944,-0.019204896,-0.0017549642,-0.014118978,0.00095177017,-0.0007205082,0.029849937,0.08288154,0.068285264,-0.09201418,0.08435172,0.02231336,0.04150642,0.035051364,-0.03341119,0.10784651,-0.032084044,-0.05226821,0.058352757,-0.034240656,0.015790373,-0.09092372,-0.013242091,-0.032544438,0.035192747,0.028197467,0.058025476,-0.046031885,-0.07799926,0.09148392,0.04891778,-0.070273556,0.08164274,0.029099407,-0.09092243,0.0077546737,-0.018577885,-0.04689235,0.008568107,0.058965325,-0.054335587,-0.030079272,-0.01670915,-0.08703617,-0.031519216,-0.0155434925,0.059714098,0.07297839,0.107677944,-0.13135915,0.06421583,-0.006056671,0.00022140257,0.045118038,-0.034139086,0.098170064,-0.031888817,-0.13542244,4.7996933e-33,-0.01891679,-0.085528105,-0.0070962175,-0.031528994,0.021105783,0.066338174,-0.0906526,0.02593487,0.033807386,-0.04973764,0.011264211,-0.03679523,0.09709408,0.03790834,0.062381145,0.03266637,-0.034488793,0.053403437,-0.055515047,-0.05418558,-0.05119471,-0.01010016,0.036574244,-0.0942779,-0.046301212,0.066461325,0.032477036,0.015795432,-0.016567476,-0.0117613925,0.029166568,0.021842316,-0.00038158483,-0.08990108,0.01672432,0.04207871,7.337115e-05,-0.031127768,0.009922451,-0.04122373,-0.08488145,-0.0070652314,0.07798567,-0.06971334,-0.027898448,0.013097285,-0.009412334,-0.0017919913,-0.09265477,-0.03083738,0.022218108,-0.11716573,-0.009857979,0.062059686,-0.05829492,0.015500644,-0.006742584,0.045757484,-0.0017811634,0.014968431,0.042978786,0.014736601,0.010784484,-0.068157,-0.02755507,0.012109257,-0.0045332634,0.052115258,0.038786016,-0.008435728,0.05563499,0.013150555,0.051076956,0.049017806,0.0019539306,0.11137036,0.05515833,0.038526416,0.011149137,0.014104223,0.039767522,0.03705514,-0.060776252,-0.0064079845,-0.008478768,0.04663508,0.003292975,-0.0066787386,-0.06994344,0.05900715,-0.06934662,0.041051652,-0.009291432,0.026248971,-0.013207329,-6.638707e-33,-0.0598647,0.099282295,0.021361072,-0.025187124,0.010574615,-0.0010820193,-0.01253963,-0.03558562,-0.053643767,-0.007074315,0.012617367,-0.031074775,0.015161508,0.0035787835,-0.08643807,0.037711993,-0.052607786,0.017580017,0.04800205,0.024370605,0.07158787,0.014752411,-0.09316719,0.037287783,-0.017010033,0.017758474,0.014340276,0.019803984,-0.017412497,-0.023243368,-0.0126972385,0.017822048,-0.028124793,0.007754287,0.029738195,0.00899476,0.003669625,-0.02946244,0.038208142,0.029371405,-0.049435254,0.06335948,-0.03799654,-0.03714783,0.019208467,-0.13852766,0.06262823,-0.023917621,0.08813315,0.024845226,-0.047611553,0.0035569568,-0.030807959,0.07956703,0.017671367,0.044756263,-0.030451696,-0.05432067,-0.06385317,0.0528015,-0.07076709,-0.014530396,0.04539064,0.08817292,-0.050701246,-0.023887698,0.0033555091,0.09033186,-0.0072367145,0.067120366,-0.06324974,0.07693372,0.105296835,0.012147263,0.00031778208,-0.07398427,-0.011167023,0.04647325,-0.010097146,0.017361945,-0.059163097,-0.081971504,-0.057256922,0.06289956,0.055700988,-0.08218406,0.08084544,0.034352727,0.07684957,-0.0577829,-0.04220439,0.07453109,0.05761371,-0.017490216,-0.046122253,-6.430787e-08,-0.04736366,-0.049559347,0.0359645,-0.0023215641,0.07097987,0.070072114,-0.065142706,0.024050383,-0.107527174,-0.017548764,0.018384803,0.0020312197,-0.110885575,-0.050027512,0.027804775,0.07067457,0.025421808,0.115076184,-0.013245264,0.0021680007,0.074578136,-0.08797222,-0.0135146985,0.011940522,0.024542738,-0.12390157,0.036735244,0.061861116,-0.01611707,-0.023981137,-0.08809561,0.012330853,-0.04086471,-0.0039021485,0.0561369,0.019910209,-0.016903235,-0.023194773,-0.041252192,0.036932882,0.033375505,-0.052563444,0.0044639786,0.0103479,0.10690619,0.05374521,0.0045501688,-0.017668989,0.029043922,-0.008494912,0.013386079,0.08692937,-0.040961612,0.052386973,0.03009378,0.107417345,-0.057263568,-0.051295567,0.019468842,-0.00024960496,0.0060673365,-0.03928149,-0.009301834,-0.044381775,13,1.0180693,-27.615759,10
483,"in todays class we learned about logistics regression and talked about weights . logistic regression is a way to predict outcomes that are either 0 or 1. it works by taking input values, multiplying them by weights, adding a bias, and passing the result through a sigmoid function to get a probability. the model is trained by adjusting weights using the gradient descent method to minimize errors. the likelihood function helps in optimizing weights by maximizing correct predictions. the model's performance is evaluated using a confusion matrix, accuracy, precision, and recall. if the predicted probability is greater than 0.5, the output is 1; otherwise, itâ€™s 0.",-0.046842653,-0.0063932454,-0.06640787,0.048327282,0.013190082,-0.10863309,0.057482183,0.008317165,0.0034714176,0.048488684,0.042062595,-0.007879202,0.04331166,0.0596283,-0.036759123,-0.03829782,0.047773037,0.045325883,-0.11250881,-0.047182333,0.008820498,0.027659632,-0.0085818125,0.03936688,0.0055043637,-0.10165292,-0.08782959,0.03360099,-0.062444795,-0.043690022,-0.039991252,-0.0190369,0.035938933,0.04394607,-0.1328848,0.0045577027,-0.04867556,-0.07562139,0.00018140573,0.010093751,-0.05607935,-0.056962654,-0.023512108,-0.0013570797,-0.005556807,0.00787289,-0.052809186,0.014248266,-0.024254594,0.0286623,-0.02437028,0.011234437,0.01964588,-0.038180478,-0.08503114,0.004054591,0.08239937,-0.008541361,-0.03776171,-0.047727227,-0.04680747,-0.053009227,-0.00080202275,0.0032796822,0.026870707,-0.10464455,-0.07857388,-0.009807448,0.017781952,0.0045401216,0.03883075,-0.075197145,-0.02394427,0.021531483,0.04887699,0.016805753,0.10831518,0.015400212,0.005625661,0.12654425,-0.03659292,0.041158304,0.011275802,0.02882156,0.047101256,-0.05503483,0.03388032,0.101408646,0.034451384,0.0036166718,-0.02843812,-0.03324779,-0.01532337,0.03483169,-0.018618139,0.04372552,-0.06981279,-0.0267785,-0.017208915,-0.030085001,-0.027252162,0.078205824,0.09660576,-0.02757394,-0.018602876,-0.055734638,0.037911285,0.027544316,-0.016934149,-0.060792096,0.008392526,0.053365514,-0.027289996,0.0039105434,-0.019546568,0.012366524,0.015831878,0.019221677,-0.16619393,0.08325532,-0.024289971,0.012406385,-0.0051087975,-0.07460615,-0.023686757,-0.0057744053,-0.04623449,2.9996054e-33,-0.054419424,-0.013892309,0.00037094977,-0.0642647,0.0077840285,-0.016188128,-0.104628086,0.020893544,0.030255444,0.0066555296,-0.09400232,0.028148603,0.02401245,0.14588845,0.028858986,-0.05788399,-0.06074736,0.100370295,0.050075423,-0.033106554,-0.046010427,-0.06596861,0.038264107,-0.11739492,0.09707248,0.04803797,0.036007322,0.005240162,0.01689814,-0.000988917,0.055718936,0.025728201,-0.046102922,-0.09053601,0.016765984,0.043351956,-0.038435463,0.015094621,0.034858823,0.030811949,-0.08813859,0.01069972,0.014460418,-0.00045498335,-0.08696281,-0.00019785104,0.00018188827,-0.048112884,-0.072200954,-0.034929384,0.009794462,-0.08498563,-0.010199307,-0.0073385322,-0.000990228,0.011247985,0.09231436,-0.052927442,-0.045932304,-0.036372595,0.051890925,0.0013610198,0.037125397,0.027514266,0.048924934,0.0096789645,0.05699724,0.010123755,0.027127624,-0.026206415,0.0494569,0.008503495,0.10471561,0.028452007,0.02159214,0.08316442,0.07765049,-0.06432369,0.00848478,-0.012230877,0.003131424,0.058762267,-0.01937542,-0.025530118,0.010420816,0.056349516,-0.075120755,0.00660683,-0.03492739,-0.004051402,-0.05783359,0.037610896,-0.08947633,0.018290248,0.023298409,-4.8966977e-33,-0.047997653,0.082394935,0.004815992,-0.007061493,0.016331626,-0.01321467,-0.024083205,-0.08789253,0.039787296,0.03208281,-0.045219902,-0.008116692,-0.007549543,0.07914129,0.05166013,0.015722362,-0.08745522,-0.011526189,-0.0717219,-0.02286446,0.053052373,0.009880402,-0.12771621,0.0040932763,-0.0069230916,-0.033569954,-0.016856823,0.021531321,0.009457737,-0.060222436,0.011048577,-0.085664205,0.01906957,-0.036053926,0.06775596,0.015279727,0.05463468,0.045001857,0.045844126,-0.0038608327,-0.003899983,0.026787277,0.0343602,-0.04829172,-0.04418912,-0.11277657,0.066556424,-0.06699346,0.09401588,-0.03192918,-0.028141703,0.008325957,-0.020004233,0.06857956,-0.025405552,0.047214564,-0.07633755,-0.032938436,-0.02366336,-0.017485358,-0.07833968,0.058347117,0.071469046,0.06575951,-0.08798189,-0.0055228733,-0.027576348,-0.025573228,-0.020384315,-0.0022906163,0.00961111,0.025526796,0.1301033,0.09500689,-0.032558285,-0.07208658,-0.046499953,0.016427288,-0.038864065,0.027754664,0.05619387,-0.072850525,0.03614931,0.07272415,0.043475784,-0.021279493,0.06872894,0.020338265,0.001246334,-0.045826178,0.0021751928,0.04112536,0.013604137,0.02873631,-0.13994846,-5.6620518e-08,-0.029092219,-0.01517835,0.1050612,0.038397904,-0.0073319655,0.04207168,0.05003238,0.00960229,-0.06611981,0.017272701,-0.033841275,0.0733728,-0.09241818,-0.02271478,0.044000186,0.089986935,0.021060528,0.07684387,0.046892177,-0.003802179,0.077968314,-0.027716553,-0.05369028,-0.0038428558,0.05459801,-0.06827301,-0.038152847,0.077396676,0.043177124,-0.003921459,0.04029268,0.08985421,0.00296183,0.00482851,0.0450594,0.06098675,0.05927503,-0.06876221,-0.058422524,-0.04260199,0.03841054,0.024209784,0.021148827,-0.0064727683,0.097316556,0.039357956,-0.053581405,-0.0032473933,-0.033865966,-0.008710732,0.031054877,0.061808106,-0.0044472488,0.0860788,0.06517029,0.039987903,0.023532983,-0.058380354,-0.008715841,0.09964537,-0.05327908,0.007414591,0.10253943,-0.018433377,13,-1.1765176,-25.155441,10
499,"the class started with a brief discussion about the previous lecture, and then we moved on to logistic regression and regression with multiple variables. we looked at the methodology of assigning weights to different parameters to make the model more accurate. after this, we looked at the confusion matrix, which was used to check the quality of the model and different calculations for the four types of entries, namely true positive and negative and false positive and negative. the class concluded by discussing assignment one and how it can be improved. ",-0.007834175,0.013495076,-0.06469303,-0.026736341,-0.031165253,-0.082667075,0.0980092,-0.007109918,-0.013892721,0.04553982,0.069882035,0.018478235,0.059906185,0.0017951184,-0.06605753,-0.027135184,0.015394433,0.036869142,-0.08792295,0.018804105,0.033596918,0.0029013995,0.022662472,0.042700365,-0.02589759,-0.054521196,-0.053461775,0.015138087,-0.0025243023,-0.034340028,-0.07790401,0.09665828,0.021064686,0.055051114,-0.09852284,0.0061297356,-0.020823415,0.030691372,-0.020582668,0.08539689,-0.07614618,-0.03877666,0.018126355,-0.02735877,-0.0057136593,0.012409311,-0.09551007,-0.0016433429,0.034717698,-0.0067255734,-0.04522836,0.04662768,-0.05423354,-0.09322239,-0.09941558,-0.042138483,0.015443766,-0.016695147,-0.04513025,-0.006525011,-0.00026555022,0.0062108207,-0.031580754,0.03754715,0.10072229,-0.006856197,-0.15616603,-0.02763693,-0.008517867,0.08222344,-0.013928348,-0.030671906,-0.021038527,0.002492663,-0.050444968,0.038541075,0.09383476,0.03317304,-0.023746641,0.069179654,0.005710195,0.034758702,-0.031247381,-0.008880279,0.118482225,-0.0848601,0.004777483,0.0038322622,0.0007466416,0.05449684,0.06407329,0.006230368,0.040894236,0.026703877,0.026461622,0.0754216,-0.024408817,-0.037478633,0.022539208,0.052417543,-0.013434603,0.06948488,0.054134265,-0.021574289,-0.039950546,-0.09884328,0.02430999,0.0043501556,0.016272638,-0.105903804,-0.051559906,-0.006086923,-0.1039542,0.021684207,0.015786042,0.05933259,0.062923275,0.13023782,-0.107515536,0.02533765,-0.03909738,-0.0019543546,0.03591487,-0.0141459275,0.011351971,-0.048275195,-0.14404242,-2.1809793e-33,-0.0296515,0.05305354,0.010801557,0.075143665,-0.03869683,0.014839095,-0.11408516,0.009863436,0.04330541,-0.0038721901,0.045505702,0.028215703,0.029446254,0.13481443,0.01172731,-0.0038522985,-0.10604517,0.051643863,-0.009798513,-0.024078378,0.04698491,-0.012366906,0.056649227,-0.10187974,0.03965744,0.039080493,0.015799886,0.000633024,-0.04979852,-0.014434173,0.027328456,-0.01438859,-0.017554719,-0.05111111,0.057945423,0.029827805,-0.017720532,0.032640725,0.043697104,-0.08859611,-0.02156182,0.021230679,0.048030753,-0.022702849,-0.0067251544,0.056606293,0.039292708,-0.011343503,-0.024936164,0.042069323,-0.06308414,-0.06046809,-0.021137988,0.039631087,-0.041099913,0.016915776,0.020379143,-0.00022161506,-0.039533585,-0.017045574,0.015443707,0.045071963,-0.04101135,-0.07326681,-0.04438549,0.056593202,-0.024893336,-0.026482826,0.10018467,-0.0312998,-0.032737147,-0.019929672,-0.026260935,0.016905637,-0.023284866,0.030184751,-0.028492257,0.0001306562,-0.01922813,-0.08605896,0.06070835,0.078231685,-0.03099057,-0.025650395,-0.14660406,-0.001959815,-0.007952167,-0.044523567,-0.030832907,0.08230793,-0.0034246976,0.048445527,-0.026383895,0.04358388,0.02041777,-2.280895e-33,-0.10327972,0.035146892,-0.04974373,0.038368836,0.0069661993,-0.027741766,-0.017589767,-0.051475093,0.018671399,0.022574583,0.029048933,-0.003390714,-0.075406544,0.059089184,-0.0049767233,0.026842427,-0.047086,-0.01740496,-0.044013817,-0.060833693,0.056594957,0.1270112,-0.057185326,-0.060014106,-0.021556344,0.06867327,0.00599869,-0.015651338,-0.007495801,-0.051918376,-0.04321419,-0.007348633,-0.028982917,0.102465786,0.053203344,0.02231406,0.03197017,-0.009409357,0.014354541,0.0662536,0.01682138,0.048062358,0.0066412073,-0.030484991,-0.009532968,-0.02503011,0.066826314,0.02144475,0.056497715,-0.024812657,-0.04730285,-0.041977834,-0.030559296,0.09437874,0.04546433,0.06663466,-0.02804354,-0.10863603,-0.013409283,0.039281487,-0.103071995,0.042967156,-0.029765498,0.025353568,-0.08660508,-0.051652744,0.01220018,0.004496253,-0.017820595,-0.022697283,-0.018882386,-0.015195747,0.065231286,-0.056939997,-0.025613572,-0.048456382,-0.0722338,0.019286225,-0.06652715,0.008620988,-0.047951054,-0.07122337,0.001426782,0.12067324,-0.06377721,0.025000323,0.08476095,0.02451355,-0.013654795,0.021479836,0.008256913,0.07391608,0.019071408,-0.005742308,-0.06006674,-5.230703e-08,-0.02401344,-0.0013314701,0.033865746,-0.014521477,-0.022458794,-0.040952276,-0.01672303,0.012374116,-0.07277432,0.09493925,-0.0032524394,0.06146407,-0.09445443,-0.03330837,-0.019167023,0.00041596688,-0.048663035,0.07697885,-0.004295465,-0.020947974,0.16319306,-0.06858862,-0.034292545,0.028534608,0.07392866,-0.023219489,-0.012482134,0.08480659,-0.033100653,0.036209874,0.028472537,0.09380163,0.0013861848,-0.011764412,0.05398629,0.06951838,0.0009107257,-0.02713726,-0.00052189437,0.01799732,-0.061367907,-0.022963274,-0.042915653,0.032869063,0.10584853,0.050967634,-0.08800023,-0.040229537,-0.0613463,-0.02275426,-0.034252413,0.060073644,-0.051648527,0.09780339,0.03925137,0.019854492,-0.0031694374,-0.07070188,-0.06736868,0.019607535,0.02809734,0.0888573,-0.03697397,0.0027309968,13,4.385294,-24.657557,10
504,"in the class first we started by taking some doubts in which we learned about the standard deviation of mean of samples which is equal to sigma divided by root x and its square is the variance of mean whereas sigma square is the variance of sample. can we learn about different methods like sturges theorem to calculate the number of bins in the histogram.
then we started with logistic regression . there are two different classifications then it is easy to classify them when the clusters are far away and when the clusters are not far away then we use probability like if probability is greater than 0.5 then it is x1 otherwise it is x2. our goal is to minimise the error between the predicted value and the actual value of all the samples. then we learnt thatp(y/x) = sigma(wtx+b). we want to maximise the likelihood of predicted outcomes being closed to targets if we get 100 percent correct classification of training data then there can be higher error in the test data we need to generalise the boundary of classification. then we learnt about confusion matrix and how to calculate it then we learn some definitions like accuracy which is how often is the classifier correct precision of the events you have detected like how many have you detected correctly recall is for the class events like how many of a class 1 events have you correctly detected. and f1 value is just the harmonic mean of precision and recall so that we get a performance mean of the precision and recall. then then our assignments were being assessed and the reviews were given about them in the later part of the class. ",0.05179896,-0.049249068,-0.06561784,0.038945656,0.032366257,-0.02096157,0.025389966,0.08895359,-0.009666234,0.061158333,-0.013914648,-0.007879725,0.04437501,-0.022427373,-0.041009832,-0.08015995,-0.0035010935,0.046445515,-0.11280908,-0.027473645,0.04816731,0.003947689,0.06607149,0.06913777,-0.038445584,-0.012049053,0.020469986,0.0066639567,-0.03244942,-0.057463843,0.0025307906,0.0047528236,0.054630637,-0.0068953927,-0.09204464,-0.0016179389,0.060000744,0.044315476,0.056287926,0.041520234,-0.028320285,-0.025889352,-0.0011158459,0.02770454,0.049920786,0.03189728,-0.037134267,-0.005003816,-0.02768175,0.03467021,-0.059600923,0.05607804,-0.037999872,-0.06308515,-0.10179253,-0.024679618,0.046622813,-0.015760846,0.045048088,-0.00087267166,-0.014638778,-0.115077704,0.014048043,-0.056831088,-0.045773637,-0.05571333,0.019384602,0.05076953,0.10067615,-0.022110956,-0.0355679,0.045620367,-0.04599882,0.04155879,0.040422186,-0.018720496,0.05220676,0.07978057,-0.021732317,0.07760334,-0.005505686,0.05722216,0.03534553,0.0052499734,0.08389808,0.014372221,0.011092478,0.0957914,-0.024577264,-0.0715899,-0.01055488,-0.010017612,-0.059494063,0.003922596,0.03684082,0.025125565,-0.024182511,-0.039661292,0.12983805,0.033967614,-0.009655356,0.015283978,0.000111049616,-0.052640773,0.019404037,-0.01694516,0.042677555,-0.028349442,0.084960595,-0.07945705,-0.0029232954,0.025822856,-0.14966951,-0.016051244,0.015038309,0.02322295,0.0060658883,0.027712572,-0.104422286,0.06593943,-0.013392901,0.024690036,0.059366576,-0.003271917,0.08501785,-0.04829271,-0.10130019,6.055184e-33,0.011143436,-0.017336223,-0.048223127,0.060098086,-0.026025126,0.021949243,-0.035510812,0.023249364,0.0893007,-0.017109783,0.03798185,-0.021977743,0.07353472,0.0359343,0.08321589,0.03600937,-0.08120624,0.008009346,-0.07597832,-0.05747509,0.0066863326,-0.019932011,0.10982225,-0.12258437,-0.0014386775,0.049227897,0.028100314,0.005189852,-0.0045820656,0.008672984,0.03215005,0.0025787898,-0.01562457,0.008822897,0.054267846,0.0625018,0.006943957,0.0086349305,0.002433,-0.05056142,-0.07059643,0.013900922,0.04809352,-0.012006864,-0.019685632,-0.025207434,0.019843662,-0.07048067,-0.016369795,-0.07088153,-0.06497594,-0.096850276,-0.027292233,0.030118223,0.01427391,0.096762575,0.04159896,-0.04820511,-0.039028525,0.038727038,-0.013451194,-0.030960811,-0.0044941767,0.042197693,-0.062442146,0.045535218,0.023944624,0.028680505,0.0474002,-0.046730578,0.01066859,-0.01617354,0.007456872,0.010787666,-0.026060507,0.06635426,0.04002464,0.04109607,-0.038827807,-0.008190983,0.04658005,-0.032832503,-0.10343639,-0.08212029,-0.084500045,0.033518396,0.02389957,-0.011814451,-0.052003536,0.017330812,-0.101241946,0.025738062,-0.019151697,-0.014948506,-0.012620413,-8.680208e-33,-0.066109225,0.12911548,-0.026886543,0.0397903,-0.02672778,-0.018557644,-0.029231703,-0.01766595,-0.059148274,-0.07281514,-0.055035967,0.010661039,0.042727903,0.07884485,-0.081023864,0.03914646,0.012192313,0.021613177,-0.036373183,-0.042301256,0.04837542,0.050654538,-0.09407542,-0.08869967,-0.02694555,0.0024780578,-0.03989254,0.04370421,-0.00839716,-0.062928125,-0.01659767,-0.04016302,-0.045040395,-0.04134574,0.051499236,-0.0015179056,-0.0079911165,-0.042350024,0.014391722,0.024627578,-0.012708539,0.029736547,-0.03250151,-0.1117438,0.013934632,-0.062179737,0.06005284,0.06712184,0.057119947,0.005283554,0.06514538,0.005500679,-0.054685634,0.05538552,0.039683547,0.017918248,-0.036283195,-0.048938364,-0.06536928,0.05648699,-0.040017933,0.016476342,-0.02093767,0.1173189,-0.13046883,-0.033750504,-0.10435002,0.042517923,0.03958219,0.019178607,-0.119690016,0.068321675,0.04904205,0.01749305,-0.027347181,-0.031121995,-0.07639147,-0.018310253,-0.02053208,0.03138297,-0.0027975535,-0.07607845,-0.031006368,0.06305298,0.027962413,0.04786216,0.11347292,0.00917981,0.044221036,-0.08888168,-0.0518157,0.01633017,0.023092717,-0.004799979,-0.08588958,-7.42737e-08,-0.06407466,-0.024369918,0.12201145,-0.011464004,0.046999346,0.061075103,-0.119614154,0.050883517,-0.06411312,0.026495352,0.00056070514,0.03187927,-0.11722269,-0.057547368,0.021093966,0.094939634,0.033950243,0.095794484,0.028558044,0.06893514,0.039727993,-0.14383918,0.033384427,-0.0017700414,0.052049145,-0.07219835,0.01083471,0.0882905,-0.02924025,0.010507371,-0.022844331,0.008894985,-0.00026947734,0.017195001,0.034253184,0.02298603,0.013583121,-0.033958063,-0.012964389,0.03828051,-0.0017885193,-0.017747454,0.008043381,0.02023011,0.009864526,0.038089383,0.008527355,-0.006565394,-0.019688979,-0.0041620075,-0.039832592,0.018229062,-0.058206484,0.0345614,0.021637356,0.056780763,-0.04545447,-0.10026291,-0.05197888,0.060874972,-0.014182986,0.10907772,-0.049187616,-0.01718049,13,-0.16198325,-28.943089,10
516,"we began the class with logistic regression and calculating weights of parameters in order to maximize likelihood of predicted outcomes being equal to targets. if t=1 we maximize p and if t=0 we maximize 1-p. maximizing likelihood simply means minimizing error. since there are product terms which are difficult to work with we take logarithm to get sum. a confusion matrix is created- true negative, true positive, false negative, false positive with the accuracy found using the formula- (true positive+ true negative)/total no. of events, precision defined as true positive/(true positive+true negative), recall as true positive + true negative /total and f1 value as harmonic mean of precision and accuracy",-0.02759014,-0.032330327,-0.058959223,-0.06874348,-0.008719364,-0.07972374,0.08310004,0.057683866,-0.021196807,0.047337968,0.0587278,-0.015922865,0.06734052,0.024783097,-0.08169711,0.0015710708,0.054579806,0.095927015,-0.085757546,0.02322605,0.0067806193,0.018930022,0.018868178,0.040656876,-0.01609891,-0.12403415,-0.0017751239,0.0030029374,0.020180458,-0.0829387,-0.03977047,0.047594838,0.044448607,0.018861363,-0.059752613,-0.020069443,-0.06318608,-0.024317238,0.036302295,0.05341014,-0.02589231,-0.063767105,-0.021296673,-0.022468884,-0.02614545,0.043161433,-0.070654325,0.039485358,0.03675722,0.0040938957,-0.095643215,0.10154487,-0.038630333,-0.06484339,-0.08814573,-0.06986234,-0.0045453114,-0.017234376,-0.01329136,-0.0050341776,-0.04656802,-0.03444544,-0.050979115,-0.018208051,0.0779008,0.008939042,-0.10044263,-0.018643502,0.012548542,0.11930489,-0.0007439205,-0.028835393,-0.055305883,0.0051068305,-0.019259922,0.0661733,0.051504645,0.057791892,0.0014538955,0.12302373,0.022396136,0.011518968,-0.013384804,-0.025660787,0.1318993,-0.069134936,-0.0061331815,0.10800865,0.10676787,-0.002075773,-0.06638074,-0.049511492,0.026146563,0.011524076,-0.023055699,0.021802904,-0.036965262,-0.035572328,0.006759877,0.050746668,-0.06257757,0.0069980905,0.004896074,-0.07029694,-0.03711326,-0.08329573,0.0028709562,0.034679756,0.0653523,-0.074292354,0.006331676,-0.0056035775,-0.09303024,-0.010253994,0.020878542,0.039279245,0.045626502,0.11790518,-0.04379196,-0.0076128715,0.00831556,0.03137028,0.049835965,0.021739557,-0.0154337725,-0.040230453,-0.127945,4.2072287e-33,-0.0584954,0.052873176,0.034686565,-0.0014740031,-0.0483678,0.047819164,-0.1004934,0.006958716,0.043759804,0.024564102,-0.016230302,0.012348742,-0.009729926,0.06831182,0.032552097,-0.00094532454,-0.092612684,0.089552194,-0.06583924,-0.082512125,0.021948412,-0.012975679,0.062931225,-0.11170612,0.03552579,0.028403994,0.0414673,0.02140477,-0.030226894,-0.005301448,0.053258404,-0.021997746,0.028231898,-0.046882927,0.08214061,0.054139014,-0.07265972,0.015659682,0.042447742,0.013119078,-0.1115844,0.033138506,0.010220164,-0.09148028,-0.06890178,-0.03346073,0.014581248,-0.0031103103,-0.02721997,-0.015255059,-0.070862524,-0.0728216,-0.035194118,0.037916306,-0.050547443,-0.010567112,0.030228432,0.055778664,-0.003676836,0.0032980165,-0.0006142682,-0.0042541083,0.017216176,-0.06259481,-0.062805995,0.06929136,-0.008795064,-0.0065304013,0.1308747,0.004573799,0.03405044,0.032031123,0.09068942,-0.015891673,0.0063236225,0.037123427,0.052707735,0.033970594,0.015843123,0.02383979,0.060386565,0.04986035,-0.017347375,-0.03178615,-0.09289529,0.020653393,-0.035434183,0.008730322,-0.12654832,0.054135412,-0.023417259,0.026991248,-0.020337932,0.06778604,-0.039588585,-6.358929e-33,-0.10939814,0.04960257,0.021494184,-0.009162094,0.026059842,-0.014629759,-0.022884417,-0.030278731,-0.031712607,0.019680446,0.03415524,-0.025433753,-0.053058546,0.015506043,-0.022426825,-0.016431691,-0.029509598,0.048660036,0.0010593181,-0.027892075,0.0741714,0.10602557,-0.09981442,0.013171527,0.0074539394,0.06234394,0.007897954,0.046311148,-0.0044728653,-0.036104094,-0.06771876,-0.00268138,-0.078145064,0.029453574,-0.0063049784,-0.005908638,0.06661844,-0.030531913,0.042239726,0.036350157,0.010716449,0.019046566,0.018425193,-0.05025519,0.0051747095,-0.077852346,0.05712825,5.564352e-05,0.07632383,-0.036939304,-0.009928311,-0.017875379,-0.06699409,0.10834004,-0.017212117,0.046490654,-0.054566473,-0.057793334,-0.04293595,0.03203132,-0.113009624,0.036966726,-0.06249761,0.05335003,-0.067604065,-0.008361052,-0.044257708,0.014448976,-0.023639016,-0.009139988,0.004395299,0.10655186,0.098834485,-0.010038384,-0.029519439,-0.051959697,-0.06626262,0.027906138,0.0061467453,-0.012594909,-0.027892234,-0.07131797,0.00859681,0.08053737,0.007625501,0.018627496,0.051781334,0.05985867,0.0017968733,0.0010867069,-0.03859642,0.058280036,0.07418277,0.0037905062,-0.04773167,-5.5490407e-08,-0.0028204196,-0.013609023,-0.0024871877,-0.0070364717,0.04912114,0.0071359356,-0.058446415,0.008516135,-0.04497891,-0.016900511,0.0017642562,0.034458913,-0.10741269,-0.047063388,-0.028287599,0.030444209,-0.07590087,0.12436922,0.029129207,-0.013570083,0.11320593,-0.058315575,-0.005032243,0.0027557637,0.045670167,-0.010720856,-0.036931593,0.16031602,-0.015508086,0.04023621,0.004566415,0.024425304,0.008277017,0.024504816,0.0068159383,0.07744715,-0.013849208,-0.01667914,-0.06996372,-0.012438137,-0.04625268,0.038871657,-0.04913143,0.017733332,0.06339757,0.0724454,-0.054647624,-0.037737824,-0.04282655,-0.0022590612,0.041933738,0.07179051,-0.038733218,0.049141508,0.053316046,0.028778836,-0.0008120891,-0.09497282,-0.055164646,0.041708536,0.047995813,0.052359827,0.002596106,0.009746969,13,2.1450188,-25.521845,10
561,"we define the boundary for classification as a linear combination of features. this is then passed through the sigmoid function so that we can treat the output as a probability, specifically the probability that the label is 1 given the features. the prediction output is done by rounding this probability to the nearest integer (0 or 1).
the four possible cases of predicted and actual labels (both can be 0 or 1) are represented in a matrix form called the confusion matrix. many metrics can be derived here, for e.g. precision, recall, accuracy. f1 score is the harmonic mean of precision and recall and it is a better metric than accuracy in the case where the data is imbalanced. when there are more sample from a particular class in the training dataset, then the classifier can achieve a high accuracy by trivially detecting every input as belonging to the dominant class. f1 score counters this by providing a value based on the precision and recall which themselves focus on number of correct predictions from those predicted and for each class how many were identified correctly respectively.",-0.031092003,0.012612792,-0.080463886,-0.04875664,0.052356876,0.027909366,0.015499859,0.087735854,0.0115376795,-0.040472534,-0.079428725,-0.057720426,0.05721554,-0.01501998,-0.069523744,-0.04914345,0.029345252,0.04882861,-0.0110035995,0.0002783021,0.027751423,0.039566513,0.07239616,0.05994955,-0.0365765,-0.031909868,-0.033593606,0.013565341,-0.027915495,-0.05458907,0.011310422,0.08323861,0.047368925,0.012724684,-0.13677864,0.005175904,-0.038898773,0.0103339935,0.04499741,0.0265915,-0.0044700815,-0.021115111,0.039032374,0.081614,0.024957454,0.06342311,-0.025485847,-0.011133737,-0.030022264,0.08293839,-0.045289245,0.056615863,-0.04356357,0.04718943,-0.13345104,-0.012958536,0.066237465,0.004492635,-0.046507724,0.050928067,-0.014627209,-0.14326426,-0.010622965,-0.013542016,0.028835125,-0.050610885,-0.012877696,-0.033815064,-0.015085104,0.024729457,0.057171665,0.07904542,-0.033735555,0.036321416,0.017706847,0.03884787,0.0031352062,0.052692395,0.008861362,0.11738388,-0.024025273,0.0527699,-0.030834662,-0.061924838,0.15753815,-0.04176741,0.027184667,0.018362656,-0.030778825,0.048556853,-0.046193406,-0.06469954,0.024478758,-0.056307144,-0.054477498,0.0720594,-0.00039618378,-0.048279315,0.020517852,0.042597353,-0.052379772,0.034792162,-0.01430738,-0.043158397,0.08706421,-0.010417652,0.031537272,0.008584486,0.11624295,-0.16845012,0.017868476,-0.06680829,-0.041912314,0.014583516,-0.0065166014,0.026313026,0.06663559,0.046608984,-0.024063816,0.10304695,-0.114473835,0.00031941096,0.010365226,0.023740469,0.051194966,0.03810437,-0.084225684,3.89489e-33,-0.02432836,-0.029792547,0.0029824106,-0.016130988,-0.009293701,-0.057254355,-0.079353474,-0.026336607,0.047978636,0.040696662,0.017662436,0.04233282,0.042477578,0.06234471,0.03787832,0.010697736,-0.073830076,0.0042774207,-0.032055844,-0.0803359,0.0055280174,0.005731104,0.11390058,-0.108725905,-0.015559854,0.029197529,-0.05669221,0.068970226,-0.022802465,-0.00025977785,-0.04010864,-0.0314541,0.04256475,0.009777368,0.025019834,0.005297315,-0.021817738,0.0009579827,0.042123273,0.016789427,-0.068537414,-0.016637847,-0.006255799,-0.04739957,-0.006797151,-0.03155487,-0.0018594173,-0.0055207903,-0.030807074,0.039485287,-0.012996329,-0.08012191,-0.036564134,0.04877248,-0.015617449,0.055997357,-0.012369717,-0.003355901,-0.063597426,0.023277652,0.0052638585,0.0077795233,-0.0025743018,0.030499954,-0.044842422,0.039632883,0.038141556,0.07031283,0.0377396,0.025310518,0.02379585,-0.006635257,-0.0750596,0.0028400556,0.0071450565,0.06451,0.043687694,-0.060752396,0.003080097,0.02720491,0.011643926,0.018636325,-0.036557894,-0.034526475,-0.112818286,0.03365113,0.0637696,-0.02345903,-0.03228053,0.050204083,-0.11696945,-0.023352934,0.038267918,0.04021213,-0.07724903,-4.5455658e-33,-0.097321056,0.01775083,-0.05257424,0.03223339,-0.026548978,-0.0077981194,-0.025388723,0.005103492,-0.09210704,0.00079571275,0.0068374486,0.02749501,-0.032210734,-0.015141707,-0.063318886,-0.01285952,-0.03391331,0.024673915,-0.0354328,0.027985968,0.07394484,0.07008197,-0.056468017,0.084804654,-0.06531169,0.035109762,-0.03391756,0.010534778,0.005556115,-0.03763514,-0.018709855,-0.092020914,0.028482823,-0.017521895,0.05859635,-0.039304767,0.09815692,-0.051889185,0.014756016,0.13719518,0.015298135,0.058141377,-0.08438053,0.026119985,-0.034420326,-0.05038473,0.004338311,0.024756117,0.0014593792,0.010958326,-0.0019196381,0.055549193,-0.092144236,0.09891621,-0.010177492,-0.019155977,-0.033580612,0.0090454575,-0.053573813,0.15109581,-0.060244028,0.059451703,-0.01815332,0.020041138,0.044005785,-0.03955094,-0.043159004,0.047229815,-0.007449482,0.14387141,0.02039275,0.054858055,0.017876433,-0.022040484,-0.035628438,-0.0019149359,0.0064924466,-0.0077769873,-0.029442813,-0.00868052,-0.0035832205,-0.056346383,0.015818832,0.107541375,-0.0026705754,0.03872522,0.098652296,-0.029326696,0.04599679,-0.05476075,0.029110644,0.072753266,-0.029685058,0.040452622,-0.11479846,-6.144298e-08,-0.02459588,-0.014668977,0.006991389,0.0005592207,0.008478797,0.028348072,-0.044340283,0.036021378,-0.04919467,0.016005661,0.04261546,0.02108135,-0.1170606,-0.059330422,-0.0005762306,0.03543873,0.014565171,0.13665241,0.022056952,0.03672371,0.03637814,-0.07507608,0.026026249,-0.06332312,0.025339283,-0.09202366,-0.0153135,0.12644123,0.04426384,0.078907005,-0.030810684,0.00038698377,0.012066368,-0.011520758,0.043640096,0.096038386,0.031431027,-0.066319294,-0.022767551,0.031021588,-0.023643924,0.053558994,-0.023976091,-0.017416893,0.02109964,0.022686671,0.032751136,-0.0032561836,0.0081910305,-0.02930563,-0.046266437,0.021701258,-0.04010629,0.09099365,0.053338423,-0.020468777,-0.06991896,-0.0951581,-0.04994479,0.00852651,0.039012253,0.06528526,-0.0020199942,-0.023436893,13,5.4148183,-28.753109,10
564,"in this session, we began by addressing doubts from the previous class, where expectation algebra was briefly introduced. we then derived the relationship between the variance of sample means and the variance of the population. moving forward, we covered the basics of logistic regression, discussing its mathematical formulation and applying gradient descent to minimize the defined function. it explained the concept of confusion matrix, elaborating that the explanation of misclassifications is an important part of understanding false negatives, considered to be worse, such as being declared ""ok"" when not ok. additional key evaluation metrics discussed include precision and how many of the detected events were correctly identified. the recall measures how instances of a particular class are successful in being detected. then, the f1 score represents the harmonic mean of precision and recall. we emphasized how the f1 score is more reliable than simple accuracy, as the later sometimes misleads when implemented as a selection criterion.",-0.0029199906,-0.013358964,-0.072777905,0.0124400165,0.03893538,0.013136135,0.029458804,0.05391824,-0.032464046,0.017454645,0.027022053,0.04169347,0.026847253,-0.00027470553,-0.078966826,-0.052997887,-0.0010262065,0.056735672,-0.09490507,0.009361467,-0.0056308582,0.042508513,0.080831006,0.06886972,-0.07203488,-0.10214627,-0.07486851,0.017184254,-0.062452685,-0.022410436,-0.0029020465,0.054557666,0.052594043,0.01921073,-0.091729715,-0.026033852,0.016019115,0.041609112,0.036394928,0.011581405,-0.061089415,-0.06866062,0.015887342,-0.002180706,0.035614226,-0.02489296,-0.041684963,-0.009133854,-0.05323356,0.027122693,-0.06682947,0.07325025,-0.056673404,-0.064192474,-0.098865524,0.0026473668,0.026301352,-0.03342156,-0.039450318,-0.0070466786,-0.025301931,-0.10686348,-0.027542252,-0.025862847,-0.02509925,-0.033433486,-0.042596012,-0.07927532,0.04970618,0.0484177,-0.024196878,-0.0003407099,-0.031270437,0.014587588,0.03444447,0.045146246,0.032182943,0.06684473,0.00048545172,0.08319193,0.014159461,0.049714535,-0.0006207063,0.013524001,0.14055535,-0.053456713,0.026487516,0.013637887,-0.03264784,0.023835571,-0.016688133,-0.084647246,0.035546087,0.015482437,0.017477129,0.01848917,0.007422287,-0.05147232,0.02626237,0.055076703,-0.04340111,0.109585784,-0.0048158793,-0.04725494,-0.0020307603,-0.09636744,0.03709468,0.002417374,0.021228911,-0.07041946,-0.027281571,0.02983468,-0.08988165,0.03692663,-0.017155936,0.038343348,0.041415643,0.09628617,-0.040298298,0.13934676,-0.0048850286,0.011776422,0.07603539,0.02479235,0.051406767,-0.07404911,-0.0703592,3.612028e-33,-0.01867362,0.036837824,-0.00076728984,0.0002528769,-0.03073432,0.01854953,-0.1134446,-0.029853407,0.04310463,0.04477838,0.0069950223,-0.0031469879,0.034508627,0.13406171,0.048853695,0.06468684,-0.14469823,0.067188844,-0.035141513,-0.03166343,0.006918909,-0.0056675044,0.05910476,-0.11353318,0.018197775,0.050296757,-0.010328892,0.046022836,-0.0032020088,0.016208624,0.005052763,-0.0016683342,-0.0005328698,-0.012763393,0.054506853,0.07369895,0.0112068225,-0.013784839,0.050012503,-0.039055742,-0.07227683,0.034450848,-0.002716929,-0.032234207,-0.04994735,-0.024373772,0.059652414,-0.035895616,-0.0113103315,0.01568152,-0.05263078,-0.03795643,0.01316448,0.039092556,-0.03441645,0.076269455,0.046754185,0.04618244,-0.07264716,-0.008081287,0.0024955536,0.03125102,-0.0029866612,-0.0014971696,-0.083158374,0.015749747,0.01633517,0.015841411,0.04235362,-0.0072225486,0.010097509,-0.027687915,-0.037024066,-0.01877437,-0.021364942,0.080062695,0.0924285,0.001483017,0.04969677,-0.073687576,0.050206255,-0.04099075,-0.078060344,-0.0703476,-0.096669696,0.05221441,0.029251715,-0.012224706,-0.044454083,0.033137687,-0.03921997,0.033999562,0.001786913,0.056147173,-0.04309146,-6.1708796e-33,-0.09565786,0.03787249,-0.02043308,0.056327343,-0.024776258,-0.028190799,0.02286318,-0.056654077,-0.033629388,-0.03157656,0.0003345316,0.0115723545,-0.00092515623,0.043037236,-0.058275286,0.023149159,-0.041801885,-0.0010845141,-0.01826759,-0.012541433,0.09725945,0.06469027,-0.044836525,-0.050373524,-0.054943204,0.045981638,0.0203796,-0.004584545,-0.06083913,-0.15253541,-0.006709382,0.0038218054,-0.025941027,0.047086574,0.0829246,0.019791229,0.0855446,-0.018191705,-0.029420638,0.0903054,-0.011155686,0.102524824,-0.052104615,-0.046644732,-0.007435031,-0.023058634,0.06672684,0.07350491,0.07451241,-0.0022062312,-0.052388735,-0.017029114,-0.08185583,0.08126781,-0.03868106,0.025438791,0.017060783,-0.08368642,-0.14247967,0.11447783,-0.05720313,0.06714735,-0.045473754,0.07879477,-0.06887463,-0.08208599,0.00057954143,0.0010693196,-0.012065748,0.012201462,0.0071024606,-0.03488989,0.017040854,-0.033618554,-0.009529185,-0.03616144,-0.058786917,0.03611804,-0.032515887,-0.046608478,0.028780328,-0.067494534,-0.007671604,0.062249027,-0.015620613,0.01815574,0.0745582,-0.0037496637,0.018569319,-0.04691708,-0.026156452,0.07803296,0.016167883,-0.038757946,-0.087922595,-6.149887e-08,0.0057105804,0.04725055,0.04976071,0.018565016,0.027036425,-0.023175217,-0.090104826,0.016039388,-0.09904159,-0.019681077,0.047123045,0.037403148,-0.12480321,-0.07355251,0.028193075,0.018951677,0.04571095,0.14426376,0.01117059,0.063201435,0.062009204,-0.05068455,0.016513275,-0.049244992,0.04021184,-0.039802518,0.0219669,0.13225336,0.012847987,-0.0049532335,0.0091922125,0.051724475,0.019330421,-0.030537013,-0.0024254317,0.045640245,0.00053644227,-0.06679714,-0.0063664736,0.011192678,-0.009257005,-0.023540461,-0.0025454205,0.015594152,-0.012006573,0.022756537,-0.019791253,0.008659292,-0.04374302,-0.021753363,0.025183987,0.017212303,-0.039070066,0.08010238,0.09284074,0.08836749,0.04840871,-0.086199544,-0.059055325,0.00059689773,0.036947966,0.10899623,-0.048053812,-0.017094148,13,3.9577188,-26.853996,10
566,"today's session focused on logistic regression, a fundamental algorithm for classification problems. it predicts probabilities using the sigmoid function, which maps inputs to a range between 0 and 1. a decision boundary, typically set at 0.5, determines class labels.

we also explored clustering as a technique for organizing data before classification, improving model performance. the importance of true positive and true negative rates in evaluating classification accuracy was discussed, along with outlier detection techniques to handle irregular data points. additionally, we examined loss functions, which guide model optimization by minimizing errors. the session concluded with practical approaches to implementing logistic regression and improving classification performance.",-0.005243823,-0.018566119,0.00416965,0.010861326,0.053421512,-0.041767534,0.06847385,0.017166276,-0.054847408,0.0065724193,0.0030660115,-0.015729694,0.07985708,0.0009596378,-0.052510753,-0.07393796,0.035441205,0.035159625,-0.054840002,-0.011384644,-0.05280984,0.07331564,0.02331357,0.031026214,0.030332377,-0.050220482,0.00731948,0.023075506,-0.061646856,-0.054525364,-0.07090575,0.03907983,0.048670325,-0.0033771174,-0.06071027,-0.027047642,-0.02397413,0.035475537,-0.017324572,0.012840081,-0.02608646,-0.051747326,0.03135473,0.004311684,-0.028922511,0.021866057,-0.02723628,-0.01607162,0.014443536,0.026959987,-0.024926294,0.02471766,0.013916099,-0.0046759374,-0.18592513,0.03314964,0.028113468,-0.02504429,-0.0151093,0.02198646,0.057516545,-0.042043194,0.011939247,-0.015506049,-0.021057047,-0.0011020611,0.00055231916,0.023967689,0.0722214,0.04578948,0.048602276,0.04148207,-0.036955643,0.054418866,0.0009580094,-0.003283725,0.068530306,0.049540784,0.026088342,0.01018815,-0.0019235061,0.07518922,-0.013812004,0.032288574,0.15876389,-0.020140592,0.030965151,0.049226318,-0.0025985169,0.018153409,-0.06390408,0.022734214,0.043319214,-0.036588438,-0.048149154,-0.0051291743,-0.03976895,-0.033398163,0.04735878,0.03554678,-0.09577333,0.049475364,0.0022154336,-0.049970098,0.07705947,-0.04635305,0.053414006,-0.026376152,0.07855643,-0.07103077,0.0012268841,0.03296855,-0.0646137,-0.052972373,0.0128166275,0.036346614,0.10401907,0.027788976,-0.12843463,0.0966712,-0.04693605,0.053119127,0.018688392,-0.0104290815,0.0509947,0.0010662214,-0.10230894,2.7844124e-33,-0.00974348,-0.008425377,0.0027376495,-0.11285226,-0.002571669,0.008535745,-0.103463136,0.02975577,0.022288892,0.0153470505,-0.031288307,-0.037046272,0.045665905,0.07499838,0.11526634,-0.022107378,-0.02085281,0.09916999,-0.015646866,-0.08233461,-0.06481408,-0.102065876,0.051891025,-0.12390275,-0.020553213,0.124806814,0.028334852,0.0041390792,0.018618565,0.001311709,0.039171427,0.0026127095,0.01660975,-0.07036363,0.10389731,0.059379544,-0.0404071,0.05165456,0.047475994,0.018689357,-0.17793421,0.015222693,-0.007540514,-0.04070071,-0.006970172,0.019225236,0.060278174,-0.05391842,0.0072943224,-0.08846694,-0.032095566,-0.09893432,0.012288629,0.078635484,-0.07283409,0.021850474,0.060117446,-0.002949352,-0.06565845,0.0060962415,0.023926243,-0.03144852,-0.0052942373,-0.034740202,-0.032196004,-0.031503502,0.051205162,0.03955516,0.05323707,0.023632683,0.05731849,0.011233563,0.023048481,0.0033521967,-0.0048330924,0.10919834,0.06292798,-0.008512907,0.009438708,-0.054424323,-0.017588174,-0.036389507,-0.045369286,-0.06552419,-0.03148119,0.028174898,0.06401466,0.022225212,-0.069271155,0.035369832,-0.14385904,0.04987054,-0.036914036,0.03646265,-0.022686752,-3.6930407e-33,-0.07622291,0.05659735,-0.0462581,-0.02571235,-0.028669316,-0.011129145,-0.06675763,0.03338925,-0.014871073,0.030652262,0.008473921,0.004591233,0.02569918,-0.023531554,-0.048690744,0.023701947,-0.05873008,0.035920955,0.0018580259,0.00920928,0.049578033,0.04667661,-0.10406914,-0.017336307,-0.06764042,0.024679434,-0.051028185,0.04586669,-0.03343574,-0.04936102,-0.08205884,0.023173735,-0.032401584,-0.033371717,0.087945566,0.0156752,-0.02223766,-0.043378405,0.004827759,-0.0077406787,-0.036826216,0.077299096,-0.061908912,-0.0061848573,0.018703816,-0.104102485,0.024285305,0.017586289,-0.013540017,0.04980383,-0.07000085,-0.045914482,0.009126459,0.07098488,-0.017231772,0.08118823,-0.04515114,-0.042333294,-0.07416873,0.105572335,-0.09661437,0.03850022,0.09361223,0.042928454,-0.049695008,-0.07914957,0.00425947,0.05827863,-0.011215286,0.02210263,0.017160218,0.06604112,0.028426114,0.046929795,-0.082235284,-0.06565543,-0.031000517,-0.012471877,0.0064675086,0.030568294,0.009530078,-0.05399392,-0.013814373,0.10861587,0.057102367,0.0329707,0.0849056,0.046941184,0.105987504,-0.044722456,-0.045252997,0.059292696,0.017374039,0.030403877,-0.08434001,-4.522757e-08,-0.04577528,-0.00523758,0.028597565,0.042813018,0.005933994,0.04259682,-0.111440316,0.12064599,-0.05944054,-0.017859021,0.03590649,0.0059524984,-0.069202505,-0.022569243,0.027292978,0.007821969,0.07236638,0.06608719,-0.0017778102,0.02983417,0.03401763,-0.027446881,0.030020736,-0.025436608,0.008950146,-0.089340396,0.02078597,0.09724704,-0.01968371,-0.015258033,-0.014859368,0.046390235,-0.020786969,0.05279455,0.045648932,0.1251178,0.007503751,-0.04417553,-0.08944711,0.009773659,-0.0050654397,0.0004889004,-0.015734822,-0.009436173,-0.022979368,0.036740694,0.04664832,-0.031379525,0.043924283,-0.042307496,0.005811263,0.021639798,-0.034221295,0.06926291,0.055546958,0.014244132,-0.04655459,-0.07235935,-0.015151607,0.016393175,-0.0034869344,0.005844274,-0.02569199,-0.0032521065,13,2.2207215,-29.370022,10
583,"discussion upon logistic regression was taken further ,logistic regression is used to predict binary outcomes (0 or 1). it operates by taking input values, applying weights, adding a bias, and passing the result through a sigmoid function to obtain a probability. training involves adjusting the weights using gradient descent to minimize errors. the likelihood function aids in optimizing weights by maximizing the number of correct predictions. the modelâ€™s performance is assessed using a confusion matrix, accuracy, precision, and recall. a predicted probability above 0.5 results in an output of 1; otherwise, it is 0.

",0.0062665325,-0.0378189,-0.091414355,0.016523616,0.003202136,-0.06436068,0.039120235,0.012063345,-0.035483222,0.042854663,0.011719363,-0.02682197,0.04231187,-0.014763114,-0.03473716,0.008334615,0.0071431636,0.042623762,-0.048586957,-0.008636715,-0.002316627,0.027884496,0.032162506,0.018608721,-0.0033714261,-0.141685,-0.029277021,0.045734722,-0.055280972,-0.041670717,0.005698365,-0.012064114,0.07528798,0.016449647,-0.13129126,-0.029517353,-0.066888936,-0.04951015,0.0014232139,0.012747157,-0.055470638,-0.0706131,-0.008146048,-0.0119074,0.046953164,0.051790778,-0.028455885,0.04582834,-0.01765234,0.03567822,-0.06869245,0.014734055,0.063102365,-0.04050989,-0.09187172,-0.040349882,0.013073792,7.039718e-05,0.025097491,-0.032839708,-0.03094542,-0.06077971,-0.03999102,0.013719481,0.015465943,-0.0916534,-0.089491576,-0.048536725,0.050669316,-0.046489254,0.012932686,-0.040416613,-0.040291153,0.010285782,0.008649145,0.012612957,0.075588085,0.044181127,0.015389865,0.11044964,-0.023538543,0.039091267,0.05648587,-0.015216675,0.09365999,-0.02191323,0.05739726,0.07340503,0.02389605,0.04184239,-0.03736912,-0.042424098,0.0054633915,-0.011509768,-0.050926223,0.008985219,-0.010366582,-0.0671938,-0.032562666,0.04279228,-0.054560095,0.06626837,0.033401426,-0.0053691105,0.020298705,-0.05281047,0.060872875,0.04845365,0.024671474,-0.084163696,0.014871389,0.06850537,-0.05846498,-0.03692265,0.010861554,0.013656928,-0.009041355,0.054158036,-0.11248358,0.13183801,-0.047479827,0.033912633,0.0038393247,-0.047020312,-0.01880366,-0.08091167,-0.062088456,1.580056e-33,0.002028869,0.017503139,-0.0293272,-0.044845987,-0.028874332,0.0031265793,-0.10859446,0.0545021,0.033370867,-0.030048672,-0.029003033,-0.016855441,0.026593165,0.16867271,0.028990833,0.018713716,-0.060548846,0.092458524,0.028146204,-0.001885116,-0.050631084,-0.038722713,0.027108666,-0.09343965,0.02858601,0.11460138,0.056504652,0.04290719,-0.011941998,0.005046808,0.020880725,0.025441004,-0.04161181,-0.07848689,0.04792518,0.063179485,-0.01223602,0.023895377,0.043672886,0.047447048,-0.08490981,0.010592039,0.06260163,-0.03230981,-0.070004225,-0.039488167,0.024552882,-0.025246155,-0.05248739,-0.031056445,0.028609032,-0.040661357,-0.078849986,0.061659094,-0.01658053,0.088375166,0.07936405,-0.023601461,-0.04712979,-0.008416738,0.043544836,-0.0024119837,0.00892901,-0.00013894489,-0.03372051,0.045665942,0.043315478,-0.0063972436,0.062969685,-0.029552123,-0.019035762,-0.0017589356,0.086162455,-0.018438922,-0.04997616,0.09500692,0.05490582,-0.07394853,0.07467611,0.0710698,0.040669683,0.04884964,-0.05040717,-0.055250194,0.007683346,0.081121005,-0.0073669027,-0.029462446,-0.049746837,-0.04727371,-0.080727644,0.003536076,-0.047358163,0.010821729,0.028987657,-3.1600256e-33,-0.07978481,0.044692047,-0.005817866,0.0077190287,-0.019072428,-0.033068817,-0.026061047,-0.0653539,-0.026807345,0.015525551,0.015100234,-0.0073758382,0.041668024,0.031651653,-0.012876079,0.021332042,-0.11043648,0.026518371,-0.0759633,0.016329635,0.077434786,0.05106658,-0.08031375,-0.03799257,-0.024083002,-0.03163635,0.011956944,0.051877666,0.029857572,-0.06106835,-0.027566843,-0.02829545,-0.033993945,-0.06368915,0.11353627,0.030521583,0.032932427,-0.009952114,0.021537915,0.007448841,-0.0078116264,0.03284969,-0.02144971,-0.03254677,-0.01920708,-0.055990707,0.047856912,0.006808606,0.09205817,0.00942496,-0.0150524415,0.026654007,-0.037113912,0.086441986,-0.0022347935,-0.029408593,-0.10255699,-0.03230258,-0.03582055,0.020991934,-0.07448723,0.072448045,0.05691393,0.08118474,-0.05346782,-0.011396343,-0.111721,0.02358395,-0.0029889394,0.034757357,0.039514475,0.041898996,0.11447709,0.030368911,-0.0380699,-0.07410517,-0.06192803,-0.013240596,-0.050226606,0.00896351,0.0140434,-0.04357481,-0.011017812,0.042111125,0.05379688,-0.009431224,0.0789222,0.0028537628,0.014767743,-0.035328645,-0.045749564,0.089261875,0.04581149,0.03813141,-0.16975352,-5.2610243e-08,0.0077382787,-0.0116790105,0.10666911,0.024632854,0.005116335,0.07683312,-0.015022872,0.0024302437,-0.05494522,-0.038126867,0.030023202,0.08441396,-0.1019403,-0.07356202,0.069734745,0.090868905,0.089308366,0.079566754,0.050799396,0.01861718,0.09734647,-0.05613589,-0.023685407,-0.033448324,0.04703002,-0.0680003,-0.032382958,0.13117932,-0.05709273,-0.02668642,0.010377377,0.040314525,0.036753178,0.01809559,0.026244968,0.07979176,0.06503116,-0.05134747,-0.017959611,-0.040213242,0.030544221,0.01283237,-0.022160899,0.0054047126,-0.030403227,0.035845987,0.0054841745,-0.018538542,0.0009255697,-0.013368311,-0.012083304,0.053499397,-0.02891613,0.043891806,0.07794922,0.07937161,0.02332923,-0.073424526,-0.031193754,0.09672555,-0.04268094,0.03829956,0.050208163,-0.03963183,13,-0.56006056,-25.522818,10
588,"we started with logistic regression and learnt how to calculate the weights. we want to maximize the likelihood of our predicted outcomes being close to the targets. if t=1 we would be maximizing p and if t=0 we would be maximizing (1-p).in order to find w there should be the goals across all the data(i.e. training data=n observations). maximizing likelihood is same as minimizing the error function.since there are product terms and it is difficult to work with those hence we take logarithm so that we get sums.
we create a confusion matrix:-true negative,false positive,false negative,true positive.
then we defined some terms:-
accuracy=(true positive+true negative)/total no. of events
precision:of the events we have detected how many have we detected correctly 
recall:of a specific class how many events have we identified correctly
f1 value-harmonic mean of precision and accuracy(doesn't give false hope unlike accuracy)",0.004086484,-0.039690632,-0.047693226,-0.041489445,0.028764505,-0.070118554,0.07592844,0.046018034,-0.0026522805,0.023808869,0.016247489,-0.027607339,0.06478675,0.04012439,-0.08156865,0.03203866,0.037999768,0.09964896,-0.11957175,0.0016158432,0.020954488,0.019409245,0.055625595,0.04921711,-0.047346685,-0.07706899,-0.015215042,-0.017652197,-0.008751673,-0.06250182,-0.0027222694,-0.030437058,0.037831645,0.007945607,-0.057774585,-0.01912363,-0.07517685,-0.048155848,0.038185846,0.0112400735,-0.03613612,-0.05014743,-0.020292925,3.5020214e-05,-0.0015934631,0.034165706,-0.037211142,0.049349062,0.018238576,0.047053758,-0.080552556,0.0717473,-0.012696317,-0.03630159,-0.113682695,-0.06351001,0.04549097,-0.026840921,1.5474836e-05,-0.0071131554,-0.04351915,-0.07867729,-0.053907488,-0.057711814,0.021958474,-0.016736386,-0.05649188,0.015023248,0.030865379,0.086513735,0.008406954,-0.016317027,-0.04010967,-0.034229223,-0.010247154,0.06811578,0.057133827,0.045821175,-0.021504471,0.12443319,0.035634305,0.050835293,0.032469966,0.0073006013,0.14852603,-0.06297035,-0.033586994,0.11552704,0.11501417,0.002526065,-0.10115928,-0.0793722,-0.014947242,0.008409968,0.035320774,0.04009401,-0.03571921,-0.03423301,0.01510424,0.045192152,-0.059160758,0.025691016,0.027327845,-0.065583564,-0.016082186,-0.043599207,-0.032704867,0.057096776,0.06258067,-0.098470405,0.025485314,-0.0012448693,-0.07400416,0.02330141,0.062510416,0.031796973,0.040583607,0.088162884,-0.09589924,0.058262985,-0.021545332,0.021022106,0.07664349,0.008611521,-0.004119632,-0.006282661,-0.099459976,4.777577e-33,-0.031427328,0.05688252,0.019607084,0.0017126373,-0.05670429,0.034723144,-0.08179511,0.022001795,0.016937826,0.034391463,-0.023739377,-0.0007819457,-0.006049399,0.08535478,0.05796613,-0.017190445,-0.06188743,0.07161482,-0.034963522,-0.0690305,-0.0066186516,-0.071599305,0.049918666,-0.11387156,0.043292265,-0.020591427,0.045382794,0.042067923,-0.027316963,-0.018788679,0.046903193,-0.03167037,0.024083147,-0.06654065,0.09797812,0.043380816,-0.062011,0.027492652,0.06263496,-0.0054153525,-0.07297243,0.04204699,0.0076072514,-0.052090444,-0.06233364,-0.04450712,0.0012385594,-0.030667895,-0.05070062,-0.012342465,-0.015719395,-0.07772143,-0.04986722,0.04379916,-0.04336243,0.019501414,0.014145212,0.005833739,0.036844246,0.00922679,0.004163262,-0.05011411,0.042630747,-0.002085995,-0.04687107,0.053136673,0.010214082,0.0031396386,0.08012272,0.011480931,0.019799175,-0.011422043,0.13711889,-0.012200305,0.032018468,0.040678576,0.08862293,0.017033223,0.028875202,0.020780843,0.065916605,0.1061592,0.009318856,-0.021941595,-0.07346125,-0.014336274,-0.065416455,-0.014966529,-0.14078438,0.035362024,-0.036319472,0.05036995,-0.04222919,0.04196218,-0.068638094,-5.491929e-33,-0.089904025,0.02594174,0.056401446,-0.034855098,0.017475065,-0.05011852,-0.037310757,-0.0780499,-0.013065147,-1.1723727e-05,0.039099254,-0.03340227,-0.028838452,0.049012102,-0.048323307,0.032524582,-0.017423332,0.07824049,0.039095853,-0.05328834,0.06666876,0.08134651,-0.13251516,0.015481837,0.0126750115,0.04661501,0.014806719,0.028405957,0.0043026544,-0.033634856,-0.07526032,-0.029169723,-0.055133227,0.024663297,0.038186505,0.027794562,0.04821583,-0.006962569,0.034234066,0.026317444,0.017567284,0.03728509,-0.032011487,-0.05660154,-0.017374853,-0.077979535,0.022536615,-0.008188527,0.07419723,-0.00014005888,0.0018841791,-0.01092444,-0.09051602,0.107131645,-0.009674563,0.044083618,-0.03524372,-0.075466886,-0.05393952,-0.008011884,-0.11180596,0.031045219,-0.04551221,0.08915105,-0.06092358,0.0036032114,-0.024013877,0.038355302,-0.0088589955,0.0016366768,-0.046279278,0.09702797,0.08820768,0.019681573,-0.0007223808,-0.06305097,-0.05442921,0.000877945,-0.023195961,-0.015277108,-0.02933233,-0.07630814,0.012778535,0.07557984,0.037657894,0.0590175,0.04742401,0.062138937,-0.004792009,-0.036406077,-0.0368536,0.014279905,0.08702127,-0.035491712,-0.05919943,-6.4190026e-08,-0.053944975,-0.023679122,0.03393958,-0.027891323,0.06182664,0.022163833,-0.037615757,0.018971838,-0.05813977,-0.062701635,0.012426063,0.0204783,-0.12141533,-0.05156396,-0.026355527,0.0590469,-0.06788078,0.10563027,0.021261694,0.004691065,0.09701386,-0.026600033,0.018934293,0.0030978543,0.05925464,-0.04116679,-0.028525785,0.14203261,0.0012995534,0.031206636,0.028544448,-0.028146146,0.011565832,0.02790132,0.017570369,0.048014533,0.010959312,-0.035229858,-0.09600219,-0.02341122,-0.038496956,0.0587395,0.012337302,0.008954505,0.07065687,0.06364621,-0.05172577,0.0040626726,-0.025657512,-0.016099935,0.052097645,0.06457008,-0.026363019,0.08446703,0.05943928,0.05095971,0.017272227,-0.0843534,-0.021058759,0.044652283,0.012577689,0.027646972,-0.013849185,-0.013610759,13,1.9366463,-25.318687,10
622,"logistic regression is a method for predicting binary outcomes (0 or 1). it multiplies inputs by weights, adds a bias, and applies a sigmoid function to get a probability. if the probability is above 0.5, the output is 1; otherwise, itâ€™s 0.

the model learns by adjusting weights with gradient descent to minimize errors. a likelihood function helps improve accuracy by optimizing the weights. performance is evaluated using metrics like accuracy, precision, recall, and a confusion matrix. itâ€™s commonly used in classification tasks such as spam detection and medical diagnosis.

",0.005242086,-0.03569889,-0.071323626,-0.0021157358,0.0106577445,-0.07361215,0.048757333,0.033388227,-0.03163738,0.035473943,0.02501523,-0.033750534,0.029067695,0.025214503,-0.046378378,0.018432356,0.024638215,0.040701717,-0.06737955,-0.007969062,0.00019508692,0.06496791,0.03894052,0.031735215,-0.021070635,-0.12582694,-0.03894118,0.022640888,-0.084298186,-0.042590044,-0.025542919,0.0029638833,0.059346914,0.022287536,-0.117232494,-0.024243759,-0.03204486,-0.045990277,-0.011229501,0.0028785267,-0.053636312,-0.08668086,0.0030784425,-0.030610092,0.03870231,0.05484931,-0.03126176,0.048905656,-0.011923542,0.032400794,-0.052440636,0.029772997,0.036928594,-0.028945714,-0.10411335,-0.06030368,0.0039572804,-0.04221592,-0.015411955,-0.04517921,-0.021643102,-0.03733912,-0.024260847,0.021866133,0.018031688,-0.08196969,-0.08878231,-0.06519814,0.051869206,0.0068888674,0.00712022,-0.05682638,-0.04336484,0.02903412,0.031914413,0.01659279,0.09703414,0.026023908,0.03678389,0.09441037,-0.030704753,0.04841617,0.033399787,0.016135363,0.105663314,-0.05457294,0.06130311,0.06239905,-0.011007558,0.018904593,-0.034764744,-0.035488356,0.013377175,-0.007480128,-0.052141514,-0.028732039,-0.025339566,-0.0591227,-0.04375096,0.04899257,-0.0928685,0.044671934,0.030187733,-0.030637806,0.039034992,-0.058881924,0.05875914,0.039456505,0.027464679,-0.06885607,0.010515031,0.063575886,-0.07211586,-0.030674858,0.0044018007,-0.0029202779,0.0094791725,0.04382534,-0.089470334,0.120071985,-0.012766013,0.026191385,-0.005600586,-0.06132221,-0.008635618,-0.056640375,-0.07278524,3.10026e-33,-0.009442824,0.03381131,-0.013795327,-0.052240096,-0.028270284,0.019585202,-0.119947344,0.053015336,0.022694636,0.003593095,-0.07913802,0.007343311,0.049902868,0.15952511,0.0509661,0.025806176,-0.0460143,0.116063245,0.021918548,0.003747231,-0.05341316,-0.04392798,0.01557563,-0.08257338,0.03503361,0.08736814,0.03662945,0.03289772,-0.017849978,-0.00012477359,0.02529134,0.03233048,-0.019202728,-0.087053075,0.076262385,0.045274246,-0.021173611,0.021556623,0.050958797,0.05115447,-0.10720128,0.008419362,0.035771206,-0.029055392,-0.047598895,-0.016636854,0.0039364616,-0.05010944,-0.040139858,-0.04727761,0.042482406,-0.04886237,-0.036455527,0.06430857,-0.029035458,0.072718136,0.07059496,-0.025119841,-0.040350292,-0.008456771,0.042530637,0.004872816,0.012210643,-0.011277315,-0.010845489,0.008053066,0.05707831,-0.031182274,0.07057856,-0.023983153,0.0017668247,0.018893803,0.0997546,0.01414825,-0.054541,0.09168188,0.072194,-0.094104454,0.062758066,0.046897918,0.030346131,0.01695945,-0.038668126,-0.047366142,-0.0042605917,0.07116563,-0.024776675,0.0033501035,-0.09314233,-0.036494322,-0.08494016,0.042454325,-0.056948584,0.015190649,0.030932283,-4.6901795e-33,-0.08686982,0.041210447,-0.002773434,0.018535858,-0.01393789,-0.040151324,-0.026386311,-0.057896294,-0.003872312,0.0322816,0.036867354,-0.0068621566,0.033703733,0.03959484,-0.015431916,0.067777656,-0.12752318,0.014486981,-0.06848621,0.03100884,0.062444676,0.07145589,-0.07659876,-0.056449313,-0.05421129,-0.022064373,0.007786272,0.024914319,0.023325395,-0.061087936,-0.011178724,-0.011551701,-0.041306883,-0.073116384,0.11734694,0.039868724,0.031614263,-0.018872835,0.02450297,0.0027265984,-0.0038565253,0.040204108,-0.017091334,-0.058850702,-0.006796893,-0.0752913,0.04709749,0.03009001,0.106556945,0.04435612,-0.02562602,0.01005911,-0.020057175,0.097988434,-0.03366901,-0.024974434,-0.0794007,-0.06361618,-0.068489,0.022131879,-0.07946871,0.039424278,0.052658457,0.09448543,-0.047436804,-0.016884353,-0.057524305,0.026986869,-0.012375508,-0.009209145,0.03819856,0.01854554,0.09778484,0.038349137,-0.040358637,-0.07576791,-0.05959574,-0.008791216,-0.042427916,0.028633451,0.02396173,-0.09545928,-0.0021253054,0.058277555,0.017471563,-0.017883668,0.08488391,0.018892627,0.0075892922,-0.05075841,-0.03678313,0.090714425,0.014509376,0.0044744355,-0.14887683,-5.5010528e-08,0.023793597,-0.03213801,0.1039812,0.0072235246,-0.0013371937,0.07868338,-0.044246603,0.0105040055,-0.057685606,-0.04628646,0.024978707,0.091518015,-0.10921095,-0.08960485,0.08393533,0.056555845,0.06306167,0.08336206,0.025262013,0.028686324,0.057227243,-0.044168293,-0.02841045,-0.053862162,0.029176349,-0.05390657,-0.006811426,0.10701932,-0.04546693,-0.03296914,0.034706157,0.07017678,0.030968083,0.028189555,0.024788786,0.0908021,0.055422682,-0.06471161,-0.06124847,-0.023811357,0.022585146,-0.00041883174,-0.017383719,-0.0018662377,-0.0100257415,0.02064705,0.024592249,-0.03286291,-0.008049029,0.0191595,0.012477574,0.06489899,-0.013192288,0.060135096,0.07103936,0.09146858,0.031529672,-0.057061143,-0.019899053,0.09815901,-0.0143872,0.041700948,0.049923256,-0.026506443,13,-0.5417383,-25.659243,10
634,"logistic regression-
most of terms used here are derived from medical field. we have to assign label to each point based on output. we need to find boundaries which separate regions. expectation algebra - same as discussed in probability course last semster. if we don't have standard deviation of population, we assume that standard deviation of sample is same as that of population. to find the number of bins in a histogram there are different methods such as struges rule, rice rule etc based on what we want to infer from the data. even when there are overlapping clusters, we might need a boundary to separate out non overlapping points, overlapping points are dealt later. 
logistic regression doesn't directly predict the class to which the point belongs, it predicts the probability with which it belongs to a particular class. if the number of classes are two it is simple, if probability of y=1 given x is greater than 0.5.  we need to solve the problem of finding a boundary so that misclassification is minimum. our goal is to minimise the difference between .
calculate the weights wi such that the likelihood of getting the desired targets is maximised given the observations. in training phase input data points xj and the output points yj are known. predicted probability is probability of y given x which is sigmoid of w x + b. if there are two classes, if the predicted probability lies close to 1 then it belongs to class 1. we want to maximise the likelihood of our predicted outcomes being close to the targets. log(l) is known as the log likelihood. minimising likelihood is same as maximising log likelihood. see derivation in slides. apply gradient descent to minimise likelihood. the boundary will be decided based on the flexibility. we have random forest classification and random forest regression. we are give a dataset we have to apply some model and then compare the metrics of both models. we have to create a confusion matrix which contains true positive, false positive , true negatives and false negatives. false negative is more dangerous. false positive might incur some additional costs. accuracy=true positive+ true negative / total.( how many observations you have correctly classified. when there is a data imbalance, there is not much impact on accuracy, so it is not correct measure to compare. precision is of the events that you have predicted how many you have predicted correctly. recall value of a specific class how many could you correctly identify. f1 score is the harmonic mean of precision and recall. other metrics include true positive rate, false positive rate, true negative rate. ",0.07359613,-0.009415368,-0.032922506,-0.050027154,0.06863201,-0.036329284,0.03255609,-0.018083718,-0.04425528,-0.027817663,-0.009803075,-0.06726842,0.10570445,-0.01289804,0.06511366,-0.019878205,-0.003335019,-0.03532755,-0.11227148,-0.024842946,0.022975579,0.06276544,0.0098733585,0.031207822,-0.041048322,-0.057865996,0.034370936,0.014502339,-0.058027685,-0.028433328,0.017045088,0.0002382425,0.09026942,0.0026910023,-0.038843144,-0.053025037,-0.055438373,0.05731527,0.0462408,0.021178313,-0.04845422,-0.004800773,0.027279971,0.018042821,0.029849835,0.06573867,-0.042962894,0.018081004,-0.011036496,-0.018384011,0.030732065,0.013082305,0.0055682613,0.008260973,-0.13321611,-0.08356879,0.09995585,-0.10183782,-0.007748457,0.0024775267,-0.043781795,-0.014003762,0.025301194,-0.06093142,-0.0116197895,-0.08580359,0.032496806,0.057539344,0.06523501,0.06583575,0.08162122,-0.006009116,-0.019627817,0.029104143,0.037976228,0.035187136,0.051277574,0.040491488,-0.014908594,0.007066412,-0.02335026,0.027288103,0.13672943,0.001586618,0.034987982,-0.037568636,-0.04424113,0.029035937,0.0077957427,0.023627419,-0.06661279,-0.015583754,-0.056651827,0.011615761,0.040838882,-0.0365976,-0.028973369,-0.0135272015,0.09430433,0.05802788,-0.0678274,-0.020429367,0.08377321,-0.021491323,0.058153957,-0.0015129821,0.00044135615,0.012817497,0.017452382,-0.073095106,0.05971014,-0.025264768,-0.12926799,0.030217199,0.007829125,0.0035645945,0.1331705,0.030323755,-0.1244627,0.047930256,-0.016921403,-0.01819658,-0.023591954,0.00968117,0.032796837,-0.09354946,-0.1029045,3.7907812e-33,-0.014598322,-0.09185112,-0.017580956,-0.06970641,0.024584243,0.0351909,-0.05248658,-0.03091672,0.060956266,-0.008539303,-0.071944624,-0.054917634,0.11094351,0.057788428,0.08089755,-0.024886783,-0.026514923,0.057594128,-0.06189487,-0.06984327,-0.065871276,-0.0357327,0.014971641,-0.07710382,0.000985748,0.02524389,0.0045289397,-0.047181107,0.021126961,0.03566368,0.037400257,-0.0028945042,0.057724725,0.017359423,0.053628575,0.03787777,0.03464735,0.032193784,0.03053701,-0.041045524,0.009608988,0.048595782,0.08460569,0.013044216,-0.022992224,0.0026304827,-0.0060691508,-0.0019518768,-0.041426085,0.0026361349,-0.0002296137,-0.013865427,0.034003414,-0.026071617,-0.011805581,0.029899037,0.0072540827,-0.02231214,0.017169058,0.048157696,0.030152222,-0.024645917,0.01899324,0.040264815,0.078543745,-0.005071996,-0.001759458,-0.060222145,0.06989139,-0.03451478,-0.03458862,-0.025434451,0.043026093,0.019605404,-0.036960892,0.027862513,0.047302604,0.082508415,-0.009978266,-0.048317112,0.004280081,0.033916377,-0.114696905,-0.118884176,-0.03505541,0.05552087,0.0022601068,0.007577779,-0.119942315,0.0033541305,-0.043474883,0.033743776,-0.10392932,0.065671876,0.013736207,-6.357913e-33,-0.02415628,0.096568584,0.06376691,-0.04111777,0.006153054,0.0029423807,-0.0034389878,-0.03427091,0.004058989,-0.0405319,-0.02709684,0.04941381,0.0814035,0.04582308,-0.09447687,0.0811845,-0.0127666835,0.065665185,0.003914979,0.011451265,0.083344445,-0.00039822015,-0.08024425,-0.06148189,-0.055377264,-0.031882405,-0.09201551,0.0044140583,0.01227946,-0.014731262,-0.04354491,-0.10640204,-0.032828525,-0.08259871,-0.00023770216,0.0145730125,-0.027443347,-0.0062687066,-0.003424846,-0.0058970484,0.019257784,-0.0048198826,-0.08153551,-0.053810753,0.035875715,-0.028397957,0.020930918,0.041107036,0.034196474,-0.02951753,-0.0850674,0.030644314,-0.085916854,0.032736223,0.0019238958,-0.0037525038,-0.028559335,0.009979382,-0.14166099,-0.019187653,0.009764495,-0.010197387,0.0012754824,0.07675906,-0.03960278,-0.0651146,0.01670833,0.02589425,0.056632217,0.011465789,-0.10267663,0.06471046,0.11188787,0.037453834,-0.017071595,0.0018275613,-0.017143663,-0.04416169,-0.005889577,0.0044614975,-0.043774255,-0.06545695,-0.01220363,0.10498983,0.10100539,-0.0014775624,0.091290414,0.0012061668,0.058438525,-0.024652943,0.0117614055,0.03294612,0.039427932,-0.038064275,-0.041531615,-7.35515e-08,0.006421648,-0.069523856,0.034791674,-0.0024724535,0.040505223,0.07091764,-0.09520469,-0.0028185463,-0.0039538112,0.0005360471,0.025003336,0.03949965,-0.1253706,-0.06358567,-0.074608795,0.0667773,-0.015483107,0.10326649,-0.0076018083,0.034134295,-0.054284707,-0.08387611,0.03396617,-0.036293145,0.046799146,-0.058736883,-0.04347754,-0.019340787,0.0010470431,-0.045719106,0.013238634,-0.006584414,-0.07416854,0.062312663,0.08624791,0.0074319504,-0.052128427,-0.032363933,-0.039480317,0.037078813,0.06241203,-0.053709928,0.041469805,-0.03150778,0.1556691,0.087325186,0.038150158,0.04594947,0.01856562,-0.0071514463,-0.01419949,0.01911374,0.0006457853,-0.004619138,0.052063987,0.03738565,-0.05629542,-0.048584823,0.05952683,0.078654096,-0.053210303,0.022824688,-0.06389622,-0.04775388,13,-1.3302194,-29.589844,10
647,"summary 2

in this lecture, we studied logistic regression, which helps classify data using probability scores. clustering techniques were introduced to segment similar data points. we learned about true positive and true negative classifications, essential for assessing model accuracy. outlier detection methods were explored to identify unusual data points. the role of loss functions in minimizing prediction errors was discussed. finally, solving logistic regression using optimization techniques was demonstrated.",-0.0017233919,-0.014931774,0.0065274197,0.055748057,0.057467684,-0.057324074,0.0712886,-0.029332578,-0.033445477,-0.023705194,0.050308906,-0.0084081525,0.072685055,0.027151931,-0.0556346,-0.10880855,0.045603655,0.0038479296,-0.04818013,-0.040617883,-0.06033361,0.07898305,-0.009348185,0.060228042,0.05538884,-0.01609827,0.05040034,0.03891945,-0.08683235,-0.046403132,-0.040791858,0.037510652,0.022197582,0.017159756,-0.020294363,-0.015722161,-0.010406148,0.06798182,0.025766032,-0.014713157,0.0032750599,-0.06708144,0.05849654,0.009746874,-0.00061770104,0.024513533,-0.086714454,-0.031204218,-0.0009316872,0.016524067,-0.037633426,0.034069736,0.0049015107,-0.015517636,-0.1253607,-0.012711049,0.021841371,-0.026050946,0.053587414,-0.02040694,0.09451612,-0.013071505,0.023653822,-0.021841083,0.0037848987,0.00556076,-0.00074352964,0.030064872,0.036535412,-0.013850641,0.04569964,0.025525894,-0.014189832,0.045923624,0.036459565,-0.015753184,0.06324844,0.026083298,-0.0317038,0.018410254,0.008145799,0.04101934,-0.018163497,0.055604607,0.12272358,-0.04275874,0.029143248,0.014447459,-0.026060393,-0.007366134,-0.066293076,0.04092336,0.014437426,-0.0012005069,-0.03421485,-0.0042709084,-0.027293537,-0.051922042,0.11163069,0.04526171,-0.074976504,0.07917044,0.032968424,-0.017747596,0.06610359,-0.046883713,0.06404134,-0.010401731,0.08838193,-0.040614557,0.009928401,0.03674243,-0.043179657,-0.057635628,0.033320237,-0.01639018,0.090020746,0.04341046,-0.14153036,0.07565553,-0.034590345,0.036602568,0.057805616,-0.0004768515,0.056320623,-0.03177909,-0.11744359,4.0865088e-33,-0.026990017,-0.050801173,0.059517127,-0.103336744,0.03298827,-0.00468698,-0.122129135,0.0016055505,0.03694717,-0.008540059,-0.006188016,-0.06900135,0.042980984,0.075607814,0.11815876,0.013199589,0.011569734,0.0622006,-0.04617559,-0.043815106,-0.027219193,-0.10277863,0.036994938,-0.09397144,-0.025241928,0.082821175,0.026960291,-0.00965953,0.0073725744,-0.00039004613,0.055660676,0.023145009,-0.0012337443,-0.06306476,0.10078673,0.039069425,-0.013852202,0.04163416,0.022770414,-0.0143964365,-0.08905873,-0.008839014,-0.020301053,-0.06956568,0.014256477,0.08289446,0.08058491,-0.08220398,-0.0030836836,-0.059313796,-0.044914417,-0.10043998,0.007670837,0.05230321,-0.092021674,0.03303341,0.02876715,-0.04183991,-0.026034836,-0.00423135,-0.0016349884,-0.06508965,-0.012017008,-0.094198726,-0.040440038,-0.05077447,0.041530766,0.042689137,0.04340091,0.07699182,0.03871385,0.011262273,0.049488783,0.028342355,-0.028666066,0.08996165,0.031878125,0.03310125,0.017325368,-0.095168516,-0.024847776,-0.05911688,-0.029712204,-0.06824702,-0.037945233,0.031573817,0.045811586,-0.0029531652,-0.10871899,0.01953481,-0.12169281,0.053563535,-0.085003436,0.0561396,0.006881368,-4.75604e-33,-0.06850167,0.047353752,-0.04787142,-0.049049683,-0.031161046,-0.039531372,-0.08753833,0.023869665,-0.007263164,0.005356842,0.0109978635,-0.0060731033,0.04911556,0.0036920186,-0.02354593,0.087359674,-0.034028724,0.0494372,0.038999133,0.030117054,0.06245248,0.0014491122,-0.061967116,-0.029246556,-0.012044083,0.05374005,-0.04941377,0.047487,-0.05136674,-0.06787792,-0.044927686,0.08758762,-0.020159416,-0.013933921,0.029586717,0.011812993,-0.065173864,-0.07186733,0.020761555,0.0009825926,-0.010820575,0.107378826,-0.06734415,-0.010126045,0.013060153,-0.08481932,0.07701364,0.017031739,-0.0065049157,0.060277168,-0.07449948,-0.032127805,0.007695364,0.08496488,-0.03473506,0.09343873,-0.05345435,-0.0010945355,-0.024409417,0.07599135,-0.025416575,0.02687309,0.03905888,0.10784827,-0.03312605,-0.07401808,-0.024297703,0.009914978,0.0050251125,0.010789443,-0.03014562,0.04408177,0.03765952,0.013016068,-0.055459924,-0.06935175,-0.055097546,0.0119027505,-0.00760524,0.025884738,0.046809312,-0.090025395,0.013729369,0.08919281,0.022828076,0.026635652,0.049024265,0.024401646,0.062675476,-0.06579806,-0.06579784,0.04352161,0.019704714,-0.012053343,-0.074412234,-3.9294036e-08,-0.0786631,-0.05421682,-0.009786496,0.025818055,0.008089185,0.046022214,-0.13744703,0.12857202,-0.05835809,-0.03532366,0.033464305,-0.060894944,-0.10680951,0.0038579225,0.012146928,0.01917376,0.0623236,0.08509722,-0.004810966,0.036655743,-0.0005500912,-0.009438024,0.018517839,0.015064942,-0.014948155,-0.043756045,0.01528626,0.10397168,0.013004553,-0.035460353,-0.0032554774,0.0006065742,0.01983553,0.046011224,0.038324933,0.133314,-0.015945977,-0.06780455,-0.11879096,0.041855562,0.04085272,0.00037814304,-0.046623904,-0.0012605243,0.015179609,0.038252883,0.09328139,0.003528984,0.07320429,-0.045626286,0.01225337,0.02403705,-0.021183114,0.03713861,0.013217176,0.023536863,-0.048352536,-0.0076185833,-0.011617699,0.01685529,-0.004923396,-0.018773397,-0.027491892,-0.030085504,13,2.4080312,-29.75616,10
666,"at the beginning, sir explained what are the ways to determine number of bins in order to get an idea of histogram (there are some formulas to calculate bin number or bin width, but mostly it depends upon how closely we want to get insights from the data(bird-eye view or precise view)). clustering is also useful in cases, even if there is an overlap between two clusters. then topic resumed to logistic regression and sigmoid function. if p(x/y)>0.5, push the outcome to class 1; else, push the outcome to class 2.  learnt about the notations (p, t, w, etc.) in logistic regression. in order to classify (or to come upon with a prediction/outcome), the given things are 1. number of observations(n) and 2. corresponding targets; and our goal is to 1. calculate the weights (w_i) and 2. maximizing l (likelihood) such as the desired weights outcome expression for l satisfies requirements. then we discussed about log likelihood, gradient descent method to solve it (objective function is to minimize log likelihood). then we discussed about confusion matrix (how false positive and false negative affect the outcomes; false negative is disastrous some times). then we saw some quality metrics to assess the logistic regression (like accuracy, precision, recall, f1-metric, etc.); f1 is better quality metric as compared to accuracy( accuracy may fail in case of imbalanced clusters). at the end, tas presented the assessment and insights from of exercise-1. ",0.071725294,-0.06058121,-0.080534115,-0.024826383,0.08282232,0.014750016,0.046515893,0.005770579,-0.036843497,0.03482113,0.00068523065,-0.05582753,0.05862024,0.03451164,0.0077582505,-0.010457668,0.011208321,0.006155142,-0.07559685,-0.057706922,0.041343857,0.047942273,0.08168227,0.009215581,-0.06581321,-0.014508352,-0.018036429,-0.0052774497,-0.08083817,-0.032102097,0.018133374,0.09788958,0.07572297,0.034725517,-0.1110341,0.03561845,-0.007343202,0.027191922,0.039056633,-0.0019991433,-0.09062568,-0.057790894,0.028696887,-0.019323971,0.054951504,0.012842754,-0.09517661,0.09339738,-0.022676582,0.009501045,-0.05412704,0.04140108,0.010115679,0.004977582,-0.12447274,-0.03685949,0.05109382,-0.056955267,0.017472984,-0.0287721,-0.031261586,-0.046433236,0.012416369,-0.049358085,0.01903788,-0.06823738,-0.037109412,-0.023355667,0.11489549,0.053170063,0.044562887,-0.014253431,-0.0075886594,0.014971946,-0.05689171,0.008896172,0.11062719,0.042572364,-0.026219588,0.030775022,-0.0204294,0.08323632,0.08530532,0.004237759,0.06938252,-0.021214299,-0.054419003,0.056773804,-0.030487346,0.010222119,-0.051141724,-0.019393807,-0.016377643,-0.021314643,0.061597545,-0.05449635,-0.023659674,-0.05101775,0.09311786,0.042290874,-0.043276243,0.014671822,0.00081403967,-0.07727222,0.018547775,-0.009490517,0.011669537,0.036490858,0.049206577,-0.06663866,-0.008538087,0.08992601,-0.119766966,0.010147172,0.036172103,0.008856702,0.08123289,0.059435986,-0.06977431,0.07214972,0.024691107,0.037124597,0.0037373381,-0.032562204,0.039165866,-0.05475088,-0.11351289,2.902954e-33,-0.01097988,-0.056573093,-0.028375298,0.040020578,0.0074654734,0.023573887,-0.09987801,0.01789694,0.024718227,-0.01191927,-0.009512094,0.04871475,0.0833811,0.09800212,0.083814844,-0.06641621,-0.02715977,0.081663705,-0.035704773,-0.09845047,-0.11271136,-0.051706005,0.054965246,-0.033871975,0.035650663,0.058403857,0.039720014,-0.052604415,-0.0066446112,0.026418084,0.03679447,-0.012899016,-0.002543122,-0.028012859,0.033406544,0.03146128,-0.081112124,-0.002298454,0.019802205,-0.026348488,-0.05812331,0.04738668,0.06258432,-0.0556681,-0.055613626,-0.010785392,0.02392486,-0.011405446,-0.01836047,-0.024853846,0.043539163,-0.082634,-0.007995637,-0.013489322,0.007357446,0.02869111,0.031611767,-0.051812492,-0.02118275,0.027967874,0.023625493,-0.047013674,0.028579643,-0.017803848,0.013996794,-0.012020664,0.013297547,0.018833317,0.0375399,0.015487741,-0.017700884,0.049884073,0.12226146,0.010060615,-0.032467045,0.074754566,0.053552322,-0.008761613,-0.031551365,0.011924419,0.030607022,0.02170094,-0.03821833,-0.062467467,-0.04692886,0.055620227,0.027901575,-0.023823442,-0.102381945,-0.033116348,-0.080630824,0.058650196,-0.07003232,-0.00046527662,0.007681015,-6.3353525e-33,-0.07261695,0.07310925,0.011431233,-0.03476761,-0.05031205,-0.0021712738,-0.041304253,-0.090538576,-0.01821704,-0.04283495,-0.052943055,0.011829823,0.05372576,0.03523668,-0.08613336,0.024039425,-0.03617543,0.014639081,-0.022660796,0.034944426,0.05998621,0.020521598,-0.09690525,-0.1422551,-0.08326377,0.011063468,-0.056100156,0.013517465,-0.0015160795,0.017002566,0.009699634,-0.059397284,-0.07826863,-0.064716965,0.113902405,0.022250859,-0.03808763,-0.026068864,0.014248015,0.011224285,-0.00874936,0.038415596,-0.027337786,-0.10632586,-0.00095866644,-0.076692656,0.040970415,0.030714856,0.020156922,0.016362956,-0.0038105217,0.04639383,-0.020122638,0.05987217,0.01562728,0.03552005,-0.06381829,-0.008895938,-0.05644077,-0.01676073,-0.04905289,-0.006274578,0.029569678,0.0753101,-0.0934025,-0.09369292,-0.029572347,0.018785745,0.005042605,-0.014455054,-0.075773045,0.033779055,0.11741949,0.04652883,-0.06667455,-0.029552624,-0.048641153,-0.014007858,-0.04843787,0.008167623,0.022672469,-0.088283986,-0.039005794,0.07626361,0.039853346,-0.0043323175,0.0876448,0.070172735,0.070536934,-0.033482626,-0.038835745,0.020741738,-0.0012336097,-0.023965964,-0.045828454,-7.193611e-08,-0.062923275,-0.050446443,0.073716566,-0.037633818,0.05784933,0.097582266,-0.0898965,0.054660648,-0.022505108,0.024254568,0.03159261,0.040734332,-0.1347466,-0.050599214,-0.008488601,0.02414919,0.036737893,0.027416296,0.03847484,0.05342894,0.0024488848,-0.062050387,0.0438601,0.0012063329,0.016365672,-0.062077414,0.007857204,0.06827675,-0.04189869,-0.030601103,0.0009164986,0.06477333,-0.055925086,0.038757455,0.008985955,0.037151918,-0.038508777,0.0010843849,-0.031261105,0.05907171,0.060764987,-0.011589926,-0.028087873,0.016176857,0.08499061,0.071411714,-0.0011071124,0.018153273,-0.051487803,0.007872736,-0.05191444,0.048826028,-0.023997476,0.058106884,0.06806933,0.07277186,-0.080265686,-0.07657436,0.0721277,0.08663717,-0.019293448,0.046660706,-0.07319361,-0.07199975,13,-0.72021663,-29.562004,10
