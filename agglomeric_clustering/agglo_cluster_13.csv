SerialNo,Session_Summary,BERT_Feature_0,BERT_Feature_1,BERT_Feature_2,BERT_Feature_3,BERT_Feature_4,BERT_Feature_5,BERT_Feature_6,BERT_Feature_7,BERT_Feature_8,BERT_Feature_9,BERT_Feature_10,BERT_Feature_11,BERT_Feature_12,BERT_Feature_13,BERT_Feature_14,BERT_Feature_15,BERT_Feature_16,BERT_Feature_17,BERT_Feature_18,BERT_Feature_19,BERT_Feature_20,BERT_Feature_21,BERT_Feature_22,BERT_Feature_23,BERT_Feature_24,BERT_Feature_25,BERT_Feature_26,BERT_Feature_27,BERT_Feature_28,BERT_Feature_29,BERT_Feature_30,BERT_Feature_31,BERT_Feature_32,BERT_Feature_33,BERT_Feature_34,BERT_Feature_35,BERT_Feature_36,BERT_Feature_37,BERT_Feature_38,BERT_Feature_39,BERT_Feature_40,BERT_Feature_41,BERT_Feature_42,BERT_Feature_43,BERT_Feature_44,BERT_Feature_45,BERT_Feature_46,BERT_Feature_47,BERT_Feature_48,BERT_Feature_49,BERT_Feature_50,BERT_Feature_51,BERT_Feature_52,BERT_Feature_53,BERT_Feature_54,BERT_Feature_55,BERT_Feature_56,BERT_Feature_57,BERT_Feature_58,BERT_Feature_59,BERT_Feature_60,BERT_Feature_61,BERT_Feature_62,BERT_Feature_63,BERT_Feature_64,BERT_Feature_65,BERT_Feature_66,BERT_Feature_67,BERT_Feature_68,BERT_Feature_69,BERT_Feature_70,BERT_Feature_71,BERT_Feature_72,BERT_Feature_73,BERT_Feature_74,BERT_Feature_75,BERT_Feature_76,BERT_Feature_77,BERT_Feature_78,BERT_Feature_79,BERT_Feature_80,BERT_Feature_81,BERT_Feature_82,BERT_Feature_83,BERT_Feature_84,BERT_Feature_85,BERT_Feature_86,BERT_Feature_87,BERT_Feature_88,BERT_Feature_89,BERT_Feature_90,BERT_Feature_91,BERT_Feature_92,BERT_Feature_93,BERT_Feature_94,BERT_Feature_95,BERT_Feature_96,BERT_Feature_97,BERT_Feature_98,BERT_Feature_99,BERT_Feature_100,BERT_Feature_101,BERT_Feature_102,BERT_Feature_103,BERT_Feature_104,BERT_Feature_105,BERT_Feature_106,BERT_Feature_107,BERT_Feature_108,BERT_Feature_109,BERT_Feature_110,BERT_Feature_111,BERT_Feature_112,BERT_Feature_113,BERT_Feature_114,BERT_Feature_115,BERT_Feature_116,BERT_Feature_117,BERT_Feature_118,BERT_Feature_119,BERT_Feature_120,BERT_Feature_121,BERT_Feature_122,BERT_Feature_123,BERT_Feature_124,BERT_Feature_125,BERT_Feature_126,BERT_Feature_127,BERT_Feature_128,BERT_Feature_129,BERT_Feature_130,BERT_Feature_131,BERT_Feature_132,BERT_Feature_133,BERT_Feature_134,BERT_Feature_135,BERT_Feature_136,BERT_Feature_137,BERT_Feature_138,BERT_Feature_139,BERT_Feature_140,BERT_Feature_141,BERT_Feature_142,BERT_Feature_143,BERT_Feature_144,BERT_Feature_145,BERT_Feature_146,BERT_Feature_147,BERT_Feature_148,BERT_Feature_149,BERT_Feature_150,BERT_Feature_151,BERT_Feature_152,BERT_Feature_153,BERT_Feature_154,BERT_Feature_155,BERT_Feature_156,BERT_Feature_157,BERT_Feature_158,BERT_Feature_159,BERT_Feature_160,BERT_Feature_161,BERT_Feature_162,BERT_Feature_163,BERT_Feature_164,BERT_Feature_165,BERT_Feature_166,BERT_Feature_167,BERT_Feature_168,BERT_Feature_169,BERT_Feature_170,BERT_Feature_171,BERT_Feature_172,BERT_Feature_173,BERT_Feature_174,BERT_Feature_175,BERT_Feature_176,BERT_Feature_177,BERT_Feature_178,BERT_Feature_179,BERT_Feature_180,BERT_Feature_181,BERT_Feature_182,BERT_Feature_183,BERT_Feature_184,BERT_Feature_185,BERT_Feature_186,BERT_Feature_187,BERT_Feature_188,BERT_Feature_189,BERT_Feature_190,BERT_Feature_191,BERT_Feature_192,BERT_Feature_193,BERT_Feature_194,BERT_Feature_195,BERT_Feature_196,BERT_Feature_197,BERT_Feature_198,BERT_Feature_199,BERT_Feature_200,BERT_Feature_201,BERT_Feature_202,BERT_Feature_203,BERT_Feature_204,BERT_Feature_205,BERT_Feature_206,BERT_Feature_207,BERT_Feature_208,BERT_Feature_209,BERT_Feature_210,BERT_Feature_211,BERT_Feature_212,BERT_Feature_213,BERT_Feature_214,BERT_Feature_215,BERT_Feature_216,BERT_Feature_217,BERT_Feature_218,BERT_Feature_219,BERT_Feature_220,BERT_Feature_221,BERT_Feature_222,BERT_Feature_223,BERT_Feature_224,BERT_Feature_225,BERT_Feature_226,BERT_Feature_227,BERT_Feature_228,BERT_Feature_229,BERT_Feature_230,BERT_Feature_231,BERT_Feature_232,BERT_Feature_233,BERT_Feature_234,BERT_Feature_235,BERT_Feature_236,BERT_Feature_237,BERT_Feature_238,BERT_Feature_239,BERT_Feature_240,BERT_Feature_241,BERT_Feature_242,BERT_Feature_243,BERT_Feature_244,BERT_Feature_245,BERT_Feature_246,BERT_Feature_247,BERT_Feature_248,BERT_Feature_249,BERT_Feature_250,BERT_Feature_251,BERT_Feature_252,BERT_Feature_253,BERT_Feature_254,BERT_Feature_255,BERT_Feature_256,BERT_Feature_257,BERT_Feature_258,BERT_Feature_259,BERT_Feature_260,BERT_Feature_261,BERT_Feature_262,BERT_Feature_263,BERT_Feature_264,BERT_Feature_265,BERT_Feature_266,BERT_Feature_267,BERT_Feature_268,BERT_Feature_269,BERT_Feature_270,BERT_Feature_271,BERT_Feature_272,BERT_Feature_273,BERT_Feature_274,BERT_Feature_275,BERT_Feature_276,BERT_Feature_277,BERT_Feature_278,BERT_Feature_279,BERT_Feature_280,BERT_Feature_281,BERT_Feature_282,BERT_Feature_283,BERT_Feature_284,BERT_Feature_285,BERT_Feature_286,BERT_Feature_287,BERT_Feature_288,BERT_Feature_289,BERT_Feature_290,BERT_Feature_291,BERT_Feature_292,BERT_Feature_293,BERT_Feature_294,BERT_Feature_295,BERT_Feature_296,BERT_Feature_297,BERT_Feature_298,BERT_Feature_299,BERT_Feature_300,BERT_Feature_301,BERT_Feature_302,BERT_Feature_303,BERT_Feature_304,BERT_Feature_305,BERT_Feature_306,BERT_Feature_307,BERT_Feature_308,BERT_Feature_309,BERT_Feature_310,BERT_Feature_311,BERT_Feature_312,BERT_Feature_313,BERT_Feature_314,BERT_Feature_315,BERT_Feature_316,BERT_Feature_317,BERT_Feature_318,BERT_Feature_319,BERT_Feature_320,BERT_Feature_321,BERT_Feature_322,BERT_Feature_323,BERT_Feature_324,BERT_Feature_325,BERT_Feature_326,BERT_Feature_327,BERT_Feature_328,BERT_Feature_329,BERT_Feature_330,BERT_Feature_331,BERT_Feature_332,BERT_Feature_333,BERT_Feature_334,BERT_Feature_335,BERT_Feature_336,BERT_Feature_337,BERT_Feature_338,BERT_Feature_339,BERT_Feature_340,BERT_Feature_341,BERT_Feature_342,BERT_Feature_343,BERT_Feature_344,BERT_Feature_345,BERT_Feature_346,BERT_Feature_347,BERT_Feature_348,BERT_Feature_349,BERT_Feature_350,BERT_Feature_351,BERT_Feature_352,BERT_Feature_353,BERT_Feature_354,BERT_Feature_355,BERT_Feature_356,BERT_Feature_357,BERT_Feature_358,BERT_Feature_359,BERT_Feature_360,BERT_Feature_361,BERT_Feature_362,BERT_Feature_363,BERT_Feature_364,BERT_Feature_365,BERT_Feature_366,BERT_Feature_367,BERT_Feature_368,BERT_Feature_369,BERT_Feature_370,BERT_Feature_371,BERT_Feature_372,BERT_Feature_373,BERT_Feature_374,BERT_Feature_375,BERT_Feature_376,BERT_Feature_377,BERT_Feature_378,BERT_Feature_379,BERT_Feature_380,BERT_Feature_381,BERT_Feature_382,BERT_Feature_383,kmeans_cluster,TSNE_1,TSNE_2,agglo_cluster
28,"during exploratory data analysis we get insights on type of data and understand problems present in the data. we can remove outliers based on trend followed by data: if all data follows a particular trend and some doesn't follow we can remove this data. we can use data smoothening to reduce noise in data. real data has a lot of fluctuations. this can make it difficult to find trends ir pattern in such data. simple moving average sma consider a window around every data points and average the values. window width can be varied to adjust the level of smoothening. based on application we can take window width. we can take moving average at a point as average of all past points or average of some past and some future points. there will be problems at the end of data as there won't be any points to the past of starting of data. so we have to define accordingly at end points. the moving average reduce the noise and maintains the characteristics data. another way is exponential moving average that weight nearby samples more while averaging. this is used in time series forecasting.
in data analysis, it is crucial to handle missing values and outliers before proceeding with further steps like calculating moving averages. ignoring these aspects can lead to distorted results, such as sudden changes or incorrect trend patterns. handling missing values can be done by either ignoring, discarding, or using methods like regression to fill in the gaps. outliers should be removed or adjusted to prevent them from skewing the analysis. after addressing these issues, the data is ready for further analysis, where both visual and mathematical methods are used to ensure accuracy. normalization transforms data to a 0-1 range. in this case, normalization had little effect on the target values and coefficients, with minimal changes except for one variable (x2). this shows that scaling didn't significantly impact the model. for the next messy dataset, a clustering algorithm is suggested. in the process of data analysis, independent normalization of variables (x and y) can impact the results, especially when using algorithms like k-means clustering, which rely on euclidean distance. when the data is normalized, it alters the shape and relationships between variables, leading to more refined results. algorithms sensitive to scaling, such as k-means, are particularly affected, and normalization helps produce more accurate clusters by balancing the influence of different features.


a standard normal distribution has a mean of 0 and variance of 1. to standardize data, the formula is used, where is an observation, is the mean, and is the standard deviation. standardization does not change the shape of the data's distribution, meaning the transformed data retains the original distribution's shape. this method is often used in statistical tests, especially hypothesis testing, where normal distribution is required. data transformations like box-cox are applied to achieve normality before performing such tests. descriptive statistics of the transformations should be reviewed to understand their impact.
the box-cox transformation is a technique used to make data more normally distributed. it involves raising each observation to a power (lambda), with the optimal lambda value determined through a process called maximum likelihood estimation. in this case, the lambda value is 0.17, which is chosen to best transform the data into a normal distribution. this method is useful when algorithms assume normality in the data, ensuring that the data fits those assumptions.
the box-cox transformation and similar techniques, like square root or cube root transformations, pull data points closer together, particularly those that are far from the mean. this helps in dealing with skewed distributions. for a lambda value close to 0 in box-cox, the transformation approximates ; if lambda is 0, it uses the log transformation. the result is a more normally distributed dataset, which is essential for certain algorithms. when applying transformations like box-cox or logarithmic transformations to features (e.g., x1), these same transformations must be applied consistently to future or test data to ensure accurate predictions. additionally, sometimes transformations must be reversed to interpret results in their original context. various scaling methods, such as standardization and normalization, are commonly used to handle such transformations.
the condition where data variance changes across different levels of the data is known as heteroscedasticity.
the process of scaling data, such as using log transformations, reduces the emphasis on large values and minimizes the impact of variations in data. this is crucial because many algorithms, especially those that rely on euclidean distance or hierarchical clustering, are sensitive to data scaling. various scaling methods are used to bring the data to a common scale while preserving the variation within the dataset. however, in some cases, transformations like the box-cox transformation are applied to also change the shape of the data for better analysis.
when preparing data for analysis, steps include identifying and fixing missing values, exploring correlations between features, and creating visual representations like scatter plots or matrix plots. scaling transformations are often applied before conducting these analyses to ensure accurate and consistent results.
the issue of data imbalance arises when certain classes are underrepresented in a dataset compared to other classes. this is particularly problematic in classification tasks. in the example given, there are four classes: red, green, blue, and white, where the white class is underrepresented and often merged with the blue class. when subjecting these classes to classification, boundaries are created by the algorithm based on the dominant class, leading to misclassification of the underrepresented class.
as a result, false negatives occur, where observations belonging to the underrepresented class are wrongly classified as another class. this is evident in confusion matrices and the performance metrics like precision and recall, where the underrepresented class shows poor classification. the algorithm focuses on correctly classifying the majority class, but the minority class suffers from poor performance.
data imbalance occurs when one class significantly dominates over others in a dataset, causing challenges for learning algorithms. this imbalance can negatively affect the algorithmâ€™s ability to learn, particularly in cases like medical diagnosis (e.g., diabetes detection) or fraud detection, where underrepresented classes are crucial. algorithms may become biased toward the majority class, leading to misclassification and poor performance for minority classes.
an example of this is in the detection of exoplanets based on flux values from distant star systems. most stars do not have planets, meaning the dataset is heavily skewed, with 99.3% representing stars without planets and only 0.7% representing stars with planets. in this case, the algorithm may focus on the majority class (stars without planets) and miss important patterns in the minority class.
to address this, data needs to be balanced or projected into a lower-dimensional space for better visualization and understanding. algorithms require different performance metrics (e.g., precision, recall) to detect the presence of data imbalance and ensure that the model learns effectively from both the majority and minority classes. neglecting this can lead to biased models that perform poorly on critical underrepresented classes.

in cases of data imbalance, particularly when the disparity between majority and minority classes is not extreme, dropping a small number of values from the majority class can be acceptable without significantly affecting the sample's representativeness. however, if the difference is significant, dropping too many values from the majority class may lead to a sample that is no longer representative of the population.
one approach to address data imbalance is oversampling the minority class by duplicating its data points. however, this method does not introduce new information; it merely replicates existing data, which can water down the dataset without solving the core issue of imbalance.
a more effective method is using synthetic minority over-sampling technique (smote). instead of duplicating data points, smote generates synthetic samples by selecting a minority class data point and creating new points as linear interpolations between it and its nearest neighbors. this method enhances the representation of the minority class while maintaining diversity in the dataset. by creating synthetic samples, smote helps improve the balance without simply duplicating existing data, providing a better way to handle data imbalance in machine learning tasks.
some other data balancing are adasyn. tomek links are used to to undersample data. majority class with their nearest neighbor being a minority class sample are removed. smote and tomek links are done one after other to get nice data. 
",-0.07904905,0.009768046,0.048195645,0.0523437,-0.005948169,-0.035975702,-0.0461056,0.016039042,0.032763045,-0.006209421,-0.040793087,0.055140145,-0.011388545,-0.008374,-0.02070274,0.009325482,-0.056024805,0.041375354,-0.050042633,-0.051407963,-0.01873629,-0.0156744,-0.059233215,0.0052523557,0.019987244,0.057482388,-0.04919434,0.009026318,0.015146774,-0.015090684,-0.021912383,0.02310656,0.0018739178,-0.0070838244,-0.07853083,-0.07566425,0.010396424,0.10862973,-0.001463116,-0.059993584,0.016791549,0.019337868,-0.035918713,-0.029685127,-0.032235518,-0.0074890745,-0.0006330725,-0.05164879,-0.036269635,0.038525008,-0.1098383,0.0005967314,-0.05215315,0.038339302,-0.01561309,0.0025453304,-0.026014177,-0.022274386,0.09969801,-0.04690276,0.0021648447,-0.02324685,0.06758235,-0.022078495,0.010635238,-0.048031982,0.067054376,0.08115208,0.048519984,0.10170063,-0.025324324,0.011141089,-0.0375072,-0.06400011,0.0087891705,-0.0175421,0.0061006113,0.006310051,0.0069967895,0.01353212,0.050290033,-0.0030779764,-0.05662673,0.005889732,0.013386092,-0.047500007,-0.010886277,-0.026131067,-0.018231247,0.058549218,0.034995526,0.06983564,-0.047663916,-0.04107052,0.13711566,0.03200618,-0.060796462,-0.045477003,0.13305217,0.051971853,0.027182661,0.051301308,0.025338594,0.046252247,0.023495875,-0.08363519,0.057291225,-0.013914252,-0.076035045,0.0015349836,-0.0011463198,0.016993215,-0.06673763,-0.08352253,0.047395952,-0.07781136,0.022171764,0.025864333,-0.11384959,0.021770166,-0.028561108,-0.0045181615,0.008830669,0.07196151,0.10855967,0.047599148,-0.016318338,7.637165e-33,0.0060589244,-0.010262688,-0.018651536,-0.050185382,0.029585378,0.0043685874,-0.03245057,-0.01625907,0.07158794,0.06951777,0.038958464,0.05726504,-0.019432217,-0.07171708,0.09010742,-0.082528174,0.031266965,0.011564608,0.012458589,-0.029327894,0.07836264,-0.11527039,0.045990165,-0.027019542,0.012031896,0.0035630541,0.095271535,0.040151104,-0.011751075,-0.019225607,0.092253834,0.0376698,-0.13477784,-0.04906218,0.035939585,-0.051290583,-0.035022452,-0.019394357,0.105728775,0.023138268,0.030436324,0.06901819,-0.06293739,0.015932072,0.00013535547,0.07735858,0.069554076,-0.00063865416,0.0018953057,0.030726042,-0.013354623,-0.055661663,0.01698075,-0.037979633,-0.064875,0.035663884,0.04068241,-0.06221777,-0.02794698,-0.013218915,-0.058379423,-0.07108085,0.031299856,-0.055922028,-0.076783374,-0.058733065,-0.029199867,0.14300135,-0.0683812,0.014416863,0.07417396,0.0152602075,-0.038444012,0.0057598823,0.051497743,-0.05016683,0.015359571,0.08968733,-0.048881408,-0.028527774,-0.047304988,0.022912107,0.04840691,-0.08204919,0.010131557,-0.036616445,-0.012118505,0.025532357,-0.024124088,-0.07398554,-0.04879878,0.0059911185,0.0071450276,0.108025655,-0.040458657,-7.822861e-33,0.009610639,0.040513318,0.03685301,0.034282252,0.018959483,0.0657636,-0.047908504,0.060373534,0.08807261,-0.046571337,-0.1051123,0.008853492,0.043126173,-0.036677644,0.028851086,-0.012106802,0.04733975,-0.103437945,0.028353907,-0.010566202,-0.05318828,-0.05236752,-0.06288757,-0.027819881,0.013372408,-0.0023679459,-0.07952959,0.052304327,-0.07469075,-0.04143275,-0.07025668,-0.019822905,0.031100133,-0.05054558,-0.07089865,-0.044571433,0.01135626,-0.059494443,0.002485912,0.085325755,0.052037973,0.09196349,0.013447652,0.0022354382,0.029262511,0.08861839,-0.040264875,0.04485321,-0.05652071,0.016201032,0.046671346,0.11905401,-0.052741162,-0.03062057,0.025788952,0.096281745,0.0006583277,-0.035576854,-0.13235354,0.026180316,0.039926346,-0.03296594,-0.013020623,-0.05208481,-0.021707332,0.03079652,0.06169002,-0.15453975,-0.060430422,-0.038767952,0.015220733,-0.024554586,-0.08949089,-0.00546142,0.038252756,-0.07011681,0.04769252,-0.089059256,0.045294546,0.0066572754,-0.06390166,-0.0075418698,0.04792372,-0.017282758,-0.108307205,0.062021337,0.014860099,-0.055915873,-0.026476333,0.015045104,-0.041895382,-0.061706666,-0.0022153705,0.03634042,-0.03518859,-6.339021e-08,-0.074755654,-0.04274255,0.03529703,0.052436907,0.023348922,-0.07043031,0.010530715,0.10050281,0.060548764,-0.08539217,0.05308223,-0.061901256,-0.055519883,0.026831655,0.06769158,0.03622203,0.0009662065,-0.00021272205,-0.026263442,0.007201327,-0.013298761,0.05235531,-0.014161138,0.041044828,0.06095452,0.006805795,0.031711273,0.050388217,0.026509956,-0.0396231,-0.012627319,-0.0069248076,0.07604195,0.036467,-0.048207466,0.029816277,0.07391837,-0.0021225573,-0.04219746,0.033398584,0.0207817,0.050607372,0.0009050133,-0.027162276,-0.046309613,-0.00050617516,0.007217497,-0.00801068,0.018888166,0.013693142,0.14008887,-0.020812627,0.035972912,0.06532853,0.068363704,-0.0074562165,0.03298902,-0.018381594,0.029637067,0.005223945,-0.017848141,-0.06377192,-0.1227203,0.018553868,9,10.406472,36.12427,13
50,"the session covered key steps in data preprocessing, starting with outlier analysis and removal. next, it addressed noise reduction using data smoothing techniques like simple moving average (sma) and exponential moving average (ema), where a larger window size results in more smoothing. the discussion then moved to gradient descent as an optimization technique. normalization methods were introduced, followed by the box-cox transformation and logarithmic transformations, which help stabilize variance and normalize data before time series analysis. additionally, maximum likelihood estimation (mle) was discussed. the session also touched on the kepler exoplanet dataset and the synthetic minority over-sampling technique (smote) for handling imbalancedâ data.",-0.05954271,0.055067357,0.054040555,-0.027660431,0.019707434,-0.100634225,0.011208023,-0.001364627,-0.043351132,0.027898083,-0.04240665,0.048250053,-0.042881224,-0.05662557,-0.03650708,0.013328073,-0.035015773,0.043391764,-0.054330893,-0.025036411,-0.05087244,0.08338519,-0.09438512,0.0665657,0.027459394,-0.05078682,-0.05879111,-0.041030064,0.0018728698,0.0025243752,0.010513417,0.09842162,0.074250326,0.011613701,-0.06479007,-0.06393203,-0.025193013,0.011420216,-0.0014132223,-0.012184176,0.013787265,-0.055265144,-0.056484234,-0.018264169,-0.012624898,-0.0052768844,-0.009566721,-0.03857505,0.05131133,0.025934098,-0.06136292,-0.0015865628,-0.044160523,0.047129273,-0.06451419,-0.059403203,-0.047987785,-0.027077274,0.10894138,-0.037934713,0.0028172054,-0.0060410183,-0.0070165317,0.04316775,-0.04397734,-0.10603919,0.03628866,0.0070320973,0.061864324,-0.014855176,-0.03735543,0.061200917,-0.009834825,0.00692354,-0.013119743,0.06780197,0.017890438,-0.0065895696,0.038701013,0.010597722,0.058155116,0.019660214,-0.0032711376,-0.023977552,0.06734329,-0.038198125,-0.026406556,0.039384447,0.00055455713,0.033152115,-0.029915487,0.07441359,0.023939166,0.0626718,-0.048380483,0.04458141,-0.019598447,-0.0077584395,0.08653225,0.115186535,-0.005867485,0.08677464,-0.014442848,0.04035616,-0.019719666,-0.08997666,0.09958063,-0.017434308,-0.031946193,-0.07754884,-0.050493643,0.018014874,0.0008937,-0.083491504,0.020702802,-0.027330492,0.07272152,0.07083668,-0.14774436,0.034283325,-0.014236661,-0.0011232868,0.006557941,0.047010686,0.088204235,0.08093907,-0.06231482,4.0321164e-33,0.022917772,0.0030712846,0.016457537,-0.070040025,-0.004722141,-0.013186456,-0.07258308,-0.034207318,-0.0025968407,0.075209744,0.018499197,-0.014702678,-0.01827419,-0.034895543,-0.029237648,-0.042506076,-0.027389137,0.043504555,-0.018460935,-0.038462013,0.0051656845,-0.09833289,0.016757185,0.01698884,0.01380575,0.099352166,0.035884373,0.0035586725,0.058968212,0.02864495,0.015805835,-0.015823174,-0.09998944,-0.01946085,-0.026680404,-0.00023589809,-0.0043495926,-0.019598514,0.013204146,0.11475949,-0.032266613,0.08054242,0.017667891,-0.1002744,-0.017340675,0.07175703,0.08975321,0.011268033,0.044100605,0.04855749,-0.043573346,-0.067956164,-0.021640444,-0.016509108,-0.051651694,0.08393638,0.09461342,-0.0013696994,-0.022565303,0.03081157,-0.073702976,-0.07545748,-0.0053585498,-0.04584385,-0.010953579,-0.042151857,0.019196479,0.03126442,-0.10201872,0.017204104,0.026794188,0.019724268,-0.02467595,0.06568197,-0.011363796,0.010076511,0.063076735,0.08599563,-0.03761809,-0.0027716418,-0.043703333,0.07392602,0.01509842,-0.09217125,-0.0055874633,-0.07273595,1.5836746e-05,0.024860932,0.03577064,-0.04801142,-0.12962975,-0.032758594,-0.05658292,0.003978823,-0.08299408,-2.6497391e-33,-0.07050508,0.053513266,-0.023409279,0.063347206,0.03550676,0.046136025,0.030645052,0.028036643,0.033467826,-0.0060375263,-0.04019597,-0.022803826,0.07446989,-0.07473804,0.021805236,-0.016322413,0.035381302,-0.033530124,-0.08110414,-0.012403578,-0.021057278,-0.0041674715,0.017284049,0.022102628,0.006264097,0.0087229945,-0.07147836,0.0771498,0.00621479,-0.042171534,-0.08957491,0.0019628431,0.00095976616,0.0052868077,-0.03751447,0.02763186,0.004122238,0.018287038,0.030349316,0.06834964,0.0051315376,0.10361019,-0.04032016,-0.05093856,-0.047353618,0.0079534,-0.044704963,0.047759816,0.007866796,-0.013687778,0.0020307712,0.031437475,0.06811846,0.092033036,0.06483042,0.01517136,0.051910657,0.02133656,-0.049445502,0.0047746226,0.032314043,-0.0034741024,-0.019531433,-0.07188791,-0.04273303,-0.03641757,0.07979255,-0.009817911,-0.059479732,0.0009477859,-0.0061689145,-0.009045445,-0.022926414,-0.006329239,0.019782104,-0.0918379,-0.0020261046,-0.0883795,0.04202407,-0.02017543,0.0133870095,-0.07534926,0.084984735,0.07859944,0.013971086,0.10390958,0.024506235,-0.07016191,0.031281587,-0.01229481,-0.09598461,0.046180557,0.031988095,0.0780607,-0.04641288,-4.2022144e-08,-0.0031995252,-0.050225195,0.021611253,0.05980554,0.030972159,-0.031615164,-0.067229934,0.165832,-0.0063846824,-0.055938594,0.09279399,-0.073916376,-0.0078222975,-0.036290877,0.061059073,0.023061406,0.015653841,0.025378488,-0.021668224,0.050608374,-0.035852738,0.07859466,-0.01885998,-0.028936166,0.16612266,-0.062104505,0.047565486,0.11257041,0.0665884,-0.0471323,-0.08828282,0.033240933,0.030367907,0.015049688,-0.035637736,0.018511683,0.035409145,0.031988658,-0.071025476,0.09263634,0.014286277,0.06582447,-0.081511386,-0.04155704,-0.05157863,0.059415627,-0.011289602,-0.019370895,0.03928564,-0.06304743,0.07706869,0.006126758,0.11045964,0.07514166,0.02732842,0.042090163,0.028563032,0.010296921,0.012079142,-0.0042882124,-0.0001695049,-0.07695309,-0.068315566,0.00881977,9,12.162413,33.116467,13
111,"we began by looking at how outliers affect the data plots, and how much of a difference it can make (we saw a graphical representation after removing outliers and rescaling to better understand the data). we also saw how noise can change the signal data and in order to fix it we started learning about data smoothing. fluctuations due to noise can make it difficult to identify trends and patterns, and can even lead to wrong analysis, hence the need for cleaning. sma stands for simple moving average, it a moving window average, it's a form of low effort filter, to filter and check data. we start with let's say 50 data points and underlying trend is visible after that, then we have a higher window size to do more smoothing. when we come to the end points we can choose to stop the moving average according to the type of method we're choosing (if we say that moving average of a window is at the left most point of the window, then we'll have to ignore the right most end point). there are other methods like exponential moving averages that weigh nearby samples more, also used in time series forecasting. then we observed the clustering of data, and again we went through the data of session summary and saw the graphs of plotted values of number of submissions and the length of summaries. we also saw standardized and normalized transformations. we saw how a skewed distribution can turn into normal distribution with transformation, we should be aware when to apply the transformation and how it'll be useful. then we to got to see kepler exoplanet data, plotting the flux values for stars belonging to two classes. data imbalance, and we saw techniques to see data imbalance, either by under-sampling the majority class or over-sampling the minority. or, a more interesting approach is to generate synthetic data for minority, which is a type of oversampling. we understood how smote oversamples the minority class, minority classes with samples with nearest neighbor being minority class are removed.",-0.080513336,-0.014577287,0.08822833,0.08356229,0.056282245,-0.09601318,-0.0020528305,-0.021071792,0.07135199,-0.002174065,-0.058844745,0.09053578,-0.013174709,-0.06320588,-0.067172006,-0.007942592,-0.007967619,-0.04838677,-0.06195839,-0.051482692,-0.02024115,-0.03470095,-0.066722415,-0.0015862394,0.031206239,0.05249932,-0.05696862,-0.015736548,-0.009505847,0.018498823,-0.02895646,0.055784963,0.03726983,0.005502047,-0.034678075,-0.0542634,0.027451932,0.084223405,0.0045290706,-0.0074251546,0.008228892,0.040679693,-0.04352,-0.021773973,-0.057526153,0.00096062105,-0.020847442,-0.07012223,-0.03715229,0.0013666858,-0.12093737,-0.007484785,-0.08876483,0.018483132,-0.025785133,0.008949251,-0.030255547,-0.036864106,0.09286139,-0.038102508,-0.024012795,-0.055640176,0.010233411,0.025119249,0.09815145,-0.028270137,0.05592915,0.027222697,0.058786083,0.047038283,-0.037914317,0.03222862,-0.027883038,-0.029280882,0.05516736,-0.014124575,-0.013129916,-0.022525843,0.038315695,-0.016538462,0.05503157,-0.009415561,-0.0607256,0.017535515,0.03852895,-0.003926906,-0.03441909,-0.07517144,0.04574465,0.038893875,0.016902948,0.074011445,-0.07644926,-0.018553613,0.08846171,0.037294753,0.0023958962,0.014787171,0.1165738,0.090544984,0.051612332,0.022612792,0.033617206,0.017474152,-0.002084883,-0.03570547,0.0454377,0.02357179,-0.06408914,0.017341679,-0.017328765,0.03974858,-0.06932917,-0.009465595,0.048295945,-0.028930368,0.08793789,0.047567982,-0.11175686,0.026401054,-0.03725918,-0.046612397,-0.028167292,0.020046107,0.08991402,0.07962885,-0.07385189,1.3338489e-35,0.006782124,-0.01476159,0.0142728165,-0.048996232,0.022240466,-0.02241752,-0.049784787,-0.016890097,0.088755995,0.13217025,0.010360596,0.10264902,-0.012623076,-0.0627353,0.09832874,-0.096080355,0.014422183,0.015514386,-0.04723784,-0.05141332,0.053019688,-0.082789205,0.05009497,-0.02772856,-0.021339888,0.004651481,0.09401697,0.06452286,0.015533991,0.0063220393,0.026063515,0.08117761,-0.08662075,-0.030535268,-0.0003986494,-0.029741565,-0.04152389,-0.04972156,0.0640548,0.01568956,-0.021538885,0.048201285,-0.029474499,-0.031937063,0.02244413,0.028009329,-0.019638013,0.013570742,-0.0063780188,0.03736417,0.0010603609,-0.079688005,0.012154996,0.010113628,-0.07495572,0.054932494,0.110377334,-0.12081002,-0.019071339,0.016102288,-0.043637633,-0.075647436,-0.0024995832,-0.005623832,-0.025542153,-0.0034609374,-0.017784387,0.14040647,-0.056187503,0.008150892,0.057831634,0.038765192,-0.075439356,0.030292341,0.077524185,-0.09356195,-0.00845917,0.1168979,-0.049692906,0.017031092,0.0041670734,0.0024462333,0.00912036,-0.070409834,-0.0675488,-0.06806564,-0.006608717,0.022064624,-0.03293132,-0.076363884,-0.03084581,0.018991694,0.0033368042,0.10351987,-0.061352216,-1.5404944e-33,-0.021433108,0.009167936,0.06425057,0.076602586,-0.034053694,0.044402312,-0.013583342,0.034998734,0.09800842,-0.028500514,-0.11628694,0.01057137,0.061223615,-0.050324537,0.06193064,-0.019077817,0.1000891,-0.11381095,-0.031543978,-0.040261358,0.0034605004,-0.05764572,-0.035014242,-0.023204861,0.0037461356,-0.00040989328,-0.10295525,0.039009653,-0.06868189,0.0055535976,-0.030895717,0.030895336,0.054751333,0.009770716,-0.013827642,-0.008428243,0.033876024,-0.016464697,-0.0029785729,0.036454663,0.04007799,0.058331016,0.03480045,0.047751073,0.01766301,0.09638307,-0.025982495,0.046403725,-0.07356768,0.01837337,0.058147985,0.090955734,-0.048819117,9.4013965e-05,-0.009568615,0.14043029,-0.0015060633,-0.021164732,-0.07766365,0.034688283,0.064374514,-0.013483095,-0.09256026,-0.014549323,-0.054016434,0.0025856707,0.085388586,-0.101470254,-0.007922248,-0.035148676,-0.01739206,-0.019543782,-0.081353866,-0.033211544,-0.0008576686,-0.01941019,0.04046863,-0.05176876,-0.01735103,0.04105577,-0.08356046,-0.03447517,0.050731458,-0.01709599,-0.056006715,0.047243558,0.01981536,-0.0643974,-0.008161689,0.022980314,-0.03107385,-0.07049238,-0.008713748,0.01878015,-0.04689792,-5.5036576e-08,-0.10645827,-0.068952605,0.0054722703,0.024760265,0.026677184,-0.049774382,0.016465183,0.14376408,0.03614166,-0.06957408,0.044405036,-0.04853501,-0.09912973,6.6875065e-05,0.075233325,0.013714658,-0.027331397,0.016202636,-0.03512774,0.021839177,0.039876074,0.008194892,0.010990176,0.050677106,0.059844002,-0.003491116,0.02473073,0.11551064,-0.010657015,-0.0526594,-0.029508395,-0.011306389,0.0665527,0.012612498,-0.046188913,0.045732003,0.05807362,-0.032685325,-0.033557266,0.0311596,0.017699255,0.029755414,-0.011380881,-0.020248711,-0.014957049,0.02331504,-0.015670164,-0.0008762596,0.029762052,-0.0030328773,0.09361266,0.007148983,-0.015296621,0.022858223,0.010000849,-0.038667392,0.04182262,-0.047837485,0.013559799,-0.012001061,0.0057512894,-0.025570204,-0.11603606,0.006120885,9,10.235723,36.216885,13
116,"after dropping the outliers â†’ after re-scaling the display. how do we eliminate noise? â†’ if makes it difficult to find a trend in data â†’ simple moving averages. consider a window around every data point and avg the value, window width can be varied to adjust the level of smoothing. (higher window size â†’ more smoothing). sma â†’ filling up missing value, removing outliers, eliminating noise. exponential moving averages â†’ works better time series data. we take value â†’ on both sides â†’ or only on one side. # first remove the outliers.
standardization and normalization of data. linear regression is immune to data scaling but gradient descent not. â†’ value btw [0,1] â†’ but regression statistic does not change. clustering algorithm based on euclidean distance will greatly influence by scaling the data. normalization â†’ [0,1] (xn = (x - xmin) / (xmax - xmin)). standardization â†’ ""standard normal distribution"" â†’ n(î¼,ïƒ) â†’ (0,1). # it does not change the shape of distribution of data. box cox transformation. log transformation â†’ heteroscedasticity (variance is changing).
data imbalance - certain instance of a data might show up more freq than others. ex â†’ rare disease diagnosis, forgery. â†’ undersample the majority class â†’ oversample the minority class. (1) smote. # tomek links are used to refine data.",-0.026382701,-0.019274987,0.04641464,0.07229105,0.024979291,0.009877717,-0.030564982,-0.034099765,-0.025373321,-0.034827083,0.038000755,0.093697816,0.04122747,0.0012686545,-0.046121266,-0.010698157,0.040526327,0.04575306,-0.108944185,-0.053569913,-0.10583257,-0.017716123,-0.07918385,0.08104509,0.08324192,0.00834648,-0.0013937189,0.016971493,-0.00050813623,-0.022827769,-0.014473496,-0.012007209,0.03978718,-0.023717724,-0.025310198,-0.026641404,0.054623008,0.03174429,0.02121967,-0.06559981,0.04988137,-0.016334405,-0.035806563,-0.02066459,-0.035744518,-0.06654372,-0.011785218,-0.05292165,-0.038090285,0.0012563941,-0.0065696426,-0.025409412,-0.064220004,-0.0015801652,-0.06288232,0.013087471,0.011034196,0.024346769,0.08296259,-0.0027763706,0.013709406,0.018975697,0.032768,0.064377934,-0.03969794,-0.01297764,0.0444222,-0.053552993,0.023077844,0.07377545,0.0032505651,0.0191211,0.027303299,-0.002267005,0.09362061,-0.04492925,-0.023003746,-0.04199861,0.02487476,0.06494148,0.037890263,0.021416541,-0.00472447,0.03762293,0.042388696,-0.043578703,0.01210328,-0.07262071,-0.0043567005,0.013331068,-0.011893675,0.054648537,-0.1148883,0.020939251,0.065120466,-0.03365989,-0.04604936,-0.034516156,0.10148584,0.08928461,-0.016294649,0.04572994,-0.016079152,0.047656227,0.035581257,-0.017843522,0.075785115,0.033329383,0.0057498137,0.03834347,0.0005491224,0.01441351,-0.036292963,-0.0118522765,0.017089132,-0.032358244,0.111259274,-0.0013206106,-0.1459683,-0.041481182,-0.023242887,-0.02910595,-0.010761484,0.055792537,0.04311391,0.11397118,-0.101640835,6.706264e-33,-0.063766986,-0.00025824978,0.032278564,-0.07639164,0.058251955,0.026400324,-0.0685321,-0.086675845,0.03656349,0.021801906,0.048759576,0.062248968,-0.029640023,0.020605745,0.02666901,-0.07801639,0.11241114,0.027173873,-0.014979018,-0.038337722,-0.0033870353,-0.08201165,-0.024731565,0.00957793,-0.06183239,-0.0032083772,0.04066861,0.007933895,-0.05029209,-0.02512487,0.026075551,0.0410782,-0.07727259,-0.03367282,-0.017868362,-0.060277253,0.07027037,-0.016831132,0.07315404,0.018508337,-0.009950043,0.102478586,-0.0641313,-0.057559513,0.04950243,0.07144915,0.059124067,0.0089997165,-0.009882386,-0.13892642,0.0051260376,-0.01911528,0.022806475,-0.04117703,-0.071552046,-0.012741024,0.01902133,-0.09922417,-0.027493324,-0.03364847,0.020560926,-0.076499216,-0.019797152,-0.018976001,-0.0046294755,-0.037454944,0.028828809,0.037151475,-0.0783476,-0.03610821,0.045955088,0.06311028,-0.027201105,0.056392398,0.025444288,0.016281815,0.048796985,0.015232942,-0.016614122,-0.09736216,-0.027230654,0.018645935,0.042788062,-0.02526251,-0.011864342,-0.07491529,-0.03309133,0.00796556,-0.0034092884,-0.012972202,-0.07057611,-0.012629188,0.0063624126,0.04656515,-0.05073882,-7.035043e-33,-0.0022862784,0.05516693,-0.05709597,0.012401404,0.004425643,0.061538585,-0.013889866,0.09226741,0.06004932,-0.059561484,-0.0068005337,0.030909186,-0.017058834,-0.019904898,0.027996046,-0.025546763,0.068546996,-0.03016879,-0.011147777,-0.005323829,0.020062037,-0.03237974,0.0056292536,0.02601084,0.018060174,0.011552512,-0.12207334,0.0822291,-0.055927828,-0.059158392,-0.079959095,0.0255147,0.091959886,-0.06777025,0.002185709,0.0674996,-0.063080885,-0.04435712,-0.060851187,0.10189599,0.012718798,0.08277672,-0.032294363,0.0009313411,0.089006506,0.06356004,0.0028067615,-0.020420726,-0.07528822,0.015239384,-0.020970458,0.08192504,-0.013724164,0.018300503,-0.08003295,0.13008459,-0.0015081025,-0.017271178,-0.09229193,0.021014037,0.035840947,-0.07902817,-0.11412623,-0.025632914,-0.062118925,0.011555682,0.06543116,-0.027065342,-0.018685648,-0.06178095,-0.056750488,-0.043485887,-0.034605756,-0.026067307,-0.01833069,-0.12855065,0.0044025704,-0.101035096,0.056276098,0.010052287,0.012459073,-0.015676823,0.040296286,-0.0430612,0.05726022,0.08482746,0.02064339,-0.016329294,0.026193958,0.036136035,-0.039930865,-0.0720181,-0.023668893,-0.047677528,-0.032524306,-6.230545e-08,-0.121292025,0.02550314,0.0069854283,0.040150292,-0.005769988,0.010611195,0.036870323,0.11485312,-0.023019528,-0.054954685,-0.010461446,-0.10930082,-0.08053803,0.06625839,0.026135739,-0.01248211,0.018624656,0.041093983,-0.024654808,-0.04930598,-0.097835906,0.08369696,-0.045054417,0.02906574,0.023606544,-0.05026313,0.025682667,0.08446912,0.06581083,-0.030748336,-0.044873398,0.005490207,0.109561466,0.045548387,-0.045189083,0.05310716,0.015411268,0.064299114,0.019980606,0.02534897,0.04099799,0.025317734,-0.029896993,-0.0013151447,-0.010735676,0.034793478,0.033366267,-0.0059840768,0.034861792,-0.014213531,0.19286586,-0.004241289,0.030883012,0.057389017,0.051793065,-0.029269706,-0.008633544,0.039122976,-0.0074181077,-0.014377824,-0.013729698,-0.065027915,-0.104192995,-0.009135866,9,11.963603,35.85942,13
133,"in today's class, we started with a recap of eda (exploratory data analysis). then we learnt about fixing the missing value and handling outliers using mumbai aqi value dataset. this known as data smoothing, it can be done using simple moving average sma. higher the window size for sma, more smoothing is the curve. linear regression is immune to scaling but gradient descent method gets largely affected by scaling. then sir explained about data normalisation, standardization and box cox transformation. after normalisation, things like clustering algorithms do well; because in presence of scaled noisy data, such algorithms like k means clustering which are based on euclidean distance gets highly influenced by the scale. at last ta explained about data imbalance and how to fix it with an example of detection of exo-planets. some times a data is under represented, in that case we use methods such as smote, adasyn, borderline-smote,etc. to create new synthetic data points; and methods like tomek-link to increase the minority class data points to cope-up with class imbalance. ",-0.04501248,0.018822309,0.0117801195,0.00027648895,0.021149369,-0.07283674,-0.050559882,0.0078033027,-0.020387035,0.027918449,-0.006524657,0.025779853,-5.0008275e-05,0.025889644,-0.06417316,-0.016085565,-0.0015843726,0.107295096,-0.14102796,-0.039384197,-0.038523503,0.014610388,-0.058012813,0.062524356,-0.014060487,0.00755415,-0.004929713,-0.044033077,-0.04037419,-0.01910095,-0.01470541,0.062168796,0.048960026,0.0028897196,-0.08240826,0.024832549,0.033310946,0.056495994,-0.06868775,-0.029927464,-0.01296315,-0.015489674,-0.041317042,0.0075403475,0.068189956,-0.05268858,-0.05848831,-0.05841592,-0.025515506,-0.046384074,-0.06047757,-0.04625728,-0.07715462,0.0155106345,-0.03559842,-0.052507382,0.005659872,-0.026198363,0.068439506,-0.050070915,0.0063554705,-0.07028012,0.016005006,0.015940199,0.0017010162,-0.07576203,0.06385839,-0.009245209,0.04370371,0.013034344,-0.016848568,0.04461052,0.025793906,0.046446808,0.035052136,0.044433765,-0.0023156228,0.048617616,0.014734905,0.05595555,0.03843336,0.049075034,0.020866377,0.03358702,0.069198616,-0.0068844664,-0.07716263,-0.039943825,-0.022268822,0.046475768,0.039901562,0.089072734,-0.042703375,-0.014390861,0.012431117,-0.025808517,-0.041000634,-0.052575406,0.110178374,0.074261956,-0.013482925,0.07236576,-0.005884745,0.014119754,0.007767301,-0.024422592,0.0769193,-0.018310016,0.03466013,-0.028610835,-0.08085405,0.0006903673,-0.0822814,-0.05507834,0.005614065,-0.077037565,0.09481492,-0.0020303072,-0.14791895,0.023991525,-0.039471548,-0.067149945,-0.037103537,0.052048538,0.09127732,0.111462384,-0.10795095,5.567013e-33,0.0071045184,0.0067226803,0.010131602,-0.06695077,-0.03352364,-0.056105286,-0.08341655,-0.044402707,0.007102491,0.0010578313,0.014923514,0.09365323,0.05246329,-0.01241058,0.06566661,-0.05738171,0.046696775,0.037410863,-0.012772048,-0.04303793,0.00564084,-0.104003124,0.080517106,-0.025906907,-0.045254905,0.041475497,0.027367463,0.00711391,0.04697826,0.0022098494,0.031603143,-0.012437978,-0.064936504,-0.030510282,-0.041331083,0.007107518,0.029653953,-0.031938445,0.048475135,0.05019082,0.014929178,0.088129684,0.01745417,-0.06364246,0.04208598,0.09125295,0.06604094,0.056368086,-0.0333953,-0.03799856,0.001701399,-0.04694976,-0.028563945,-0.026231375,-0.0008314305,0.09862259,0.055404004,-0.0954499,-0.018026318,0.016321832,-0.022368174,-0.10223446,-0.024719374,-0.00241985,-0.026667746,-0.041733682,0.0047805747,0.047417615,-0.07311023,0.055715706,0.055700783,0.062317166,-0.086599566,0.08006781,-0.012173396,-0.03364507,-0.005923872,0.056083646,-0.040247492,-0.004253509,-0.07374662,0.07240619,-0.018573016,-0.068421066,-0.01207227,-0.05436524,-0.002165301,0.0060682143,-0.0053696316,0.0064004282,-0.08912967,0.009596374,0.0027377633,0.086138204,-0.017999774,-4.7553504e-33,-0.034353737,0.027078336,-0.07225571,0.08451131,0.010570326,0.081355765,-0.033867076,0.044883426,0.024661018,-0.028816547,-0.026920978,0.011231196,0.0872684,-0.040953822,0.02448956,-0.011849935,0.051987655,-0.0190384,0.00038088235,-0.02464128,0.03497098,0.014721668,-0.05580984,-0.01659794,-0.015112086,0.009592458,-0.17115647,0.06423524,-0.053917613,0.002933069,-0.0208252,-0.022184068,-0.015183349,-0.014825166,-0.07120986,0.034556728,-0.006679158,-0.043799676,0.012346521,0.08477368,0.011568313,0.089266896,-0.01898473,-0.026887633,0.016274776,-0.013838316,0.0015838643,0.04552729,-0.036605738,0.01548383,0.031185461,0.048173387,0.015974049,0.03116856,0.0038897188,0.10016107,0.00011074563,0.0055436264,-0.122636594,0.022012575,0.027876174,-0.073459506,-0.060630143,0.026889084,-0.11389271,-0.017164785,0.14793947,-0.021512758,-0.052136205,0.018268911,0.011802622,0.022761624,-0.024057701,0.007657292,-0.042157922,-0.09728101,0.023561431,-0.017699366,0.03690854,0.025022637,0.031573236,-0.036452558,0.07891223,0.030334497,0.021325901,0.07729024,0.098832786,-0.060318626,0.033832297,0.037989113,-0.07532877,-0.043688897,-0.03528925,0.04484315,-0.052978326,-5.6555507e-08,-0.085068375,-0.056549616,0.00091489963,0.052699614,-0.016904294,-0.069775134,-0.036158904,0.16822256,-0.03646117,-0.004153556,0.028783416,-0.048520315,-0.048829723,0.02182255,0.09141556,0.041324794,0.07841604,0.038159337,-0.034300834,0.069193386,-0.04060259,0.046614844,-0.03280134,0.01835774,0.09675533,-0.07629948,0.027221758,0.04291623,0.056563374,-0.033056144,-0.10398095,0.001913566,0.06525863,0.005371425,-0.012198248,0.022164714,0.036720928,0.05179543,-0.03770906,0.054434694,0.08979746,0.050076652,-0.010140995,-0.016070781,0.030535929,0.08460797,0.01724644,0.001039618,0.025873067,-0.022777034,0.102827385,0.008057562,0.028998647,0.020373702,0.03907789,0.015115722,0.008920444,-0.029715186,0.042560652,0.028238272,0.031963028,-0.10721304,-0.10058392,-0.029194498,9,12.208094,34.55783,13
200,"todayâ€™s class start with a discussion on how do we eliminate noise from dataset. noise makes it difficult to find a trend in data therefore it is becomes a necessary part of exploratory data analysis. we learn one method named â€œsimple moving averageâ€ in which we consider a window around every data point and average the values. window width can be varied to adjust the level of smoothing. higher window size makes more smoothing. this method is also used for filling up missing values, replacing outliers.
another method is â€œexponential moving averageâ€ which works better on time series data. one good practice is that first removes the outliers then apply moving averages method. next we learnt about the standardization and normalization of datasets. linear regression model is immune to the data scaling but many models such as gradient descent method is not. clustering algorithm based on euclidean distance will greatly influence by scaling the data. the normalization makes value lie between [0,1] using formula such as x=[x-x(min)/ x(max)-x(min)] and the standardization creates standard normal distribution. normalization does not change the shape of distribution of the data. we use log transformation when data is heteroscadascity. next we discuss about data imbalance which happens when certain instances of a class might show up more frequency than others. example rare disease diagnostic, forgery. we can overcome this by either under sampling the majority class or over sampling the minority class. we can do this with the help of smote tool.",-0.053901825,-0.03742862,0.04271267,0.036726788,-0.005105187,-0.011647958,0.00095863023,-0.016435906,-0.026410205,-0.047996107,-0.0025468192,0.07613292,0.010841908,-0.0049142777,-0.08935546,-0.06478157,0.006434791,0.037152983,-0.058627132,-0.06672364,-0.050768565,-0.014835351,-0.05311529,0.07203355,0.071723886,0.012156033,-0.029736478,-0.016352285,0.031267423,0.01806755,-0.061408125,-0.020081982,0.040028717,-0.018292481,-0.10373524,-0.061498698,0.064266734,0.104637235,-0.023600623,-0.013671865,-0.012517743,-0.0057088966,-0.037507698,-0.023948822,-0.011021381,-0.000653154,-0.012319216,-0.09346425,-0.033173054,0.018248718,-0.06726652,-0.0077309147,-0.017758777,0.04807516,-0.03652633,-0.016858628,-0.02083179,0.042051535,0.113235466,-0.037168346,0.026460847,-0.05255916,0.01141006,0.038144752,-0.046012912,-0.043570418,0.023644092,0.02808721,0.049725045,0.0062560705,-0.031909835,0.06716061,-0.012846715,0.008905917,-0.0063207457,-0.012652613,0.052740615,-0.014768686,0.014121015,0.040388934,0.04618043,-0.011595484,0.00780443,0.010296326,0.03488659,0.01790246,-0.0068652504,-0.003971125,-0.063142516,0.013161326,0.0131234,0.061403133,-0.06093926,-0.0021141923,0.08813677,0.027897775,-0.03050962,-0.0027023456,0.13776019,0.058017306,-0.009717939,0.03530746,-0.021583924,0.03836229,0.06416736,-0.007429941,0.090890974,-0.017523495,0.031509552,-0.0011047262,-0.056903753,0.024289027,-0.09773887,-0.0430494,0.05586505,-0.08863319,0.07852126,0.021490315,-0.12811717,0.030932337,0.022313263,-0.018046346,0.0059411796,0.03486471,0.0732066,0.057551116,-0.029906008,1.0842638e-32,-0.002609874,0.0122589525,-0.015558607,-0.043486454,0.02862253,0.007102872,-0.042451095,-0.030917425,0.08901911,0.04336151,0.051321667,-0.01377101,-0.032191996,-0.0026628731,0.04752739,-0.07706535,0.041639213,-0.0132917585,0.047977217,-0.041922845,0.04120639,-0.07881467,0.029175991,-0.03421574,-0.038276233,0.078724496,0.053188518,0.006443873,0.033269715,-0.011548332,0.04451109,0.04637719,-0.12919892,-0.0025632088,0.038969066,-0.012576219,0.022920951,-0.05217932,0.1105075,-0.0043107625,-0.013281291,0.050956562,-0.051904734,-0.07777756,0.050589718,0.10777814,0.07434075,0.00029048158,0.037070997,0.010625689,-0.02362227,-0.06355056,-0.01524875,-0.05843924,-0.08071961,0.040868267,0.03340338,-0.09469204,-0.03782667,0.03250439,-0.051616073,-0.06934818,0.02538045,-0.020607937,-0.009366203,-0.033637412,0.011979461,0.08334716,-0.045698263,-0.050689925,0.054519173,0.030054733,-0.051438373,0.054409947,0.030360868,0.0010991897,0.064634666,0.03960636,-0.06914701,-0.065360606,-0.053724322,-0.005473899,0.052795798,-0.08613811,0.0063903546,0.01475287,0.026019264,0.040238813,0.024354141,-0.02781153,-0.07235981,0.02599946,-0.021268122,0.10098781,-0.05871157,-9.809919e-33,0.037246536,0.03316409,0.0014705784,0.038722888,-0.017677767,0.050873574,-0.06805344,0.06782609,0.11374641,-0.0608031,-0.07028279,-0.0012146363,0.04202577,-0.03139931,0.018836446,-0.03371294,0.05292745,-0.09482975,-0.052021082,-0.01352583,-0.027051127,-0.063977145,-0.09365174,-0.04659323,0.003450685,-0.0020399285,-0.12911414,0.08789801,-0.10244018,-0.034905255,-0.090081595,-0.0048647528,0.050190046,-0.086342014,-0.03439715,0.05280042,-0.033666033,-0.024740426,0.013370212,0.04720501,0.009882302,0.10832937,0.018690266,-0.081130855,0.030255893,0.10237005,-0.056878764,0.03757761,-0.08333941,0.02173584,0.033420935,0.063042015,-0.018252289,0.0105392635,0.02722497,0.10644044,0.0129530905,-0.02905182,-0.078740135,0.019123377,0.024046423,-0.03673814,-0.07783696,0.003388626,-0.08004096,0.020149546,0.01059761,-0.084380545,-0.03282196,-0.016283011,-0.036008373,-0.019255595,-0.007936343,-0.0070725325,-0.061203163,-0.09511568,0.06816907,-0.11177074,0.037452426,0.011854558,0.0154703995,-0.00551799,0.03050331,-0.06152453,-0.04649489,0.0935995,0.06883694,-0.058786396,-0.008449196,-0.0042170016,-0.071955316,-0.022015184,0.011179775,-0.0215059,-0.0074230754,-6.723814e-08,-0.08533007,0.01685302,0.06569408,0.001843179,0.0023404558,-0.033134185,0.018780706,0.1477253,0.0018599438,-0.049741834,0.050397202,-0.088591814,-0.055246435,0.048953366,0.049960144,0.0026156453,0.026781736,0.02762028,-0.029415574,0.010476314,-0.06082314,0.015102231,-0.016543578,0.008648415,0.09949825,-0.04117621,0.08379247,0.09798971,0.0566712,-0.023785656,-0.052483898,0.06389571,0.08507811,0.031147007,-0.027228542,0.07261727,0.029857315,0.04488178,-0.069128245,0.03149525,0.027166579,0.024577405,-0.028991593,-0.034450606,-0.014918418,0.04792332,0.034459397,-0.0441561,0.025374742,-0.010258633,0.17774293,-0.0024481649,0.038668524,0.018577365,0.0763838,0.053266525,-0.0150141455,-0.009165409,-0.0012175764,-0.026581127,-0.062239047,-0.040621374,-0.08482239,0.014466171,9,11.215991,35.631577,13
284,"data scaling, linear regr3ssions is affected by it. gradiaent descent is immune to it. methods to deal with it. normalize all samples to range between 0 and 1.

clustering. on running clustering through a normalized data we obtained an errorneous result with linear or parallel boundaries. the original data in aspect ratio 1 is spread over a narrow range of y values. when normalized, the clustering improved. because clustering makes use of euclidian distance, scaling or transforming data can improve or worsen classification.

normalization techniques. standardization is to turn a disttibution into one that resembles a standard normal distribution. boxcox transformation turns distributions far from normal into close to normal distributions. (x^lambda-1)/lambda. log transformation takes care of heteroscadacity. because euclidian distance 

caution against confusion matrix being transposed. data imbalance and clustering.

dealing with data imbalance.
case of exoplamets and timeseries flux data. overtsampling is the idea of duplicating certain values when under represented. smote performs linear sampling with two hyperparamaeters, number of samples and smote%. ",-0.02705885,-0.0013765468,-0.03845064,-0.041295074,0.0050544622,-0.0949166,-0.039227713,0.01823831,-0.006305688,-0.026076356,0.03277378,-0.004930412,-0.018201334,0.03183181,-0.008560778,-0.055267457,0.014364883,0.06981873,-0.15047683,0.037502773,0.06867555,0.011550853,-0.06470219,0.06597377,0.022860285,-0.018581575,-0.0277334,-0.07988122,-0.00010589703,-0.040860392,-0.013642668,0.08815026,0.042216577,0.042685185,-0.030742267,-0.03692878,-0.02145262,0.057887528,-0.0143340705,0.037440185,0.0073381388,-0.012559807,-0.066351555,-0.0069897976,-0.015304739,0.017774332,-0.03497347,-0.017822105,-0.023677843,0.029258328,-0.0053161443,0.033680275,-0.06428378,0.08018198,0.007959579,-0.020858843,-0.016696313,-0.0531415,0.05250699,-0.04817749,-0.0438609,0.0235533,-0.019732332,0.0020639587,-0.0012713094,-0.061937872,0.03365056,-0.014851829,0.02744683,-0.024983356,-0.049232055,0.04931039,-0.021660764,0.057131413,0.03705538,0.08157803,0.020745011,-0.008571848,-0.0022949192,-0.00960464,0.066752575,0.1101156,-0.0143394405,-0.01210406,0.037948284,-0.033964783,-0.04103832,0.06402891,0.036259666,-0.009384777,0.011912745,0.07256487,-0.018924166,0.0035471132,0.0501868,0.0051398952,-0.034848418,-0.023792053,0.15963049,0.0739578,0.040179532,-0.0030441044,0.03250661,0.002012438,-0.02740539,-0.044863228,0.07661382,-0.005746057,0.04931515,-0.041030183,-0.010109546,-0.0055488925,-0.06702055,-0.05962889,0.00016274172,-0.023178369,0.06542486,0.007404273,-0.10866508,0.00076891074,-0.007289534,-0.017331833,0.0028527172,0.10205856,0.075681105,0.06228821,-0.08862663,1.0137715e-32,-0.08359358,-0.022566166,-0.008073604,0.025210347,-0.016543616,0.080700494,-0.07014557,-0.052978817,-0.007994564,-0.011433853,0.022319848,0.1035821,0.0474187,-0.013336432,0.0016871061,-0.022715155,0.010462851,0.11928457,-0.030916737,-0.022319742,0.04116192,-0.07080948,0.030151162,0.00227995,-0.07799091,-0.0047141435,-0.06021286,-0.02466108,-0.03273255,0.013055927,0.036791317,0.005268864,-0.037528932,-0.036405522,-0.0022697398,0.077124976,0.06401375,-0.05619777,-0.017732376,0.039206654,-0.08960406,0.07625206,0.026463542,-0.07810212,0.068989396,0.066919394,0.11792734,0.038824227,-0.036018092,-0.049648754,-0.025984315,-0.054007035,-0.011891806,-0.02095875,0.045784477,0.1192825,0.05416307,-0.09363925,0.047885515,0.07998591,-0.077419154,-0.054555126,-0.06790483,-0.03144344,0.03304018,-0.03532038,-0.007469622,0.045085587,-0.062384997,0.07045649,0.02345322,-0.0064009177,-0.06581185,0.0791079,-0.01413818,0.032139916,0.06733735,0.1098539,-0.08635621,-0.045783892,-0.09189072,0.0537454,-0.045327768,-0.058056735,-0.08576766,-0.05520091,0.004059683,0.021170568,-0.0675235,0.02758435,-0.025604513,-0.012637138,-0.029043946,0.01517161,-0.046950568,-8.4486056e-33,-0.077552475,0.02304241,-0.06827113,0.044650204,0.0069370214,0.020390263,0.00750619,0.019180434,-0.01892506,-0.04416188,-0.00614302,-0.03284033,0.02719673,-0.03647908,-0.03144852,-0.010073084,0.050541267,0.06495848,-0.03493511,-0.019303458,0.044685584,0.05391098,-0.09542622,0.004462176,-0.0038349752,0.063792534,-0.12502001,0.07029661,-0.012583604,-0.03496181,-0.10054845,-0.029998919,0.009525995,-0.020925106,-0.009877322,0.011987059,-0.042834714,-0.019045733,0.057133317,-0.02007532,-0.036860608,0.099514045,-0.042543367,-0.06297791,-0.013263853,-0.017728632,0.006350023,-0.041674964,0.054489538,0.04786057,-0.018906832,0.03207363,0.0245815,0.0846301,0.009006452,0.044695906,-0.0243621,0.01169173,-0.06580001,0.010098451,0.0017310113,-0.03088828,0.019011324,-0.012498369,0.009218004,-0.068719395,0.10158071,0.03521581,0.018176906,0.017449878,0.021823637,0.025677461,0.031726006,-0.061443686,-0.024626289,-0.12218988,-0.0026740276,-0.0632764,0.017919272,0.025018498,-0.050359827,-0.055127565,0.051556118,0.10716177,0.026997996,0.050048105,0.12146516,0.008436538,0.0597273,-0.007118333,-0.07649346,-0.0041058077,-0.0321198,0.03846908,0.017284207,-5.9474896e-08,-0.12908715,-0.01518109,-0.029385898,0.023316428,-0.012342881,-0.00376913,-0.06743867,0.13075411,-0.04132427,0.0018994967,0.10526806,-0.103747666,-0.047819186,-0.0334028,0.0878144,0.017379573,0.011782271,-0.004674627,-0.031768236,0.04387796,-0.105351396,0.024887243,-0.06415512,0.005269229,0.034190506,-0.06375268,0.02729822,0.10893904,0.07163444,-0.10037606,-0.061051853,-0.0057589156,0.058666565,0.04240161,-0.05382298,-0.012456148,-0.024987802,0.063570246,-0.03793954,0.0891617,0.028927233,0.026284715,-0.0034328785,0.013269353,0.011274445,0.056063075,0.04389908,0.0055738175,0.004134979,-0.053614367,0.091192126,0.046858072,0.019508425,0.08027905,0.04466608,-0.025685575,-0.076133244,0.026152961,0.05042036,0.035411157,0.016756728,-0.06550795,-0.10130865,-0.018747466,9,12.997265,34.509857,13
432,"in today's class, we move on to data smoothening using moving averages. in the practical datasets, we see many fluctuations due to detrimental noise, which can lead to difficulties in obtaining trends and patterns in the dataset. to handle the missing values in the dataset, for example in stock analysis, we don't have data of the upcoming days but we can fill the data using moving averages. in the simple moving average(sma) method, we need to consider a window around the missing data point and average the values for this. you can select the points from both sides of the point in the window or points from the left side of the point. for points which are appearing after endpoints, we use the final endpoints more than once. after performing sma we can create the plot again, the plot will be refined based on the window size chosen by us. if we select a very large window size, the plot nearly becomes a straight line, so we need to choose the width of the window based on the data for better analysis. we also have better methods like exponential moving averages that weigh nearby samples than moving averages. if in a dataset one particular feature column's features have a larger magnitude compared to other features, then we apply normalization to this column, but due to this change only the coefficient of this column in the mlr model changes. clustering methods based on distance can be highly influenced if one column has larger magnitudes or more variance than others. then we saw the difference between normalization and standardization. in normalization, the data is transformed as xn=(x-xmin)/(xmax-xmin) while in standardization, the data is transformed to a mean of 0 and standard deviation of 1 by the transformation 
zn=(x-mean)/(std. dev), in which the mean and std. dev. are of the sample. then we learned about the box-cox transformation, in which we transform the original x to remove the skewness from the data and remove the situation of heteroskedasticity, meaning the variance is changing in the data. it is based on the maximum likelihood estimation technique. then we saw the confusion matrix of a cluster plot in which a particular class was creating data imbalance. data imbalance means certain class shows more frequency than others, we need to handle this else we will get misleading results as an algorithm trained on imbalanced data may show biases toward a class. techniques handling data imbalance are either to undersample the majority class or oversample the minority class. a better approach is to generate synthetic data for the minority class(oversampling). we do this by performing a technique called smote in which we perform linear interpolation between existing samples. smote is used to oversample the minority class while the technique of tomek links on the other hand undersample the data. majority class samples with the nearest neighbours as minority class samples are removed from the data.",-0.037472725,-0.03449806,0.06001154,0.048319865,0.041794814,-0.034637686,-0.04222481,-0.0034405834,0.06851741,-0.030879738,-0.0071923654,0.015802737,-0.035351574,-0.032581866,0.02185123,0.005162472,-0.025883488,-0.031015756,-0.048061125,-0.04094774,-0.027113944,-0.048042446,-0.074143924,-0.0072818995,0.054132886,0.027950184,-0.050109044,0.017960334,0.023572115,0.0013432596,-0.059741825,0.015111166,-0.04050483,0.009618215,-0.056211557,-0.07454297,0.027906343,0.10262701,-0.018002938,-0.01930961,0.00710267,0.07392762,0.019481929,0.052524373,-0.011175753,-0.022381114,-0.017652545,-0.074246064,0.008421921,0.030283555,-0.07466336,0.029584156,-0.07640461,0.068571225,-0.033941347,0.015949812,-0.01224779,-0.06644577,0.09207587,-0.013948151,-0.030848525,-0.017382927,0.07755168,-0.008702752,0.06684067,-0.097782835,0.044381633,0.04213906,0.060253743,0.09547067,-0.03948644,-0.020667616,-0.03966226,-0.019643784,-0.0069106845,0.0043269703,0.011113295,0.033776633,-0.005439805,-0.019723132,0.0017648038,-0.03722642,-0.055715587,0.06515583,-0.00026318236,-0.026663853,-0.020047618,0.0061866357,-0.0062782275,0.07521163,0.0024916406,0.10742036,-0.088552125,-0.030651292,0.12717287,0.039583612,-0.00013087796,-0.0012931562,0.08175724,0.063518025,0.05642611,0.006770053,0.028561974,0.0004905879,0.020615933,-0.051287018,0.00623106,0.0107048685,-0.0831391,0.039296046,-0.001978495,-0.0015786355,-0.074173376,-0.0010245041,0.097973965,-0.025092116,0.014685896,-0.042706344,-0.112661794,0.021468405,-0.046724457,-0.0262121,0.018756302,0.042484246,0.059115134,0.07296322,-0.030412694,2.2753888e-33,0.0028243824,-0.03935327,0.0039388156,-0.027965577,0.013262544,0.00035149278,-0.065958865,-0.015154452,0.080733605,0.09904059,-0.04414884,0.050895326,0.015716305,-0.07627825,0.039773207,-0.10057125,0.07500745,-0.01983506,0.0039610644,-0.011757295,0.06714628,-0.08728736,0.07533838,-0.037347436,0.006054281,0.01852574,0.042732,0.056591082,0.0072296644,-0.012834028,-0.014412032,0.08477966,-0.028893517,-0.089149155,0.015818188,-0.081186034,0.016094252,-0.034349192,0.11428622,0.0077613215,0.03401862,0.018563053,-0.05186173,0.0034229495,-0.009222797,0.03677669,0.048252013,0.041731432,0.00082090293,0.07503169,-0.03083814,-0.05513105,0.014463827,-0.0301061,-0.054035574,0.018794732,0.056028504,-0.12076294,-0.04726126,-0.0318851,-0.0392968,-0.043722637,-0.014720191,-0.044107724,-0.04227772,-0.014912042,-0.04002994,0.09830989,-0.055165444,-0.008545021,0.06747959,0.012827,-0.06775088,0.034002706,0.07176815,-0.10266751,-0.03512186,0.11084352,-0.04999352,-0.015847594,0.031602792,-0.0061369683,-0.009345226,-0.115976945,0.01055324,-0.04330428,0.024341002,-0.020175472,-0.06018298,-0.14073136,-0.029708533,0.0052777184,-0.0009794988,0.12685134,-0.023982925,-3.6074632e-33,-0.04873984,0.058973007,0.042013112,0.092857756,0.007241748,0.07202412,0.0005201865,-0.0048797233,0.1400439,-0.04007354,-0.12295621,-0.008044299,0.026092395,-0.042004753,0.053132657,0.019807104,0.041045826,-0.11670637,0.01268857,-0.037749924,-0.019022819,-0.083512135,-0.062234405,-0.018386876,0.052735843,0.01689585,-0.09619316,-0.02678922,-0.100992404,-0.05672508,-0.02129749,-0.03744204,0.011288293,-0.07280323,-0.04827041,-0.01292683,0.030834371,-0.019820355,-0.02480517,0.086017996,0.041109588,0.025294214,0.04358254,0.076802425,0.0032803765,0.12976146,-0.013191518,0.07514288,-0.051150244,0.04033236,0.070162915,0.09196443,-0.045853995,-0.01885382,0.04111979,0.12520717,0.004683327,-0.0029649776,-0.11235745,-0.019034214,0.02861389,-0.062997155,-0.04918118,-0.04675583,-0.056528285,0.014732207,0.06964886,-0.121572666,-0.043171726,-0.016005242,0.0123831825,0.005954513,-0.082787804,-0.064880915,0.04702667,0.018065426,0.06776132,-0.028277399,0.02671682,0.04004744,-0.029898623,-0.025535008,0.085928656,-0.01517921,-0.08704259,0.081075385,0.0069306754,-0.047722846,-0.00214233,0.04053312,-0.025564598,-0.011529929,-0.0011890826,0.018536495,-0.012323703,-5.934852e-08,-0.08826811,-0.04250202,0.018747078,0.013029567,0.017045619,-0.053590752,0.0502414,0.113753274,0.050360795,-0.028282663,0.020391101,-0.035865262,-0.056185875,0.019763632,0.025462603,0.0074694157,0.066271305,0.0050052027,0.004053974,0.026821917,-0.023308765,-0.004508799,0.015027903,0.032133564,0.02327754,-0.0049599023,0.0073522674,0.04717744,-0.010963865,-0.025793785,-0.03104241,-0.041083004,0.07011894,0.045383353,-0.0037321295,0.018972097,0.07154693,0.031510215,-0.05778828,0.029246556,-0.016789716,0.029580168,0.034417257,-0.046319082,0.016716173,0.049037334,-0.0051426506,0.013735649,-0.008731427,0.0009534688,0.09889548,-0.0249648,-0.03924267,0.039256066,0.045623,-0.023146534,0.01543572,0.022636646,0.038064152,-0.026359588,-0.078290194,-0.045979563,-0.14857395,-0.010191595,9,10.009717,36.33358,13
451,"we continued our discussions on eda. eda is the first step before performing any complex algorithms on the available data. it involves discovering problems associated with the data as well as few possibilities and insights from the data. in todayâ€™s class we looked at some specific problems associated with the raw data and the methods/ algorithms used to tackle these.
the first problem which we discussed was that of presence of lots of noise in the data, which causes hindrance in detecting the true signal. we discussed few methods to solve this problem. we specifically looked at /moving averages for data smootheningâ€™, in which we first learnt about the simple moving average. in this method, we take nearly about 50 data points surrounding a particular data point and evaluate their average inorder to get an average value associated with that particular point. we do the same with all the other points and finally get a smoother curve with the values as averages of some 50-60 points in the neighborhood. the smoothness of the curve depends on the number of data points we are choosing to evaluate the average. as we increase this number, the curve becomes smoother and smoother and eventually becomes a flat horizontal line, when all the points in the set are included in the window. this creates an â€˜artificial signalâ€™ and the original variations in the data vanish completely.  there can also be different ways in which we consider these points. for example, we may consider only those points which lie behind the selected point. this causes a problem at the end point, particularly the one on the left. similarly, we can also consider that window which includes some points behind the selected point and some ahead of it. this also causes problem at the end points. we can choose any of this, however our choice should be justifiable, according to the particular data set. 
next, we can also use exponential/ weighted averages, in which we assign higher weights to the points in the proximity. we must select an optimal size of window or the moving average method, such that we are able to extract the signal from the highly fluctuating values. 
before creating moving averages, we should address the problems of missing values and outliers, else they would cause trouble while creating the ma. 
the next problem is that some values in a column have significantly higher values than some others. this makes the gradient decent algorithm, more biased towards the larger values, thereby causing data imbalance and giving false results. hence, it is important to normalize the values in the columns so that every value lies between 0 and 1. standardization is another process wherein we convert the existing data into standard normal distributions, with mean 0 and standard deviation 1. both of these do not change the shape of the data. if we transform or scale the data, then any algorithm based on calculating euclidean distances would be largely affected. it is important to transform the data first because some algorithms assume that the data is already normally distributed. 
there is another kind of transformation- box cox transformation in which we evaluate the transformed value of each x by using a parameter lambda, which is optimized such that we get the closest approximation to the normally distributed data set. 
apart from transforming the features, we also have to reverse transform the transform the response variables, to get the values in the original form back. 
the third problem which we discussed was regarding the data imbalance, which occurs whenever we have a class whose number of samples are very small compared to the other classes. hence, the class is eventually suppressed by the others. we need to fix this problem, as we may get incorrect results/ predictions on using the imbalanced data. also, the algorithm may not learn well using the imbalanced data. 
to fix this data imbalance, we can do the following:
1)	under sample the majority class.
2)	over sample the minority class. the most naive method is to duplicate the values of the existing points.
3)	ideally, we should sample more points in the surrounding of these points by linearly interpolating between any two points. this is known as smote.
4)	next, instead of just randomly dropping any sample value from the majority class, we can drop those values which have the nearest sample belonging from the minor class. this has two advantages- first is that it creates a distinct boundary between the classes and also in this way we can get rid of some possibly misclassified points. this is known as tomek links.
5)	so, first we can apply smote then use tomek links to improve the quality of the classification model.
",-0.14612709,-0.053868387,0.045973524,0.0036931515,0.0010815712,-0.13080336,-0.007294014,-0.03233759,0.00105748,-0.022937693,-0.07637154,0.0037136935,0.01772876,-0.017222347,-0.026798058,-0.0027294736,0.0020401273,0.03791072,-0.054765265,-0.036318015,-0.014913281,-0.05028073,-0.036215577,-0.018623007,0.0766871,0.049183786,0.045271866,-0.0038853837,-0.0061737034,-0.05720541,-0.010917545,-0.00860648,0.036776897,0.029566992,-0.046222024,-0.058893606,0.042934466,0.07914917,0.035562508,0.056439456,-0.011929611,0.036164355,0.01788818,0.01655413,0.06778695,-0.031301174,-0.040564038,-0.059680518,-0.029426394,0.047669433,-0.053691983,0.0006293127,-0.058348384,0.06383278,-0.017515555,0.0037370631,0.027127117,-0.03650691,0.08075312,0.021131353,0.02556703,-0.06952826,0.043444097,0.014966617,-0.04852272,-0.035814963,0.05788029,0.016292073,0.050948028,0.0402856,-0.041057676,0.08107401,-0.034778394,0.014248407,-0.00010831104,-0.04744151,-0.0493005,0.012826282,-0.017045377,-0.02753669,0.036266256,-0.05956567,-0.044470556,-0.016664667,0.0228475,0.0091955485,-0.004813882,0.0044461717,0.05362423,0.037522282,-0.025798488,0.064234234,-0.08637997,0.019158572,0.08290046,0.026651055,0.04034836,-0.035080824,0.10486389,0.10803586,-0.016463637,0.0435879,-0.04638301,0.015904872,0.013011692,-0.02169287,0.06609495,0.001281605,-0.0386319,0.008580856,-0.023196781,-0.051521916,-0.024287393,0.037946414,0.017538348,-0.08999626,0.056038607,0.005985653,-0.077087134,-0.017734412,-0.016726438,-0.07091639,0.033817153,0.097845025,0.06801342,0.050628237,-0.028314864,4.245556e-33,0.0021287,0.0125103695,-0.013206471,-0.03965192,-0.013335913,0.0005220555,-0.07028979,0.0016184748,0.03487491,0.08061834,-0.03846773,0.045219634,0.009336522,0.02026614,0.09371414,-0.055034522,0.04831898,0.01756541,-0.05552749,-0.022807596,0.027295139,-0.0973802,0.06200302,-0.08696314,-0.026529856,0.027221821,0.019456413,0.04675033,0.071170874,-0.008072836,-0.051021487,-0.008975801,-0.017034544,-0.019168006,0.016669828,-0.015771754,0.02180714,-0.0029839962,0.018053483,0.030022776,0.07419231,0.06140005,0.01641616,-0.082768515,-0.044942595,0.049583275,-0.030517198,0.08424889,-0.029052408,0.010214648,-0.01835415,-0.05270805,-0.024527846,-0.029862272,-0.08639309,0.04361893,-0.0061462778,-0.019939266,0.0041033947,0.07787587,-0.009887108,-0.006980658,-0.0025699504,-0.073772736,-0.0797001,0.019332232,-0.031127557,0.06152139,0.020520423,0.03641249,0.054381836,0.067829415,-0.079444505,-0.032921318,0.08167585,-0.0649553,-0.029349769,0.103448294,-0.007927924,-0.057824105,0.0008945926,-0.027246214,0.047410652,-0.10825729,0.03356355,-0.0065216334,-0.04169108,0.002495255,-0.07485568,-0.07240473,-0.08180599,0.06627887,-0.04079136,0.062291186,-0.005419202,-4.09337e-33,-0.040879127,0.0763551,0.009158139,0.14790651,0.008522743,0.034250252,-0.047748625,0.022848124,0.11860272,-0.028786331,-0.047241054,-0.002973189,0.044527184,-0.0108844265,0.051719457,-0.003085973,0.0046497914,-0.0852576,0.0064914403,0.0120722605,-0.023152072,-0.04364208,-0.08796622,-0.043868616,-0.0049478086,0.06089402,-0.09657088,0.040607415,-0.087424815,-0.016219273,-0.013869956,-0.019415285,-0.07349808,-0.122032,-0.056880686,0.032208133,0.042177264,0.0024619335,-0.012241898,0.06777474,0.045526456,0.07715802,0.022118976,-0.01857999,0.034839895,0.037796848,-0.03776814,0.13808556,-0.12014811,0.053427022,0.035877995,0.08640297,-0.02079995,0.044744965,0.035564944,0.13006794,-0.049040824,0.0011257674,-0.04816877,0.012057502,0.0014941274,-0.024998851,-0.15106061,0.074804194,0.0016049353,0.038960006,0.017401338,-0.15213878,-0.05489176,0.038854834,0.090123676,0.007570397,-0.082009256,-0.04200821,0.008192786,-0.017546883,0.04487631,-0.032678004,0.035856795,0.030537287,-0.010181468,0.004923553,0.079189636,-0.02020354,-0.07239895,0.057368338,0.029857209,-0.093310475,0.0012145928,0.0009202488,-0.008450739,-0.019929444,-0.051459353,0.015153407,-0.0065549333,-5.843952e-08,-0.10145789,-0.065007314,-0.018752497,0.011540844,0.07597985,-0.05901718,0.023630887,0.09679008,0.0015145792,-0.14850524,0.10969818,-0.07908648,-0.07045703,0.032375608,0.059576534,0.054473076,0.021799657,0.047338836,-0.049959153,0.03822638,-0.004380839,0.06199897,0.016215887,0.013536538,0.08161747,0.012604737,0.04834163,0.06836895,0.017111354,-0.02453241,-0.03305545,-0.009757051,0.07313426,0.010639834,0.041152403,0.046172336,0.026234774,0.027470946,-0.03236439,0.0020706453,-0.03200763,0.06387689,-0.03231587,-0.057169076,0.056179814,0.0024582169,0.06290348,-0.049210876,0.016773725,-0.022748942,0.050103832,3.1068565e-05,-0.009602157,-0.05646874,0.022837719,-0.01630023,-0.037160575,-0.051712066,-0.005196361,0.029940173,-0.01692895,-0.018696355,-0.09442463,-0.004603027,9,9.5192995,35.28013,13
502,"we started off the class by discussing about outliers and how they can be found out using scatter plots and box plots. they can also be found out by descriptive statistics and line charts. however whether to actually ignore and drop the outliers or to perform some processing on them, depends upon the domain and thus required domain knowledge. sometimes, outliers can be hidden in the data, which can be observed by maybe rescaling the data or by dropping the true outliers and then observing the remaining data again. now we need to smoothen out our data and remove all the noise, in order to observe some trends in the actual data. thus, we perform data smothering, which could be done by various methods. we discussed about simple moving averages, where we consider a window around every data point and average the values in that window. the window width can be adjusted according to the level of smoothening. but in doing so, we need to take care about missing values as well, or else our smoothening algorithm might not work correctly. the higher the window size, the smoother our data becomes. we also have methods called exponential moving average or weighted moving average, which weighs the nearer points more as compared to the farther points. 
we then moved on to handling data where each column has a different scale. suppose we have a data where one column has very large values as compared to another column. this becomes a problem when we try to run a model like multiple regression, as the larger value data creates a very large maxima, which leads to all the other minimas getting overshadowed. hence gradient descent is not able to find the most optimum minima and the algorithm fails. hence we need to scale our data appropriately to prevent this. the most common method of scaling is to scale each column data to lie between 0 and 1. we then discussed that any algorithm which depends upon euclidean distances, like the k-means clustering algorithm, will be affected by the normalisation of the data. we could also perform an operation known as standardisation, which comes from the term â€˜standard normal distributionâ€™ i.e. n (0, 1). standardisation and normalisation do not change the shape of the distribution of the data.
the difference between the mean and the median of the data also gives us an idea about the skewness of the data, where a larger difference indicates more skewness. 
we then discussed about various other transformations like logarithmic transformation, where we de-emphasise the higher values. this transformation actually changes the shape of the distribution of the data. 
we then moved on to data imbalances, which is mainly used in the context of classification. it means that data from one class is highly under-represented as compared to the other classes. in such cases, we can either under-sample the majority class, or generate data belonging to the minority class. we could use smote algorithm, which creates synthetic samples based on linear interpolation between the existing samples. the hyper parameters for this algorithm are the number of neighbours we want and the smote percentage. we also have tomek links, which under sample the majority class data. ",-0.09617981,-0.034447495,0.079588525,0.053724412,-0.00033016756,-0.13254794,0.0040379413,-0.03346939,-0.036166135,-0.028163709,-0.038945004,0.04050183,0.07075934,-0.03796574,-0.02416074,-0.008060936,0.026703412,0.06706478,-0.049897827,-0.04525443,-0.067615494,-0.028958183,-0.095528714,0.0051405304,0.054447953,0.012932092,-0.006136261,-0.014471376,0.007905277,-0.03826386,-0.007497581,-0.016453074,-0.032658327,0.010612536,-0.045249384,-0.065478116,0.029451786,0.17192738,0.024440076,0.008960259,-0.04851925,0.027100844,0.01894077,-0.03007627,-0.01680464,-0.019826977,-0.028461138,-0.09114794,-0.062218923,0.015183375,-0.07670639,-0.022203518,-0.041560456,0.024252923,-0.038117096,-0.017658556,-0.022837996,-0.04731885,0.051586583,-0.0070495605,0.026529014,-0.03788482,0.026860561,0.004184417,0.0015917046,-0.044906154,0.072105326,0.060101666,0.061846834,0.1033396,0.009912915,0.051127058,-0.009869233,0.012921613,-0.03056407,-0.038395107,-0.051916428,-0.009669957,-0.0051695947,-0.034643084,0.049452204,-0.015350337,-0.05142983,-0.01444382,0.00035930055,-0.014241665,-0.014858867,0.010466329,-0.03993657,0.0021649157,-0.017338304,0.09599823,-0.07008725,0.020313317,0.09025963,0.040972672,0.051262073,0.00062216073,0.12063816,0.08928162,-0.009462392,0.026930455,-0.030341536,0.026034072,0.050949547,-0.053085912,0.04641944,0.038777668,-0.037072048,-0.013827246,-0.019225093,-0.023087578,-0.06265895,-0.025223471,0.049794775,-0.04473102,0.11236743,0.0025318875,-0.14205904,-0.02820451,-0.029199572,0.005853664,0.010486702,0.045803756,0.06744991,0.063948005,-0.057236075,4.490855e-33,-0.020575916,0.02578094,-0.03964312,-0.053039715,0.046457164,0.0034080583,-0.054159965,-0.037744056,0.052033905,0.11287876,0.0272985,0.02273005,0.008574332,-0.038200874,0.0809609,-0.096021585,0.039175082,0.021383751,-0.008860825,-0.014139599,0.0068131513,-0.08919846,0.018381622,0.0005227466,-0.004509006,0.049865536,0.06960765,0.051774144,0.058920126,-0.0021245074,0.06191133,0.0756634,-0.09039835,-0.03786937,0.02576882,-0.078134656,-0.013138709,-0.024303192,0.07450425,0.015295122,0.00073516066,0.03925492,-0.04130187,-0.019489076,-0.010749754,0.06153874,0.048021108,0.03149771,-0.017676027,-0.022135584,-0.013574923,-0.011303739,0.034698095,-0.014258695,-0.07617834,0.013601003,0.08361311,-0.088509955,-0.0028845984,0.035231803,-0.025769936,-0.06028248,-0.027066534,-0.04538232,-0.033767063,-0.018842915,-0.019978827,0.08274622,-0.04032925,-0.026547218,0.044416536,0.07888801,-0.06004364,0.0023896534,0.027991746,-0.019633763,0.028170647,0.07169171,-0.0409182,-0.10892232,-0.03700399,-0.0108929835,0.0074347216,-0.060387217,-0.023924548,-0.032126058,-0.028530816,0.023825388,0.007780035,-0.05398226,-0.09938008,-0.00965892,-0.03755115,0.05951599,-0.05897203,-4.009342e-33,-0.018844666,0.10448107,-0.025296733,0.10556995,0.03548679,0.09544138,-0.009554322,0.029023202,0.11926229,-0.07830997,-0.11125432,0.025950719,0.012602317,-0.01004668,0.04061535,-0.0052098986,0.038014974,-0.10630805,-0.048647,-0.005486724,-0.019633636,-0.043338668,-0.081999175,-0.014744217,0.021320395,0.071520336,-0.13171133,0.015055486,-0.0933878,0.0005807029,-0.03514234,-0.002086238,0.013462135,-0.16360755,-0.040508434,0.0017368149,0.035428893,-0.009384584,-0.048443343,0.021547468,0.0020062877,0.08643962,0.00016331238,0.016249713,0.051965695,0.10903439,-0.056020748,0.059301563,-0.06327408,0.06987792,0.032760695,0.08582683,-0.01191163,0.019243987,0.027680848,0.0940538,0.004355745,-0.034635726,-0.07688406,0.04606794,-0.020209808,-0.038148418,-0.11427767,0.020384206,-0.048562065,-0.0054937704,0.003237458,-0.078558594,-0.011757736,-0.010238244,0.0020338167,-0.042284183,-0.065861285,-0.040212747,0.043068707,-0.021637473,0.053479075,-0.06519402,0.041767348,0.07006444,0.03350021,-0.03444367,0.11912381,-0.01701158,-0.041010216,0.06336978,0.0036698612,-0.10002086,0.036631986,-0.018240543,-0.02889205,-0.035189573,0.021354292,-0.043029867,-0.034777664,-5.4280914e-08,-0.0754715,-0.046311107,0.025370684,0.031199077,0.06414797,-0.017673552,0.009527384,0.15043335,0.047990963,-0.10591583,0.05339807,-0.11465591,-0.042210273,0.012004815,0.08697743,0.008932068,0.033614848,0.04931535,-0.03316286,0.018489867,-0.041067842,0.054229382,0.0062766583,0.049928088,0.09314207,-0.013454254,0.065862894,0.04866149,0.03133024,0.0053614443,-0.041605953,-0.007165265,0.07819003,0.047800448,0.053159025,0.061831467,-0.014128187,0.020011662,-0.046458174,0.056118704,0.0036581699,0.0529541,-0.025492936,-0.012455656,0.011228301,0.0152005665,0.0039111204,-0.056774873,0.039804883,0.014440291,0.04252295,0.0006349602,0.016421493,0.014931934,0.029343285,-0.030996244,-0.012238123,-0.007898799,-0.0047405176,0.00084756856,-0.028498467,-0.019377427,-0.08062949,0.021543046,9,10.020183,35.39112,13
539,"today we learnt topic about data smoothening to reduce noise. one of the method used in data smoothening is moving averages. this method makes the data less noise and  helps in detecting the trend better , another method is exponential moving averages these are mainly utilised in time series here weighted averages are taken. its important to handle missing values and outliers before applying moving averages. few techniques to handle missing values are winzorisation , imputation, trimming, cap data and robust estimation. we then saw the importance of data scaling and its various methods like normalisation, standardisation and box-cox methods. all euclidian distance models are not immune to the data without data scaling. few models assume normal data distribution in such scenarios we have to use box-cox scaling method. standardisation and normalisation doesn't change the shape of the original distribution. log transformation is used when there is heteroscedasticity in the data that is variance keeps on changing across the dataset. log deemphasises large values. we later saw how to handle data imbalance which primarily occurs in clustering problems in this we have three options 
1) under sample the majority 
2)oversample the minority- that is duplication of the same minority data many times
3) produce synthetic data.
the first two methods are naã¯ve and not usually recommended.
to do the third method we use smote method which helps us in synthesising new data points for the minority class. it works on euclidean distance method it connects two minority points and forms a new point on the line (uses k-nearest neighbours method to do this) the data should be scaled before passing it on to smote, other methods used for synthetic data production are adasyn, borderline-smote. tomek links is a method used to under sample the majority class it deletes the majority points which are very close to the border of the minority points. usually smote and then tomek links are used to form a good dataset",-0.019278947,0.017710593,0.034952704,0.014304897,0.0038922203,-0.0512704,-0.0012686506,0.018338157,0.004977995,-0.019783799,0.038195822,-0.0068606846,0.006410859,0.026045984,-0.040011525,-0.07627998,-0.010856475,0.073295206,-0.1381774,-0.009745042,-0.04124911,0.041849595,-0.029857963,0.09387136,0.024220519,-0.010697856,0.033111837,-0.044807386,0.004495196,-0.030599644,-0.04115061,0.02685447,0.027110167,0.021077033,-0.06349354,-0.03142466,0.02334118,0.052440565,-0.037083164,-0.023343816,0.038468625,-0.01572896,-0.03499079,-0.041458823,-0.037589844,-0.03277907,0.0024102244,-0.039103113,-0.079476096,0.032133527,-0.010407171,0.015830506,-0.045404255,0.06824802,-0.054359887,-0.07309943,-0.0018221899,0.0056124837,0.062669925,-0.042244527,-0.0366067,0.015344431,0.0046315305,0.014520692,-0.038111985,-0.050776206,0.04127337,-0.0071377796,0.020770045,0.09905698,-0.016957724,0.01381103,-0.047863748,0.062313076,0.021825684,0.01742405,0.016004335,-0.003223567,0.013441279,0.0145452935,0.013402095,-0.0022290342,-0.00084005797,0.00269234,0.058352843,-0.013992218,-0.032430537,0.035319325,-0.009869151,-0.016431997,0.007920494,0.09376494,-0.013951209,0.047551867,0.08170153,0.0324722,-0.037051044,0.03719888,0.1092155,0.07083264,0.0065581384,0.08644966,-0.0151185,0.033559322,0.03217235,-0.040887944,0.07467429,-0.004211181,0.021371722,-0.009202844,-0.0094488105,0.038996417,-0.0513692,-0.039577343,0.03198275,-0.08980499,0.098192625,-0.03691104,-0.11825887,0.0394142,0.023768142,-0.01689684,0.0040057963,0.04641569,0.09408234,0.12698762,-0.045871016,6.873478e-33,-0.021141384,0.025731152,0.018576114,-0.06486386,-0.017561326,0.026602028,-0.105767876,-0.061630167,-0.010243807,0.006543188,0.0031656502,0.10697543,0.03800597,-0.07290164,-0.024672419,-0.044565804,0.048889264,0.060593892,0.004181769,-0.008942092,0.07111119,-0.07843917,0.03976626,0.036483336,-0.033663053,-0.0035399487,-0.0050324383,-0.019357273,0.058776766,0.0065841503,-0.008355613,-0.0034601325,-0.06019649,-0.027786931,-0.031314936,0.022563608,0.01331132,-0.06268287,0.00856491,0.026355917,-0.0030122679,0.053349398,-0.065353185,-0.0480377,0.042873863,0.009761285,0.10319596,0.024932336,-0.086334266,-0.035147313,0.040335156,-0.032828618,0.004818385,-0.10828043,-0.06120285,0.043110017,0.09053846,-0.10538367,0.008472765,0.02993518,-0.031704977,-0.1097314,-0.027437149,0.0056499206,0.0730896,-0.0049734237,0.0035416274,0.05478464,-0.11626169,-0.032344792,0.08227072,0.010898534,-0.052320648,0.101056755,-0.032607935,0.015180603,0.030134927,0.036999643,-0.03723823,-0.025385324,-0.07771297,0.0023627246,-0.013659179,-0.048905514,-0.006268672,-0.035681374,-0.0075371494,-0.0074412674,-0.025645768,-0.0063547604,-0.025564423,-0.015166271,-0.03955456,0.007521422,-0.042367954,-5.4775374e-33,-0.04372817,-0.00030346907,-0.07141262,0.07337766,0.027741695,0.01839892,0.0026526763,0.04709203,0.077621326,-0.1303564,-0.022993205,-0.04829737,0.05447807,-0.03564666,-0.04304613,0.008037452,0.08743614,-0.0033943718,0.012677708,-0.0007407705,0.06357681,-0.0232776,-0.11401409,0.07102889,-0.0152440285,0.00015980787,-0.15321878,0.021961875,-0.03358857,-0.036221128,-0.06023,-0.037452936,0.054973997,-0.11311603,-0.04548768,0.06136169,-0.012402601,0.0028074773,-0.008612588,0.031131241,-0.03626349,0.08491818,-0.046654258,-0.02031731,0.056319855,0.026075022,-0.015607551,-0.00034903633,-0.03533324,0.034415085,0.015071197,0.037790418,0.020489533,0.10928479,-0.035000592,0.11212321,-0.00394938,-0.009264416,-0.08808599,0.019745395,0.019844744,-0.0408161,-0.037656106,-0.024856057,-0.095295906,-0.041725114,0.05379596,-0.06968783,-0.05228761,-0.025696982,-0.013851716,-0.03220793,-0.012221079,-0.0163328,-0.020139085,-0.0551406,0.08630918,-0.042464156,0.035540074,0.026577882,0.022386493,-0.039661124,0.06338525,-0.01726967,-0.022446966,0.057418868,0.08556565,-0.081670545,0.008872359,0.026795223,-0.09408172,-0.03206663,-0.007903196,0.0010172216,-0.047026444,-5.7257925e-08,-0.076434635,0.024486681,-0.0052506337,0.06817197,0.025396373,-0.0027199595,-0.034902327,0.14298657,-0.02824976,-0.031131694,0.036651365,-0.08708989,-0.024301656,0.02455761,0.09281326,0.054202903,0.076240025,0.025918568,-0.038143303,0.05365991,-0.08952159,0.09337281,-0.04702834,0.004015489,0.07811854,-0.04341022,0.057513587,0.051015347,0.06351294,-0.06896701,-0.07382151,0.018638384,0.09000094,0.050385877,-0.04091485,0.02806316,0.0140854055,0.09795013,-0.08237874,0.060623724,0.09328839,0.036224183,-0.012358104,-0.04255541,0.025429327,0.054273833,0.0017260192,0.03896317,0.03069632,-0.0138119105,0.12424639,-0.019210795,0.04974059,0.059328005,0.115805954,0.020879798,0.04586565,0.005647003,0.061227296,-0.0039567924,0.020919856,-0.14528228,-0.08607767,-0.0008780935,9,12.259113,34.951538,13
605,"started with identifying and eliminating outliers.
covered ways to reduce noise using smoothing methods like sma and ema, noting that bigger windows give smoother results.
explained how gradient descent works for optimization.
introduced scaling methods and transformations like box-cox and log transforms to adjust variance and prepare data for time series.
discussed maximum likelihood estimation (mle).
mentioned the kepler exoplanet dataset.
talked about smote to manage unbalanced datasets.",-0.05196952,0.0112024145,0.05580017,0.016710566,-0.014309338,-0.11293488,-0.02650656,-0.002020174,-0.10291404,-0.00021149653,-0.036078926,0.02287288,-0.008454976,-0.03043135,-0.059654213,0.01728464,0.009564929,0.09217332,-0.067552105,-0.08169768,-0.112805754,0.04737412,-0.056235865,0.062703766,0.029901925,-0.05485852,-0.04757535,-0.014970428,-0.03338428,-0.03146349,0.07337766,0.029150506,0.021107405,-0.029234285,-0.039254237,-0.023559906,0.0016827424,0.002059801,-0.035630673,-0.05552295,0.0025471877,-0.050862618,-0.030253332,-0.040203635,0.016445832,-0.048420817,0.035992563,-0.050810542,0.0016659695,-0.03754168,-0.054511026,-0.09966777,-0.033024825,-0.023557242,-0.046227004,-0.051079243,-0.017750653,-0.041043572,0.08853984,-0.109154865,0.040559594,-0.08823859,-0.06615727,0.023593087,-0.030732563,-0.07827578,0.054260835,0.018066864,0.07485286,0.016190648,-0.017234523,0.07499615,-0.03182806,0.024837684,0.004788543,0.042398006,0.050876748,0.010694073,0.06505021,0.0055318517,0.054056197,0.03544015,-0.05919882,0.05225865,0.05570732,-0.08140361,-0.037336074,0.009603295,-0.04675435,0.0073138173,0.026922397,0.05480657,-0.023851734,0.0032009294,-0.029371152,-0.002597475,0.01345503,-0.090971656,0.06991656,0.09401515,-0.017766684,0.06701281,-0.014358751,0.023725744,-0.0060443133,-0.053719807,0.11338294,0.05171429,-0.0133192055,-0.07222756,-0.057360113,0.04368529,-0.023085536,-0.081628256,0.05430728,-0.0150794005,0.07195061,0.051235016,-0.09169192,0.058328398,-0.07282749,-0.039287277,-0.021329548,0.024200806,0.080849424,0.11451691,-0.0999355,6.298406e-33,0.00901967,0.055218853,0.02406657,-0.056109477,0.049781494,-0.011079583,-0.058590688,-0.036342863,-0.025013363,0.034758937,-0.0030454744,0.020764055,-0.028563013,-0.01460699,0.058400128,-0.12480886,-0.028770993,0.06606712,-0.027989374,-0.01787,0.0068986695,-0.12606297,0.041800622,-0.007868978,0.048399866,0.059501696,0.12620124,0.012824097,0.04105758,0.037570685,-0.001476013,0.022367679,-0.083906636,0.018794209,-0.0025276926,-0.0020620858,-0.0504434,-0.029514708,0.039283063,0.053774953,-0.022350216,0.0546743,0.016673386,-0.06678768,-0.0029924354,0.07335167,0.033986233,0.02352923,0.059591465,0.005767537,-0.024572346,-0.025385894,-0.027536355,-0.014309622,-0.011362957,0.047235373,0.0646605,-0.07448623,-0.015681636,0.046260647,-0.002605867,-0.06250572,-0.03650743,-0.07600323,-0.04036004,-0.028365545,0.025519118,0.025415111,-0.10047249,0.014331409,0.04051588,0.038165405,0.032568008,0.0050696176,0.036498718,-0.0076655583,0.030154949,0.055300865,-0.022631431,-0.024808807,-0.06975596,0.07715842,0.015236617,-0.0766793,-0.015937353,-0.064745896,-0.01537094,0.014468001,0.010928994,-0.010258732,-0.15546681,-0.016975926,0.019504935,0.048462316,-0.08727912,-4.422772e-33,-0.07126758,0.07062853,0.0068388013,0.04803648,0.060576685,0.056442086,-0.07209387,0.038758595,0.038093448,-0.0058283214,-0.06649192,0.015011543,0.056623343,-0.04393054,0.036791567,-0.038683042,0.059403937,-0.08165775,0.010456618,-0.00955908,0.002483786,-0.0096344445,-0.028201448,-0.012340312,-0.0029648237,0.0136757605,-0.10638277,0.07604321,-0.04346682,-0.052134894,-0.08530847,0.03608959,0.0044507235,-0.030508593,-0.019203357,0.046487432,0.006485104,-0.012269012,0.008715468,0.09122129,0.010746196,0.084456325,0.030712277,-0.03365631,-0.0027361643,-0.0006113841,-0.02954063,0.027944425,-0.020949125,0.056541245,0.03469464,0.023408443,0.0020031598,0.06376658,0.03319355,0.028851697,0.00956045,-0.012336157,0.00025674579,0.027440833,-0.039750833,-0.032485098,-0.08358859,0.038795527,-0.042079307,-0.043061443,0.07160022,-0.008438033,-0.04614739,0.02128425,-0.015936041,-0.006555656,-0.027979571,0.022765724,0.0010051028,-0.057943232,-0.042037,-0.079819284,0.025357276,0.0005120353,0.06089288,-0.043399855,0.07466267,0.059554514,0.0055989446,0.106765024,0.059696212,-0.07572519,0.027788203,0.004794696,-0.08586107,-0.012568608,0.035857443,0.051227503,-0.018915275,-3.8712308e-08,-0.044939253,-0.057641782,0.02385705,0.037897978,0.011133239,-0.04169799,-0.02668161,0.1934436,-0.0041199382,-0.032935552,0.068397015,-0.07821175,-0.01759508,0.011075124,0.061752655,0.03670407,0.04548723,0.12286922,-0.0495761,0.03771101,0.003949125,0.13600698,0.040516,0.0038876834,0.11946368,-0.05854795,0.06704243,0.090117514,0.081972025,-0.04065382,-0.06580469,0.02706755,0.049203113,-0.011477418,-0.056913245,0.05655905,0.0076947426,0.019469263,-0.024904119,0.06795603,0.011691712,0.077346176,-0.045962185,-0.019084156,-0.037387732,0.07328825,-0.008398429,-0.043512076,-0.009623475,-0.001789203,0.04517742,0.02702052,0.06134786,0.045882933,0.05523806,0.022537816,0.024555992,-0.038792577,0.018043611,0.033078816,0.0036368459,-0.02690842,-0.110107854,0.048175868,9,11.833223,33.44102,13
612,"the session covered key steps in data preprocessing, starting with outlier analysis and removal. it then explored the kepler exoplanet dataset before discussing the synthetic minority over-sampling technique (smote) for handling imbalanced data. noise reduction techniques like simple moving average (sma) and exponential moving average (ema) were introduced, where a larger window size results in more smoothing. gradient descent was discussed as an optimization technique, followed by normalization methods, including the box-cox and logarithmic transformations, which help stabilize variance and normalize data before time series analysis. finally, maximum likelihood estimation (mle) was covered.

",-0.039716132,0.05723565,0.07808282,-0.024192031,0.04030419,-0.10982426,-0.03359278,-0.0019090773,-0.015139546,0.035673726,-0.04081305,0.050740544,-0.03549091,-0.06839728,-0.04198177,0.006145746,-0.022596968,0.058193874,-0.04359828,-0.020212777,-0.07510879,0.0770397,-0.07254272,0.080609895,0.033205952,-0.051121604,-0.05693811,-0.022922076,-0.006563145,0.0028017526,0.028849082,0.10843937,0.0617876,0.0033748017,-0.06834781,-0.048604976,-0.048462633,-0.016979236,-0.015411222,-0.0143199675,0.03309245,-0.05550496,-0.049358573,-0.02114884,0.00020514267,-0.004121173,-0.008283027,-0.033273645,0.018302238,0.020550817,-0.056505732,-0.0027213995,-0.053768955,0.013731172,-0.04535592,-0.07073001,-0.02174062,-0.037687343,0.11634737,-0.061939135,0.003685562,-0.031163614,-0.022450851,0.043639302,-0.050105907,-0.091253094,0.045896772,-0.0019768737,0.066703565,-0.006676228,-0.04677143,0.08665026,-7.5778e-05,-0.0054675415,0.0040679076,0.064866304,0.0063535115,0.011236081,0.024230594,-0.0368287,0.089085974,0.021376096,-0.029105993,0.00062911207,0.07631734,-0.022351788,-0.041189447,0.026931087,0.011897419,0.016063582,-0.03084789,0.056791633,0.012475807,0.025859127,-0.047049318,0.039330535,-0.016949337,-0.014863094,0.08909763,0.08813686,-0.0072500715,0.08193478,-0.0142048765,0.04838506,-0.021492092,-0.09161947,0.10084507,0.0047371793,-0.033398233,-0.09421108,-0.05136116,0.004940505,0.012325821,-0.07732584,-0.0021147558,-0.033910245,0.09236241,0.076021194,-0.12817681,0.046947524,-0.043889627,-0.018035173,-0.00079204387,0.046374924,0.08230388,0.08360582,-0.07100996,5.4159187e-33,0.026505744,0.014856037,0.026569953,-0.056093607,-0.00046873666,-0.0056735906,-0.087577865,-0.0274786,0.0080495505,0.052251954,0.041316226,-0.034781784,-0.033997886,-0.014239395,-0.015042283,-0.033314425,-0.008489712,0.046568155,-0.03911584,-0.04547703,-0.015559921,-0.119762525,0.014943807,-0.0008033407,0.00089097366,0.10519912,0.03791579,-0.0065782904,0.012741266,0.019624092,0.015159643,-0.0069370056,-0.10444747,0.009873069,-0.013746345,0.041112997,0.0023248824,-0.02188874,0.017776228,0.11642612,-0.030274088,0.06836882,0.025837405,-0.09301703,-0.017658057,0.0405069,0.07520153,-0.0016687898,0.052835893,0.07206394,-0.03224995,-0.067828424,-0.024596708,-0.04266879,-0.06214274,0.091643795,0.07914974,-0.031139756,-0.04363265,0.03493186,-0.06072932,-0.053363375,-0.006248251,-0.03659108,-0.017528838,-0.036968835,0.02519756,0.015930718,-0.10106247,0.03114846,0.05333298,0.007994496,-0.015222491,0.07683314,-0.0033148741,0.024240533,0.07433394,0.085070916,0.00036870278,-0.022745255,-0.047463626,0.06826883,-0.0002876518,-0.10693061,-0.018874096,-0.06740497,0.0014765796,0.03273937,0.06570741,-0.05307981,-0.11455742,-0.06878443,-0.07130495,-0.000973378,-0.102556765,-4.4689963e-33,-0.100946575,0.07805992,-0.008301552,0.04987274,0.022373179,0.046480153,0.0052567245,0.021617282,0.03802881,-0.0047683218,-0.047834154,0.0009773712,0.07577637,-0.08308517,0.035521135,-0.019269692,0.043237817,-0.034147084,-0.02997925,-0.008724851,-0.0044824253,-0.041867595,0.039588366,0.012860299,0.0037478595,0.019249676,-0.04942659,0.08100135,0.007512935,-0.061051585,-0.091121875,0.009178786,-0.004347395,0.02461956,-0.030624716,0.046722226,-0.020673528,0.0368752,0.02045244,0.06717832,-0.029899726,0.09443765,-0.034599196,-0.052853618,-0.01452561,-0.009906365,-0.021431863,0.033421215,0.0388046,-0.002513953,0.0012621916,0.037465595,0.06261654,0.10732993,0.047701042,0.013983354,0.052681103,0.041517872,-0.02480654,-0.0039460296,0.023359146,0.015480158,-0.025666991,-0.05941336,-0.044953227,-0.04527519,0.08268361,-0.014277252,-0.0675561,0.0043618404,-0.010611761,-0.00045480957,-0.014920382,0.0004145841,0.0070527727,-0.09984527,-0.02911815,-0.071725406,0.028526928,-0.027728524,0.0020031265,-0.07393045,0.06675839,0.0765853,0.004370675,0.0883238,0.007359358,-0.05140059,0.019620262,-0.026446894,-0.08419869,0.04707109,0.032431483,0.06704829,-0.046456255,-4.658959e-08,0.0012456216,-0.0364497,0.01707481,0.06596843,0.05645287,-0.02919431,-0.0691196,0.15319172,-0.016149895,-0.073724866,0.06791593,-0.080434926,0.023626534,-0.033229135,0.048315365,0.03332119,0.031838324,0.02326225,-0.027501028,0.08051751,-0.03695347,0.09418794,-0.0005366282,-0.049649317,0.16188893,-0.060604863,0.037797835,0.086578496,0.058522705,-0.055487774,-0.07745913,0.028988026,0.019077368,0.02025779,-0.038915634,0.01602968,0.022229137,0.040909603,-0.046417836,0.0965282,-0.005654874,0.072844334,-0.08521797,-0.021815127,-0.04016268,0.06877836,-0.022374846,-0.05002982,0.013640941,-0.05079849,0.08037761,0.016808927,0.10332334,0.06101179,0.015962638,0.025685351,0.03752251,0.019890733,0.00641154,-0.006908507,-0.007711407,-0.08556285,-0.0763945,0.009593549,9,12.151884,33.083275,13
614,"in today's lecture we talked about moving averages for data smoothing. in it we discussed about simple moving average(sma). in it higher window size implies more smoothing. we also talked about term imputation. in it we focus on a single column (aqi). linear imputation seems more natural.what are better methods ? there are exponential moving average that weigh nearly samples more while averages. also utilized in time series forecasting to understand trends seasonality in the data and forecast it. we then talked about normalization that forces column values to lie between 0 and 1 , there avoiding the problem of overshadowing. clustering algorithms are greatly influenced by normalizing of data . thus same input of data one with normalization and other without it will give different cluster plots.
then we talked about data imbalance.algorithm trained on data imbalance can be biased towards the majority class( that is class with higher frequency).
then we discussed about t-nse plot. it  converts multidimensional data into lower dimensional data.",-0.05831887,-0.021562798,0.04928792,0.03271167,0.038888987,-0.044896074,0.0056445184,0.0414561,0.015500753,0.03568738,0.0025232048,0.03841113,0.0015690177,-0.010102771,-0.067824736,-0.0060489085,0.0051364214,0.004037685,-0.060332935,-0.05791184,-0.0016246737,0.005113732,-0.022759361,0.025658824,0.049286958,0.036012344,-0.04009677,-0.002668712,0.010124209,0.0016531564,-0.12500319,0.047345124,0.07036037,0.018236222,-0.14548,-0.027098618,0.05464625,0.10031537,-0.008993783,-0.0016757285,0.0038379296,0.01814098,-0.024188884,-0.016978161,-0.052689202,0.009815649,-0.027534202,-0.08631277,-0.054612793,0.031076267,-0.10512544,0.04207212,-0.104917146,0.0815885,-0.031309552,-0.0028136543,-0.092560925,-0.006527163,0.104199335,-0.0023960115,-0.07897641,0.010040416,0.06990761,0.026618104,0.021118538,-0.023290338,0.029931352,0.08204658,0.05459825,0.042823363,-0.032829516,0.009448461,-0.02597645,0.011674809,-0.0071006026,-0.0093823895,0.029733052,0.014379773,-0.026484285,0.024936639,-0.008996267,-0.032367166,-0.013192864,0.0046091345,0.005452151,0.0097179115,-0.06926537,-0.008530747,-0.019749928,-0.020437416,-0.023937587,0.10142974,-0.02264915,-0.030676436,0.07150123,0.05561919,-0.01731732,-0.021260418,0.09525774,0.090574294,0.0056122667,0.021447653,0.0016562894,0.0430488,0.0070804753,-0.07881424,0.10674969,-0.048212394,-0.015798902,0.03670137,-0.01962886,0.029237708,-0.07395507,-0.029329704,0.017505938,-0.0726499,0.08793994,0.030166509,-0.106025994,0.03482309,-0.0210469,-0.022964198,0.038384475,0.0616941,0.07193586,0.06712922,-0.043666955,4.13236e-33,-0.027424233,-0.0042428747,-0.009074682,-0.021811772,-0.049055126,-0.020308247,-0.028643584,-0.009610561,0.1009273,0.06403685,0.043677155,0.10892116,0.0062940926,0.0060243057,0.038768552,-0.078887075,0.07125063,0.021285376,-0.009562204,-0.056119077,0.07471961,-0.025591858,0.04282823,-0.021967256,-0.00484766,-0.005566374,0.041493744,-0.025357418,0.07750448,-0.02013736,0.010216534,0.04750958,-0.08304935,-0.07666503,-0.02048836,0.016909327,-0.008727748,-0.0068552587,0.04031188,0.027312325,-0.044096023,0.058853768,0.008204051,-0.10531081,0.062273163,0.012621251,0.06327657,0.013542418,-0.03919842,0.005980237,-0.025248665,-0.09816244,0.041221004,-0.016505895,-0.052785974,0.048780154,0.07938273,-0.098005824,-0.04091524,0.016150743,-0.0482638,-0.08880879,-0.04057035,-0.044434015,-0.0412562,-0.024024518,-0.057280254,0.072106145,-0.021840334,-0.0019300325,0.08278402,0.021014709,-0.07885614,0.049078878,0.009513815,0.0014511817,-0.01821227,0.046285868,-0.123668805,-0.059449214,-0.017480273,0.0171925,-0.023322538,-0.08477133,0.026122259,-0.0135786105,0.045273963,0.025153026,-0.016630195,-0.087239735,-0.060694467,-0.004057247,-0.056423053,0.07610094,-0.045446984,-4.075908e-33,-0.020462824,0.04283287,-0.006412904,0.12647255,-0.009528176,0.05493629,0.011819164,0.011955771,0.11667548,-0.08528784,-0.066481374,-0.041434925,0.10803037,-0.02559006,-0.00906094,-0.012890715,0.04774045,-0.06227265,0.0017514386,-0.04943995,-0.018245935,-0.020258503,-0.05704316,0.024199676,0.008854488,0.024438431,-0.11128133,0.049152773,-0.107963726,-0.03187617,-0.060953066,-0.08761107,0.034640614,-0.06795545,-0.05959602,0.054903675,-0.0050711166,0.035518557,-0.056886185,0.016095374,0.01201026,0.055774957,0.031454615,0.0123382285,0.011455242,0.087961026,-0.062405173,0.017230624,-0.0249364,0.011928872,0.0036115502,0.028335264,-0.015831914,-0.013144713,-0.0109768845,0.10351947,0.028754631,-0.0048199855,-0.08800103,-0.008308882,0.044053152,-0.022790806,-0.07215042,-0.028552936,-0.11880989,-0.032284927,0.10739726,-0.108072706,-0.05061576,-0.013996385,-0.008225283,0.03175235,-0.014081247,-0.029950682,0.031664673,-0.03273758,0.09321754,-0.035610914,-0.016172238,0.013881645,0.007468614,-0.031184098,0.0185207,-0.0717853,-0.03926219,0.10192384,0.06262851,-0.1200855,0.010379522,-0.0019880603,-0.03568788,-0.030664856,-0.011898842,-0.0031559407,-0.059106264,-5.4712046e-08,-0.022462588,0.0023457303,0.02742393,0.020431064,0.027417809,-0.034015194,-0.00610031,0.09788146,0.028101383,-0.03646808,0.02993731,-0.05931116,-0.08034753,0.027268907,0.06703157,0.05046964,0.012771777,0.028203797,-0.0070930454,0.036476206,-0.032889657,0.022781843,-0.053190295,0.02327261,0.08799844,-0.020811811,0.039274693,0.018585756,0.027009038,0.0007522803,-0.09779594,0.031834625,0.101656534,0.024414377,-0.023920774,0.015455183,0.08177734,0.055421293,-0.11098898,0.0131959785,0.07625534,0.061206225,0.010654574,-0.06455019,0.050401345,0.03166815,0.024741134,0.0034350364,0.0818045,-0.019044535,0.11799385,-0.026554935,0.036964536,0.035991848,0.08106048,0.0074820505,0.02752788,-0.02023095,0.05196215,0.01783149,0.029649066,-0.08350013,-0.104480304,0.016808307,9,10.999694,35.388176,13
