SerialNo,Session_Summary,BERT_Feature_0,BERT_Feature_1,BERT_Feature_2,BERT_Feature_3,BERT_Feature_4,BERT_Feature_5,BERT_Feature_6,BERT_Feature_7,BERT_Feature_8,BERT_Feature_9,BERT_Feature_10,BERT_Feature_11,BERT_Feature_12,BERT_Feature_13,BERT_Feature_14,BERT_Feature_15,BERT_Feature_16,BERT_Feature_17,BERT_Feature_18,BERT_Feature_19,BERT_Feature_20,BERT_Feature_21,BERT_Feature_22,BERT_Feature_23,BERT_Feature_24,BERT_Feature_25,BERT_Feature_26,BERT_Feature_27,BERT_Feature_28,BERT_Feature_29,BERT_Feature_30,BERT_Feature_31,BERT_Feature_32,BERT_Feature_33,BERT_Feature_34,BERT_Feature_35,BERT_Feature_36,BERT_Feature_37,BERT_Feature_38,BERT_Feature_39,BERT_Feature_40,BERT_Feature_41,BERT_Feature_42,BERT_Feature_43,BERT_Feature_44,BERT_Feature_45,BERT_Feature_46,BERT_Feature_47,BERT_Feature_48,BERT_Feature_49,BERT_Feature_50,BERT_Feature_51,BERT_Feature_52,BERT_Feature_53,BERT_Feature_54,BERT_Feature_55,BERT_Feature_56,BERT_Feature_57,BERT_Feature_58,BERT_Feature_59,BERT_Feature_60,BERT_Feature_61,BERT_Feature_62,BERT_Feature_63,BERT_Feature_64,BERT_Feature_65,BERT_Feature_66,BERT_Feature_67,BERT_Feature_68,BERT_Feature_69,BERT_Feature_70,BERT_Feature_71,BERT_Feature_72,BERT_Feature_73,BERT_Feature_74,BERT_Feature_75,BERT_Feature_76,BERT_Feature_77,BERT_Feature_78,BERT_Feature_79,BERT_Feature_80,BERT_Feature_81,BERT_Feature_82,BERT_Feature_83,BERT_Feature_84,BERT_Feature_85,BERT_Feature_86,BERT_Feature_87,BERT_Feature_88,BERT_Feature_89,BERT_Feature_90,BERT_Feature_91,BERT_Feature_92,BERT_Feature_93,BERT_Feature_94,BERT_Feature_95,BERT_Feature_96,BERT_Feature_97,BERT_Feature_98,BERT_Feature_99,BERT_Feature_100,BERT_Feature_101,BERT_Feature_102,BERT_Feature_103,BERT_Feature_104,BERT_Feature_105,BERT_Feature_106,BERT_Feature_107,BERT_Feature_108,BERT_Feature_109,BERT_Feature_110,BERT_Feature_111,BERT_Feature_112,BERT_Feature_113,BERT_Feature_114,BERT_Feature_115,BERT_Feature_116,BERT_Feature_117,BERT_Feature_118,BERT_Feature_119,BERT_Feature_120,BERT_Feature_121,BERT_Feature_122,BERT_Feature_123,BERT_Feature_124,BERT_Feature_125,BERT_Feature_126,BERT_Feature_127,BERT_Feature_128,BERT_Feature_129,BERT_Feature_130,BERT_Feature_131,BERT_Feature_132,BERT_Feature_133,BERT_Feature_134,BERT_Feature_135,BERT_Feature_136,BERT_Feature_137,BERT_Feature_138,BERT_Feature_139,BERT_Feature_140,BERT_Feature_141,BERT_Feature_142,BERT_Feature_143,BERT_Feature_144,BERT_Feature_145,BERT_Feature_146,BERT_Feature_147,BERT_Feature_148,BERT_Feature_149,BERT_Feature_150,BERT_Feature_151,BERT_Feature_152,BERT_Feature_153,BERT_Feature_154,BERT_Feature_155,BERT_Feature_156,BERT_Feature_157,BERT_Feature_158,BERT_Feature_159,BERT_Feature_160,BERT_Feature_161,BERT_Feature_162,BERT_Feature_163,BERT_Feature_164,BERT_Feature_165,BERT_Feature_166,BERT_Feature_167,BERT_Feature_168,BERT_Feature_169,BERT_Feature_170,BERT_Feature_171,BERT_Feature_172,BERT_Feature_173,BERT_Feature_174,BERT_Feature_175,BERT_Feature_176,BERT_Feature_177,BERT_Feature_178,BERT_Feature_179,BERT_Feature_180,BERT_Feature_181,BERT_Feature_182,BERT_Feature_183,BERT_Feature_184,BERT_Feature_185,BERT_Feature_186,BERT_Feature_187,BERT_Feature_188,BERT_Feature_189,BERT_Feature_190,BERT_Feature_191,BERT_Feature_192,BERT_Feature_193,BERT_Feature_194,BERT_Feature_195,BERT_Feature_196,BERT_Feature_197,BERT_Feature_198,BERT_Feature_199,BERT_Feature_200,BERT_Feature_201,BERT_Feature_202,BERT_Feature_203,BERT_Feature_204,BERT_Feature_205,BERT_Feature_206,BERT_Feature_207,BERT_Feature_208,BERT_Feature_209,BERT_Feature_210,BERT_Feature_211,BERT_Feature_212,BERT_Feature_213,BERT_Feature_214,BERT_Feature_215,BERT_Feature_216,BERT_Feature_217,BERT_Feature_218,BERT_Feature_219,BERT_Feature_220,BERT_Feature_221,BERT_Feature_222,BERT_Feature_223,BERT_Feature_224,BERT_Feature_225,BERT_Feature_226,BERT_Feature_227,BERT_Feature_228,BERT_Feature_229,BERT_Feature_230,BERT_Feature_231,BERT_Feature_232,BERT_Feature_233,BERT_Feature_234,BERT_Feature_235,BERT_Feature_236,BERT_Feature_237,BERT_Feature_238,BERT_Feature_239,BERT_Feature_240,BERT_Feature_241,BERT_Feature_242,BERT_Feature_243,BERT_Feature_244,BERT_Feature_245,BERT_Feature_246,BERT_Feature_247,BERT_Feature_248,BERT_Feature_249,BERT_Feature_250,BERT_Feature_251,BERT_Feature_252,BERT_Feature_253,BERT_Feature_254,BERT_Feature_255,BERT_Feature_256,BERT_Feature_257,BERT_Feature_258,BERT_Feature_259,BERT_Feature_260,BERT_Feature_261,BERT_Feature_262,BERT_Feature_263,BERT_Feature_264,BERT_Feature_265,BERT_Feature_266,BERT_Feature_267,BERT_Feature_268,BERT_Feature_269,BERT_Feature_270,BERT_Feature_271,BERT_Feature_272,BERT_Feature_273,BERT_Feature_274,BERT_Feature_275,BERT_Feature_276,BERT_Feature_277,BERT_Feature_278,BERT_Feature_279,BERT_Feature_280,BERT_Feature_281,BERT_Feature_282,BERT_Feature_283,BERT_Feature_284,BERT_Feature_285,BERT_Feature_286,BERT_Feature_287,BERT_Feature_288,BERT_Feature_289,BERT_Feature_290,BERT_Feature_291,BERT_Feature_292,BERT_Feature_293,BERT_Feature_294,BERT_Feature_295,BERT_Feature_296,BERT_Feature_297,BERT_Feature_298,BERT_Feature_299,BERT_Feature_300,BERT_Feature_301,BERT_Feature_302,BERT_Feature_303,BERT_Feature_304,BERT_Feature_305,BERT_Feature_306,BERT_Feature_307,BERT_Feature_308,BERT_Feature_309,BERT_Feature_310,BERT_Feature_311,BERT_Feature_312,BERT_Feature_313,BERT_Feature_314,BERT_Feature_315,BERT_Feature_316,BERT_Feature_317,BERT_Feature_318,BERT_Feature_319,BERT_Feature_320,BERT_Feature_321,BERT_Feature_322,BERT_Feature_323,BERT_Feature_324,BERT_Feature_325,BERT_Feature_326,BERT_Feature_327,BERT_Feature_328,BERT_Feature_329,BERT_Feature_330,BERT_Feature_331,BERT_Feature_332,BERT_Feature_333,BERT_Feature_334,BERT_Feature_335,BERT_Feature_336,BERT_Feature_337,BERT_Feature_338,BERT_Feature_339,BERT_Feature_340,BERT_Feature_341,BERT_Feature_342,BERT_Feature_343,BERT_Feature_344,BERT_Feature_345,BERT_Feature_346,BERT_Feature_347,BERT_Feature_348,BERT_Feature_349,BERT_Feature_350,BERT_Feature_351,BERT_Feature_352,BERT_Feature_353,BERT_Feature_354,BERT_Feature_355,BERT_Feature_356,BERT_Feature_357,BERT_Feature_358,BERT_Feature_359,BERT_Feature_360,BERT_Feature_361,BERT_Feature_362,BERT_Feature_363,BERT_Feature_364,BERT_Feature_365,BERT_Feature_366,BERT_Feature_367,BERT_Feature_368,BERT_Feature_369,BERT_Feature_370,BERT_Feature_371,BERT_Feature_372,BERT_Feature_373,BERT_Feature_374,BERT_Feature_375,BERT_Feature_376,BERT_Feature_377,BERT_Feature_378,BERT_Feature_379,BERT_Feature_380,BERT_Feature_381,BERT_Feature_382,BERT_Feature_383,kmeans_cluster,TSNE_1,TSNE_2,agglo_cluster
4,"we first looked at all the summaries and observed from the graph that number of people who are submitting the summaries are decreasing with the session and the average number of words in the summaries are increasing.
then we took an example where the function looked similar to sine curve and with one feature x1, we engineered other features as x1â²,x1â³,x1â´.this is called polynomial regression.from this we obtained a p-value.then we added another feature which was sin(x1). the p value obtained in second case was found less than that of first case which suggested that the sine feature that was added is significant for this model.however, adding large number of features such that they do not give a better estimation beyond a certain point is not preferred and will decrease the adjusted râ² values.also,if a single model can represent the data good enough then it should always be chosen over having multiple models.
then we discussed about parametric method like neural network. in this there is an input layer which takes in the features and then another layer which performs computations. if computations are performed in multiple layers it is referred to as deep learning. input layer is connected to computational layers through links which represent the degrees of freedom. increasing the degrees of freedom can lead to overfitting.",-0.04405212,-0.0034452057,-0.017719746,0.057968196,0.0511854,0.09212551,-0.037055537,0.04202647,0.099095866,-0.029097041,-0.02637489,0.030219093,0.053225696,0.058164347,-0.023032326,0.003917955,0.065183155,0.045428284,-0.07317091,-0.10248007,0.09446167,0.00693828,0.017697077,-0.0109801255,0.053533398,0.0015523175,-0.011252336,-0.0030467317,0.006884471,-0.011233256,-0.0010890668,0.1012489,0.049222115,0.027820455,-0.09916091,-0.037022542,-0.11571191,0.07102915,0.016301539,-0.04745824,0.00064455904,-0.060736053,0.0026745726,-0.0042658267,0.12840267,-0.00086545316,-0.010953211,-0.025074549,0.0035381983,-0.028910186,-0.03975173,0.024241457,-0.062757954,0.023865594,-0.02132604,-0.046525754,-0.047790047,-0.04037163,0.025665376,-0.034798093,0.042852815,-0.02826356,-0.034982614,0.01659607,0.07149029,-0.013691676,0.027402278,-0.0019142446,-0.041828487,0.09676333,0.035815712,0.137742,-0.06477631,-0.023460332,-0.022156568,-0.025461975,0.015503406,0.053311393,0.03468037,-0.04767168,0.028799532,-0.016766794,0.020984951,-0.066018194,0.0272856,0.020445284,-0.01285548,-0.052943695,-0.064694375,-0.00017119142,-0.03786368,-0.033240993,0.008260669,-0.028795304,-0.035537206,0.044894204,0.007659353,-0.040286697,0.0051426846,0.12185827,-0.005446385,0.052552875,0.022350723,-0.093219325,0.055723324,-0.022758003,0.034026798,-0.013840071,-0.009856482,-0.014112044,-0.019506563,0.07497147,-0.088233076,0.017007476,0.06781181,-0.0053154286,0.036773827,0.008659092,-0.0064280187,0.05161801,-0.0024255402,0.0032527763,0.029476414,0.07576587,-0.002604822,-0.018619515,-0.079166465,9.692782e-33,0.030996423,0.051789135,-0.03277883,0.0033776246,0.020872533,0.015254047,-0.04083086,0.041710984,0.066906326,0.033966783,-0.109067075,0.027409991,0.030000994,0.04522218,0.0681229,-0.03188552,0.00037416024,0.07223663,-0.028036237,-0.003891885,0.055202562,-0.119234115,0.09908528,-0.0075691906,-0.08388816,0.030344125,0.02161249,-0.0054540844,-0.08493776,0.006309884,-0.03248729,-0.010027061,-0.0001520913,-0.0056855413,0.015148192,-0.0432943,-0.0024575386,-0.09810823,0.07273979,0.0028061753,-0.04484353,0.014900568,0.021995364,-0.086537845,-0.049425073,0.060581338,-0.0145428665,-0.054277774,-0.06551254,0.06324288,-0.038336843,0.00077211234,0.050659314,0.043970574,-0.038224727,-0.0005957263,-0.013477391,-0.071599826,-0.053479087,0.023990571,-0.008153896,-0.019062882,-0.024552308,-0.053049244,0.03141705,0.087625906,-0.024693657,0.042773448,0.12551562,-0.023143338,0.000991057,-0.006109138,-0.02422626,-0.030304052,0.060069755,-0.03699122,0.03218624,-0.018477777,-0.0026127289,0.021061135,-0.021788562,-0.03364142,0.082841195,-0.13398847,-0.009738411,0.00019221057,0.05125297,-0.07780973,0.00988123,-0.026300922,-0.071757756,0.05712688,-0.01895675,0.06429988,0.06231744,-8.9197224e-33,-0.12782803,0.057709634,-0.041464258,0.03993738,0.016552825,0.0149507,-0.02579944,0.0249011,-0.047383126,-0.07629746,-0.000507755,-0.062089123,0.098536365,-0.033311736,0.08461331,0.003950128,0.020086836,-0.08177358,0.074159294,0.06830711,-0.035090532,0.09712781,-0.07114387,0.007975227,-0.022628406,-0.010388857,-0.09728897,-0.05851254,0.0050281705,0.018902928,-0.073240966,0.013943753,-0.09506282,-0.028901163,-0.04851753,-0.02435746,0.001550578,-0.044993635,-0.020988083,0.123413086,0.086290985,0.00070746493,-0.045259878,-0.014816781,-0.07121868,-0.0040455433,-0.02753567,0.010376668,0.04849315,0.04670985,-0.0016048657,-0.019964982,-0.066479295,0.01880503,-0.087118156,-0.05889663,-0.044809073,-0.052928086,0.023399822,-0.023387905,-0.078915834,-0.019399682,-0.017862728,0.01825943,0.064402014,-0.01702355,0.010800488,-0.02444547,-0.029068235,0.04754602,-0.01994021,-0.07308174,-0.0023068676,0.036469482,-0.0037571066,-0.041553203,-0.022432497,-0.062308695,-0.09610608,-0.043455757,-0.014293417,0.027771369,0.030196603,-0.022231031,-0.020606373,0.00036698175,0.010996018,0.06951623,-0.031497642,0.01839659,-0.051952563,0.07347894,-0.09784002,0.043561824,0.0028432244,-7.056795e-08,-0.11079401,0.019905865,-0.040520955,0.02002467,0.05705662,-0.07034296,0.016473437,0.09124901,-0.045601163,0.0026849168,-0.0041973284,0.016556136,-0.0058829812,0.022826968,-0.032888472,0.14301915,-0.03855766,0.034601882,-0.018336643,-0.11882711,0.15137795,0.029304314,-0.018068304,-0.03137205,0.080767386,-0.007824705,-0.06775664,0.08021654,-0.026286872,0.02729639,0.04540776,0.014425833,-0.018875867,0.051555213,0.07931497,0.089344114,0.07638973,-0.047348674,0.014619429,0.041585285,-0.026916618,0.113935016,0.00586548,0.07176811,0.11375007,-0.0057308567,0.032839153,-0.09333358,0.030486489,-0.038449716,0.020626461,0.032803062,-0.036996827,-0.045217857,0.052052785,0.057919122,-0.0071711782,0.0040279254,0.03724311,0.027531335,-0.005778167,-0.034364063,-0.030862277,-0.071743235,5,2.7753508,-5.114213,9
9,"we looked at the course summary we initially talked about ways of improving the quality of our results, either by improving the sample, the method, or fine tuning/properly using the method.
we studies the errors in the residual scatter plots, and since the errors were distributed very systematically, we looked at how introduction of new variables (with polynomial relations with known variables) make the model better.
we looked at why one model is not fitting a data well, and then looked at multiple models, the results or different statistical quantities which we were getting and then tried to derive the meaning of them.
we looked at neural networks and deep klearning methods.
we discussed logistic regression, sigmoid function at the end
",0.008701653,-0.056836892,0.058676466,0.06803851,0.040396404,-0.0454852,-0.064877704,-0.02333674,-0.023564443,0.00100008,0.0043119886,0.06967449,0.027777158,0.030589996,-0.034412157,-0.04084558,0.03859609,0.044856653,-0.09050896,-0.06970068,-0.020444201,0.010730118,0.015896581,0.0069441535,0.03528129,-0.022675525,-0.041744508,0.00110177,-0.0908117,-0.024106648,-0.013600119,0.029948175,0.00015265937,0.046514817,-0.042847555,0.004159302,-0.03155964,0.08114732,0.016535526,-0.0024859228,-0.013031853,-0.021085555,0.056652315,0.035569552,0.1066008,-0.037954852,-0.0076015955,-0.065717936,0.0031738398,0.05612645,-0.081232205,-0.03180761,-0.020633163,0.013710911,-0.003291531,0.0075500826,-0.012868656,-0.0023398802,0.01718667,-0.02532821,0.049738307,-0.004130113,-0.10108894,-0.013805475,0.06113087,-0.06042314,-0.016319128,0.05785605,0.0463649,0.11595806,0.025280312,0.050999284,-0.08108592,0.048637193,0.0043064095,0.060517356,0.0033257355,0.020547219,0.013470551,-0.04850468,0.030510187,0.03442558,-0.025610486,0.00961296,0.11735498,-0.019246873,0.025884189,-0.017309504,-0.101491526,-0.020327464,0.038646795,0.03062166,-0.048068773,0.0144614745,-0.034157783,0.06335463,0.065933995,-0.1050374,0.041476756,0.09775959,-0.014103982,0.11807949,0.03763554,-0.10240222,-0.0051896647,0.0075570866,0.061714828,0.057812337,-0.019184912,-0.024463976,-0.024301568,0.03349472,-0.11392721,0.013351021,0.14139497,0.012043077,0.039344784,0.012495933,-0.08417671,0.10822623,-0.023702037,-0.007937034,0.023235839,0.010926021,0.028176256,0.033588387,-0.12163579,2.7757958e-33,0.019543644,0.048566572,0.007843877,0.036753293,0.0021859831,-0.030996786,-0.06562168,0.053057365,0.045369443,0.035834357,-0.028475102,0.028824598,-0.004219856,0.06847884,0.04304259,-0.027897717,-0.09246041,0.0851907,0.0057402304,0.13176712,-0.03763765,-0.07153281,0.033181258,-0.11469052,0.010384963,0.08733317,0.041292753,0.10539271,-0.044466034,0.014376978,0.0391618,0.081539035,0.006793312,0.050432052,-0.021574697,-0.037061922,-0.02940555,0.018345607,0.06861686,-0.017653469,-0.039143078,-0.009402176,0.040351804,-0.0074470183,-0.020840837,0.06999792,-0.020480601,-0.09241391,-0.018893508,-0.019579627,-0.075174384,-0.042722493,-0.06640119,0.0020741154,-0.09066539,0.07004646,0.028535986,-0.07938335,-0.09074051,-0.03608217,0.0467003,0.03516399,-0.060372572,0.010184759,0.028865812,0.01509719,0.01091568,0.012891831,-0.0033090173,-0.054311637,-0.060833916,-0.00024573173,-0.02774488,-0.012087875,0.053518083,-0.014498464,-0.0059374147,-0.010373867,-0.04359615,-0.031376056,0.015935456,0.028507873,-0.028741103,-0.09973498,-0.09276321,0.061091293,-0.001077569,-0.014805307,0.030729247,0.02438119,-0.067264326,0.046482738,-0.05213402,0.0056154337,-0.0023870666,-1.7156707e-33,-0.053017512,0.083274335,-0.028322604,0.10882377,-0.054775454,0.039879058,-0.058341496,-0.0026406967,0.023499351,-0.013575213,0.027294097,-0.037868127,-0.025790155,0.040097117,0.0517028,0.011288607,-0.0004481817,-0.04751215,0.018034626,-0.018275673,0.020001305,0.08524596,-0.030828858,-0.10708291,-0.04223006,0.008765664,-0.051405713,0.035986125,-0.0915257,-0.034508426,0.0076476657,-0.05946561,-0.055485915,-0.037558492,0.06805707,0.12550324,0.035969418,-0.06485841,-0.015268963,-0.0037941379,0.032772403,0.044592407,-0.0065330355,-0.061613314,-0.0047946717,-0.0686087,0.0024806643,0.05619775,0.049890522,0.06730032,0.00072188483,-0.0358925,-0.08440824,-0.03608612,0.011449713,0.00026496054,0.036196187,-0.07111566,0.006829953,0.015329623,-0.13531767,-0.018018074,-0.04745439,0.026086515,-0.051848184,-0.04696902,-0.0045580273,0.04675015,0.039892208,0.03686579,-0.03246375,-0.098259136,0.052790407,0.033452347,-0.024707261,-0.029324137,-0.06285694,0.021793332,-0.10877002,0.017260745,0.08141497,-0.080504246,-0.02889418,0.063340105,0.03248185,0.05659775,0.042212393,0.04306114,0.030469686,-0.01232984,-0.0399181,0.02261749,-0.025367139,0.009400332,-0.06543901,-5.7089117e-08,-0.0293425,-0.03216854,0.020276258,0.0013868785,0.0065964386,-0.025419306,-0.06441565,0.14601235,-0.041026115,-0.0029457088,0.00062471733,0.036019623,-0.09957957,0.010270257,0.026871463,0.011761469,0.05014371,0.119876616,-0.02818333,-0.04249924,0.06455998,0.055345744,0.012362877,-0.028219441,0.11507927,-0.000504648,0.009110634,0.053989887,-0.085029595,0.059685923,0.04492554,0.03695709,0.0038240778,-0.028644852,0.07103853,0.11435608,0.07896778,-0.027216693,-0.017704103,0.042786174,-0.06629622,0.10067254,-0.059168685,0.0701387,0.06507202,-0.026682762,0.006875074,-0.01360561,0.009229563,-0.04169636,-0.021952704,0.016106904,-0.05016201,-0.012629961,0.024838252,0.043229826,0.02200494,-0.031268876,-0.023996366,0.07459171,-0.034532156,0.02663392,-0.02205509,0.038559318,5,-1.5751435,-14.354085,9
21,https://docs.google.com/forms/d/e/1faipqlsffcpe8ytvk7pee7cslz0xgjhjk3_a8y7jo1abmkldxnrim4a/closedform,-0.054833855,0.019691981,0.017697625,-0.03403747,-0.039801724,0.08312872,-0.014376844,-0.004199623,0.011053629,0.056535143,0.05129485,0.025535565,-0.027185218,-0.046524353,-0.04680022,0.025789214,-0.109948106,-0.044317577,-0.03777685,0.018161582,0.09133766,0.038701605,-0.016642312,-0.045972217,-0.023757413,-0.03541134,0.032742545,0.02101426,0.044982873,-0.04990858,-0.0762494,-0.016102485,0.05149694,-0.09456953,0.07378406,-0.017768856,-0.005323624,-0.0070616314,0.034620862,-0.032772254,-0.052489944,-0.059424266,-0.015069335,0.030426916,-0.0041895225,0.030463861,-0.056851912,-0.04355988,0.04889992,-0.00082642224,-0.08002912,-0.048079446,-0.07083302,0.011439471,-0.009186335,-0.017657856,-0.0064717215,-0.041694626,-0.02675295,0.033172894,-0.014826691,0.01773187,-0.097883195,0.029832248,0.04388881,0.08018644,-0.033524226,-0.14369026,-0.035448167,-0.08094532,-0.060625207,0.05203701,-0.050918218,-0.054782853,-0.02786079,-0.05066276,-0.10480818,-0.0026718506,0.007452439,0.007362004,-0.002567482,0.010071497,-0.004731815,-0.039500646,0.04440891,-0.035963666,0.0041964157,-0.040878747,0.09602291,-0.004500918,0.012788174,0.022621663,0.061409604,0.054457564,-0.041363187,-0.029466413,0.0061200545,0.07663486,0.008850642,0.021113895,0.012431216,-0.018816711,-0.025583833,-0.00928655,-0.029132338,-0.078685865,0.030387107,0.061952773,0.01898384,0.013834777,-0.044406522,-0.09377367,-0.06361286,0.004222429,-0.016163658,0.05953497,-0.037322927,-0.015439108,0.025242826,-0.026415711,0.035751138,0.030042347,-0.105377406,-0.012405694,-0.0881378,-0.10995197,0.08442482,8.8213754e-33,0.051723987,0.007859022,-0.057721194,0.07686809,0.020525,0.060391944,-0.088843286,0.024723625,-0.008445617,0.04530796,-0.039083384,0.040640797,-0.004233037,-0.024367176,-0.07783875,-0.07623196,-0.0032479584,0.16650268,-0.020633006,-0.05119508,0.095487796,0.071987875,0.064867176,-0.044602424,0.047154676,0.11548974,0.027415901,-0.025274644,-0.0049176826,0.01489765,-0.031089317,-0.1313507,0.029389609,0.036154095,-0.0038676863,0.10229418,-0.037347417,-0.05857096,0.013030169,0.0013894653,0.03339977,0.005252655,-0.028369213,-0.04422103,0.012073391,-0.08267414,0.049636535,0.01121069,0.10561004,0.03099573,-0.0053926758,-0.0078766495,-0.13523953,-0.08434533,0.020789064,0.049793456,-0.06922485,0.04787145,-0.06287162,-0.05071015,0.04517511,0.0831457,-0.042916145,-0.07107805,-0.15443224,-0.10427529,-0.047308676,-0.092576936,0.07124835,-0.03889355,0.019320859,-0.030535238,-0.0344336,-0.020854574,-0.0098824585,-0.016530868,-0.05171225,0.0035231665,0.0136603685,0.006515068,0.044115905,-0.059496384,-0.089415796,0.051400244,-0.011963863,-0.042336322,0.020287758,0.06541192,-0.013250793,0.0030419466,-0.13168764,-0.040480997,-0.07523542,0.0006249438,0.053090893,-1.0011103e-32,-0.07340231,-0.052220836,-0.024910353,-0.029239219,0.049812667,0.030841669,0.12804712,0.04079329,0.07488761,0.08838355,0.04447924,0.03380623,0.08100825,-0.006825295,-0.029428009,-0.075748205,-0.0029689188,0.09565806,0.026101107,-0.009480067,-0.077604614,0.097104125,0.023565782,0.05245608,0.042126518,-0.021945884,-0.07089789,0.015223582,0.06000316,0.03511665,0.06246038,-0.085448146,-0.0677858,0.085330494,0.006106763,-0.041166242,0.02103551,0.04032251,0.024387043,0.011061698,0.03487451,0.027650885,0.04818515,0.021390041,0.035415854,0.013042963,0.024912432,-0.02440295,0.019044753,-0.021576663,-0.02433451,-0.053145193,0.020159656,0.012129458,-0.02511997,0.07532005,0.06530938,-0.01588677,0.02112751,-0.0041163587,0.07261067,0.047278095,0.0008657215,-0.01948671,0.16206926,-0.04090585,-0.0043133376,0.052234195,0.057367813,0.018455815,-0.010330965,-0.037751738,0.07977148,0.010885472,0.08180493,-0.04662479,0.04912274,0.043588743,0.033868343,-0.025325144,0.05433813,0.018067023,-0.035908353,0.017896164,0.0051027196,-0.017227316,0.049417894,0.017547017,-0.019064529,0.07184729,-0.11687792,0.0720565,0.042419255,0.022550765,-0.058148026,-4.6922818e-08,-0.011034863,0.045336027,0.087357216,0.00561892,0.037671745,-0.07587504,0.04622177,0.0012723864,0.01634709,0.0019065612,0.057430938,0.0013308672,-0.03297583,-0.00860045,-0.049746454,0.013394507,-0.030062651,0.024737129,0.0058955397,-0.09751194,0.028393723,-0.04616048,-0.0018157635,-0.0036944135,-0.0026224097,0.0075495206,-0.0436872,0.025226265,-0.026300374,0.012005438,-0.02490158,0.035083197,0.047229744,-0.035740443,-0.07884374,0.017746475,0.015239925,0.027332168,-0.018821906,0.08982372,0.049833823,0.04644534,0.043754574,0.020074697,0.027494846,0.015512834,0.02345771,-0.024388565,-0.011295713,0.06274662,-0.054927904,0.0010260149,0.023457836,0.016503032,-0.045219254,-0.03230228,0.020608274,-0.030979645,0.037974373,-0.010990589,0.045047194,0.09349524,-0.013553165,0.031789534,1,5.246482,-21.971329,9
39,"sir said 3 ways are available or improving result quality. 
(1)	sample improvement
(2)	mehod improvement
(3)	for fine tuning
now sir started talking about mlr with many features,i.e., mlr for non linear cases.
adjustedrsquare decreases as more terms are included. p values makes  only the significant term to remain at the end.
(polynomial regression) we took help of feature engineering for features in form of polynomials or trigonometric functons.  
sir said about 
(1) backward eng
(2) forward eng
(3) overfit issues
we then talked about trigonometry and straight line. a single model model which is the combination of the two is required.
sir then talked about random forest, parametric, non parametric and dela analysis.
sir then asked us if we knew about neural networks and only few people know about it.
then he said that chatgpt is also a neural network with billions of features and requires a lot of data space.
",-0.062198907,-0.07182157,0.08114156,-0.018937819,-0.027656842,-0.022455556,-0.0707389,0.06890948,-0.023845676,-0.0050773607,-0.027978992,0.06166099,-0.06302179,0.025774587,0.033635147,0.04442331,0.0253863,0.020287337,-0.046425715,-0.08960602,-0.015504156,0.016342685,-0.032248076,0.00020230075,-0.016981762,-0.049977213,-0.01258283,-0.037139148,0.048022937,-0.007553948,0.016588453,0.081868075,0.04905882,-0.02802297,-0.1303559,0.030456562,0.02019124,0.058387093,0.009727912,0.012888493,-0.025361223,-0.117843404,-0.00411683,-0.039144564,0.13641728,0.010827262,-0.017867325,-0.0508926,-0.040159706,0.031966496,-0.12982343,0.02665122,-0.068716265,-0.0054913433,-0.010027388,-0.006623775,-0.01633758,0.09698314,-0.02066155,-0.04759363,0.0071779024,-0.0065627694,-0.088274054,0.006017311,0.033724207,-0.0070068603,-0.005895118,0.01915309,0.08976364,0.038205095,0.01485599,0.011930579,-0.07182342,-0.020930018,-0.026604488,0.022126613,0.054860145,0.03032409,0.02250358,0.034850102,0.02049304,0.07193784,0.02181958,-0.03218457,-0.009460323,-0.01515054,-0.056872316,-0.042017456,-0.05374159,-0.038520463,0.051479455,-0.054361112,-0.04628153,0.01629399,-0.01013181,0.021042483,0.017953778,-0.07139936,-0.0023173366,0.054010555,-0.01772413,0.016392099,-0.014796615,-0.08568384,-0.027093826,0.034929395,0.04435279,0.040744923,0.11892931,0.0063027306,-0.045487236,0.011894529,-0.025305154,-0.02727313,0.044499733,-0.039545562,-0.03235571,0.03118489,0.064166464,0.046925493,-0.030728541,0.0008165467,0.0042165425,-0.02272823,0.022348436,0.040261522,-0.077633925,9.789204e-33,-0.027386654,0.04907011,0.008045173,0.03814855,-0.02582931,0.04183742,-0.028125742,0.009574855,0.12628888,-0.008045224,-0.023834445,-0.039985675,-0.019902378,0.02042115,0.09020841,-0.048618678,-0.045162518,0.009812937,-0.002410587,0.022817615,0.0053160614,-0.05447147,0.019465309,-0.019268375,0.027087707,0.04011392,0.09649921,-0.018313669,-0.06056998,0.0048785913,0.01656217,0.03851618,-0.033048645,0.06905624,-0.021234,-0.010198377,-0.010983931,-0.12430142,0.0050943093,0.00929165,-0.063567504,-0.008168264,0.06542468,-0.05454269,0.019635249,-0.013864646,0.0013003346,-0.07140072,-0.052359886,0.003022259,-0.046563756,0.020220293,-0.09008502,-0.008270191,0.022349725,0.0025033748,-0.09980241,-0.03171276,-0.008471015,0.12990667,0.002547339,-0.03657394,0.025174756,-0.058375753,0.013716477,-0.077001296,0.009951334,-0.030709699,0.024050217,-0.035834957,-0.022275632,-0.012731509,-0.03213679,-0.04970051,0.005146964,-0.02761754,-0.025215952,0.043139536,0.05695778,0.025085764,-0.030569587,0.10341977,0.027284816,-0.10325212,0.044943158,-0.0039396267,0.036652584,-0.037032124,0.080050215,0.007353054,-0.14915591,0.046522163,-0.021368695,0.07369568,-0.003953035,-8.902284e-33,-0.0995168,0.046507634,-0.00854538,0.09435283,-0.062339183,-0.0002668112,-0.00028096954,-0.04792432,0.08228277,-0.04677401,-0.040401008,-0.04617043,0.06335964,-0.035467166,0.04053999,0.007097964,-0.030833302,-0.109102525,-0.034565642,0.065499894,0.06599552,0.06159138,-0.12265666,-0.003960495,-0.061808586,-0.02695137,-0.117934234,0.015376871,-0.0017992185,0.041744765,-0.023391085,-0.012969763,-0.06831296,-0.027431952,0.040175233,0.018839533,0.09098621,-0.0649347,0.01777015,0.052673504,0.11869716,0.0138007635,0.01079872,-0.07151406,0.025521267,-0.04982645,-0.078366585,-0.021113116,0.024105778,0.0316064,0.030101363,-0.03780711,-0.008914455,0.0064333924,-0.03986718,-0.06829822,0.035126705,-0.028447408,0.022564247,0.047889955,-0.049784176,-0.047090307,0.035809107,0.034096647,0.01693566,0.04892195,0.04822778,0.051426638,0.02468218,0.008769935,-0.073174916,-0.07121303,0.060420856,0.052651204,-0.035706665,-0.039109465,0.028045107,-0.047085755,-0.037684344,-0.08777922,0.03331011,-0.073396616,0.030681834,-0.009653836,0.07875188,0.035002705,0.06856927,-0.013636273,0.024779951,-0.064766794,-0.03091836,0.04217065,-0.026697677,0.06565692,0.0690701,-6.655997e-08,-0.06774815,-0.037957173,0.027340833,0.057177477,0.026111588,-0.038436744,-0.017917736,0.13913612,-0.05256415,0.022260064,-0.015830446,-0.007373123,-0.07144417,0.004784938,0.16767928,0.057437263,0.0013067614,0.095168754,-0.017194308,-0.058549356,0.117803484,-0.008987602,0.027692903,-0.01036544,0.09741658,-0.057561886,0.007455378,0.03127364,-0.03936255,0.013876012,0.0021343543,0.048919592,0.03951595,0.030200256,0.1077935,-0.012321579,0.088783756,-0.025606718,-0.035464108,0.05510891,-0.0614284,0.1104299,-0.046446245,0.036528245,0.11395266,-0.052249666,0.033773396,-0.0964916,-0.064896494,0.014090498,0.033576693,0.061765578,0.08303131,-0.03928276,0.040523086,0.0521718,0.04937466,-0.001718243,-0.03958856,0.032806154,-0.02467288,-0.023468666,-0.006825826,-0.043669567,5,-2.2867916,-11.700759,9
65,"today's session basically started off with some review of the previous lectures, where we discussed that how if the error plot is not random, and shows some non linear relation i.e. if there is a non linear relation between the independent and dependent variable, then we need to introduce polynomials of the independent variable also as features to our model. this is part of feature engineering, and using polynomial functions of the independent variable as a feature for the mlr model, is known as polynomial regression. we also discussed how any non-linear and non-polynomial relation (eg. sin, cos, log) can also be converted to a polynomial relation using taylor series expansion. we also learnt that even complex neural networks employed some kind of polynomial regression itself, in order to make good predictions.
then we moved on to data which displays different kinds of relation in different ranges. we saw that we could use a variety of models for such data, and we could even use multiple models for a single dataset in case there was a completely different relation. however using one single model for the entire dataset is always more beneficial and easier than using multiple models, as it will create complications when implementing for the test dataset. also in such cases, we concluded that the best method of finding out the best model for our data was to try out all possible models and then choose the one with the best prediction accuracy according to us. we saw the responses of various models over a dataset and we observed that even though one model (random forest) was fitting our data highly accurately, we observed that it might be too good a fit for our model to be able to make any useful predictions on the test data. however, some other models, although not fitting the data that accurately, were doing quite well at prediction as compared to random forest. hence, choosing them would be a better choice than choosing a model which accurately fits our data but is not able to make accurate predictions. 
we then moved on to logistic regression and classification. we noted that this scheme applies to nominal and ordinal levels of measurement and not to ratios as classes are discrete and need to be dealt with in that manner itself. we also said that unlike linear regression, logistic regression was not meant to predict the data values, but rather the class in which an observation belonged. the output of such a model is the class, and we are basically trying to predict the boundaries between the classes. the expression for calculation is somewhat similar to that of linear regression, however in this case, we use weights instead of coefficients (beta) as per the nomenclature. the idea behind both the regression algorithms was similar; to reach a mean or mediocre value. in linear regression we were trying to predict the value of data points based on the mean of observations, whereas here, we are trying to find the mean boundary between classes so that we can accurately classify our observation points. ",-0.111203216,-0.039465662,-0.016168369,0.086803384,0.0058664056,0.010736599,-0.077985756,-0.04275561,0.09118357,-0.06276209,0.04702412,0.0016263016,0.0066087185,0.060480163,0.023951232,0.009397265,-0.017698646,0.05074267,-0.048414953,-0.0005366741,0.01701848,-0.04599431,-0.10215068,0.017864024,0.10519345,-0.06173654,0.001819074,0.037538603,-0.049795,0.051841903,0.08518438,0.028954519,-0.048656385,-0.019821646,-0.06241315,-0.0018277421,0.0014516467,0.08269353,-0.0003871407,0.0021739474,0.06335965,-0.029743738,0.076880105,0.027618844,0.09687145,-0.03545172,0.018067256,-0.08041131,-0.03421029,0.002402054,-0.05654984,0.028239097,-0.036550187,0.03127022,0.028146038,-0.028115459,-0.01006756,0.06701409,-0.008377563,0.039595943,0.009808246,-0.0004923031,-0.025365956,0.028444936,0.029492728,-0.01862907,0.0080744345,0.059217907,-0.010258405,0.089659646,-0.009652746,0.08496431,-0.09696591,-0.009774745,-0.019001955,0.011687751,0.053864524,0.009476837,0.0071989796,0.0028522373,-0.010557315,0.056723833,0.07038001,-0.034408197,0.043700654,-0.0065400163,0.024469916,-0.020200472,-0.010177088,0.047345035,-0.016709473,0.0043807193,-0.030121692,0.008588943,0.00440578,0.03235644,0.07422979,-0.058011103,0.022796167,0.064236686,-0.0051493053,0.04700622,0.06351369,-0.067804076,0.014687276,0.023473514,0.02079095,-0.020213563,0.021397997,-0.03493197,-0.03245072,0.031317256,-0.092660956,-0.008420659,0.07732876,-0.06593814,-0.044432376,-0.026026387,-0.008398177,0.063376226,-0.068812944,-0.027500372,0.02541765,0.076966554,0.04019885,-0.026839636,-0.09362928,5.072463e-33,-0.012939686,0.01267358,-0.019106733,-0.03781401,0.024055855,-0.030483952,-0.001571602,0.026929542,0.085593075,0.0627469,-0.038984586,0.051039338,0.029021457,-0.03397179,0.078767695,0.086665384,0.008931353,-0.010443882,0.085485786,0.06507363,0.0062064547,-0.044752035,0.014019779,0.010238714,-0.014131163,0.06567082,0.0546388,0.049618013,-0.059525393,-0.008296361,-0.010263824,0.060192052,0.0019862724,-0.038204808,0.029292239,-0.013261888,-0.025550537,-0.098191954,-0.003613186,0.0062746084,0.044844244,-0.050296586,0.107450604,-0.0054511013,-0.0010135266,-0.018624056,-0.010274102,-0.035447903,-0.062419724,-0.008891475,-0.07284883,0.031930417,-0.08569933,-0.034873698,-0.054911707,-0.0037881893,-0.08019515,-0.03495452,-0.04939493,0.01785005,-0.11329976,-0.051877983,0.009438317,-0.080323845,-0.0048833154,0.033257946,-0.008669617,-0.045785654,0.022437684,-0.08564309,0.01548479,-0.028706418,-0.066843845,-0.08434399,0.07744501,0.031380773,-0.04299706,-0.080082215,0.009828967,-0.005560753,-0.037612464,0.023814661,0.04439563,0.007288271,-0.07893257,0.003423545,0.019378245,0.033370808,-0.008254907,-0.045717426,-0.0879468,0.055892803,0.015804257,-0.018433811,0.054683432,-5.8985947e-33,-0.05967499,0.02207253,0.01557297,-0.041384295,0.00534268,0.07393538,-0.001269243,-0.04121762,0.06962005,-0.015602132,0.00087817514,-0.030492347,0.030240152,0.0016368316,0.049473025,0.061401717,-0.076317325,-0.059230212,-0.010991624,-0.0005216209,0.033571303,0.122240126,-0.0975942,0.015292419,-0.034278408,0.030346233,-0.116293244,-0.038162183,-0.020259691,0.04396745,-0.048933383,-0.025955962,-0.023236241,-0.11339085,0.04106737,0.065691635,-0.01093229,-0.08295263,0.033872984,0.007695162,0.024849059,0.038829703,0.052190196,-0.012480565,0.020442769,-0.035462078,-0.018307578,0.005637643,0.092346855,0.1004156,-0.015963638,0.010180241,-0.14379074,-0.045336537,-0.102493584,-0.051905595,-0.0047363313,0.001211757,-0.00073441636,0.008501867,-0.07554956,-0.08096775,0.065466605,0.052057948,-0.07380069,0.021230191,0.0049727606,-0.0121479435,0.061585497,-0.03210624,-0.012771011,-0.072279826,0.001518569,0.045600444,-0.07930205,-0.09931535,0.05214693,-0.099824265,-0.06513917,0.0027626809,0.058849115,0.022589132,0.07504335,-0.021607894,0.02664607,-0.04174756,0.06418056,-0.015758725,0.05628325,-0.12731372,0.010482132,0.050503533,-0.041476388,0.061469708,0.019721122,-6.3203785e-08,-0.057864796,0.043493923,0.02548459,0.05957072,0.018226935,-0.057581037,0.06603244,0.03464284,-0.06969915,0.04782782,0.04182227,0.0144162625,-0.029067628,0.005120673,0.11514828,0.028725745,0.014232641,0.02523795,-0.043097354,-0.07757654,0.03749168,-0.027405778,-0.10748454,-0.018816892,0.12855811,-0.088742316,-0.033493284,0.052597895,-0.025681257,0.06212361,0.009889753,0.018419214,-0.018059064,0.036042128,0.07017248,0.07116679,0.12541063,-0.051366203,-0.03802388,0.0024427297,-0.056866594,0.052337285,-0.02119851,0.0789202,0.06122959,-0.011292602,0.07051375,-0.14663057,-0.054189164,0.07546759,0.021852778,0.074490614,-0.031780608,-0.038086005,0.0324367,0.044848364,0.00083637366,-0.022026548,-0.04030906,0.043526113,0.018316494,-0.025636313,0.03525861,-0.08265634,5,2.3075416,-8.413553,9
66,"we started our discussion with how to improve the quality of results:
1. by improving the sample --> quality of the sample
                                                 --> size of the sample (increasing it)
2. improve the method
    --> use multiple methods and use the best one.
3. fine tune/properly use the methods.

then we talked about linear regression:
where output is expressed as a linear combination of independent variables.
it is not necessary that the output has to be a line (linear).
like: x1->x1, x2->x1^2, x3->x1^3
y = b0 + b1*x1 + b2*x2 + ......

although, on increasing the number of features the adjusted r^2 value might reduce as we are adding more unnecessary features.

 ",0.04006641,0.009322424,0.012055826,0.012088773,0.0013243253,0.061311092,-0.07404763,0.014044322,-0.039302137,-0.021327497,-0.04306927,0.08276464,0.020401787,0.033858493,0.024646455,0.07712427,0.060696032,0.0963353,-0.05283549,-0.040226765,0.05133247,-0.0748914,0.01830298,0.035226975,0.065192305,-0.042081356,0.008290405,0.0035902627,0.0035537947,-0.023665663,0.0057035997,0.08386413,0.05504416,-0.0621572,-0.031247452,-0.08072304,-0.038038317,0.050521974,-0.018579755,0.021602038,0.011815311,-0.02363892,0.046472482,-0.01221237,0.05284998,0.036823343,-0.030440547,-0.089654185,-0.0012026305,-0.024180569,-0.062089577,0.043052565,-0.042996794,-0.0065671704,0.03584142,-0.09961432,-0.0845504,0.017191524,0.005821554,-0.040236153,0.122285046,0.03329729,-0.01896881,0.0010728948,-0.034023203,-0.027257646,-0.035248604,0.0014632618,0.015531725,0.10153183,-0.012009854,0.11463089,-0.052761607,-0.020514969,-0.033358205,0.030697709,-0.016212238,0.021584317,-0.014661608,0.07534123,0.046739105,0.066018164,-0.034834374,-0.00926947,-0.065422155,0.008173759,0.016757866,-0.16228443,-0.09984907,0.060277127,0.051652335,0.052116696,-0.0026732937,-0.021741413,-0.072564684,-0.003979931,0.041639067,-0.08473423,0.056037415,0.058920722,0.029032247,-0.00044488392,0.017265692,-0.026586263,0.011930344,0.016699329,0.041829612,0.026410887,0.031685174,-0.00015061205,0.08306628,-0.004759851,-0.09855333,0.008499422,0.077348135,0.037139434,0.010388908,0.0721575,0.01615865,0.0092007285,0.031951036,-0.018904202,0.01632078,-0.026477516,0.0023445953,0.08009476,-0.057525013,6.0505563e-33,-0.040480413,0.029530767,-0.0021740797,0.001870621,-0.016716436,-0.025588004,-0.0161809,-0.0076028835,0.09327032,0.031758483,-0.026972922,-0.039119802,0.041534483,0.09114186,0.055402786,-0.020967335,0.032166068,0.06050263,-0.010871988,0.120976925,-0.042289075,-0.11567096,0.06996163,-0.039788302,-0.031575747,-0.05590664,0.01782042,0.042767793,-0.115387164,-0.018078687,0.040765114,0.02184863,-0.01292281,0.052254677,-0.049581554,-0.058083605,0.022702487,-0.028122451,0.022169352,0.055187374,0.042530663,0.02275423,-0.005050284,0.011129641,0.06390888,0.11985031,0.031166531,0.009048877,-0.051014345,0.04993567,-0.04538069,0.05288121,0.008990462,0.04972975,0.016663577,0.007618259,-0.083444394,-0.04133875,0.032444626,0.07600089,-0.12119002,-0.006467775,0.037751008,-0.094619505,0.025615167,0.03884833,0.08492942,-0.004380164,-0.014734366,-0.027329668,-0.04677697,-0.060294025,-0.04202464,-0.0011887371,0.038910296,-0.06866164,0.03780224,-0.021443807,0.082265295,-0.075021505,-0.030796535,0.109088264,0.04075134,-0.058475584,0.02253576,-0.022189671,-0.06138886,-0.084150665,0.04218122,0.036740858,-0.07799144,0.071399115,0.012119947,0.027722314,0.0018034593,-5.5145118e-33,0.04778565,0.03954616,-0.022847323,0.029887542,-0.042611606,0.011262566,-0.034336843,-0.081067376,0.034735847,-0.019382738,0.016092176,-0.040783335,-0.028004566,-0.009814825,0.05779698,0.010065336,-0.01024196,-0.07385247,0.007843456,0.0248089,0.018861402,0.07693859,0.05485056,0.02098607,-0.043248113,-0.008137331,-0.0944517,-0.028460203,0.019037744,-0.033167284,-0.025165536,-0.0056630727,-0.04027137,-0.054409992,0.0054326677,0.034901567,0.031488,-0.06415792,-0.0005791108,0.11270244,0.05492066,0.044497907,-0.04261074,-0.07073334,0.021928482,-0.02589221,0.0016329993,-0.0029961944,0.013059846,0.050205484,-0.0218157,0.033694353,-0.093869194,0.061019894,0.048567563,-0.03647481,-0.035863213,-0.004858802,0.10340476,0.0067789243,-0.08476259,-0.042899385,-0.12142761,0.05706279,0.04875682,-0.008159268,0.031339645,0.017970448,0.022129022,0.043593578,-0.060534243,-0.10428983,0.017466018,0.017605284,-0.06400131,-0.019654548,0.018136939,-0.088151604,-0.020708846,0.046214845,0.054183926,-0.027715271,0.04955928,-0.021838801,-0.0519272,0.03929049,-0.03421554,-0.009411613,0.038543493,0.06664213,-0.023034807,0.018737681,-0.05622567,0.03658812,0.051296894,-6.880477e-08,-0.047581714,-0.053725418,0.008369872,0.016301623,-0.005747636,-0.041071307,-0.04883695,0.17041737,-0.06860752,-0.072618425,-0.034918968,8.09087e-05,-0.069941856,0.058045167,0.06501051,-0.008063704,-0.051446173,0.05410068,-0.04834497,0.00989712,0.039018545,-0.02240704,4.625737e-05,-0.08267585,0.045687284,-0.054722536,0.07065734,0.06787975,-0.052221265,-0.013907934,0.04099783,0.028397046,0.011393643,0.0301701,0.11372266,-0.020382352,-0.016708044,-0.020871343,-0.01279439,0.002880235,-0.11052845,0.13626185,-0.0104026105,0.026608275,0.041546237,-0.059962653,0.06458876,-0.07057616,-0.079406954,-0.1054461,-0.0073767398,0.006869624,0.0068518226,-0.040383738,0.020689609,0.036036234,-0.08464926,0.046601515,-0.021942558,-0.0053344863,-0.049223762,-0.025135007,0.004374042,-0.11136787,5,-2.979833,-7.5983214,9
72,"we looked at three ways to improve the quality of results: improving the sample, improving the method, and fine-tuning existing methods. from there, we moved on to multiple linear regression (mlr) for nonlinear cases, where we used feature engineering to add polynomial terms like  and trigonometric functions like . this led us to polynomial regression. we noticed that as we added more terms, adjusted  tended to drop, and only the most significant terms remained based on p-values.

we also explored forward and backward feature selection and the risk of overfitting. when dealing with nonlinear models that combine trigonometry and linear elements, we considered whether a single model could handle both instead of relying on separate approaches. random forest turned out to be a method that naturally does this by combining different models.

along the way, we compared parametric and non-parametric methods, introduced delta analysis, and briefly touched on neural networks and deep learning, particularly models with multiple hidden layers. finally, we wrapped up with classification, covering regression vs. logistic regression, the concept of weights, and the sigmoid function, including its graphs and explanations.

",-0.06303333,-0.049697477,0.03238818,0.08040713,0.10198557,0.011522979,-0.07575027,-0.04773203,-0.00816363,-0.0067117596,-0.047241226,-0.012652868,-0.014464562,0.05705302,0.047450203,0.02208866,0.029925397,0.0620628,-0.055787988,-0.03847861,0.051757812,-0.0062302053,-0.028926663,-0.036024142,0.009164691,-0.05176551,-0.017595576,-0.019292345,-0.077376366,0.00542066,0.001150251,0.0036631587,-0.03278739,-0.024576103,-0.14623736,-0.017055552,-0.09170526,0.043126903,-0.013004529,-0.012087405,-0.020869808,-0.06276964,-0.012286256,0.028768362,0.097375855,0.004273332,0.004306396,-0.040283818,0.039348178,0.013512231,-0.03656123,-0.016845433,-0.08483931,0.019773839,-0.023315772,-0.0374158,-0.008597268,0.033106886,0.033237662,-0.022966692,0.09219644,-0.008517999,-0.0558707,0.010680232,0.04304204,-0.011405954,0.0028026358,-0.004580118,-0.014056565,0.042872828,0.07354049,0.09032704,-0.05816699,0.0400563,0.0125159435,0.03504866,0.051032785,0.08359656,0.008315435,-0.013604852,-0.029985776,0.057992496,-0.0149428155,-0.030340794,0.10138352,0.051732715,-0.06298929,0.006657386,-0.06759484,-0.022579491,0.053628087,-0.02692088,0.023554958,-0.0021439837,-0.051102184,0.021199426,0.02168889,-0.12529002,-0.025330205,0.05067401,-0.030707512,0.052116964,0.030478103,-0.09318494,0.026529942,0.02770804,0.031194502,0.014326247,0.12063676,-0.05365992,-0.009205044,0.06677343,-0.033675678,0.006823771,0.088302925,0.00089606136,0.009905905,0.011325703,0.02388231,0.11722282,-0.06564856,0.042296145,0.028724393,0.034321383,0.057135813,0.04078155,-0.07714838,2.5932144e-33,0.008924588,0.04628985,0.002401878,-0.02717217,0.044082463,-0.051517528,0.022373712,0.0066588847,0.037317976,0.09159205,-0.085396685,0.02053783,-0.023198694,0.02089033,0.10529893,0.033504017,-0.038323525,0.04377121,0.024905797,0.043606665,-0.031684827,-0.08438961,-0.0014711982,-0.013059259,0.014880486,0.093204,0.054016564,0.06620463,-0.08190113,0.016587779,-0.02150912,0.038409982,0.0023235483,0.063509755,0.02605742,-0.03949543,-0.041333184,-0.017951017,0.002497075,-0.0017771696,-0.058347896,0.012247131,0.007858037,-0.026456017,0.020949256,-0.0015395987,0.016411658,-0.061280645,-0.09107565,0.012765545,-0.023346547,0.027029688,-0.019636786,0.025819117,-0.086376645,0.044949353,-0.023079181,-0.035564173,-0.034980662,0.020524876,-0.007181408,0.0035282976,-0.023143293,-0.052165803,-0.028275682,0.003812447,0.049223166,-1.4564201e-05,0.009248454,-0.05320532,0.0088324025,-0.024303097,-0.05215261,-0.048836328,0.06557397,-0.014462216,0.054085594,-0.039425343,0.046018407,-0.00030209206,-0.016476447,0.07737312,0.046984192,-0.05884472,-0.027249055,0.01574761,-0.003084425,-0.050751105,-0.035077292,0.01379964,-0.1648712,0.10644022,0.0031335114,0.0043461197,0.04281666,-1.6071926e-33,-0.11994893,0.027596714,0.008367723,0.0030220144,-0.057050917,0.07440864,-0.081398875,-0.11546908,0.03518311,-0.069606535,0.012032709,-0.014208762,0.027377661,-0.048376646,0.06447792,0.04051184,-0.02449219,-0.03167491,-0.014539824,0.047131043,0.03500048,0.09372253,-0.11349703,0.019814441,-0.04456016,-0.0048988056,-0.051815107,0.025512664,0.013642997,-0.008783303,-0.03491219,-0.034947608,0.011006672,-0.098269634,0.004529268,0.012241891,0.052222062,-0.05934184,0.024330776,0.013393585,0.05415702,0.017416213,0.019152548,-0.01066832,-0.03309236,-0.049627837,-0.014806008,-0.0036264283,0.0013575689,0.11793415,-0.0057965354,-0.02635339,-0.14983894,0.018784082,-0.04250614,-0.032585215,-0.0039975853,0.007557682,0.041413885,0.023254136,-0.1364455,-0.01110594,0.022581931,0.070108995,-0.019325336,-0.02339984,0.054171246,0.02507924,-0.0068865395,0.015296059,-0.05264426,-0.04876867,0.021438612,0.053923216,-0.11989034,-0.05957218,0.034859784,-0.05836081,-0.072842225,0.0015852187,0.0988765,-0.02997311,0.014414835,0.018537205,0.061179813,0.050307266,0.04504845,0.020096071,0.05895404,-0.047765527,-0.05562176,0.07499424,-0.06849301,0.026522307,0.021993233,-5.5648663e-08,-0.02862832,0.13367613,0.013279766,0.022045935,0.0048500863,-0.025752114,0.04749518,0.16181144,-0.09359829,-0.04803211,0.04523998,0.04469435,-0.011891106,0.040647015,0.0691384,0.02746586,-0.04449039,0.03777301,-0.009608507,-0.016401427,0.08053171,-0.003596538,-0.009487098,-0.052375283,0.11873238,-0.07222529,-0.019303508,0.04183893,0.024986366,0.06220701,0.0081546325,0.028160911,-0.01182664,0.06700193,0.050238937,0.062290773,0.08511357,-0.0068920166,-0.017437007,0.022484178,-0.023430413,0.1116023,-0.05244572,0.044789877,0.05574405,-0.014221787,0.0980616,-0.09162054,0.046434283,-0.0051752017,0.027621616,0.008410949,-0.026053892,0.002861694,0.06760891,0.088366665,0.015847651,-0.023165405,-0.036576852,0.0630639,-0.030814886,-0.16208403,-0.016228123,-0.016056834,5,0.057269394,-11.852964,9
74,"we started our discussion with multiple linear regression. we saw that we can derive more features from given features also that might help us in fitting a better curve. with more features the adjusted r2-score decreases. and hence in a situation of more features we should analyze the p-values of all the features, the features which best signify the dataset will have lower p-value and one with higher p-values can be dropped. if we force a model to fit the training data then it would lead to overfitting and the model will not able to generalize well. the dimensionality of the problem can be reduced using methods like pca or t-sne etc. we should try multiple models from linear-regression to xgboost etc. and one which performs best should be chosen. then we switched to classification and discussed about it. we started discussing about logistic regression. looked into logit/sigmoid function and it's plot.",0.05014863,-0.024556857,0.032655362,0.01916229,0.07335925,0.05246338,-0.04338939,0.0766991,-0.07562149,-0.036651604,-0.060279377,-0.0004377803,-0.04972309,0.06891634,0.04330312,0.0041856826,0.023803828,0.058413696,-0.010280267,0.03763562,0.023874937,-0.11259143,0.039415617,0.06660173,-0.04624843,0.020918995,0.015876038,0.0032788855,-0.08385403,-0.019865056,0.04301224,0.05396568,-0.035030916,-0.00053107337,-0.039919563,-0.030319544,-0.08036541,0.07444412,-0.0076160207,-0.031771883,0.056812298,-0.07302605,-0.007721785,0.0005243262,0.11889195,-0.0001788852,-0.04228768,-0.034751955,-0.026693204,-0.043758232,-0.019306434,0.015129589,-0.041816745,0.03165839,-0.019927008,-0.071732,-0.015678432,0.014144067,0.04480341,-0.070781685,0.023171835,0.039634835,-0.0034255146,0.03315569,0.03651351,0.01317041,-0.007842553,-0.022187524,0.0058977962,0.080387756,-0.022961546,0.059003565,-0.029302368,-0.013546706,-0.022057971,-0.016636223,0.01990157,0.043711234,-0.028907314,0.04668558,0.042754702,0.02837181,-0.04042164,-0.033410367,0.033145487,-0.035810564,-0.045497417,-0.09250463,-0.12975182,0.027647201,0.057940952,0.0045349034,0.040888645,0.053319186,0.0012127755,0.021833884,0.029277451,-0.12154933,0.019421842,0.026068952,-0.063121416,0.046779767,0.071557194,0.020542547,0.09000363,-0.016695736,-0.050499775,-0.04714493,0.067729175,-0.04582818,0.0033380694,0.008719713,-0.08669763,0.020153042,0.08647289,0.09925486,-0.0047623864,0.09293502,-0.06794549,0.0931081,-0.085983604,-0.015449674,0.01392907,-0.011129097,0.07985549,0.046403352,-0.17517225,6.7786564e-33,-0.022255968,-0.005204427,-0.052733306,-0.05839302,-0.0011430809,-0.005101885,-0.014665168,0.048207853,0.05560267,0.03656094,-0.0682816,-0.035386067,0.008350363,0.044417135,0.08477832,0.011659372,-0.055294745,0.052129414,0.01865814,0.046766523,0.0052021495,-0.08360845,0.031361356,-0.021722335,-0.0013364851,-0.019166984,0.016295241,0.04051932,-0.1021677,-0.0013360354,0.025286743,0.032363962,0.0017611025,0.053561606,-0.054985676,-0.08604296,-0.057304543,-0.00715709,0.031808525,-0.03795557,-0.07379938,-0.04271183,-0.033148155,0.00796868,0.047251523,0.09214835,0.03869742,-0.056973714,-0.14786519,0.020069288,0.010265981,-0.025186967,-0.06575937,0.042425856,-0.075608894,-0.03464781,-0.06777736,-0.05227303,-0.017681794,-0.018075246,0.038465668,-0.02742615,0.037501488,-0.09694575,0.010959899,-0.05495374,0.1006086,0.035679627,-0.05052198,0.046235222,-0.04502172,-0.07606956,0.009968537,-0.055914998,0.08883804,-0.031815913,0.09012739,0.031202082,0.05824538,-0.015505159,-0.019857716,0.0807448,0.080337,-0.048830956,0.065395705,0.0059210407,0.0013019688,-0.06918462,-0.033998717,0.027789714,-0.08874176,0.056893215,-0.0566695,0.046262007,0.037401743,-4.778323e-33,-0.0341652,0.052311327,-0.01900356,0.04645859,0.0021865892,-0.020346744,-0.0017215111,-0.05915504,-0.037855364,-0.06587615,0.05647787,-0.052982274,0.07707343,-0.0012319798,0.027841331,0.0019884496,-0.04525077,-0.01565069,0.05938128,0.039703548,0.016906101,0.03719203,-0.07786288,-0.024614934,-0.04399208,0.016389012,-0.07817473,-0.031121591,0.023299584,-0.028935486,-0.049615007,0.07469184,-0.034245644,-0.026455432,0.039895017,0.047952417,-0.04680279,-0.012645778,0.03638926,0.18696882,0.079270825,0.033982642,-0.016483678,0.009312367,0.02786637,-0.026783368,0.006518882,-0.019915087,0.07710274,0.059207853,-0.016888484,-0.014344487,0.019124866,0.041339807,-0.020656088,0.00039520135,0.003975677,-0.04466564,-0.00785862,-0.0042429855,-0.018311922,-0.023557968,0.023096204,0.11201194,0.0007482904,-0.03195294,0.099361554,0.069149815,-0.06659917,0.0128097925,-0.0046763364,-0.05357968,0.081051566,0.060901415,-0.020856317,-0.05078709,-0.07182002,-0.0054672537,-0.02926732,0.022556039,0.025069378,0.011442146,-0.028937003,0.029134974,-0.008931977,0.04133968,-0.010311615,0.010280167,0.044764068,-0.080696434,-0.016727187,0.099811755,-0.016263649,0.04128896,0.038987514,-6.5142416e-08,-0.038729757,-0.06261317,-0.017153371,0.028912792,0.07439975,-0.04815774,-0.10251742,0.1546835,-0.0052382443,-0.020020077,0.01645372,0.028285764,-0.023766011,0.063516065,0.050788265,-0.02183814,-0.01826854,0.03385742,-0.016853567,0.049009435,0.0379793,-0.06972314,0.019735593,-0.06339735,0.11322107,-0.09180997,0.024675297,0.040504653,-0.057172816,0.048299313,0.026031218,0.002776895,0.019396866,0.002531379,0.06540928,0.06030014,0.043031108,-0.06646549,-0.029913275,0.080204956,-0.02770259,0.03845371,0.001602185,0.024986153,0.049504135,-0.03358103,0.042980198,-0.04195804,0.030501217,-0.012283241,6.0731323e-05,0.033736784,-0.025380097,0.003586624,0.050464813,0.04369861,-0.04589056,0.081188016,-0.02427494,0.010107822,-0.078809425,-0.13541827,0.012108983,-0.008332414,5,-1.345943,-6.1764026,9
82,"the session started with a question of how to improve the quality of the results. one solution is to improve the sample. to improve the sample, we can either increase the sample size or increase the sample quality. but how do we know if the sample selected is of good quality. another way to improve the quality of result is to improve the method. we can use multiple method and then select the best one. but we noticed that if the nature of data changes, then even the best method may not be the best for the new data. another solution to improve quality is to fine tune or properly use the method. 
then we went back to mlr. the base idea is that in multiple linear regression the outcome may not be linear but linear combination of independent variables. 
then we saw a sample. after the regression the error showed a specific pattern like a function or polynomial. for such cases we can use polynomial regression where we create new parameter x1, x2, x3, x4, which are polynomial or function of original x parameter. ex. if the error shows the pattern like a sine function, then taking x4 = sin(x) can be helpful. and now in mlr, the coefficient for sine function should appear significant and other all parameters should drop. 
it is not possible that we get a 100% accurate model. but at least we should try there should not be too many features and overfitting. 
then we discussed in short about forward feature engineering and backward feature engineering. 
though, we can use multiple models to extract data but it always better to have a single model. 
then we analyzed several models together. models were like random forest and neural network. 
by the end of the class, we moved to classification. it is applicable when y is either nominal or ordinal. we studied about the boundaries between the groups of y and that to calculate the boundaries we need equations. for which we use the sigmoid function. ",-0.04270453,-0.03577651,0.04689839,-0.036811303,0.046439096,9.003247e-05,-0.050945938,-0.016629461,0.05567653,-0.03378576,0.0023167655,0.07644133,0.015143664,-0.039290518,0.017553255,0.046810932,0.026513094,0.108515985,-0.117695086,-0.07712844,0.030462716,-0.060583048,-0.058043838,0.03188163,-0.038458336,-0.0034072192,-0.036418814,-0.036630463,0.027289765,-0.02979766,0.06709658,0.07206415,0.025286384,-0.035942305,-0.08441155,0.026107486,-0.066398725,0.054711282,0.047102474,0.024329027,-0.014595972,-0.048179734,0.012149797,-0.07545748,0.013558257,-0.013813029,0.022222675,-0.12257171,0.02820846,0.0071375067,-0.01168956,0.0017554397,-0.04725982,-0.07866485,-0.008153796,-0.11383941,-0.025363076,0.03232704,0.018793141,-0.0027543772,0.035798725,0.041267022,-0.03948623,0.027730811,0.045795966,-0.03607632,0.035705343,-0.024022914,-0.057437602,0.07994407,-0.022365782,0.030148879,-0.078034654,0.03257927,0.0014639269,0.025862357,0.018486036,-0.010015539,0.018998299,0.03241239,0.0662627,0.08140951,-0.007301035,0.0051637557,0.036591217,-0.04938862,-0.059737694,-0.10815315,0.041166104,0.022209998,0.019635616,0.03127761,-0.15508142,0.06033355,0.010827545,-0.021929065,0.0039808354,-0.008882055,0.13923341,0.07129508,0.014789962,0.056117203,0.00038996091,-0.074618965,0.0014036031,-0.045617964,0.096227035,0.05718505,0.045754667,0.018282056,-0.015312391,0.053046606,-0.012326939,0.013303492,0.09403749,-0.062383275,-0.039074786,0.052640453,0.008368662,0.015994627,0.013173567,-0.042044137,0.11997347,-0.013391517,0.06375269,0.016067537,-0.07722831,5.3388867e-33,-0.07442249,0.048459183,0.021256244,-0.0071857343,-0.019206518,-0.0003402353,-0.03256113,-0.006920816,0.08851356,0.0034523245,0.009343871,-0.032603133,0.0034703,-0.002456569,0.069198065,0.06562037,-0.013028253,0.010958047,-0.04720011,0.060776986,0.011294534,-0.07340072,0.08478726,-0.05182173,-0.056212507,0.04537602,0.093989365,0.051161725,-0.08844576,-0.02305265,-0.00043269116,-0.02019215,-0.005306505,0.06590102,0.031038443,0.028103815,0.011286472,-0.0118516395,-0.016258996,0.07564042,-0.0012909841,0.032799672,0.11660324,0.026897935,0.048663847,0.05188213,-0.07613625,-0.06715282,-0.061384175,-0.029169621,-0.027444031,-0.028412985,0.005203359,0.03217607,-0.03613601,0.04569719,-0.13500926,-0.046845533,-0.010341744,0.022982579,-0.09203927,-0.056452315,-0.050646488,-0.012916728,0.0765144,0.007582074,0.085278355,-0.050450277,0.0090900315,-0.08478533,0.00057920243,-0.0290445,0.005129486,-0.029348793,0.04739744,-0.07744419,-0.03455536,0.016336516,0.03584713,-0.0646387,0.040370204,0.09322624,0.027591037,-0.065264866,-0.053958897,0.0082648555,0.02322338,-0.03552557,0.045290697,0.0036156708,-0.09657925,0.028352128,-0.018050294,-0.008248081,0.055679776,-6.366921e-33,0.0147404075,0.026875863,0.08479422,0.068322584,-0.08853824,0.023146464,-0.0056255297,-0.095354326,0.005994298,-0.034800675,0.008135684,-0.06135385,0.0372774,-0.0024761774,-0.0022428636,0.021562705,-0.02749385,-0.04793709,-0.054291613,0.015260427,-0.011953242,0.07803307,0.014710009,-0.029438082,-0.026039205,0.0067915423,-0.093438335,0.021136494,0.02467447,-0.039877407,0.038319867,-0.0109369,0.02863687,-0.017199336,0.08467918,0.07525213,0.044596117,-0.029211503,0.012265529,0.07697864,0.039197117,0.019071173,0.00019633671,-0.09234354,0.023869365,-0.0137892,-0.017731521,-0.026253078,0.058617014,0.049416173,0.012815946,-0.058046866,-0.15103817,0.040968105,0.005190387,-0.04287335,-0.04626975,-0.015287984,0.031252224,-0.00582381,-0.112853944,-0.019073123,-0.028096197,0.06497758,0.01021603,0.03502822,0.012026251,-0.027141554,0.07785055,0.05338755,-0.14225242,-0.1128616,0.023772525,0.020135187,0.006265699,-0.027035898,-0.039360132,-0.10961685,-0.09168109,0.009342976,0.015787378,-0.088212095,0.002924892,-0.054843243,-0.022839122,-0.02116325,0.044382222,0.043812107,-0.0038156107,-0.11022867,0.002170386,-0.01115673,0.0090010865,-0.025654575,-0.01942912,-7.053804e-08,-0.049810372,-0.021770364,-0.019656055,0.008733822,-0.029715989,-0.0384351,-0.07232937,0.11570927,-0.03154744,-0.020070862,0.029692987,-0.00057845545,-0.035152975,-0.025232278,0.10578473,0.01996631,-0.058047526,0.056278314,-0.030051729,-0.03853245,0.02580128,0.03980155,-0.0222404,-0.096708484,0.06523158,-0.012061191,0.05176492,0.04608448,-0.04646469,-0.01643738,0.04779567,0.06078191,0.029610934,0.05452434,0.0862513,0.028853755,0.05646306,-0.02974557,0.070798896,0.028981,-0.02920516,0.12797084,-0.10336036,0.03440197,0.08115803,-0.019038413,0.03239331,-0.024568178,-0.07353724,-0.022947371,0.03360664,0.0324787,0.03611821,-0.0015141662,0.05595948,0.0030362138,-0.03573543,0.008435457,0.0064383713,0.0051777908,-0.041276585,-0.025723003,0.0019346167,-0.09094264,5,-4.0032496,-9.272634,9
97,"to improve the quality of results we can improve the sample by either increasing quality of sample or size of sample. we can improve the method by using multiple methods and select best one. also we can fine tune and properly choose parameters. linear regression- outcome is expressed as a linear combination of independent variables. taylor seriers expansion- any function can be written as a sum of powers of x. so we can fit any curve by defining x2=x^2,x3=x^3,...... and use multiple linear regression. if we keep on increasing number of feature, adjusted r squared value decreases after a certain point. linear regression of non linear independent variables. the resulting regression method is known as polynomial regression. the technique of starting with all features and remove features one by one until the model performance reaches a peak is known as backward feature engineering. sir gave two exercises to solve based on feature selection and polynomial regression. for different datasets linear regression does not always give proper fit. there are many models under supervised machine learning. based on data and exploratory data analysis we choose which models to try on this data. each model can handle a different type of dataset better. we can use multiple models at a time also. we need to fit a single good model rather than fitting multiple models to predict output. we have to think in long term. if we fit multiple models, total cost of ownership increases. linear regression and similar methods are parametric methods. random forest is a non parametric model. with parametric model we can do delta analysis(if there is certain change in feature what would be the change in output). xg boost is also a nice non parametric model it gives residuals in better normal distribution as compared to random forest. neural networks are examples of parametric models, most of the present day ml models are based on this. if there are more than one layer it is a deep learning network. but deep learning requires a lot of data. when y is nominal or ordinal we use classification techniques. when it is internal and ratio, it becomes a regression problem. we then went on to logistic regression which is a classification method. regress- to go back to the mean. in logistic regression we try to find boundaries between groups. we need to find the boundary so that the misclassification is minimised.  the output in this case is a categorical variable which denotes to which group the given point belongs. we need a function that will convert any variable in between 0 and 1. we use the sigmoid function for this purpose. ",-0.0014649967,0.00038287792,0.028618082,0.04008242,0.01453555,-0.0036226807,-0.041147973,-0.020247217,-0.050451744,-0.03719993,0.0033204681,0.03575249,0.00586417,0.04221907,0.027764954,0.054606944,0.0048745624,0.08113289,-0.016280865,-0.032986663,0.06498104,-0.04535151,-0.074288435,0.043918304,0.036429245,-0.037163988,-0.026440293,-0.016678968,-0.041054234,-0.0007766716,0.020478537,-0.024520814,0.03278088,-0.05536563,-0.058413778,0.006990914,-0.06676675,0.08705228,-0.011958237,0.0038156267,-0.0031385086,-0.0716048,-0.011494023,-0.015963318,0.15170497,0.034300018,-0.013565285,-0.049661983,0.00067249336,-0.010103831,-0.026246086,0.013610335,-0.059572272,0.005780061,0.020324942,-0.11812332,-0.025185524,0.03901789,0.05637546,-0.0023757091,0.05689052,-0.01051687,-0.044105265,-0.0017745243,0.048072107,-0.017534167,0.01835502,-0.020197054,0.023485878,0.08723343,-0.0009463843,0.056134637,-0.04759626,-0.00545257,-0.03886629,0.04732678,0.05605754,0.028758055,-0.014373134,0.0045198896,0.02571523,0.03917245,-0.0082764495,0.0011664769,-0.007641968,-0.0029382915,0.0026565988,-0.08049971,-0.10644583,0.09192468,-0.007026958,0.06782949,-0.018252414,0.0128998235,-0.012173669,0.01765822,0.05019814,-0.11252494,0.071726315,0.09985815,0.014779548,0.044696324,0.05322432,-0.017923791,0.054647285,0.0015121822,0.03183689,0.0345747,0.011423176,0.00985483,-0.023259578,0.0033762907,-0.10481822,-0.012718842,0.117301926,0.013058441,-0.000573241,0.06555462,-0.044117466,0.06616554,-0.02532456,-0.017264105,0.057994824,0.013070233,0.050004065,0.05143349,-0.056228206,1.01550215e-32,-0.044530194,-0.028237177,-0.018325474,-0.044421557,0.012788635,-0.028863462,0.014375074,0.018128011,0.08420961,0.049951117,-0.019808853,-0.052924465,0.02607235,0.029802967,0.09513943,-0.03602799,0.015632326,0.053354207,0.020678692,0.059840623,0.043477505,-0.11661257,0.056870673,-0.037992485,-0.013330942,-0.0028240164,0.052008376,0.030202908,-0.104657054,0.0045069177,0.035219684,0.029256076,-0.05303166,0.02220131,-0.03297543,-0.05219585,-0.0041685826,-0.1050751,0.036981944,0.08924653,0.0026915653,0.016432665,0.030057915,0.0027294885,0.014428396,0.08050783,0.04172419,-0.07290287,-0.032091405,0.028402667,-0.03944127,-0.0024176908,-0.026992157,-0.04277876,-0.085085645,0.036292918,-0.06769864,-0.058023635,-0.0042278003,0.039132174,-0.06219557,-0.059330795,-0.040780775,-0.051445697,-0.0161224,-0.019948423,0.097322434,-0.009303876,-0.013333168,-0.052044347,-0.031730812,-0.044602424,0.042402238,-0.07227371,0.05988979,-0.013050387,0.044323847,0.0031898655,0.033866763,-0.058882944,-0.1045166,0.07566759,0.020927763,-0.11034497,0.10024776,0.015149567,-0.029019395,-0.06878339,0.018931488,-0.017500916,-0.14277391,0.11393339,-0.00778984,0.047547393,0.046032928,-7.660944e-33,0.016573275,0.024671903,0.012575923,0.06442223,0.0044646785,0.061241247,-0.085952096,-0.025988713,0.02734373,-0.064254485,0.0029601324,-0.050352965,0.09496543,-0.027542673,0.011199075,0.050233517,-0.028278537,-0.09936655,-0.044945147,0.06553385,-0.042627845,0.07692462,-0.027704146,-0.0078382455,-0.06093321,0.0007538139,-0.116582006,-0.019513942,0.024560886,-0.013082698,-0.070924334,0.019946644,-0.0017685513,-0.0030948617,0.02164828,0.025648694,-0.009142539,-0.06828634,0.02259007,0.1140502,0.08935843,0.091553755,-0.0122543145,-0.065397486,-0.012805433,-0.032586847,-0.00090133987,0.023071941,0.028870795,0.049806148,0.017507536,-0.0086079165,-0.07988992,0.005151327,-0.03787966,-0.048122007,0.022033647,-0.03260974,0.030020813,0.015059982,-0.042704478,-0.019462988,0.022690006,0.09980495,0.025907207,0.0042731864,0.015353012,0.0423794,-0.06467122,0.010833528,-0.104404636,-0.076385885,0.016780645,-0.0066479463,-0.05130241,-0.08284521,-0.025067564,-0.07958155,-0.04572655,0.022558035,0.1308224,0.052398253,0.0044706287,0.0325195,-0.027449204,0.061315324,0.016947642,0.028659655,0.020820996,-0.032757107,-0.01911105,0.04567308,-0.028257295,0.061042685,0.059500042,-5.8720694e-08,-0.068634935,-0.006058758,0.024114124,0.031721156,-0.0067478428,-0.042479128,-0.05076608,0.17803226,-0.015398319,-0.098438606,0.048156604,-0.0022705358,-0.03756049,0.07248286,0.08278179,-0.0085949,-0.021023728,0.06426269,-0.05063511,-0.037378702,-0.009183934,-0.042363197,0.0059515033,-0.09551132,0.1007577,-0.08500297,0.007940781,0.04352768,-0.02893421,0.036607448,-0.03429177,-0.012026848,0.009487193,0.056657705,0.12271916,0.036178943,0.0930832,-0.052615598,-0.039303612,0.07794996,-0.029494891,0.117533654,-0.025328735,0.011034541,0.0067661256,-0.051569898,0.07540124,-0.06638534,-0.006163388,-0.0065453076,0.05686155,-0.0025107756,0.0116088055,0.006497998,0.041791774,0.092182204,-0.073635764,0.012771067,-0.041664068,0.04343486,-0.020733962,-0.0856425,0.048176542,-0.06953108,5,-1.2841934,-8.266131,9
98,"summary
todayâ€™s class discussion start on a topic of how to improve the quality of results ? we can do this by 1) improving the sample like quality and size of sample 2) improving the method like using multiple methods and selecting the best one 3) fine tuning of methods or properly using the methods example â€” gridsearch
the new definition of linear regression is that outcome is expressed as a linear combination of independent variables.
when using mlr for any datasets and if the error plot is not random follows a pattern. this indicates that forcing a line to model this data results in incorrect results. we need to introduce non-linear independent variables in the system so that the multiple linear regression method can use this non-linearity to produce the desired non-linear y_cap. this method is still linear regression but it is linear regression of non-linear independent variables. the method is known as polynomial regression as polynomial terms are introduced as independent variables to handle non-linearity in y. another term is feature engineering in which we introduced additional x variables to improve the performance of ml methods. there are two types of feature engineering 1) forward engineering - it starts with an empty feature set and iteratively adds one feature at a time based on their performance and 2) backward engineering - it starts with a complete set of features and removes features one by one until the model performance reaches a peak. both the techniques have their advantages and disadvantages and can be used in combination to optimise the feature selection process. next we talked about some non-parametric models like random forest, xg boost, knn, etc and their features. some glimpses of neural networks which is an example of parametric models are composed of neuronâ€™s and links with their weights. if we start increasing the layer it becomes deep learning but it needs more and more data but chances of overfitting also increases.
if the difference between r_squared of train and test data is more then it is case of overfitting. by comparing r_squared and mse value we can select best model. next we learned about the classification when y is nominal or ordinal values. if y and x are available then it is supervised learning. in logistic regression which is used for classification creates boundary along the data using sigmoid function.",-0.049918164,-0.046135418,0.03548203,0.024437778,0.023837278,-0.042329427,-0.012181412,-0.05860798,-0.05452055,-0.027123505,0.005609382,0.046245914,0.037440892,-0.004603012,-0.01719885,0.026181245,0.02477816,0.064888895,-0.0461525,0.010594378,0.09087133,-0.06478214,-0.06306388,0.048212323,0.043130815,-0.040136114,-0.011208876,-0.008836735,-0.018765505,-0.030465234,-0.020376036,0.037708525,0.0060311323,-0.012879372,-0.0507207,-0.019449005,-0.045136143,0.06345168,-0.011334967,0.034116633,-0.046559416,-0.049315322,0.025337119,0.004269277,0.116545245,-0.010270771,-0.010862364,-0.14100537,-0.019823829,-0.056340892,-0.08861199,-0.033506718,-0.0163659,-0.042080265,0.018419905,-0.064082414,0.008928179,0.025433455,0.063353494,0.005501552,0.036480792,0.02581426,-0.06685246,0.024601651,0.022566533,-0.047620017,-0.037486095,0.08918956,-0.027199302,0.09913574,-0.03957054,0.058651518,-0.043376762,0.0014454147,-0.023420935,0.051132005,0.034355946,0.05018884,0.02987609,0.0701064,0.013759587,0.039366174,5.5781762e-05,-0.005749268,0.0006770293,-0.036266785,-0.0591858,-0.0853475,-0.046629753,0.027676571,0.006917162,0.07312432,-0.031817026,0.07851338,-0.011323416,0.003882642,0.09839952,-0.09761914,0.10608132,0.097612455,0.0055836686,0.06195869,-0.0115952,-0.061986174,0.0462585,0.04289907,0.090509236,0.057263043,-0.019819116,-0.026757454,-0.0033124553,-0.028590368,-0.113535404,-0.021320539,0.10179578,-0.013573808,0.0324284,0.031614006,0.008144998,-0.03427125,-0.016511591,-0.04415407,0.091291316,-0.009574024,0.030906055,0.097339235,-0.101544894,8.212295e-33,-0.06182647,0.010257885,-0.044001315,-0.053979144,-0.04436,-0.024358382,-0.011947251,-0.003652946,0.05438556,0.042315904,0.033228572,-0.062529214,0.014114585,0.048928767,0.11442553,-0.024122693,0.056587152,0.03163342,0.0372703,0.10263227,-0.00513949,-0.068763204,0.08301206,-0.08620975,0.030009478,0.024120072,0.016317887,-0.020390855,-0.06920063,0.03524248,0.043158498,0.09760077,0.01618494,0.03618408,-0.04788243,-0.028197398,-0.036885254,-0.039506797,-0.021280238,0.053323,0.037036974,-0.011883264,0.057887644,-0.050989605,0.029851064,0.06629653,0.020648569,0.0068243146,-0.04019291,0.04389184,-0.051698677,0.041984446,-0.05534576,-0.031602275,-0.058390193,0.02424113,-0.08054509,-0.027469296,0.011702502,0.053455036,-0.1113714,-0.05089041,-0.010259106,-0.087705575,-0.0037851604,0.02646208,0.08243924,-0.047914565,0.015321441,-0.08894565,-0.023264246,-0.040844396,-0.059193414,-0.01828585,0.0918428,-0.07297802,0.01880966,0.061927944,0.05609577,-0.11783037,-0.005750886,0.104574926,-0.01832331,-0.06112743,-0.009488604,0.0045678564,-0.033959135,0.06852383,-0.0026146676,0.016912736,-0.11502092,0.055675853,-0.025154965,0.055478584,0.047250982,-8.064262e-33,0.036271714,0.094855525,-0.038846735,-0.015387749,-0.018721461,0.034143876,-0.011713041,-0.09799232,0.06836337,-0.0560851,-0.021622239,-0.07075617,-0.02743782,0.06586345,0.012796059,0.020264076,-0.04739938,-0.07426779,-0.05354524,0.0156712,0.041758884,0.08082991,-0.04031549,0.008865987,-0.009653826,-0.00945271,-0.16531144,0.07073561,-0.029100444,-0.0057552285,-0.0037881301,0.017295564,0.013385506,-0.06453128,0.022664877,0.08021853,0.0043933643,-0.057830613,0.051171154,0.05471982,0.07931019,0.027500598,-0.0096042,-0.10361679,0.025309822,0.0020784112,0.003301803,-0.0076243957,0.046324536,0.075735755,-0.022316596,0.0076344768,-0.104870774,0.020425234,-0.02124013,-0.043491486,0.005589985,-0.037129927,0.028235586,-0.037427727,-0.0445959,-0.040001642,-0.013413772,0.06520365,0.049439177,-0.037611052,0.022232233,-0.017842798,0.012586803,-1.7266662e-05,-0.06323873,-0.047793094,-0.03844149,0.009652685,-0.051766686,-0.05025467,0.008555069,-0.073282614,-0.064243086,0.024692435,0.08819377,0.007378418,0.05670908,0.008605981,-0.016291551,0.006792967,0.06984879,-0.016363928,0.06432102,-0.066482894,0.0072352174,-0.034248795,-0.07294694,0.086641885,0.06735606,-6.4267724e-08,-0.024281805,-0.023056073,0.012803449,0.071511954,-0.01824721,0.013052468,-0.04641464,0.1040223,-0.037805546,-0.071624994,0.006498148,0.030581705,-0.04032533,0.027116003,0.05410624,-0.020925358,-0.07458673,0.099484816,-0.049870387,-0.057565846,0.028291924,-0.021447819,-0.059243947,-0.05238075,0.104493275,-0.06162953,-0.0067610918,0.0411833,-0.013168613,-0.011519178,0.023255408,0.06339367,-0.018691,0.050055325,0.09724682,0.042555504,0.0808401,-0.030506464,-0.019578991,0.04322628,-0.06281664,0.1040379,-0.057037614,0.0050811926,0.0663116,-0.00010281078,0.026813986,-0.09498174,0.0072002425,0.011702564,0.03749441,-0.030348687,0.0067213424,0.0018821077,0.051106982,0.0638631,-0.032158833,0.0049860002,-0.036549397,0.03410476,-0.040744707,-0.047972705,0.012846249,-0.072202876,5,-3.03864,-8.5289135,9
103,"touched upon how taylor expansion allows functions of x to be expressed as linear combination of powers of x

when we have just one independent variable to describe y- why not allow more variables to be included?- make variables as powers of x

this is the basis of polynomial regression

introduced to backward and forward selection
backward- get all features and keep eliminating features one-by-one

get data- preprocess- good data- multiple methods- compare- select best

is fitting one model necessary- answer is still ambiguous to me.

looked into a lot of models- like slr, svm, random forest, knn, ann and more
we were simultaneously classifying the models as parametric v/s non-parametric models

random forest good for prediction for the value of y, but lets say we want to find the delta that is change in y as we change x, then rf is not as helpful

what are neural networks?

learned about the basic of these models; special emphasis given on how anns work by associating weights to the various variables and how an ann with multiple layer is called deep learning model

moved on the next topic-
classification

it is also regression!!! ""logistic regression""?
what does the word regress mean- ""coming back to mediocrity"" (found this definition quite interesting)

logistic regression is about finding boundariesâ€¨classifier- gives you the boundary

once the model is made ie., the boundaries are determined we proceed by assigning metrics- false positive; false negative

labels: # distinct labels = # distinct classes

we are not predicting a continuous value here

we can very well have a line defining the boundary between the clusters

sigmoid function- gives 0 or 1 value- like a switch
s(a) is this functionâ€¨
a is obtained by the indep var and the weights associated with them- w1x1 + w2x2 + â€¦
so, interestingly, we need to find the weights- exactly like linear regression

using the training data to find w1, w2, â€¦",-0.01591865,-0.10685501,-0.015301945,0.08594612,0.11122709,-0.0018673519,-0.06003061,-0.03340362,0.06297508,0.00919426,0.0009655002,0.059607554,-0.017777981,0.040716883,0.050493106,0.018342867,-0.020906482,0.026377019,-0.09476664,0.01207773,0.025176378,-0.025565714,-0.107466705,0.0052445154,0.069409125,-0.054346144,-0.08627599,-0.0016189299,-0.09022739,0.043062214,0.03574459,0.026331002,-0.046378776,0.0037042114,-0.1065515,-0.016962223,-0.0063945064,0.08825752,-0.056136236,0.010909489,-0.050051916,-0.06697681,0.015513796,0.019172544,0.0591846,-0.06744249,0.00034389528,-0.08639588,-0.06841191,-0.028338661,-0.038895704,-0.06661443,-0.071072854,0.025242614,0.045690816,-0.0656752,-0.045226935,0.030899497,-0.036406044,0.029012619,-0.03826969,-0.04576287,-0.045333464,0.0026105572,0.07167649,-0.077109195,-0.0042904364,-0.008455681,-0.020119404,0.049482487,0.0013436014,0.040756483,0.01753788,0.0074115237,0.039666507,0.020777976,0.14368269,0.09630355,0.05224375,-0.02953532,-0.039527338,0.13656384,0.031139854,-0.036067106,0.057691004,0.058205336,-0.05203022,-0.052030217,-0.024553588,-0.007703304,0.024030806,-0.009253964,0.028199095,-0.030515544,0.014349348,0.0039225514,0.020005353,-0.09660275,0.04258095,0.03858777,-0.038392957,0.06704104,0.08395638,-0.069956,0.063397415,-0.060382422,-0.029314933,-0.0033453552,0.015955117,-0.035861798,-0.005210389,0.031867944,-0.031261258,0.0024269552,0.03555097,-0.049111653,0.0055537364,-0.004202202,0.029052114,0.02254134,-0.109654546,-0.018381333,0.016925056,0.050857443,0.059766833,0.016275173,-0.06931645,9.486572e-33,-0.044361763,0.02247084,0.012790215,0.0308326,0.021527555,-0.059190094,0.008344949,-0.015551922,0.08565999,0.07956505,-0.060949024,0.07355776,0.024894992,0.028825924,0.09159106,0.023174617,0.0012421556,0.03093556,0.0641415,0.05073276,0.06909141,-0.0077394135,0.03067617,-0.008325933,-0.014714831,0.0023743077,0.013416932,0.043318007,-0.08896878,0.0047285296,0.027499406,0.024744913,-0.004742148,0.018584874,0.0024887861,-0.040672652,0.02395012,-0.051633652,-0.0025150205,-0.023515625,0.00901796,0.017710263,-0.0032710559,0.0058448217,0.0049282,0.05082235,0.03609424,-0.042841196,-0.084552884,0.0027090765,-0.06310085,0.0299733,-0.04228625,-0.060931366,0.014704761,0.024665697,-0.043693315,-0.038309883,-0.046927597,0.01022848,-0.09154784,-0.05292002,0.023962127,-0.04077455,0.03445555,-0.023843551,0.055489752,0.008960278,0.042590626,-0.062173955,-0.04302699,-0.020165324,-0.01376425,-0.06664203,0.105150886,-0.033208683,0.043296233,-0.08403656,0.06230009,-0.044942643,-0.12255443,0.09928811,0.04171921,-0.01897668,0.026980605,-0.013743004,0.015752412,-0.014748597,-0.008255877,-0.015328075,-0.10473242,0.039300807,-0.01476513,-0.033555456,0.029314289,-9.590452e-33,-0.050888214,-0.06396169,-0.054801468,0.017325928,-0.026492627,0.04055646,-0.078876525,-0.07765134,0.008378124,-0.15784463,-0.018299883,0.010963662,0.025591765,0.017212786,0.014335915,0.018860549,-0.038278386,-0.070398174,-0.013677998,0.021783603,0.023288893,0.1243896,-0.05268179,-0.03628843,-0.03640447,-0.03678671,-0.11967387,0.066165894,-0.01837568,0.03962498,-0.007974954,-0.009056233,0.028407112,-0.03210686,-0.0011882109,-0.013760234,-0.0054993345,0.0036491624,0.037359226,0.090109,-0.037913665,0.022498963,0.023777027,-0.03707792,-0.03456442,0.020144545,-0.004558482,0.044863064,0.028837427,0.039831057,0.08577637,-0.06422355,-0.09074739,-0.027891573,-0.08899424,0.0030727005,0.016753756,-0.017760051,0.0537426,0.046904676,-0.026911706,-0.013765922,0.03711792,0.07587954,-0.07112439,0.021522429,0.022782957,-0.00287835,0.06812365,-0.07325275,-0.05879724,-0.06485515,-0.010058316,0.053056926,-0.08622674,-0.039381523,0.058143996,-0.11778077,-0.02170294,-0.02446885,0.014879408,0.044742372,0.05410314,-0.006276494,0.015122091,0.07047865,0.07919994,0.039331254,0.056040544,-0.11065049,0.017972253,0.03372099,-0.03831359,0.06281183,-0.010489831,-6.8504605e-08,-0.05879721,0.08893056,0.023664158,0.035026964,0.0398424,-0.046578307,0.06110948,0.060299575,0.01112361,0.017661177,0.07188594,0.0665464,0.01715415,0.011257901,0.091271944,-0.036919054,0.02311284,0.04831625,-0.058772307,0.046492122,0.028956475,-0.042186685,-0.07224696,-0.05738886,0.19418308,-0.066446744,-0.08959834,0.03759172,0.032446925,0.064514175,0.007628607,0.051111445,0.018481044,0.098916024,0.07288218,0.08467042,0.049002845,-0.051168825,-0.06106011,-0.018229086,-0.027350333,-0.020723363,-0.0021017762,0.021486167,-0.05005406,-0.017015368,0.051099773,-0.08383211,-0.026727334,0.023829903,0.024035778,0.039293505,-0.008923319,-0.036093578,0.059036084,0.043753747,-0.013503926,-0.023379724,-0.05214661,0.070993274,-0.036649156,-0.05639764,0.008829892,-0.02662948,5,2.4286647,-8.900837,9
128,"first, we discussed the large language models used to summarise the topics of the summary sent by the students. using the superset of those topics, assignments are to be evaluated. then, we discussed improving the result quality using multiple parameters. then we discussed mlr with an example of taylor series expansion. we looked at a scatter plot which resembled the function of sinx and then discussed about how the model will remove the polynomial parameter coefficients and give more weightage to trignometrical parameters for fitting the curve onto sinx. then we discussed the difference between parametric and non-parametric models and their uses like knn, random forest, etc. then there was a brief discussion about neural network. after that, sir explained in detail the mse and r-squared metrics interpretation. then, we ended the class by briefly discussing nominal and ordinal variables.",-0.04436462,-0.05951322,0.00011063912,-0.009169319,-0.0067022336,0.03646269,-0.026482647,0.07626906,0.03189035,-0.01534269,0.017348895,0.05802713,0.029731138,0.048255134,-0.049327664,-0.011472161,0.004624379,0.026831076,-0.060396798,-0.08525235,0.09618289,0.036714446,0.0003889593,0.0014677176,0.04665895,0.02873516,0.003953941,-0.03495701,-0.024013715,0.0127025265,-0.027593391,0.0850242,-0.007644266,0.053882804,-0.100652605,-0.0014268577,0.013813358,0.03672898,0.021213457,0.025315056,-0.092994176,-0.018686803,0.07735357,-0.00417452,0.12945485,-0.092744656,-0.054688238,-0.07380929,-0.030701423,0.05912062,-0.1174189,-0.044331618,-0.06919287,-0.003986229,-0.035662744,0.018878516,-0.02020463,-0.030219268,-0.025304291,-0.056452546,-0.0988414,-0.089411855,-0.062366027,0.045772437,0.03327142,-0.020913068,-0.040495127,0.0318623,0.04312058,0.08462522,-0.012577999,0.069011725,-0.014824422,0.0770098,-0.0075017842,0.035064332,0.009426973,0.037187055,0.04702415,-0.059652284,-0.015031216,0.018919874,0.026391525,-0.0026002966,0.08425586,0.013481716,0.072914496,-0.016848914,-0.05259662,-0.0017273976,0.006012697,0.03443824,0.0006310662,0.020236192,0.01294057,0.044402737,0.024243144,-0.08749401,0.04531845,0.06445594,-0.008790374,0.090035506,-0.015809717,-0.13925001,-0.08194149,-0.062386565,0.069773965,-0.020032315,0.058558606,-0.08704116,0.0038841015,0.0600959,-0.1089666,-0.0021728245,0.04504111,-0.050324477,0.110459514,-0.019319035,0.031752255,0.026625218,-0.031882226,0.041492563,-0.017221617,0.00696446,0.05335117,0.0037128536,-0.08569836,3.0111823e-33,0.0009407839,0.08232844,-0.048365556,0.08114973,-0.016624229,-0.031951264,0.01693951,0.029153127,0.07934909,0.004226868,-0.04909321,0.057749875,-0.019891806,0.04939037,0.10110132,-0.024850309,-0.0033132376,0.05589181,-0.07858189,0.018858721,0.029167403,0.0017072911,0.13425471,-0.051556483,-0.0002474794,0.06144204,0.0349903,-0.038722046,-0.016091106,-0.00052397395,0.007063858,-0.041167192,-0.037648965,-0.0130487075,0.026506819,-0.03178663,0.037601292,-0.020472476,0.08548387,0.0024137502,-0.011900153,0.010827087,0.064019755,-0.024510497,-0.04311271,0.021071482,0.015878325,0.007884274,0.063115574,-0.044819452,-0.12719017,-0.07522204,-0.029852886,-0.036770146,0.024077835,0.03453228,0.014269641,-0.002695081,-0.103430815,0.029856306,0.009929251,0.05070232,0.05048428,-0.057024978,0.008324216,-0.009306479,-0.061826717,-0.003412995,0.12715675,-0.076994345,-0.038036335,0.009982394,0.000371004,-0.03168051,0.008020401,0.042729795,0.0038303805,-0.06731446,-0.008432074,0.051709175,-0.008377356,-0.06015376,0.04906738,-0.15902653,-0.09375468,0.02076972,0.10263297,-0.075426735,0.061602995,-0.026407726,-0.044826142,0.029552378,-0.055814803,0.026508233,-0.010153769,-3.8956728e-33,-0.07232581,0.086046346,-0.06484252,0.07401548,-0.024589293,0.062239327,-0.064148046,0.044422496,0.0049590026,-0.021919556,-0.029564377,-0.04002237,0.031728983,0.014696625,0.0720485,-0.03866215,-0.030405214,-0.06207389,-0.02152923,0.003138283,0.002589045,0.10105463,-0.128254,-0.0299895,-0.013474156,0.017238647,-0.050295986,0.006045803,-0.06834165,0.027562782,0.040236227,-0.056768022,-0.013157139,-0.007393407,-0.02073691,0.044281684,0.10245743,-0.073139355,-0.027461583,0.09532871,0.059039067,-0.027397666,0.008342293,-0.05912208,-0.01431034,-0.034922972,-0.0508525,0.030919805,-0.015550938,0.044330787,-0.01275148,-0.0045980136,-0.03040018,-0.024402276,-0.01738679,-0.028037963,-0.031186756,-0.10059137,0.043524448,0.032895908,-0.0371705,-0.042654965,0.03409751,0.0011835934,-0.059197694,-0.044997938,-0.0061947885,0.012994716,-0.03654588,0.010979007,-0.012727751,0.03273271,0.036443688,0.0059348047,-0.04878882,-0.011972325,-0.011184583,-0.02995048,-0.06114136,-0.034997758,0.04787429,0.01616671,-0.009794684,0.015433751,0.0131199425,0.060760044,0.075335175,0.06615744,0.020907365,-0.004935678,-0.016946562,0.04460819,-0.010004086,0.028563282,-0.044701446,-5.962469e-08,-0.08109413,0.006491257,-0.016826125,0.06526014,0.0036154261,-0.038672302,-0.025403373,0.08481244,-0.045241732,0.105785444,-0.00036337765,-0.0426991,-0.07148407,-0.03745562,0.03175471,0.06337813,0.0050766855,0.10400029,-0.0015929083,-0.06866011,0.15537186,-0.0069601797,-0.017243825,0.017318055,0.06645526,-0.028339006,-0.0044311085,0.09693733,-0.040176023,0.030438686,-0.013807049,0.064666614,-0.010359323,-0.028567476,0.010767936,0.09584268,0.055989802,0.031102236,0.045558672,0.06538769,-0.0711284,0.0013189054,-0.054707367,0.057983063,0.12177083,-0.018113969,-0.067944564,-0.07424485,0.050195698,-0.011821231,-0.037871633,0.0013054207,-0.056121636,-0.029713167,0.018569825,0.029772518,-0.024489773,-0.027348273,-0.055982128,0.028357299,-0.00029372386,0.07597664,-0.078312956,0.02147935,1,6.088735,-7.710106,9
139,"today's class started with a question asked in one of the session summary that is how to check the correctness of the metrics without increasing the sample size and the answer to this was to improving the method or fine tuning it. then we started to discuss about how to capture non linearity of the data set. also we discussed about the magic of multiple linear regression on how it adjusts the coefficient to make the model efficient ( example of some coefficient being the taylor series terms for sine and one coefficient being the sine itself ). then we discussed some facts that the adjusted r square decreases if features increases, some features needs to be eliminated in order to make the model stable and forcing the error to fit normally can lead to overfitting. further we discussed what are parametric methods and how we can ask them a ' what if ? ' question. also we discussed about forward and backward feature engineering on how forward uses an iterative method to add features and backward removes features to improve the accuracy. we then looked at sample fitting done by various models like random forest, xg boost, neural networks, knn etc. we deeply discussed about neural network on how they have weights associated to their features. large sample size is needed for training a neural network properly to avoid overfitting. lastly we discussed about logistic regression.",-0.08136584,0.0026030294,0.02705008,0.051315837,0.042957906,0.029284015,-0.031028958,0.04729729,-0.068329826,-0.02092122,-0.05443721,0.070837095,-0.027341418,-0.024071349,-0.06294789,-0.03886888,0.01972111,0.083378986,-0.112509966,0.0023148237,0.06372956,-0.027558837,0.00042216553,0.033145584,-0.05678699,-0.017921505,-0.027474696,-0.06889183,-0.091841444,0.013292782,-0.020184781,0.05476391,-0.063094385,0.0075714416,-0.090113714,-0.0025524518,0.006251899,0.082859725,-0.00801109,0.0042253155,-0.049167953,-0.05319058,0.00634761,0.020541545,0.124231815,-0.01724207,-0.001471864,-0.061460987,-0.025684917,0.00020143611,-0.07245776,0.01036413,-0.0700668,0.0039152284,0.0099208215,-0.030004239,-0.024955112,0.055027198,0.0056985873,0.028396325,0.007379258,-0.07184295,-0.102630846,0.045950193,0.0048516286,-0.050805416,0.013085807,0.00721065,0.025633743,0.05347926,-0.042820714,0.07349685,-0.037153684,0.043548573,-0.0066704997,0.042613786,0.033816863,0.08710238,0.012868394,-0.04112617,0.016211933,0.041829582,0.07555008,-0.03180372,0.097362705,0.047724105,-0.014325215,-0.026138963,-0.077181004,0.01761916,0.038749617,-0.0007519182,-0.05006475,-0.015136265,0.058305014,0.042018566,0.0029544756,-0.079600334,0.0045249304,0.114276804,0.0029232008,0.10348296,0.07228261,-0.023414092,0.053749267,0.010757594,0.0650866,0.016757064,0.10675673,-0.06749583,0.0016916478,0.013705326,-0.08058522,-0.051830243,0.111282825,-0.050190564,-0.012522604,0.021488156,-0.06420244,0.04962946,-0.074308716,0.037812002,0.0051043537,0.005937665,0.06999267,0.063566655,-0.0913466,6.9195534e-33,0.018752499,0.056593306,0.013775325,0.029351335,-0.07858624,-0.051704552,0.004883552,0.06309094,0.10035965,0.05749547,0.026272893,-0.01387821,-0.024771536,0.08639714,0.09018382,-0.00044414902,-0.058892015,0.046676937,0.004127394,0.0034778302,0.042338464,-0.13449708,0.067242645,-0.037126347,-0.03275295,0.054906406,-0.006423319,0.009720582,-0.108064294,0.022159202,-0.02311559,-0.013638986,-0.060462225,0.016648164,-0.00020662275,-0.053621687,0.016247362,-0.040747344,0.036092885,0.036957934,0.004875824,0.040459238,0.008027829,0.013120589,-0.0068829367,-0.015321148,0.07165115,-0.082455195,-0.052253723,-0.0039729048,-0.020964531,-0.033056095,-0.047767475,-0.025179302,-0.04629328,0.043059174,-0.01220739,-0.031010376,-0.025691548,0.08916993,-0.024442364,-0.03554142,-0.029241238,-0.021254955,-0.060505707,0.0091038365,0.01414102,0.022171734,0.043450218,-0.074399166,-0.03548124,-0.05649197,-0.058323927,-0.04168873,0.05328854,0.033563193,0.055526674,-0.013528055,-0.004090602,-0.0016060349,-0.05606754,0.06768452,0.027654747,-0.13021785,-0.009267402,-0.025447395,0.077323414,-0.0900279,0.037640654,-0.021117369,-0.10719324,0.02360031,-0.019715527,0.0038545022,0.008862443,-5.9102515e-33,-0.021572286,0.039356414,-0.103600726,0.12159496,-0.017700613,0.024425425,-0.033135653,0.02049842,0.008619121,-0.09546738,-0.007693527,-0.05657532,0.10993316,-0.0076036933,0.004618383,0.015190037,-0.048799485,-0.06460692,-0.0035211383,0.0052595837,-0.011317216,0.08334195,-0.06460038,-0.008994273,-0.06146054,0.01812798,-0.12253126,0.025249282,-0.03272964,-0.03104111,-0.0019351441,-0.03046181,0.023789993,0.0679361,0.014109923,0.06216655,0.05549907,-0.028369494,0.012100169,0.12750149,0.10225605,0.034593873,-0.06709683,-0.0010523278,-0.013119208,-0.020252129,-0.0058713835,0.026210226,0.0073444354,0.03445068,0.074370585,-0.012312009,-0.021782232,0.0047585876,-0.016962618,0.0020383908,-0.011936723,-0.018559854,0.06956025,0.051565774,-0.0862973,-0.03133793,0.0069818995,0.03240809,-0.0069687003,-0.013451465,0.0096475445,0.02526614,0.0068657817,0.018003793,-0.077923514,-0.028290663,0.019660888,0.018736739,-0.07696148,-0.03052285,0.05235497,-0.12577184,-0.07689454,-0.02192737,0.025620298,-0.00039927862,-0.0017013693,0.057610285,0.022937102,0.096267,0.080389254,0.074348904,0.0041678236,0.010195765,-0.04201555,0.05625458,-0.045655683,0.03821313,0.029213337,-7.9765016e-08,-0.09698372,0.06751475,0.042018168,0.07816799,0.037068255,-0.046347093,0.062289663,0.14607161,-0.048746504,-0.028567126,0.058507275,0.015572626,-0.011843759,0.06889473,0.03503594,-0.0070160464,-0.051150847,0.061204694,-0.07818797,0.0118998755,0.06097557,-0.030118668,-0.03687645,-0.00899226,0.10068702,-0.08007807,-0.020959346,0.1067477,-0.022185741,0.025376432,-0.008731882,0.04842788,-0.0026936193,0.012697164,0.06772562,0.018749181,0.05989073,-0.027628735,-0.005079432,-0.004495164,-0.07295273,0.107842326,-0.017256765,0.021689028,0.03190984,-0.059671808,0.008030453,-0.039343566,-0.068738475,-0.045831986,0.034882553,0.009887517,-0.0029296156,0.024069391,0.06104082,0.028806796,-0.056193683,0.023290988,-0.09203051,0.04883494,-0.05434696,0.0008709026,-0.059000995,-0.045287248,5,3.0367465,-10.591901,9
152,"today's class covered forward and backward feature selection. forward selection adds features one by one, while backward selection removes the least important ones, both aiming to improve model performance. we also discussed and compared machine learning models including random forest, knn, artificial neural networks, xgboost, etc. 
also that the life of a data scientist includes spending time with numbers, models, and comparisons. we also discussed what a neural network is: a model made up of layers of nodes (neurons), including input, hidden, and output layers, where hidden layers help detect patterns and improve predictions by adjusting connections between neurons. in regression, we discussed the idea that regression means moving towards mediocrity, implying that extreme values tend to average out over time. logistic regression, however, is not about fitting a line but rather about modeling probabilities using the logistic function. the logistic unit, based on the sigmoid function 1/(1+e^(-x)), acts as a switch by mapping input values between 0 and 1, making it suitable for classification tasks.",-0.037805762,-0.08998037,0.023953773,0.083958216,0.075139746,-0.0049401945,-0.040348772,0.020079484,-0.012204782,0.009038559,-0.05362825,0.005844556,-0.028062128,-0.012203591,-0.06067978,0.013851238,0.021066893,0.0026674897,-0.028212631,0.012296978,-0.04360736,0.04391824,-0.025036514,0.008347965,0.029708594,-0.0645095,-0.018645193,0.028210472,-0.097896166,0.017133774,-0.08948992,-0.027797494,0.03686833,0.051997665,-0.16079922,0.007649558,-0.03154623,0.030788504,-0.066912204,-0.025424486,-0.0097290715,-0.05864247,-0.013894537,-0.0072161364,0.07615466,0.02244493,-0.016050367,-0.06511005,-0.020038888,0.023176251,-0.048146263,0.007990661,-0.005922611,0.021686742,-0.038786348,0.023601124,0.03136937,0.04267432,-0.034864057,0.03803538,0.036035333,-0.05714557,-0.011994816,-0.04324619,0.06810263,-0.026534472,-0.06256966,-0.0036196269,0.03904045,0.02669735,0.001442684,0.05657324,-0.024904383,0.015016136,0.066237465,0.02444305,0.1139447,0.057637814,0.051722683,-0.027685788,0.0029736687,0.047441464,0.026850635,0.019217324,0.07579306,0.0009930026,-0.032993075,0.03938176,-0.023243602,0.0073914877,0.016077206,-0.056934983,0.03862596,-0.046490464,0.0051434627,0.013911521,-0.024747543,-0.07978525,-0.026439302,-0.0074718357,-0.09295167,0.056908164,0.08700188,0.007440073,0.09625793,-0.05046434,0.08039211,-0.02259457,0.04625866,-0.065546684,-0.016870948,0.03209055,-0.065126814,-0.035927422,0.050677106,-0.059050273,-0.044477105,0.027818203,-0.033491228,0.122695506,-0.074058965,0.07639107,-0.025238352,-0.0009481991,0.09656422,-0.009198526,-0.054394964,6.2114356e-33,0.03845719,-0.043240365,-0.0067017963,-0.082903765,0.014142895,-0.04000454,-0.025096796,0.03100398,0.09938779,0.083157025,-0.12612015,0.0037524952,0.0065476582,0.13769694,0.060165912,-0.008923452,-0.03967205,0.039564278,0.002034784,-0.045735184,-0.014591643,-0.07405319,-0.0026291446,-0.10745686,-0.010551369,0.08058602,0.016899819,-0.040235624,-0.06678325,0.014830238,-0.01721918,0.041380223,-0.045081705,-0.041557223,0.04871163,0.055157516,-0.039437976,-0.022263559,0.10891085,0.050857153,-0.088137366,0.03152226,-0.011380632,0.024392905,0.01114346,-0.0029485605,0.028671617,-0.023719849,-0.06760536,-0.030590491,-0.007231421,-0.04033434,-0.048177592,-0.051564176,-0.044172026,0.03329143,0.0664749,0.04031859,-0.08831925,0.059722666,0.025865668,0.031237109,-0.009281946,0.0036933105,-0.029436959,0.022373347,0.05766007,0.018305842,0.045237802,-0.024139954,-0.0018829152,-0.0010421943,0.0027794752,-0.0658873,0.0035800363,0.063350484,0.07912026,-0.08822589,0.042158227,0.0042997217,-0.059207708,0.03449167,-0.06890307,-0.076080196,0.053499162,0.07700345,0.060830805,-0.07925821,-0.019013457,-0.016812468,-0.07412382,0.026866278,0.0127778435,0.035346936,-0.047057036,-5.8745007e-33,-0.06545285,0.025084864,-0.039536174,0.074733965,-0.017208798,0.01963214,-0.049674474,-0.012394653,-0.008509438,-0.007209239,0.060684025,0.026112916,0.10501229,0.011728849,0.022787692,0.024763057,-0.0944039,-0.025760708,0.0019420556,-0.020074189,-0.0090134945,0.05689931,-0.10770844,-0.0017883946,-0.11092067,-0.0507197,-0.012637624,0.07446704,-0.029218024,-0.10452567,-0.07498645,0.016755974,-0.0075744838,-0.072382785,0.0524958,0.06373584,-0.03434769,-0.032025445,0.00027136516,0.04098634,0.033716314,0.05481366,0.00803605,-0.010650093,-0.021810263,-0.013483102,-0.05437855,0.035486136,0.043025035,0.038034327,0.026799403,0.016280307,-0.007961471,-0.018158495,-0.04290715,0.04032123,-0.047340617,-0.008917197,0.0035159038,0.113923855,-0.06477352,-0.0263336,0.12581001,0.060234275,-0.05430335,0.0027491818,0.021634184,0.040721357,-0.002018223,-0.075430244,0.072941594,0.036515515,0.07004805,0.017612856,-0.11925189,-0.06316048,-0.014805796,-0.012315545,0.004249252,0.05801863,0.009040886,0.0076807234,-0.027850019,0.0390345,0.05834871,0.08374147,0.0484544,0.055288244,0.027665276,-0.086689465,-0.03369075,0.048307654,-0.066505335,0.000326559,-0.04008027,-5.7980873e-08,-0.030264763,0.051748328,0.10303347,0.018373476,0.102065325,-0.019969217,0.06346542,0.10737448,-0.10173583,0.003491169,0.08829868,0.06870173,-0.026257105,0.00927409,0.08535961,0.0047867084,0.07390946,-0.021062942,-0.008754063,0.01620229,0.045448765,-0.0712191,-0.0016978207,-0.038402516,0.08362061,-0.094698004,-0.046554163,0.040907282,0.0037687207,0.043491207,-0.0062806164,0.057080854,0.031070242,0.03095388,0.06093535,0.09208243,0.07047878,-0.037772305,-0.069816865,0.035056375,-0.032958698,-0.013420859,-0.008256934,0.02223993,-0.06514201,-0.00028625436,0.078227185,-0.065935954,-0.0037070808,-0.058386363,0.05037395,-0.011689497,0.03253415,-0.0006561967,0.07485261,0.08222205,-0.02325645,-0.047959246,-0.06290891,0.09641276,-0.032002795,-0.0438218,-0.029117482,-0.028450057,5,2.1955845,-13.321889,9
155,"in this session, we walked through the real-world machine learning (ml) cycle, starting from getting data â†’ exploring it â†’ cleaning & preprocessing â†’ trying multiple models â†’ choosing the best one (or even a mix of models).
feature selection & model search
â€¢	grid search: tries all possible parameter combinations to find the best one.
â€¢	forward & backward elimination: adds or removes features step by step to improve performance.
understanding ml models
â€¢	linear regression: predicts outcomes using a linear relationship with features (parametric, allows delta-based analysis).
â€¢	random forest: a non-parametric model that works on decision trees; no direct delta-based analysis.
â€¢	neural networks & xgboost: examples of models where weights are learned. more layers = deep learning. too much data with fewer constraints can cause overfitting.
classification & logistic regression
â€¢	logistic regression: used when the outcome is a category (supervised learning). the different groups are called labels.
",-0.035960708,-0.055506237,0.0017828601,0.05212153,0.09656391,-0.0355721,-0.04919597,-0.050629344,-0.011087614,0.0036409246,-0.025606366,-0.0011108516,-0.04857278,0.00055219646,0.039522503,-0.0075899595,0.04498352,0.036983743,-0.0784157,-0.024797538,0.024277056,-0.0017930593,-0.073129125,0.078165196,-0.0139927305,-0.042118732,-0.024458794,-0.017349977,-0.034071147,-0.014592292,-0.031628422,-0.013721173,0.009637404,0.03474591,-0.10848328,0.0001224119,-0.023071155,0.016397595,0.013107037,-0.042286094,-0.037071798,-0.07335376,-0.00272472,-0.062060997,0.11464482,0.020148749,-0.032771897,-0.054007843,-0.052451234,0.012421041,-0.09820105,-0.052635003,-0.028251054,0.060039356,-0.033575505,-0.027757686,0.010072876,0.012356596,0.046044037,-0.0007899135,-0.025928589,-0.034105375,-0.057043534,0.046085052,-0.0031867416,-0.040993042,-0.043249413,0.009180502,0.066192016,-0.0094685415,-0.008085855,0.029426074,-0.04442248,0.033400014,0.011691069,0.010970688,0.13795792,0.03485035,0.035041306,0.022493763,-0.039901268,0.10545907,-0.002790267,0.031882085,0.08887165,-0.062575966,-0.0518253,0.005324521,-0.016225187,-0.0157653,0.032875918,0.01293673,0.0018969289,-0.038100414,-0.020956742,0.07852753,-0.0016765202,-0.0998823,-0.014342355,0.02603189,-0.050966922,0.073258124,0.06373127,0.0014316783,0.047729913,-0.04692591,-0.0024122647,0.019080881,0.076526076,-0.04996948,-0.019227834,-0.0016981572,-0.0501202,-0.025177935,0.043674827,-0.03174731,0.00020571778,0.004960104,-0.011687466,0.100447305,-0.10773734,0.0015769107,0.044657253,-0.043076277,0.033068918,0.008645771,-0.12548819,5.5665664e-33,-0.015465813,-0.031287536,-0.011324009,-0.027738828,0.02182381,-0.07935967,0.008915183,0.031706825,0.12197372,0.09970307,-0.032431893,0.046234317,-0.023446003,0.11507794,0.10561892,0.035674676,-0.08903615,0.047754887,-0.045667384,0.018505583,0.056619737,-0.009254593,0.02319529,-0.013601555,-0.005938954,0.045350928,0.039588924,0.0020384856,-0.09183869,0.045386475,0.012740061,0.013387549,-0.03968778,0.04619685,0.0051604556,0.022883372,-0.046706147,-0.04202988,0.0019752907,0.01153491,-0.086245365,-0.027883288,-0.009639331,0.009416028,0.016872853,0.03520521,-0.032994937,-0.08768425,-0.029209731,0.024305427,-0.006861632,-0.026723895,-0.044091664,-0.024776377,-0.0507996,0.08380562,0.025663873,0.012981307,-0.048734654,0.06853499,-0.012947774,-0.023582343,-0.0013698252,0.008316872,-0.0045847027,-0.053881783,0.094442524,0.05260303,0.028754076,-0.09145238,-0.065593265,-0.036460727,0.070987,-0.08195753,0.07933136,-0.03947996,0.013982603,-0.022929788,0.017701713,0.05756314,-0.03314325,0.08805969,0.009870422,-0.058130313,0.026278282,0.005335184,-0.054604344,-0.040193412,-0.025865585,-0.027893184,-0.1720979,0.09047084,-8.539159e-05,0.03288876,0.0049262126,-5.8924795e-33,-0.0030684632,0.02167382,-0.016795354,0.08981732,0.0026270645,-0.015419255,-0.022000486,-0.088827044,-0.034123152,-0.06985482,-0.02677991,0.040202796,0.08533367,0.085267484,0.0038823497,0.012678151,-0.06369803,-0.057176344,-0.029070793,0.037612066,-0.024833996,0.09474232,-0.100306645,-0.0154751055,-0.037107877,-0.035111357,-0.1072143,0.07682139,0.033990834,0.012137896,-0.0056280596,0.07246962,0.034983944,-0.063976206,0.027851418,0.023831867,0.014263475,-0.045498632,0.03278567,0.083352104,0.048367545,0.0075607393,-0.017724004,-0.05189613,-0.037416603,0.010036939,-0.0834867,-0.015474301,0.040735826,0.00731362,-0.016768936,0.007671516,-0.07178411,0.029117981,-0.019640934,-0.011761013,-0.027897982,0.016114235,0.04813807,0.058248118,-0.06702785,-0.0017485097,0.06425664,0.09142334,-0.04165649,-0.002872314,0.09298701,0.058136582,-0.021689111,-0.0629408,-0.052198883,0.028408336,0.017926965,0.05010473,-0.067401886,-0.042720787,-0.03187547,-0.09267574,-0.053550284,-0.00487539,0.00257724,-0.056900065,-0.017927492,0.06609695,0.03249585,0.0534176,-0.027806692,0.015071823,0.026530769,-0.10474298,-0.03324039,0.016681425,-0.04442989,0.09092294,0.0032672177,-5.863717e-08,-0.011080279,0.06351609,0.113058776,-0.0064870073,0.050489776,-0.002614543,-0.027172847,0.1681247,-0.05364393,0.022781378,0.07548363,0.0058663497,-0.05831794,0.016680013,0.03502596,-0.0066165393,0.0053745583,-0.0005360678,0.010824575,0.04095128,0.031447615,-0.06872621,-0.011271635,-0.0402997,0.14428087,-0.12899046,-0.032963067,0.05978132,-0.0063163037,-0.0007728796,-0.06378255,0.0679837,0.06633778,0.046811882,0.021306185,0.095037624,0.080162734,-0.12723935,-0.07401306,0.049589388,0.0035212685,0.06311174,-0.06933638,-0.027590817,-0.011521177,0.018526241,0.064962365,-0.06466932,0.015227392,0.052712783,-0.012810875,0.011243768,0.037914194,0.035056517,0.054832388,0.062049553,-0.0051573496,-0.0127677,0.038523883,0.09371605,0.04055705,-0.046049397,-0.0040646773,0.02808164,5,1.5949992,-12.52481,9
163,"in today's class we have learned about how to improve quality of result 1. improving quality of sample 2. using different model and choosing the best one . linear regression models the relationship between independent variables and an outcome as a linear combination but the model can be non linear also. each function can be computed from the taylor series . we also learned that if we add unnecessary parameters adusted r^2 value will decrease . models that can be categorized 
as  parametric and non parametric. parametric models are flexible and allow for delta analysis directly  ex- simple linear regression and family. observing how the label changes upon changing a particular feature. non parametric models examples - random forest, xg boost, k nearest neighbors etc. then we saw neural networks where computer have mapping similar to human brain consisting of many hidden layer if hidden layers are more 1 then it is a deep network.",-0.079970784,-0.04797495,-0.019473594,0.016337164,0.04161943,0.018951392,-0.07106789,-0.0065735653,0.06440575,-0.0586156,0.02616734,0.048974987,-0.00512279,0.038797937,0.03157403,-0.06714229,0.06608895,0.07496048,-0.08882246,-0.05870727,0.026006363,-0.038333368,-0.08604518,-0.00928763,-0.0079450905,-0.0426004,-0.014671159,-0.048952453,-0.028674068,-0.034359273,0.009714399,-0.02904232,-0.06539439,0.05619651,-0.055424523,-0.009915389,-0.04164877,0.085964695,-0.0035177225,-0.055615906,-0.04977315,-0.031554192,-0.004237842,0.0028526667,0.10925476,-0.022279792,0.024094395,-0.07213069,-0.06882684,0.0414506,-0.12709591,0.00741044,-0.008564014,0.013472246,0.06860553,0.0039181206,-0.020143162,0.054650415,-0.046158526,0.026673816,0.037124764,-0.04635565,-0.049959596,-0.0052185217,0.016856825,0.03617407,-0.016987491,-0.018373739,0.03893272,0.028781405,0.0051884716,0.08248185,-0.005755252,-0.03494471,-0.023696022,-0.012947417,0.080188096,0.086267166,0.032105014,-0.055296026,0.026487192,0.0007312007,-0.010785867,0.025911858,0.0669882,0.042723343,-0.0411399,-0.036663953,-0.10588897,-0.00972473,0.006653778,0.032797433,-0.04409642,-0.055686094,-0.04144606,0.004210278,0.054041315,-0.09135924,0.017349673,0.10621836,-0.009693245,0.03915236,0.0012624717,-0.052153062,0.111147106,0.030095998,0.10381929,-0.017362801,0.09181637,-0.007063904,-0.022286262,0.031132286,-0.06402972,0.0025632943,0.023226136,-0.098773085,0.048113182,-0.0055278344,-0.023709318,0.018591793,-0.06195201,-0.008280377,-0.019711034,-0.0456514,-0.010389223,0.027231244,-0.091258764,8.303652e-33,-0.015496559,-0.007216069,0.027350526,-0.026244061,-0.027012115,-0.1064424,-0.033496622,-0.030764114,0.1379386,0.04151888,-0.02709063,0.027529618,-0.036581147,0.14894149,0.08460444,-0.009743529,-0.058778133,0.051491678,0.02387459,0.03895819,0.00037478266,-0.038972735,0.04925095,-0.03003107,-0.027233299,-0.0009452474,-0.017242711,0.013430701,-0.072100215,0.019472776,-0.027855525,0.04099755,0.016084438,0.0010053632,0.02855974,0.013999797,0.016928816,-0.008805751,0.032434482,0.005543927,0.021132682,0.04602647,0.039602533,0.0010876509,-0.024816804,-0.024217349,0.06812632,-0.04305436,-0.06348465,-0.09638187,-0.04598159,0.0002737856,-0.035203144,-0.05385619,-0.010141652,0.06756757,0.005920992,-0.0035652907,-0.013618414,0.086334914,0.0106358575,-0.006981847,-0.031470057,-0.012510735,0.06110814,0.0106427455,0.0042647757,0.020605559,0.057324603,-0.083883695,0.019369679,0.004538241,-0.057661023,-0.044177987,0.061987024,-0.026982669,-0.02715921,-0.046451848,-0.027770638,0.02747425,-0.033532076,0.015632868,-0.0045828135,-0.051868714,0.012092185,0.04893528,0.003246375,-0.021922374,0.04062417,-0.01787174,-0.07669446,0.09876536,-0.024522461,0.028632952,0.041230958,-6.61154e-33,-0.111577414,0.07107409,-0.020538544,0.09822818,-0.012075642,0.038452,0.009656169,-0.011489592,0.0013971417,-0.0062026707,-0.038917236,-0.02329433,0.06679987,0.00869869,0.03777553,-0.024130546,-0.09339442,-0.050961226,0.019328002,-0.032118846,0.019462204,0.13983944,-0.09623107,0.009551745,-0.048537146,0.042778935,-0.18921076,0.123659864,-0.010758489,-0.006693268,-0.024647728,0.028312545,-0.012789214,-0.011885717,0.04151706,0.07607867,0.051643662,-0.015407938,-0.030779824,0.013697259,-0.030565316,0.038125448,-0.013457632,-0.044292208,-0.013564901,0.029962854,0.012605181,-0.036055963,-0.08104896,0.040320925,0.059697278,0.022669058,-0.11280686,-0.040749945,-0.018016824,-0.03577296,-0.06550798,-0.0628372,0.112581365,0.094697766,-0.0517016,-0.029004827,-0.055561807,0.08801489,-0.026914736,-0.00014673501,0.04779642,0.049999468,0.0017629847,0.014076736,0.025034424,-0.020700902,0.031066062,0.027749142,-0.06302418,-0.062888525,-0.04025989,-0.045303762,-0.054642323,-0.037740164,0.06837624,-0.038902074,-0.028355127,-0.011397353,-0.002169829,0.10780503,0.056601778,-0.030280871,-0.009961732,-0.025510313,-0.020569937,0.02298737,-0.10919729,0.035603315,-0.0025459353,-5.7863296e-08,-0.0057157944,0.046194393,0.06411711,0.055747785,-0.031905875,-0.0836811,0.058692347,0.16320826,-0.03418469,-0.005749897,-0.030116262,-0.0101249,-0.0018223765,-0.0044709123,0.03974498,0.041702956,0.0614693,0.067767456,0.023447417,-0.0057739895,0.07891244,0.034072585,-0.05098845,-0.019373931,0.12754717,-0.07679506,-0.041897282,0.036637247,-0.020635515,0.027373496,0.044928282,0.07308295,0.022446804,0.034187537,0.04972326,0.07019103,0.079808004,-0.027020045,-0.07005499,0.022776624,-0.047606386,0.09441903,-0.091758065,0.023518195,0.011932726,-0.05444535,0.112402305,-0.09217689,0.016213328,-0.045577064,0.050863028,0.019450216,-0.019475434,-0.04038323,0.02286753,0.023881536,0.022316743,0.013742947,-0.0049577085,0.10790546,0.045249406,0.0037499974,-0.03007301,-0.06518267,5,3.707054,-11.322696,9
164,"in todayâ€™s class, i learnt about the key concepts related to classification and the importance of feature engineering in improving the performance of classifiers. if we engineer new features from the provided features it will be a better model and capture the dependencies. sometimes using the basic features cannot help in providing better results.

we also learnt the concept of standard error, which represents the standard deviation of the sampling distribution of the sample mean. it was explained that if the population standard deviation is known, the standard error can be derived.",-0.01112698,0.037368592,0.025775708,0.015538173,0.02844961,0.009904723,-0.021132886,0.027011534,-0.08538929,0.0044534113,-0.028387783,0.039385382,0.045683872,-0.059650432,-0.025769686,-0.05438593,0.045917235,0.11402023,-0.022363445,-0.0023443454,0.018024385,0.02027011,-0.03953025,0.064599,0.0003630331,0.024533575,0.0031689166,0.0053146584,0.06859274,-0.04920373,-0.009508388,0.05427804,0.083751045,-0.080295436,-0.06222953,-0.0112606445,0.040279217,0.008421451,-0.01507175,0.030904125,0.029063443,-0.063648134,-0.011508752,0.064620085,0.024281964,0.09594961,0.035001792,-0.026455013,-0.0032960004,0.0031951368,-0.014287841,0.050713032,-0.11268918,-0.037561193,0.019568803,-0.008005026,-0.029499939,0.01068398,-0.013230834,-0.0034143683,0.052019898,-0.02981074,0.044803414,0.0071312175,0.031051887,-0.08940957,0.0014538893,-0.033357196,0.11991268,-0.048356146,-0.0472467,0.056789726,-0.028902134,0.017933512,-0.024562588,-0.0062108506,-0.049841404,0.08661342,0.06054739,0.008675321,-0.09127536,-0.009738332,0.023684787,-0.04014869,0.12288734,0.0859825,-0.025244966,-0.009610758,-0.111196846,-0.047616843,0.08855424,-0.058230847,0.008192047,-0.023235667,-0.062924266,0.0058051855,0.0021740163,-0.060513556,0.0094206035,0.062745556,-0.031148594,0.050643962,0.027270854,-0.0022613427,0.070316315,-0.009552629,0.04142597,-0.04492328,0.040682428,-0.0060030515,-0.018187301,-0.044751413,-0.14157803,-0.09110154,0.06410901,-0.02341615,-0.06548939,-0.0019176631,-0.012644049,-0.0013569404,-0.019649051,-0.059750095,-0.010078518,-0.0017379081,0.026995812,0.018480778,-0.020892205,7.0875385e-34,-0.006724836,-0.033949476,-0.058747582,0.045388598,-0.09531773,-0.0015465555,-0.011328622,0.017647518,0.101840004,0.009196374,0.009135829,-0.05448069,0.036731463,0.052735005,0.076014616,0.08453465,-0.06719682,0.017945023,0.038332865,-0.036279395,0.07532797,-0.05325825,0.10809035,-0.020142812,-0.02527548,0.05213797,-0.040619407,-0.016683847,-0.050568223,0.01974521,0.03776191,-0.030048223,0.035636835,0.0028242571,-0.04002075,-0.025983864,0.057604324,-0.03038617,0.024020735,0.02576191,-0.012394858,-0.038200412,0.011368068,0.028163165,-0.011679014,0.081172325,0.000924825,-0.047937766,0.037404407,-0.005346565,-0.09718018,-0.04954285,-0.022257905,0.013159937,0.03552396,0.07515847,-0.035122346,-0.034591425,-0.10927019,0.041485135,-0.07340118,0.07166599,0.018068349,0.025280826,0.04143979,0.04668268,0.09067543,0.09945873,-0.042531263,-0.050218817,-0.025712632,-0.05495862,-0.121349454,-0.035174366,-0.0062722163,-0.022591375,0.07575621,0.00037509456,0.03433557,-0.004874291,-0.008907316,-0.010801235,-0.059054174,-0.09762314,0.012026012,0.079182245,0.049601577,-0.041090284,-0.011219369,0.058256444,-0.017232427,0.038647197,0.021288265,0.08821699,-0.004167774,-2.4046866e-33,-0.09264162,0.05974973,-0.03430603,0.04553282,-0.03599702,0.00032725837,-0.060229737,-0.03395197,-0.102063425,0.035415683,-0.054801907,-0.030428296,0.013893312,-0.030878576,-0.07723605,0.06426116,-0.081646666,-0.043741543,-0.017803676,0.075400494,0.02267139,0.03898339,-0.041850287,-0.028329965,-0.041290782,-0.0075187017,-0.08383074,0.0076762056,0.014647024,-0.11863188,-0.0025985714,-0.06878923,-0.08560425,0.018604564,-0.0050050113,-0.068704434,-0.006246206,-0.048359636,0.062204394,0.084731445,0.049141657,0.058482822,-0.042302627,-0.024535084,0.043129783,-0.04048732,0.0054507526,0.026433865,0.03786858,-0.04147092,0.10717335,-0.033893973,-0.013884401,0.01479044,0.014749934,0.06081181,0.04155726,0.00088400685,0.04854548,0.09543066,-0.060304727,0.002303461,-0.026989808,0.042452056,0.022223277,-0.006724566,-0.028788822,0.0668115,-0.023317095,-0.011377945,-0.029867839,0.004095455,-0.038370162,-0.0067694527,-0.08554602,-0.07633614,0.011442291,-0.03248203,-0.008535059,0.0066404394,0.003523566,0.023110352,0.058378413,0.020560095,0.056420945,0.040653475,0.088017166,-0.043396857,0.05466051,-0.025040012,-0.03408894,0.08749975,-0.07987518,0.08319817,0.022107204,-5.2544948e-08,-0.11321492,0.014961629,0.027566522,-0.045925796,0.028333358,-0.00031729022,-0.10164162,0.052247267,-0.10018736,-0.012615905,0.013826516,0.023723649,-0.04537487,0.07828659,0.12167592,0.062577866,0.0030313851,0.06762717,-0.02056282,-0.0389518,0.11675287,-0.09700757,0.105605446,-0.07759773,-0.017821804,-0.048596088,0.021168385,0.020653395,0.017005589,0.04655236,-0.03427658,0.031608623,0.029237807,-0.0030408483,0.17684369,0.06195846,0.035326414,-0.073490955,-0.058320392,0.056150097,-0.03468937,0.06183066,0.0001010257,0.044965416,-0.047830038,-0.006140394,-0.00039026403,-0.071648955,-0.053461213,0.03486062,0.052247748,-0.021532454,-0.0133718895,0.014119236,-0.018976424,0.040575076,0.008141767,-0.059129205,-0.034897152,-0.04410835,-0.014498441,0.08497523,0.034382213,-0.007811246,1,8.569392,-9.423185,9
192,"the lecture discusses two main ways to improve the results of a particular model:

improving the sample: this can be done by making the sample more representative of the data or by increasing the size of the sample. a more representative sample ensures that the model reflects the real-world situation better, while increasing the size of the sample generally helps the model generalize better.

improving the method: this refers to improving the model itself or adjusting hyperparameters, among other things. one example discussed is linear regression, where the outcome is expressed as a linear combination of independent variables. however, to accommodate different types of functions, we can create new independent variables by applying nonlinear transformations to the original variables. for example, the model can be expressed as:
y = î²â‚€ + î²â‚ xâ‚ + î²â‚‚ xâ‚‚ + î²â‚ƒ xâ‚ƒ
where xâ‚‚ = sin(xâ‚) and xâ‚ƒ = xâ‚â².

applying such transformations can make the model more complex, but it also causes the p-value to increase and the adjusted r-squared to decrease, indicating that the model is becoming more unstable. a technique for selecting the right features is backward feature engineering, where we start with all features and remove the least significant ones based on p-values. similarly, there are forward feature engineering and mixed methods to determine the most appropriate set of features to use in a linear regression model.

the lecture then moves on to supervised learning methods, which are generally used to solve various types of problems. for these problems, we gather the data and apply multiple methods to it, eventually selecting the one that gives the best results. sometimes, more than one method might be combined to achieve better performance, such as by averaging the results or using some other approach to integrate the outputs. for example, in the case presented in the slides, the random forest method yields the best results. however, even in such cases, a linear regression model might still be chosen if the priority is expandability, as linear models are often easier to modify and apply to other data.

next, the lecture covers classification, which is another type of supervised learning, where the goal is to predict categorical labels for data. the labels could be binary (yes/no, 0/1) or have multiple categories (such as classifying animals as mammals, reptiles, etc.). the simplest method for classification is logistic regression. in logistic regression, the goal is to separate the space into regions, each corresponding to a different label. in the simplest case, the model can be written as:
a = wâ‚ xâ‚ + wâ‚‚ xâ‚‚ + wâ‚ƒ xâ‚ƒ + v
where a is a linear combination of the input features. to convert this result into a binary label (0 or 1), we apply the sigmoid function:
sigmoid(a) = 1 / (1 + e^(-a))
this function maps the linear combination of features to a value between 0 and 1, which can then be interpreted as the probability of the data belonging to one of the two classes.",0.00011387916,-0.007827235,0.008441912,0.076408826,0.02129854,0.015529118,-0.08563674,0.02175005,0.03801505,0.005264579,0.023633122,0.084872395,-0.016149914,0.011331782,0.054776046,-0.00295722,0.062275827,0.14480822,-0.1105783,-0.036617897,0.05257477,-0.0678534,-0.037930313,-0.0031428253,-0.0029883925,-0.008047493,-0.005642502,0.025113001,0.015889117,-0.055102613,0.0059908866,0.062732704,0.0016829337,-0.057310957,-0.05008117,0.0058312705,-0.06043369,0.12235859,0.015255892,-0.0098788105,0.008144112,-0.0012765185,0.004530417,-0.011825221,0.07716523,-0.025452873,0.024601394,-0.04231105,-0.06404586,0.009280307,-0.0807872,0.039873347,-0.05590307,0.0056542363,0.030214189,-0.06357662,0.010274652,0.03661117,0.021709118,-0.04564696,0.0716241,0.024027238,-0.022151986,0.019330706,0.015152258,-0.05067094,0.018572459,0.04700455,-0.014079155,0.10175195,-0.023691613,0.056596335,-0.06968734,-0.023755558,0.056657,-0.03909845,-0.01684361,0.03733435,0.07669614,0.023987893,0.078234516,0.06072929,-0.0049483054,-0.04510689,0.0660024,0.012257664,0.012827965,-0.1461035,0.017698271,-0.017513681,0.07157781,0.03949968,-0.023361987,-0.011950183,0.031606086,-0.009906695,0.06904731,-0.068508476,0.06449701,0.10194746,0.09347127,0.03021332,0.053293753,-0.025719756,-0.0034704853,-0.057885036,0.033523407,0.06368882,-0.0053836796,0.028850354,-0.020369375,0.061006106,-0.09552373,-0.03325403,0.058348436,0.04174653,0.04335055,0.037671074,-0.029175507,-0.0051718643,0.030222207,-0.03412833,0.018147856,0.0036601615,0.011470922,0.07852687,-0.15618089,6.164139e-33,-0.012906934,0.054651543,-0.023821067,-0.004897471,-0.0076034674,-0.040292583,0.032115936,-0.01477799,0.05597264,0.06434172,-0.044670723,-0.026050026,0.01496878,0.06382911,0.016415562,-0.040714554,-0.10541452,0.077381276,0.012872346,0.059337344,0.022104746,-0.06556999,0.07459805,-0.09033771,-0.018803606,0.03225545,0.04699946,0.015793515,-0.15664263,-0.01117321,0.0370218,0.04169264,-0.10748955,-0.02840487,-0.02750737,-0.04896599,-0.034688212,-0.07470453,0.042219993,0.010194536,-0.025049785,-0.0013551194,0.035773415,0.07341678,0.04777995,0.07627069,-0.01342544,0.008824605,-0.06943441,0.014011701,-0.05439357,0.015466982,-0.062582485,-0.020040844,-0.07492327,0.032043673,-0.0015052109,-0.08029411,-0.027008008,0.038703628,-0.047097627,-0.02483998,-0.0034326091,0.018975807,0.07130984,0.01562655,0.041749783,-0.02857785,0.05238599,-0.03329424,-0.06538868,-0.07843887,-0.003570352,-0.00040972224,0.020270694,-0.10401448,-0.03618147,-0.021540254,-0.018716615,-0.09408463,-0.033566166,0.0693544,0.037016574,-0.061918326,-0.02232781,-0.064137585,-0.07453196,-0.051932108,0.060530175,-0.07886264,-0.07009587,0.09209895,-0.028663201,0.012314616,0.029671598,-6.6470885e-33,0.015748098,-0.00727372,-0.020485971,0.08790082,-0.038807604,-0.03907913,-0.005912892,-0.0745724,-0.033069603,-0.1259606,-0.03602255,-0.030280432,-0.012550708,0.033593416,0.057747245,0.048316408,-0.037493523,-0.0059830924,0.0047774487,0.02992136,-0.012027652,0.054695684,0.0004608342,0.010392004,-0.047042985,-0.004803735,-0.036634184,0.0025706233,0.082578056,-0.041541222,-0.046290595,0.013972094,-0.08935306,0.012654161,-0.018308194,0.08451759,-0.041095655,-0.098892525,-0.012987684,0.10492007,0.04014056,0.08113622,0.009852888,-0.0018096131,0.037517585,0.04182328,0.023003625,0.03395066,0.020016544,0.030075956,0.046190772,-0.03983359,-0.09709078,0.04160493,-0.014107019,-0.041248497,-0.014844322,-0.061637957,0.048400875,0.018223265,-0.12297356,-0.019754594,0.018056413,0.024590774,-0.022954026,-0.035791885,-0.004626056,-0.052006714,0.054418746,-0.0038366984,-0.04485603,-0.11813356,0.0032463633,-0.005405426,-0.0246969,-0.09009325,-0.0026406052,-0.018066388,-0.064588636,-0.025001997,0.034459148,-0.0062223896,0.016866494,-0.007582748,-0.0812394,0.071921475,0.0022842567,0.027297152,-0.0017487005,-0.008762982,-0.018307295,0.052878678,-0.055345275,0.043367784,0.04235867,-6.9343834e-08,-0.065841526,0.0021184185,-0.010667504,0.026589999,0.00786538,-0.07569858,-0.07011467,0.11432744,-0.012903986,-0.04748748,0.032553986,0.01620064,0.056921728,0.021224618,0.04485226,0.07077858,-0.0060310722,0.104218,-0.039686188,-0.047347758,0.029106736,0.014370879,-0.033540964,-0.07848176,0.13359995,-0.051866774,-0.029762732,0.04514238,-0.041902736,0.036525752,0.057916027,0.044001203,0.021325465,0.03820562,0.07740505,0.058484465,0.012338542,-0.105706476,0.037366584,-0.017345183,-0.07803161,0.10569728,-0.05985869,0.03248059,0.031846665,-0.04909042,0.04214018,-0.10674451,-0.005000026,-0.0012878389,0.026397219,-0.0018111849,0.0014935113,-0.060466032,0.021770807,0.06921315,-0.027654257,0.033516474,0.03175184,0.061315857,0.0052134506,-0.028262345,-0.06591074,-0.078964934,5,-2.185431,-8.045936,9
199,"n this class, we explored various aspects of feature engineering and its significance in improving model performance. we then delved into multiple linear regression (mlr), discussing how adding more terms can impact the model's ð‘…2
value and p-value, helping to assess the goodness of fit and statistical significance of predictors. moving beyond mlr, we examined different types of supervised learning techniques, including random forest, which is an ensemble method known for its robustness and ability to handle complex data patterns. we also covered neural networks for regression, highlighting their ability to capture non-linear relationships in data. lastly, we studied logistic regression, a fundamental classification algorithm used for binary and multi-class classification problems. this session provided a comprehensive understanding of various regression and supervised learning techniques, equipping us with valuable insights into predictive modeling.",-0.013508979,-0.07516637,0.022012746,0.031532634,0.10896835,0.014592469,0.012147246,-0.06338247,-0.06397543,-0.040464506,-0.06802282,-0.0019541786,0.031150714,0.025801618,-0.032418683,0.034620784,0.0048408257,0.011600211,-0.09401676,0.0040389323,0.056340136,0.044779405,0.03296613,0.04767976,0.04203607,-0.0012255053,0.028667666,0.014658985,-0.068067566,-0.033539873,-0.020718304,-0.016245345,-0.038099933,0.0046478068,-0.06657927,-0.031908203,-0.0376351,0.049995113,-0.0042326534,-0.025566628,-0.00026904017,-0.10621425,0.012537024,-0.04141665,0.11325183,0.03560878,-0.033249468,-0.05601216,0.029710183,-0.041472502,-0.04774477,-0.07492823,-0.054709572,0.025262864,-0.025999736,-0.025604751,0.01743942,-0.00425162,0.035042666,0.040965207,0.077138744,-0.026260607,0.006350193,0.007215622,0.005852067,0.0018507825,-0.09474236,0.030394575,-0.04118319,-0.008487053,-0.026240682,0.04921892,-0.06547979,0.013945096,0.056020383,0.034978867,0.068076156,0.07716737,0.043680456,0.018965162,-0.07359602,0.069540314,-0.01376008,-0.004335361,0.11344015,0.0068285614,0.012091553,0.023987735,-0.11088365,0.06292454,-0.01635514,-0.0339578,0.09899484,0.011390842,-0.08217811,-0.0076746806,-0.024130978,-0.14254881,-0.0026611064,0.034691777,-0.06885704,0.09306905,0.062058125,-0.050976694,0.032867726,-0.018679874,0.04202415,-0.016988989,0.07902018,-0.08013325,0.0034369214,0.036852494,-0.08279197,0.010178414,0.036941223,0.0062150895,-0.058520254,0.04346375,-0.008600511,0.09619912,-0.01134016,0.0142330285,0.023575198,0.018293709,0.051702823,0.007431827,-0.07445613,1.8462467e-33,0.023568546,-0.010192095,-0.026491448,0.021813184,0.06060392,-0.062551945,-0.096068874,0.036659658,0.06318373,0.032445595,0.00469471,-0.013533062,0.0037980832,0.11038723,0.032591835,0.036568116,-0.051102385,0.07011914,0.024267782,0.033347424,0.0024590963,-0.014873528,-0.006877131,-0.061371244,0.020216275,-0.00259048,0.021955766,0.041009445,-0.043565504,0.031095611,0.0033493137,0.019029826,-0.00047904236,0.0012766409,0.012773916,-0.029308379,-0.045736402,-0.025014298,0.03864839,0.072898336,-0.108887166,-0.05874061,-0.039710272,-0.0010501385,0.028637916,0.07986208,0.0019739596,-0.029106395,-0.10667416,-0.046496715,-0.034734476,-0.00076053134,-0.05120755,0.026802605,-0.09045313,0.051961154,0.009631204,-0.03613396,-0.108461484,0.037066225,-0.051732056,0.0605224,0.02451491,-0.08523735,-0.040198974,-0.022483058,0.08968491,0.08615014,0.017261429,0.035208333,-0.009850734,0.01953318,0.02479749,-0.06952458,0.05557426,0.022079324,0.09855436,-0.05021547,0.03625474,0.03730323,-0.014320538,0.044295404,0.030045873,-0.0630786,-0.031893935,0.042641085,-0.018612409,-0.06376698,-0.07551879,-0.0047914176,-0.10326209,0.089434355,-0.015717141,-0.01896482,0.02606599,-3.2507326e-33,-0.006248067,0.012067346,0.017300196,0.013769437,-0.010249207,-0.022492148,-0.078544885,-0.038822893,-0.010018567,-0.03252317,0.05414827,-0.028651873,0.031786304,0.0011454468,0.026386475,-0.00092716847,-0.06593434,0.011162837,0.016637081,0.08107624,0.017993638,0.092229985,-0.044254478,0.0074254978,-0.12838313,-0.04057143,-0.07421471,0.03991903,0.039847143,-0.0021787055,-0.008040159,-0.020085346,-0.018101014,-0.087082036,0.0033182465,0.023960385,-0.008400633,-0.08290515,0.0387941,0.08541113,0.056303978,0.021622252,-0.0133052915,-0.026898284,-0.016050076,-0.05682228,0.048291884,0.018522361,0.0811746,0.06477474,0.0131001035,-0.01669697,-0.05519459,0.03934888,-0.0070244535,0.0012318358,-0.0150598325,-0.047379605,0.07549588,0.063756116,-0.08265915,0.050287943,0.11089754,0.0876306,-0.0040332535,-0.075373754,-0.011881497,0.017872833,-0.051838968,0.011066104,-0.009576736,-0.036208235,0.028622221,0.05044928,-0.10266992,-0.092225,-0.0154166315,-0.09474598,-0.09954836,0.032734204,0.083299205,-0.027280692,0.01978182,0.0383553,0.0393894,0.04849408,0.06725313,0.0879858,0.11251684,-0.07085093,-0.036319297,0.13113782,-0.063091606,0.00493326,-0.021335328,-4.8459544e-08,-0.037712224,-0.00021614216,-0.0060839127,0.026637785,0.00019485064,-0.010234662,-0.02165081,0.11824568,-0.067599855,0.00371226,0.05282304,0.019208848,-0.09791828,0.021397691,0.073922545,-0.0013923921,0.0019553802,0.007363086,0.020761417,-0.029329468,0.092739925,-0.07216051,0.09611908,-0.0275263,0.03581727,-0.1140376,0.009001674,0.0718027,0.01807888,0.061222445,-0.014010844,0.05943298,-0.027941639,-0.025562925,0.088668436,0.10817056,0.07185659,-0.048470333,-0.034529407,0.012142017,-0.04278039,0.055201925,-0.038585905,0.049586885,0.021527316,-0.0057291137,0.048438612,-0.05818593,0.003245573,-0.020329712,0.008605107,-0.009334011,0.033485107,0.07913375,0.031442106,0.06539714,-0.068225585,-0.06622137,0.038989957,0.011790866,0.0065783923,-0.07705258,0.004849411,-0.014002161,5,0.813513,-11.541674,9
201,"in summary, forward feature engineering and backward feature engineering are two
techniques used in machine learning for selecting relevant features to include in a model.
forward feature engineering starts with an empty feature set and iteratively adds one
feature at a time based on their performance, while backward feature engineering starts
with a complete set of features and removes features one by one until the model
performance reaches a peak. both techniques have their advantages and disadvantages
and can be used in combination to optimize the feature selection process. the resulting regression method is known
as polynomial regression - since
polynomial terms are introduced as
independent variable to handle non-linearity
in y. in general, introducing additional x variables
to improve the performance of ml methods
is known as feature engineering",-0.012322064,-0.039000183,0.061121,0.04782543,0.05568146,0.00094215997,-0.02086308,-0.038868364,-0.044523664,-0.037201304,0.02278608,0.021226227,0.012500097,-0.027048776,0.010201931,0.072565004,0.03521886,0.04141575,0.02727003,-0.043339856,-0.0014627111,-0.014396986,-0.094096534,0.01116816,0.08223679,0.0062720897,0.0045447075,0.05205127,-0.0069622337,0.058756664,-0.042462464,0.034749072,0.008364898,-0.029375428,-0.13594972,0.04713089,-0.0720949,0.067739934,-0.06914893,-0.02391477,-0.010732084,-0.09188325,0.019869652,-0.0076834904,0.10262419,0.04881909,-0.0012705629,-0.054566707,-0.0018149588,-0.033484068,-0.010986228,-0.01523744,-0.07640914,0.022152716,-0.013813775,-0.010968923,0.012751216,0.048794195,-0.009708514,0.040868957,0.0438566,-0.033184413,-0.062536836,0.012782299,0.046376813,-0.07014381,-0.027289871,-0.0074678315,-0.011025537,0.026092099,0.00052319915,0.09992224,-0.031034553,-0.020392897,-0.0028880294,0.02598932,0.106567554,0.02803912,0.022577098,-0.0019603898,-0.033922665,0.05726844,0.0006352703,-0.0091906525,0.08371465,-0.0031306855,-0.05022884,-0.023284236,-0.07398905,0.0680625,0.01662469,-0.06277864,0.0143950125,-0.022746613,-0.039131086,0.0069935382,-0.00790808,-0.09016626,0.020890748,0.02537816,-0.045475394,0.062043708,0.07673806,0.007446848,0.06359631,-0.034797072,0.07391766,0.0052311644,0.027049718,-0.056908846,-0.029666739,-0.010720379,-0.0013641003,-0.059178747,0.011262483,0.00037733183,-0.0101839965,0.022484599,0.016848806,-0.018461948,-0.01976689,-0.017815985,0.08213112,0.014324319,0.028021079,-0.05641865,-0.07059583,3.6646253e-33,-0.013269416,-0.055739608,-0.05003894,-0.0705811,-0.02124315,-0.013113765,0.04615103,0.013241191,0.09974501,0.037231978,-0.03403875,-0.026055163,0.0036239005,0.08366515,0.059228335,-0.0010891382,0.0059759347,0.12644777,0.036186542,0.013815173,0.038384806,-0.08254546,0.031230079,-0.01986192,0.011203726,0.05930348,0.047755256,-0.033588883,-0.1218971,0.038797244,0.01631878,0.039025754,-0.03245791,0.007398783,-0.0155826025,0.027769817,-0.03149599,-0.09294053,0.12210261,0.0901608,-0.0067585236,-0.04706493,0.012903014,-0.031040922,-0.0279226,0.0202702,0.011901285,-0.08517174,-0.060343836,0.029538931,-0.005145804,-0.022860086,-0.021752264,-0.07527619,-0.07508499,0.016836546,-0.03268151,0.02033258,-0.0396046,0.059439935,-0.104577646,0.032227334,-0.027930507,-0.025798375,-0.030556811,-0.011373421,0.10483043,-0.016097113,0.0059006438,-0.021536939,-0.021885568,-0.047700036,0.0072112116,-0.084210545,0.061261084,-0.0048771114,0.042339906,-0.02241064,0.07237048,-0.07985143,-0.112741195,0.07443269,-0.0627677,-0.060427282,0.072295405,0.07870779,0.016065637,-0.050960124,-0.03432579,-0.04934893,-0.14107996,0.053158086,-0.04699877,0.0807426,0.040746104,-4.2546875e-33,-0.05767033,0.02773917,-0.017101327,-0.0038985568,-0.009291679,-0.0056798854,-0.023916965,-0.018948814,0.03464889,-0.030749729,-0.010781377,-0.049150404,0.0050811023,0.0060738805,-0.0064253695,0.08860212,-0.056091126,-0.041009452,-0.04408485,0.020173151,-0.039979693,0.08786739,-0.017033173,0.020812072,-0.02372289,-0.07503477,-0.038567606,0.01974175,-0.013679168,-0.055869132,-0.083795935,0.005995893,-0.005143014,-0.011221826,0.012739502,0.023554625,-0.09599863,-0.06677878,0.06644293,0.07189746,0.101323076,0.029295897,0.0002343367,-0.01987144,-0.02306833,-0.0028981627,-0.017035926,0.0012454658,0.12782209,0.028909918,0.04290352,0.018792111,-0.009203239,-0.02613006,-0.10403962,-0.018816322,0.015962359,-0.010569012,0.03079299,0.030256735,-0.02236234,-0.034745988,0.15459003,0.021296179,0.0035837116,0.014697987,0.02216406,0.04534933,-0.059286155,-0.0386493,-0.021112455,0.012755642,-0.023510236,0.033534206,-0.08360843,-0.099216744,0.012268978,-0.07208573,-0.03180584,0.02784798,0.07006801,-0.0023686462,0.06536265,0.034617487,0.03938369,0.021326698,0.06758143,0.04607026,0.018956868,-0.12596327,-0.018656818,0.07511102,-0.03585817,0.08748863,0.04643246,-4.0779604e-08,-0.07586556,-0.015025546,0.059218336,0.031508442,0.027262323,0.028740449,-0.053432096,0.13728452,-0.05957037,-0.11032257,0.023038415,0.0013313715,0.0048205326,0.08351357,0.06003045,0.019525804,0.020001711,0.015564635,-0.056066126,-0.088101864,0.008306616,-0.019821119,-0.0050541162,-0.065338776,0.066512786,-0.08964519,-0.017478352,0.009505821,0.052097775,-0.028514678,-0.028730996,0.06051197,-0.0063265907,0.06397514,0.11693833,0.09025398,0.10302932,-0.071992576,-0.06489602,0.024152078,-0.038004935,0.04282753,-0.007545215,0.03464383,0.0009915459,0.005451758,0.10450575,-0.103971556,-0.046899058,0.026875492,0.10268254,-0.01638229,0.05473553,0.010227114,0.059145834,0.09144205,-0.01943514,0.003317354,-0.0045249728,0.062485926,-0.051815186,-0.02870809,0.07930258,-0.02705866,5,0.53837675,-8.75402,9
213,"we first discussed how to improve the quality of our results. there are three ways to achieve this: improving the sample, selecting the best model, and fine-tuning the chosen model. then, we moved from simple linear regression (slr) to multiple linear regression (mlr). we can transition from slr to mlr by transforming some features into functions of themselves. this increases the p-value of other features, potentially making their coefficients zero. additionally, the adjusted râ² increases in mlr because we are adding more features to our data.  

then, we analyzed a dataset containing both linear and non-linear trends. among the four models we tested, random forest, knn, and ann effectively captured the trend.
then, we analyzed r-squared and mean squared error (mse) for our models. we observed that xgboost outperformed random forest. knn, being less complex, provided better results, making it a suitable choice for our dataset.  

next, we moved on to classification. the first method we explored was logistic regression, where we predict class labels. we also briefly discussed the sigmoid function, weights, and bias in logistic regression.",0.0004314484,-0.052547317,0.00040942788,0.063534364,0.10625481,0.023819396,-0.033959866,0.021833329,-0.058458928,-0.026214989,-0.07368057,0.026986297,0.0007105596,-0.0011241988,0.03555774,0.022696443,0.025884135,0.02211176,-0.072340444,0.0016216296,0.033688083,0.008990115,0.012485121,0.036228742,-0.033206623,-0.04935725,-0.0016963104,-0.016355995,-0.037694357,-0.011020546,-0.03294544,-0.0081577245,0.01488621,0.018557807,-0.117594965,-0.053567454,0.00796376,0.023099052,0.0018568115,-0.0121259075,-0.008883733,-0.109522924,0.0013428878,-0.015119942,0.07453816,0.051102526,0.010362766,-0.04199156,-0.017705468,0.027402496,-0.044539206,-0.090590894,-0.051623408,-0.0007302112,-0.060358454,-0.003874015,-0.025262028,-0.018386267,0.109289266,-0.05392162,0.09312364,0.0006177276,-0.016414776,0.05035547,-0.030265465,-0.046218503,-0.038855195,0.036924757,0.03354274,0.03586376,0.020382281,0.06729002,-0.04522966,0.024611248,-0.026147384,0.022886861,0.04920325,0.10435215,0.08035886,0.0077348617,-0.0063701603,0.019891819,-0.059166048,0.001562333,0.04353062,-0.023680087,0.021040156,0.017373377,-0.11929839,0.016376337,0.038845118,0.03958061,0.014708457,0.028253604,-0.066967666,0.037650924,0.046745434,-0.15362155,0.027333366,0.07935964,-0.049487334,0.108786404,-0.0054154773,-0.10718338,0.03199813,-0.023194041,0.024825256,-0.017103592,0.050879586,0.007159749,0.012331029,0.08336965,-0.13925941,-0.027364107,0.08275934,-0.009168576,-0.008022254,0.08683346,-0.07230741,0.1011107,0.0042686868,0.047225993,0.0058368016,-0.006648361,0.09051875,0.063899525,-0.11383001,3.752525e-33,0.03418653,0.03287878,-0.021967521,-0.027012153,0.035534415,-0.040638976,-0.00962879,0.006415954,0.041431144,0.05099932,-0.041476455,0.004700306,-0.01396939,0.03128052,0.078478314,0.018670084,-0.07660353,0.007622182,0.0012454278,0.04726229,-0.008277647,-0.05074577,0.021325763,-0.043302014,-0.039782923,0.053651847,0.020880835,-0.003171387,-0.05975404,0.006109779,0.050117183,-0.0008214952,0.014296691,-0.015300933,-0.032472998,-0.088510044,-0.019345347,-0.0033865257,0.021436663,0.0077297143,-0.07727537,0.005634344,-0.02751974,-0.056262154,0.06463165,0.095315315,0.06688053,-0.048691638,-0.04277899,0.056083165,-0.033278458,-0.03827996,-0.06454436,0.102382496,-0.074777566,0.056655623,-0.0039477027,-0.0036977609,-0.043921247,0.023124143,0.028289387,-0.010473427,0.0509733,-0.045684077,-0.0018658797,-0.04057353,0.09670343,0.041162934,-0.069938295,0.0015422403,0.03165242,-0.06877247,-0.02986996,0.008417742,0.05305997,-0.004071608,0.05676355,-0.011923623,0.033947613,-0.008046918,-0.016280465,0.05115381,0.0434897,-0.102642804,-0.006822931,-0.00038694928,-0.047755755,-0.05810615,0.00736231,0.050942115,-0.11541626,0.06692465,0.015896318,0.007594172,0.011387519,-3.542887e-33,0.0102412915,0.020118017,0.018917538,0.1050788,-0.043252792,0.019306937,-0.03731232,-0.021099677,-0.01164123,-0.058199205,0.041636717,0.04920983,0.036950003,0.06738127,0.047736168,-0.041045368,0.027932893,-0.009110776,-0.05548768,0.038463134,0.03988754,0.12887675,-0.09038476,-0.00077462324,-0.06706508,0.014245675,-0.05950476,0.0893852,0.007160305,-0.043967716,-0.030970449,0.054775313,0.0138530005,-0.11072796,0.026571246,0.004179792,0.039379936,-0.10048271,-0.023097634,0.10696386,0.05109926,0.07837864,-0.026166229,-0.0022445333,-0.023865813,-0.04809685,-0.020784488,0.039721355,0.029380213,0.07227475,0.0038625617,-0.0077850856,-0.14530423,0.0145463785,0.01426453,-0.0066244933,-0.06975918,-0.05839322,-0.058722455,0.093695775,-0.1371678,0.01880327,-0.017761411,0.003771923,-0.034353476,-0.06204168,0.044990957,0.015079135,-0.025422957,-0.017366862,0.0037036508,-0.04618549,-0.0033259336,0.00045584785,-0.084388286,-0.02246644,0.009821656,-0.040112097,0.0037720054,0.010754042,0.02653781,0.011366193,-0.0019791946,0.038605716,-0.04374159,0.07850362,0.01832129,-0.0132082,0.044736724,-0.07382767,-0.06520794,0.01415858,-0.022807615,0.08058356,0.013095817,-5.9079113e-08,-0.06691383,0.066531576,0.021806633,0.044644784,0.0011057698,-0.0030824086,-0.057086356,0.17154586,-0.090880744,-0.024906069,0.052635998,-0.010759841,-0.0918041,0.025493985,0.10324493,0.03026119,-0.0020642823,0.017930731,-0.014191715,0.0002086984,0.056471586,-0.03224487,0.08638612,-0.06321766,0.077065445,-0.083646625,0.039684694,0.084241204,0.014511673,0.04149394,-0.011239707,0.03372136,0.009899776,0.033485673,0.011942177,0.057381544,0.040330652,0.0028809765,0.016676629,0.062420145,-0.009340068,0.063827336,-0.022850532,0.0030670015,-0.037037827,0.017697932,0.03188739,-0.037798334,0.03902985,-0.118811704,-0.054450884,-0.06279089,-0.03243437,0.0520813,0.04825703,0.050847493,-0.068619624,-0.012393514,-0.024583686,0.023388999,0.010876683,-0.13461666,-0.05575119,0.027270734,5,-0.11667121,-12.449416,9
226,"sir started the class with discussion on improving the quality of results and the ways to do it one of them was to use grids for parameters and try out every point in the grid. later we moved on to continue the discussion on multiple linear regression with a topic of using taylor series to capture non linearity in datasets by seeing an example of a dataset whose error scatter plot looked like a sine wave. here we looked a back elimination of irrelevant features based on p values. here sir briefly introduced overfitting. further sir discussed the difference between forward and backward feature engineering. moving on sir gave us the difference between parametric and non parametric models and their uses by looking at various models like k nearest neighbours, linear regression, random forest, neural networks, etc. later we compared different matric values like r-squared, mse, etc for all the models to compare them. sir explained in detail the interpretation of the mse and r-squared metrics both while comparing the models amongst themselves and evaluating a model within itself. further we moved on classification of nominal and ordinal variables which began with logistic regression and further discussing the sigmoid function which acts like a switch between zero and one",-0.09556965,-0.10096928,-0.007880172,0.005364368,0.097803205,-0.02203137,-0.07341444,-0.0037222996,-0.040171146,0.029503657,-0.027345853,0.06570416,-0.01966284,0.016847607,-0.04688284,-0.034775678,0.03633828,0.0092883,-0.063303,-0.0009909964,0.03668999,0.021904979,-0.056244705,0.021228151,-0.018465292,-0.021060683,-0.058347065,-0.019641861,-0.05911161,0.007945185,-0.07791821,0.08356491,-0.066495605,0.027811022,-0.077930465,0.023236917,0.030721007,0.08227934,-0.055099964,-0.0024295687,-0.046295572,-0.07345266,-0.0068472726,-0.015896419,0.10841991,-0.03981638,-0.05539493,-0.09975557,-0.045401707,-0.044247203,-0.1119127,0.026897687,-0.08337798,-0.026050536,-0.0411194,-0.03071022,0.025108185,0.009530453,0.028507939,-0.017385114,-0.030229595,-0.06303666,-0.08672546,0.026513673,0.02899192,-0.030428704,-0.043022893,-0.023112511,0.037802357,0.11275991,-0.029740557,0.003265674,-0.015184997,0.028632542,0.009852136,0.0070051835,0.06577202,0.10579511,-0.006208179,-0.040413693,0.0068724584,0.039604913,-0.014959879,-0.042941395,0.047274996,0.016468005,-0.03212857,-0.03878738,-0.03339469,0.016675644,0.046686996,0.034489214,0.020822285,-0.01215324,0.031003566,0.0028502003,0.037437487,-0.090538174,0.044050608,0.0653597,-0.04400741,0.06598001,0.02521406,-0.035271395,0.0783152,-0.019739902,0.060729805,-0.02228643,0.10196263,-0.04603887,-0.034348052,0.040074933,-0.09204631,-0.058378026,0.049049027,-0.065075435,0.013857415,0.02304983,0.005655588,0.0048078676,-0.035116453,0.012348982,-0.021173408,0.045636117,0.08546487,0.027376171,-0.087790176,5.734368e-33,0.028481266,0.08561162,-0.009267846,-0.0749328,-0.02094455,0.002582399,-0.010197287,-0.0010456358,0.13731645,0.027633283,-0.03773228,0.051361587,-0.034052733,0.07181515,0.078545585,-0.025394335,0.026893504,0.04584471,0.0071157306,-0.004324927,0.047147427,-0.055005908,0.040418055,-0.07849406,0.0041336846,0.07527218,0.00024550536,-0.032749347,-0.08549498,0.05253134,0.015821172,0.097437665,-0.035140246,0.025027473,-0.0015632391,-0.029796537,-0.055219788,-0.068090424,0.07653426,0.046834473,-0.10084985,-0.0049511692,-0.0095907925,0.007221157,-0.009480528,-0.05088666,0.09999467,0.013033085,-0.026395159,-0.052690037,-0.05646292,-0.015351778,-0.05636717,-0.06575547,-0.059734147,-0.00034320742,0.029713383,-0.026172012,-0.08594071,0.031622868,0.009909665,-0.01931445,-0.0076135756,-0.11205923,-0.051758375,-0.06457138,-0.0043377005,0.010053165,0.061175343,-0.030464059,-0.0038210356,0.0024185851,-0.049684934,-0.0256501,0.096822485,-0.010720196,0.054559413,0.02158895,0.035869412,-0.050862826,-0.050927196,0.040766254,0.017703976,-0.09589428,-0.02116442,0.0009314277,0.056255545,-0.05731734,-0.005649598,-0.05275737,-0.1340904,0.056798108,-0.022149209,0.039052583,-0.016273169,-7.3740274e-33,-0.07307136,0.07089374,-0.023921546,0.0972943,-0.055472706,0.043419275,-0.05362193,0.003067497,-0.0013145467,-0.061346572,0.018698132,-0.0379009,0.05691927,-0.03916056,0.033975404,0.003098349,-0.046012387,-0.03252079,-0.042958766,-0.014438403,0.038581163,0.08969615,-0.09657605,0.020681553,-0.11272071,0.028194984,-0.09846037,0.0340932,-0.04927401,-0.020918112,0.00470068,0.051598754,-0.010071306,-0.03109803,0.06917216,0.12427065,0.032914117,-0.023419533,-0.026612835,0.08294388,0.050296422,0.050755862,0.052363455,-0.019422509,-0.017534424,-0.054001804,-0.029284928,0.029814798,0.024195429,-0.0021161952,0.04549628,0.068923384,0.009227142,0.03128221,-0.05120385,-0.01980062,-0.014493259,-0.069986,0.02453223,0.048693594,-0.03893918,-0.03210458,0.03790964,0.058133125,-0.0035440596,0.017361647,0.027033357,0.022335451,0.002476051,-0.007036531,-0.05150088,-0.06546301,0.03356584,-0.03111062,-0.076336004,-0.04591352,-0.0050637894,-0.07337548,-0.111573204,0.018650757,0.06804634,0.007732773,-0.036491185,0.07877922,9.035069e-05,0.04495886,0.11348291,0.02380316,0.015192755,-0.025050279,-0.00502953,0.048562925,-0.090207204,0.070244126,0.097536415,-6.879453e-08,-0.009128523,0.033381756,0.052729096,0.005038177,0.011200548,-0.05719379,0.0108854165,0.13582046,-0.056898676,0.008845454,0.014752636,0.039841723,-0.002221097,-0.011351976,0.08938552,0.04116013,-0.01983153,0.021269329,-0.016402857,0.025353057,0.1036365,-0.030574135,-0.07471626,-0.045546155,0.10872756,-0.039778683,-0.052466553,0.06894853,-0.011405884,0.061081942,0.055237114,0.056561816,0.015716625,0.041517153,-0.00488412,0.12666097,0.1284842,-0.016077656,-0.0733478,0.032591768,-0.08846892,0.028571349,-0.06214164,0.053831022,0.04534235,0.00954718,0.02126102,-0.056539614,0.009292872,-0.009604385,0.01785427,0.03391371,-0.008643317,-0.013258342,0.052916147,0.02001985,-0.0322492,-0.023116661,-0.026236659,0.06440095,0.013004925,-0.052931067,-0.020439725,0.014905719,5,2.8290358,-10.426413,9
228,"linear regression:-
linear regression does not mean that the outcome itself is linear. it means that the outcome is expressed as a linear combination of independent variables.


taylor series expansion:-
a mathematical technique used to express a function as a series expansion.

if errors exhibit specific trends (e.g., sinusoidal patterns), we can introduce new features:
uâ‚ = xâ‚
uâ‚‚ = xâ‚â² (polynomial feature)
uâ‚ƒ = xâ‚â³
uâ‚„ = sin(xâ‚)
generalized as: uâ‚™ = f(xâ‚, ...)

feature selection:-

backward feature elimination:-
eliminates features based on p-values, removing the ones with the highest p-values (typically > 0.05).


forward feature engineering:-
starts with an empty feature set and progressively adds important features to build a more refined model.


backward feature engineering:-
begins with a full feature set and eliminates irrelevant or redundant features.


building a good model:-
a good model should:
avoid excessive unnecessary variables.
prevent overfitting.
ensure generalization.


model selection and explainability:-
we learned about exploratory data analysis (eda) and how to select the best model for a given problem.

parametric vs. non-parametric models:-
parametric models allow for ""what-if"" simulations and interpretability.
non-parametric models are more flexible but may lack explainability.

classification models:-

when predicting classes of categorical variables, we use classification models such as:
logistic regression â†’ determines class probabilities.decision boundaries defines regions where data points belong to a specific class.

we also learnt some very basic introduction to neural network and knn",-0.045108154,-0.020438166,0.07048524,0.071493134,0.018787565,0.029986411,-0.09761727,-0.017790508,-0.06424151,0.02641069,0.045219455,0.018377732,-0.05040173,0.014418915,-0.005745712,0.031382326,-0.0075486773,-0.010137162,0.0064294217,-0.0013394634,0.042056706,0.013486939,-0.07210512,0.041052505,0.031503104,0.091715455,-0.016606322,0.006884781,-0.03880189,0.004385735,-0.03338804,0.050427455,0.030091459,-0.056571133,-0.061617486,0.02472005,-0.02474299,0.07393254,-0.06676212,-0.015820296,-0.028285485,-0.07460166,0.031182475,0.015481658,0.10742824,-0.07696957,-0.03251227,-0.06004596,-0.039850436,-0.027882664,0.059341922,0.02322711,-0.039996788,0.029837351,0.020311866,-0.08188652,0.029708069,0.015952768,0.03424804,-0.01430368,0.014596862,0.0088289045,-0.091673374,0.053632803,0.016712109,-0.03512129,0.03244769,-0.04923542,-0.015979817,0.06111606,-0.07094265,0.02324835,-0.023645269,0.0043942137,-0.010350734,0.050556548,0.058324844,0.035080474,0.010408221,0.041601118,0.021956207,0.07245448,-0.02440836,-0.041640073,0.055630654,0.03275073,0.00350153,-0.02867593,-0.08631311,0.08711887,0.037269983,-0.021752488,-0.01880433,0.017206341,0.012378179,0.01222701,-0.018368391,-0.16044173,0.04591852,0.027628552,0.012209988,0.031495593,0.031039948,-0.009458138,0.00883223,-0.0141398385,0.05455369,0.0130014885,0.03220913,-0.009141082,-0.052072592,-0.0010097516,-0.021408299,-0.05013539,0.07244989,-0.017853422,-0.016793668,0.024093831,0.03254826,0.007950325,0.017956324,0.0026379079,0.056442823,0.08833803,0.062097143,-0.004371588,-0.079459645,6.3789714e-33,0.0031966737,-0.03257246,-0.045304045,-0.0037420539,0.014269843,-0.01470362,0.033949334,0.03589098,0.051614016,0.09285332,-0.03636913,0.08438206,0.00082468794,0.105136976,0.08489257,0.004847312,0.005860238,0.038398717,0.045219265,0.035013482,-0.009346623,-0.12022194,0.05861103,-0.050053935,0.02133681,0.023015061,-0.035312817,-0.009503601,-0.15369762,0.010317023,-0.011369625,0.046386383,-0.049053647,-0.03342407,-0.0312962,-0.00021490047,-0.03367426,-0.12383239,0.10471145,0.096718356,0.013457256,-0.015661227,-0.071670935,0.011103882,0.023697652,0.06262218,0.044845674,-0.08185719,-0.014163618,0.039901424,-0.0037332273,-0.030914407,-0.061280157,-0.038162865,-0.09714023,0.031244617,-0.0367313,-0.045089986,-0.03484978,0.022144081,-0.027901454,0.02022758,-0.023769386,-0.099650115,-0.074038744,-0.009810549,0.038794722,0.024944536,0.024973715,-0.05481788,-0.07054803,-0.03156735,0.03187647,-0.013781474,0.07455471,0.0038072136,0.044461936,-0.04545077,0.056964282,-0.0073685776,-0.07371193,0.084932275,0.0028613699,-0.05297302,0.049456116,0.008150994,0.010662774,-0.010931715,-0.0032331955,-0.09704764,-0.039312072,0.054841604,0.02898339,0.115152396,0.0634244,-6.394575e-33,0.008483665,0.08039358,-0.011058641,-0.05945527,-0.017881285,0.008646643,-0.013255234,-0.034822293,0.04501655,-0.10365977,-0.0034234785,-0.038817503,0.011814074,-0.017100437,0.03688747,0.00659525,-0.044310812,-0.13320558,0.008184133,0.047189783,-0.03227233,0.032632023,-0.084942535,-0.0085293,-0.044857934,-0.035064347,-0.062418655,-0.025342409,0.03453192,-0.07802139,-0.03420801,0.048421074,0.03738044,-0.0022699025,-0.014250471,0.057951882,-0.038029987,-0.044749893,0.0476947,0.0954611,0.114906795,0.044018157,0.06482035,0.006615275,-0.012164276,-0.014795175,0.009940715,0.03756998,0.053703286,0.042541754,0.06449284,0.065612815,-0.049144056,0.0012936823,-0.049883578,-0.047824197,0.03681913,-0.018544063,-0.044241883,0.05118978,-0.046580367,-0.019034402,0.080678575,0.051730853,-0.00742823,0.022017809,0.024838729,0.005091969,-0.052217543,-0.029631998,-0.07159347,-0.0306961,-0.038415894,-0.003695657,-0.008848016,-0.11965006,-0.051750783,-0.12527978,-0.04562224,0.02742873,0.034571588,0.036116865,-0.018021947,0.057454914,-0.043076236,0.06204694,-0.007868617,0.026645487,0.00037307813,-0.080792345,-0.043876793,-0.003986568,-0.054051306,0.05641716,0.12371919,-6.4848614e-08,-0.0799717,0.010512269,0.029291114,0.0062021995,-0.0108214365,-0.063760884,0.038020317,0.119451195,-0.0009448625,-0.12733792,0.060722336,0.030394342,0.007616968,0.04588038,0.070440754,0.052616216,0.01830754,0.051011693,-0.005699662,-0.03034972,0.007335497,-0.068208285,-0.07679657,-0.07422304,0.05565762,-0.099113666,-0.039541863,0.129569,0.024621764,0.045347188,0.015932119,0.028092207,0.017364481,0.042129233,0.044652905,0.057268478,0.09905553,-0.07146927,-0.016377948,0.04870854,-0.036999483,0.11600741,-0.010333576,-0.044264056,-0.041528624,-0.0054529193,0.016945465,-0.09575219,-0.0066605248,-0.019318527,0.1445396,0.032043014,-0.008204234,0.03607623,0.050659128,0.09599444,-0.02976053,0.019673577,-0.036834113,0.024286242,-0.04556503,-0.061714485,0.03186003,-0.050036598,5,-0.058560982,-7.5786386,9
234,"the following topics were discussed in today's class:-
1. linear regression with higher order terms: linear regression is just the linear combination of many variables, those variables could be higher powers of the a single independent variable. thus increasing the complexity of the model and its capability to generalize patterns. those higher powers of the same variables are examples of engineered features, this regression with higher order terms is called polynomial regression. we also learnt that based on the problem in hand, one can use more than one models for the same problem.
2. forward and backward feature engineering:  in forward feature engineering the independent variables are chosen one by one with proper analysis. in the other case, all the available variables are added into the model and the redundant ones are removed based on the p-values
3. parametric and non parametric models: in parametric models, the form of the function is assumed with a number of parameters. the ultimate aim of the problem then becomes finding optimal parameters minimizing loss function. these models ability is limited as they can't capture intricate patterns, causing underfitting issues. in case of non-parameteric models, any assumption is not made about the function and the data itself is used to make the prediction. these models have greater complexity and capture patterns better, but have the issue of overfitting. examples like random forest, x-boost, k-nn
4. neural networks: a model having multiple layers connected to each other communicating with each other to finally give a output. if the number of layers is more than one, then it is called deep learning.
5. introduction to classification problem: the objective of this problem is to come with a decision boundary that best separates the given classes. little bit about the sigmoid function was also discussed.",-0.07325652,-0.054607484,0.010542308,0.030367728,0.047405936,0.022167912,-0.08516041,-0.035007983,0.03117774,0.025971659,0.04895165,0.07376351,0.042845786,0.030007057,0.057257872,-0.008576326,-0.016804742,0.03586308,-0.069626786,-0.016938288,0.057491515,-0.025280865,-0.10891238,0.0005423098,0.044934236,-0.005181595,-0.012848819,0.023244735,-0.04224835,0.021326033,-0.086571336,0.02683571,-0.020449284,-0.010347977,-0.045592833,0.015204752,-0.080417596,0.041567996,-0.051527247,0.003158061,-0.012318742,-0.07475624,0.0040894076,0.010179979,0.10225589,-0.0105720125,0.007678205,-0.056551125,-0.06339427,-0.10591939,-0.04447531,-0.002717436,-0.085935086,0.041347824,0.0037526095,-0.07074622,-0.02249978,0.063977264,-0.033438344,0.027871715,0.008193463,-0.028181698,-0.06333751,-0.0014286634,0.049901422,-0.039750475,-0.0001414937,-0.012105062,0.015288864,0.13242425,-0.027665662,0.020566568,-0.05827937,-0.077089354,0.005019836,-0.010761148,0.11084828,0.082868814,-0.088620916,-0.034547932,0.03554322,0.028059293,-0.041358802,-0.043683585,0.0583426,0.0023625758,-0.088797435,-0.047628928,-0.057434443,0.058613997,0.014110254,-0.042485286,-0.0296004,0.00485556,0.07587757,0.01317387,0.058260065,-0.11292852,0.029642627,0.040147703,0.010507076,0.026258694,0.08056934,0.04797202,0.08205053,-0.0028285077,0.006253656,-0.043267705,0.011320936,0.020227082,-0.033633213,0.010598861,0.008580254,-0.07171134,0.00020350845,-0.05043632,-0.013108731,0.024757393,0.056194738,-0.026592439,-0.047573745,-0.03330767,0.06165849,0.024043014,0.036234118,-0.007974485,-0.07912533,5.6754355e-33,-0.024658436,0.042350497,-0.028583422,-0.051709227,-0.0063209767,0.018664772,0.039409216,0.025634391,0.078606255,0.07602251,-0.07374511,-0.010827327,-0.0028876099,0.083485566,0.084334545,0.015547922,-0.011190856,0.12265581,0.041182455,0.04206508,0.06849409,-0.0715149,0.024229247,-0.00034125632,0.008774924,0.0006530435,0.036350552,-0.0397212,-0.14387842,0.015591299,0.027712503,0.03336825,-0.0088037625,0.0049382723,-0.07351582,0.014723243,-0.07051814,-0.10744329,0.094069555,-0.013135978,-0.010412864,0.014084061,-0.020135194,0.01868663,-0.010038291,-0.009113797,0.071678646,-0.014410022,-0.05859525,-0.016654711,-0.058509205,0.01695627,-0.033562534,-0.10988262,-0.016104477,-0.03235239,-0.025535535,-0.039632898,-0.069347814,0.04905641,-0.076183245,0.020785354,-0.0068332613,-0.063415885,0.023710331,-0.019885058,0.014074905,0.05716115,0.069102496,-0.03467439,-0.032663196,-0.022276212,-0.043391548,-0.058381006,0.10060025,0.01809173,0.017455608,-0.05250871,0.06244064,-0.085707545,-0.12934764,0.088896565,0.010814313,-0.00020145066,0.0137088075,0.08509455,0.0014547621,-0.0664607,0.040042557,-0.05119437,-0.080815785,0.05643341,-0.025718847,0.045115422,0.048974767,-6.58767e-33,-0.014988762,0.04442516,-0.053662,-0.03793472,0.01760262,-0.034862738,-0.05960455,-0.012242178,0.054185633,-0.073514156,-0.03416648,-0.025175726,0.0070839743,-0.044181526,0.010579171,0.04776911,-0.071678266,-0.06429201,-0.006802934,-0.03695947,0.012522629,0.08394119,-0.072988845,0.039850377,-0.088480406,-0.024523688,-0.11285249,0.030292878,0.010009313,0.04681431,-0.04914962,0.034060244,0.008632,0.057352748,-0.02718208,0.04212519,-0.10360645,-0.020551035,0.041796338,0.06943168,0.0724026,0.0429,0.033236966,-0.050432205,0.020978332,0.008220601,-0.001168129,-0.029434588,0.0717251,0.009318653,0.05533101,0.044209283,-0.002248981,-0.012364427,-0.057818796,-0.018571923,-0.020146197,-0.011251496,0.040990848,0.01273968,0.03942738,-0.02724968,0.087637715,0.0475338,-0.036095563,0.042410217,0.04278198,-0.01365408,0.01014801,-0.03615761,-0.050043073,-0.04203766,-0.02651378,0.039314155,-0.076246075,-0.0665293,-0.0067178435,0.013242418,-0.085382536,0.02884524,0.09330814,0.040471315,0.03220262,0.048047625,-0.02453016,0.051439513,0.05041248,0.050359454,0.027444093,-0.06727889,-0.0009208122,0.0696911,-0.02916009,0.11631267,0.08212192,-6.290014e-08,-0.020459028,0.009751554,0.015310891,0.016040698,-0.039246418,-0.04997862,0.038704716,0.10854122,0.009975402,-0.031603884,-0.022423312,0.00034977568,0.039964296,0.06468332,0.07049082,0.031483814,-0.02275089,-0.011175867,-0.05751282,-0.1115489,0.015719978,-0.003463442,-0.06333092,-0.0611745,0.11567788,-0.054007787,-0.04567146,0.0042466247,0.015515202,0.06981073,0.04060384,0.046111573,-0.004917239,0.074813336,0.03796952,0.07724351,0.12424734,-0.048260696,-0.06322396,0.005965817,-0.057332467,0.035919987,-0.058208838,0.03057768,0.064879775,-0.015039636,0.07172595,-0.09306478,-0.05793019,0.015071926,0.07091672,0.06976681,0.018588882,-0.014117663,0.031233396,0.10892283,-0.03121752,0.031020835,-0.0060754153,0.022287305,0.0021371301,-0.08453976,0.080188766,-0.031118905,5,1.11463,-8.513275,9
262,"at the start of class, we derived the closed form solution for mlr and showed that they are possible only in theory and not in practice. later, sir showed his data analysis on previous class's summary. this time the heatmap was more red than the previous one and only two similar submissions. sir showed us the two similar submission and we notice how even the text were written differently, ai tools could figure out that they are similar in meaning and understanding. 
we moved on to the topic of sample. we learned that if we have a sample, we should not entirely use it to train a model. we should always split the sample in two parts: one for training the model and the other for testing the model. this splitting, however, depends on the quantity of available data and the model. 
we noticed that even if we separate the sample in two parts, it is possible that the testing data may not give predictions with similar accuracy like that of the training data. this depends on the r-square values for the training and testing data. if the r-square values are nearby, only then will we get desired results. we also studied that r-square value for the training data is not desired to be one, because it will not be compatible to the testing data otherwise. 
we understood the terminology for r-square, adjusted r-square and multiple r. we how using (n-1) in calculated adjusted r-square would not be a biased estimator. 
in mlr model, the outcome of the linear regression need not to be linear. it would be the linear combination of independent variables. 
we moved on to the python-based approach for data analysis. we used two new modules: scklearn and statsmodel for analysing data. scklearn do not give parametric values liek r-square, p-value for its model. these values can be obtained in statsmodel. 
we also learned how the previous observation that 'p-value less than 0.5 is desired' can lead us in trouble during hypothesis testing. ",-0.12929308,-0.01546865,0.025839463,0.022772264,0.12195253,0.0018005259,-0.06402177,-0.005153953,0.016259829,-0.01246669,-0.036326226,-0.005664835,0.03273521,-0.03616006,0.07989568,-0.025562756,-0.009436959,-0.004699016,-0.09810968,-0.039206244,0.028669901,0.019939585,-0.05208947,0.08084262,-0.039563097,0.060586777,0.10123621,0.03657389,0.0041234605,0.0081143705,0.12397066,-0.022195846,-0.04669056,0.005095408,-0.001602266,0.009859102,-0.070492044,0.059159752,0.054638308,0.03438609,0.005116479,-0.08371444,0.029206384,0.016822098,0.07189726,0.048251763,-0.042033464,-0.07130053,-0.012421253,0.036785424,-0.10022806,0.0092075635,-0.055853717,-0.035602823,-0.027898807,-0.008926631,-0.019173523,-0.062128425,-0.029499928,-0.003146497,-0.03031701,-0.02996577,-0.09277061,-0.008332959,0.011945576,-0.0861824,-0.017516911,-0.0026302747,0.08127474,-0.0044466443,0.0011969943,0.14323197,-0.030122718,0.003938438,-0.0036712496,0.0021865806,-0.0063171494,0.016657343,0.018439023,-0.033362255,0.047049113,-0.013925704,0.00022617493,-0.004471159,-0.068936996,-0.025614996,0.06909362,-0.03354451,0.04103287,-0.0592211,0.053275123,0.005134552,-0.03898121,0.06908781,0.050577775,0.0434556,0.011738427,0.049242705,0.04315316,0.050381243,-0.014613385,0.06589483,-0.014522949,-0.018720414,-0.005472376,-0.10375319,0.012909775,-0.069079645,0.12963851,-0.030632693,0.06274017,-0.023093926,-0.05914358,0.041089375,0.055730533,-0.03898233,-0.06364127,0.0013311541,-0.08708994,0.10471985,-0.07651183,0.028514277,0.08763393,0.0014622925,0.096733764,-0.05589648,-0.07624531,4.9347146e-33,-0.022524677,0.047743957,-0.013921355,0.07771332,0.0713512,-0.02602573,-0.047649335,0.0057206317,0.08654718,0.032426678,-0.0063637467,-0.03381354,-0.019751795,0.069003984,0.01923426,0.086867325,-0.039065454,0.0453074,-0.11233865,0.01734497,0.058530264,0.051003274,0.060757075,-0.05660339,-0.08928544,0.052316498,0.05359021,0.022417821,-0.0077539035,0.009028071,-0.07532714,0.014977801,-0.04295891,0.075052924,0.043009482,0.054999627,0.035899945,-0.052845534,-0.03781627,-0.0280765,0.042207584,0.04982345,0.105013564,0.007750086,0.025404492,-0.07233597,-0.030430533,-0.06182565,-0.031959537,-0.043160778,-0.008910669,-0.029986624,-0.02574285,-0.049505014,0.00730325,0.035415664,-0.052275095,0.044260703,0.025430199,0.06910962,-0.06387412,0.003346515,-0.008865902,0.05180814,-0.02445088,0.050197914,-0.011329256,-0.062318064,0.058697414,-0.023600504,0.013178929,-0.035435956,-0.116406135,-0.034124315,0.037788372,-0.08402803,0.089833,0.019667877,0.04171561,-0.005860885,0.016601501,-0.043388646,-0.059582457,-0.085252374,-0.09808624,-0.10530689,0.031947188,-0.016690625,0.017994283,-0.02113599,-0.11659141,-0.01208931,-0.09463407,0.013694176,0.04982071,-6.081875e-33,-0.056743182,0.0076743467,-0.036531005,0.09370828,-0.026356563,-0.025039997,0.029747277,-0.0520944,-0.022672223,0.00952805,0.025491677,-0.06630226,0.02108314,-0.034982555,-0.02216541,-0.018486533,0.038527865,0.024944942,-0.028765768,0.08202038,-0.041247625,0.089610755,-0.06253793,-0.006458244,-0.03500409,0.015296064,-0.020522611,0.05494468,0.03464451,-0.040469315,0.040189493,-0.04898171,-0.03805142,-0.079400234,-0.037864454,0.07617709,0.11709282,-0.015943559,0.061673313,0.14453039,0.07675979,-0.0073429993,-0.077728465,-0.018948888,-0.0025574127,-0.04558891,0.031048855,-0.050820485,0.033111632,-0.045367595,0.021225352,-0.040512335,-0.083703466,0.020773595,-0.008872239,-0.051767316,-0.030471371,0.015072695,-0.046773035,0.06487851,0.01554917,0.01637052,-0.034320913,0.04172625,0.011484931,0.007674797,0.05541959,0.0134287,0.08697831,0.04429304,-0.042861164,-0.05551031,0.044540223,0.047705,0.044017166,0.009608371,-0.02674726,-0.04511284,-0.028248427,-0.06299721,-0.06497201,-0.062328804,-0.03619827,0.050536606,0.045938578,-0.012289756,0.060870852,-0.04503178,-0.055423543,-0.058646377,-0.019354636,0.022996051,0.028010072,0.12104099,-0.06407241,-7.433723e-08,-0.030790452,-0.008394356,0.08608715,0.059380297,0.04609728,-0.008558065,-0.06411258,-0.013721243,-0.024517627,0.058829855,0.07416852,-0.013647663,-0.054333504,-0.019074049,0.03698538,0.016877575,0.0023306517,0.0048295846,-0.010067465,0.04390787,0.047398202,-0.009646417,0.005396356,-0.051405337,0.045872234,0.024988519,-0.010885153,0.093259275,-0.056202266,-0.012432924,-0.040110163,0.024105046,0.047809064,0.007973466,0.06546645,-0.025896547,0.10116385,0.0069431528,-0.013547898,0.01764516,-0.04757499,0.053443387,-0.06974557,-0.0063608713,0.08534735,-0.004448373,-0.04185844,-0.12010637,-0.027300864,0.019978628,-0.046795968,-0.0037889439,0.0020384789,0.06522324,0.03974541,-0.018812185,-0.06284731,-0.018858442,-0.024980208,0.004730711,0.040973373,0.0072307065,-0.10757707,-0.023017267,8,-1.2169139,-0.22276865,9
271,"in order to improve results, improve samples by improving quality of samples or by increasing the quantity. we can also improve method by using multiple methods and then selecting best based on your knowledge of metrics used. grid search is one of the ways to select best methods where we use n dimensional grid with all possible combinations of parameters. 
in polynomial regression we use powers of features based on domain knowledge and data exploration. we can use forward or backward feature selection methods. in forward we start with empty set of features and we keep adding features to improve the results. in backward process, we start with all features and reduce them to reach better results. 
neural networks is used in ml. it has hidden layers and weights connecting the neurons.",-0.0036822674,-0.0063666794,-0.007487129,0.025437344,0.0034876568,0.036334187,-0.032304674,-0.027958335,0.019126233,0.018557735,-0.014937852,0.060346514,-0.0077623217,0.0024749604,-0.012503113,0.041161478,0.06274216,0.08812564,0.004844947,-0.043003287,0.008675775,-0.061553497,-0.044276327,-0.04050297,0.04858429,0.037405312,-0.038803305,0.018657869,0.011453707,0.05418698,0.015588554,0.01748545,-0.0043788017,-0.0063108797,-0.124172166,0.07100216,-0.084025726,0.066978574,0.0055559226,0.007761556,0.018026594,-0.051967658,-0.0057604504,0.011275811,0.13124351,0.08287352,-0.030997423,-0.023598075,-0.0022831769,-0.028060088,-0.08501949,-0.021064702,-0.013373227,0.023333795,0.0026436397,-0.058631465,0.012387168,0.05276722,0.016831381,-0.0041043526,0.06570619,-0.07360721,-0.10750234,0.048143186,0.082260795,-0.023978809,-0.014509462,-0.0076328926,0.017763505,0.0348895,0.084250845,0.09996223,-0.08092372,0.013720753,0.004243671,0.03316254,0.074027166,-0.04942115,-0.023107095,-0.011759635,0.047821797,0.0957822,-0.014544187,0.04006637,0.07127851,0.017431509,-0.029923243,-0.023051772,-0.052043717,0.0116537055,0.034557518,0.0039013738,-0.09445615,0.0090924455,-0.10078283,0.003537629,-0.0036431274,-0.08470127,0.016107557,0.048118137,-0.013230763,0.033095714,0.057073995,-0.044418126,0.02873051,0.022616263,0.06464087,0.04313447,0.043991122,-0.056379676,-0.044719897,-0.048239086,-0.060282823,-0.03282295,0.119502366,-0.03773825,0.06336959,0.034820758,0.010069499,0.016164873,-0.09270823,-0.008293842,0.021597113,-0.062630735,0.01560596,0.016911704,-0.112042606,5.3812952e-33,-0.017689288,-0.034530714,-0.014080517,-0.08523241,-0.0057844585,0.034913633,0.117737375,-0.016314525,0.1346594,0.06640646,-0.082060225,0.0067784605,-0.019012438,0.07587325,0.074252404,-0.049916055,0.02210638,0.058920894,0.0010818217,0.014030469,0.06252598,-0.11911257,0.07102074,-0.060516078,0.01568408,0.0126722,0.048812397,-0.024717808,-0.05276999,-0.010933455,0.033746272,0.039398123,-0.08574485,0.0028028328,-0.0016553241,0.018587979,0.0032943347,-0.029028505,0.070186846,0.01871493,0.010170283,-0.028620088,0.08500369,-0.0115328925,0.013247994,0.014956827,-0.06318981,-0.09968638,0.015730478,0.041159816,0.016935227,0.010546819,-0.056068063,-0.06017602,-0.04273052,0.047130972,-0.059140343,0.03608991,0.030359779,0.07389752,-0.048159253,-0.06224083,-0.028980399,-0.019132562,0.0063742464,-0.024439014,0.03644399,-0.07565883,0.018357158,-0.02501219,-0.0146480445,-0.0625702,0.019537723,-0.02950745,0.05060655,0.0045140423,-0.030000374,-0.038946647,0.037214406,-0.08144144,-0.07635917,0.06521912,-0.05351177,-0.050576985,0.047604453,0.025241459,0.010134244,-0.06581036,-0.013684603,-0.03326145,-0.18010682,0.06675533,0.0077749146,0.06941146,-0.022730753,-4.9574645e-33,-0.017169109,0.031581122,-0.0064707207,0.08610969,-0.048092727,0.0390199,-0.038650803,-0.031530883,0.010854942,0.0011763123,0.020311227,-0.0050482815,0.07165519,0.024657007,0.02908111,0.092613555,-0.08286593,-0.033914454,0.018297024,-0.014956675,-0.07716017,0.15255013,-0.052050885,0.058914833,-0.013891671,-0.06238097,0.0086629,-0.013771378,-0.01276136,0.0065530203,-0.08039541,-0.02856218,-0.029357685,0.04336578,0.04222399,0.049739316,-0.036469925,-0.08986327,0.028596982,0.07919837,0.04003635,0.039979264,-0.01470193,-0.04516032,-0.02682574,-0.053247187,-0.079602756,0.026916366,0.06300816,0.05660226,0.03064043,-0.006587729,-0.09056564,-0.041095886,-0.007982964,-0.025939373,0.008727425,0.018074162,0.041929007,-0.03396104,-0.09776014,-0.05231391,0.05520546,0.08115505,0.0011599994,0.067256145,-0.016156482,0.034880903,0.003059818,0.013098037,-0.035633057,0.007009292,0.023647228,0.008701481,-0.08442254,-0.08934028,0.020378454,-0.025346717,0.0017747923,0.01433951,0.031201135,0.028594935,0.034026127,0.049320504,0.009105968,0.052864622,0.09895348,0.002662632,-0.018023536,-0.06856035,0.0017870384,-0.0051399698,-0.0101475855,0.034402072,-0.0033864228,-4.731295e-08,-0.08705365,0.0014582204,0.10722092,0.01888842,0.043020673,-0.055960674,-0.009065642,0.14651743,-0.08605454,-0.04930274,0.057980794,0.011610897,-0.016480247,0.059374113,0.09802712,0.008820432,-0.0012783163,0.036093414,-0.06414295,-0.09376582,0.0043451614,-0.0029740585,-0.04748192,0.037508696,0.13420452,-0.032450005,-0.033645734,-0.014837121,0.010370641,0.0378648,0.017428895,0.0079372795,0.030236006,0.02995672,0.07849169,0.11430174,0.04178036,-0.0924751,-0.069714114,0.02216285,-0.06819397,0.02808673,0.031800486,-0.03524495,-0.06495069,-0.0036314682,0.09565877,-0.074317,-0.070594974,0.0007895408,0.047279082,-0.013343809,-0.010927485,-0.03866161,0.013089177,0.069469504,-0.0565231,-0.039607737,-0.016974527,0.03524865,-0.012363841,-0.012875546,0.028840836,-0.03719736,5,-0.26045224,-9.00365,9
272,"
we began by discussing how to improve the quality of a modelâ€™s results. there are three key approaches:  
1. improving the sample â€“ this can be done by either increasing the sample size or enhancing the quality of the sample.  
2. improving the method â€“ using multiple methods and selecting the best one.  
3. fine-tuning the method â€“ properly adjusting and optimizing the selected method.  

we then discussed polynomial regression followed by feature engineering, which consists of:  
- forward selection â€“ adding features one by one.  
- backward selection â€“ starting with all features and removing the ones that arenâ€™t needed.  

after that, we outlined the modeling workflow:  
1. collect data  
2. process data  
3. obtain clean data  
4. apply various models and compare metrics  
5. select the best model  

we also emphasized that if a single model can predict data effectively, it is preferable to using multiple models. we then experimented with different models like linear regression, svm regression, and random forest. the random forest model gave the best results, but we noted that it is a non-parametric model, making it less suitable for tasks like delta analysis (which requires parametric models like linear regression).  

we then introduced neural networks, explaining that they are parametric models as they involve weights. a neural network consists of:  
- input layer  
- hidden layers (where transformations occur: y = f(wixi))  
- output layer  

as the number of hidden layers increases, it transitions into deep learning. we also examined model evaluation using râ² and mse:  
- if training and test râ²/mse are similar, the model is well-fitted.  
- if training error is significantly lower than test error, the model is overfitting.  

lastly, we started with classification, which is used for nominal or ordinal data. in supervised learning, both x (input) and y (output) are available, while regression aims to pull data points back to their mean values. we then covered logistic regression, which is used for classification tasks. it helps define decision boundaries between different classes using the sigmoid function.",0.016763879,-0.0035982504,0.024584007,0.06200055,0.030918851,-0.055482555,-0.045883894,0.0129624605,-0.01066001,0.010808317,-0.08629705,-0.00013260802,-0.0019006367,0.03524297,0.048976503,0.045774586,0.060608972,0.033978082,-0.023180148,-0.0751634,0.02150962,-0.0154568795,-0.02043404,0.038340878,-0.037794493,-0.05960902,-0.0018174065,-0.023611024,-0.04493902,-0.03535942,-0.035391442,0.012255999,0.017523654,-0.027100397,-0.10711751,0.03521596,-0.0070493952,0.029609518,-0.0002030425,-0.013752013,0.0008838136,-0.04392226,-0.035723057,-0.025806047,0.04778139,-0.023227714,-0.014437932,-0.023497924,0.009859536,0.008856986,-0.045748565,-0.04736439,-0.06879224,-0.007633212,-0.023791488,-0.05946495,0.0015754808,-0.02590038,0.014043339,-0.068488955,0.056911223,0.0020761273,-0.11591983,0.036950447,0.04482799,-0.06389747,-0.05149708,0.0038107573,0.004965976,0.02953181,0.0087366495,0.070642106,-0.06971643,0.02267662,-0.05037007,0.015825605,0.050108876,0.081979476,0.06488277,-0.013145493,-0.013060413,0.042577982,-0.04692394,-0.008067121,0.03919046,0.0300643,-0.018588638,-0.05743391,-0.020172372,0.00864401,0.071041636,0.073645435,0.011846914,-0.010154214,-0.01105373,0.06352881,0.06529374,-0.11360213,0.0020148484,0.08106338,0.03122247,0.113966964,0.048641413,-0.097014636,-0.0015295175,0.0014981542,0.014063618,0.045377914,0.0012385317,-0.011334273,0.014361589,0.018162694,-0.091564484,-0.015048318,0.11006505,0.030187292,-0.025214199,0.049782243,-0.05017902,0.12719359,-0.07609545,0.037764225,0.03905673,-0.024542572,0.032094963,0.035991855,-0.053777803,7.7350144e-33,0.010051098,0.014144754,0.016894322,0.008811139,0.064132996,-0.025527868,0.039555628,0.030016176,0.06577972,0.053524867,-0.04955057,-0.001692039,-0.040157296,-0.0012333731,0.10231882,0.0009371564,-0.1047497,0.06482044,-0.034364384,0.11253969,0.08727859,-0.07801994,0.012078468,0.0029080294,0.02744475,0.0323403,0.05576115,0.038972482,-0.11022575,0.03582458,-0.0061988076,0.0023222703,-0.025521493,0.05882839,-0.042307876,-0.020125825,-0.00010898467,-0.061381098,0.044518374,-0.04612137,0.030926783,0.006648208,0.009113678,-0.02549787,0.0013588981,0.08435647,-0.0053768535,-0.09626668,-0.041570116,0.070373386,0.069648445,-0.012735234,-0.031240573,-0.018748438,-0.16016391,0.06633388,-0.010432415,-0.037520956,0.009269088,0.08178665,-0.058358595,-0.03846029,-0.053508468,-0.019244174,0.031889174,-0.0283759,0.06531098,0.0003492884,-0.0022395132,-0.13073385,-0.04368889,-0.055694122,0.00083445176,-0.041656237,0.043543216,-0.06086655,0.07320348,-0.006460894,0.011742236,-0.022484431,-0.04098478,0.1061798,0.0005219359,-0.11521274,0.06687278,0.0032094128,-0.022696706,-0.017912285,-0.05257206,0.037432145,-0.08709233,0.1429892,0.010443473,0.065519676,0.0623507,-6.815208e-33,0.014322055,-0.044330228,0.054758586,0.11648662,-0.014208947,0.011800663,-0.047779493,-0.05638095,0.03214265,-0.10316416,-0.034539774,0.014043254,0.09754449,0.013503938,0.017285112,0.01287885,0.03864578,-0.060938384,-0.021438714,0.075953126,-0.017949605,0.12948008,-0.07439057,0.007553158,-0.05387417,-0.0037871816,-0.05986662,0.01679265,0.034559738,-0.035227332,-0.005511149,0.021344101,0.041498035,-0.054784838,0.00046938902,-0.053355973,0.008935332,-0.061432075,0.06158198,0.1128274,0.06411012,0.048427444,-0.077745564,-0.027409617,-0.012292076,-0.04063509,-0.056110468,0.013680197,-0.0010167313,-0.0020560925,0.0066882465,-0.045524057,-0.12108236,0.0264772,0.008441133,-0.024512174,0.004632868,-0.063822754,0.02907618,0.049918883,-0.11075639,0.06594565,0.017345041,0.030430784,0.03678738,0.0003998372,0.05261303,0.0029484783,-0.0049717105,0.008583014,-0.077904224,-0.007760705,-0.018310692,0.021521484,-0.033610124,-0.069860086,0.03167215,-0.062131684,-0.008884976,0.038829036,-0.018641185,0.048074305,-0.009080865,-0.00031994563,0.0072714747,0.048838798,0.045137767,0.001391126,0.0002943922,-0.018845338,-0.06603666,0.02085199,0.023155697,0.072657205,0.04208652,-6.766006e-08,-0.029617028,0.06331235,0.059671123,0.024232168,-0.058662716,-0.06412154,-0.06509245,0.1524654,-0.02161484,-0.059074268,0.015476873,-0.031534865,-0.09209571,0.09048217,0.10110311,0.011322353,0.0026551648,0.082677,-0.086329505,-0.032805268,0.057264052,-0.039773438,0.04003044,-0.036679816,0.12869321,-0.066875584,0.013774135,0.024068568,0.037703726,-0.0029852048,-0.028695086,0.026345035,0.0045315307,0.05700649,0.026818575,-0.0027316192,0.077161506,-0.04387534,-0.021459538,0.04339962,0.03803166,0.097117536,-0.06761315,-0.016373714,-0.02338423,-0.037704993,0.04071492,-0.054723326,0.0017351023,0.016630882,0.0028723353,-0.023544244,-0.034350213,0.006947318,0.012320863,0.07467918,-0.052446604,0.012753614,-0.01618655,-0.034438614,0.059321504,-0.13047455,-0.056349695,0.018940879,5,-0.6472843,-11.091407,9
277,"to improve our results, we focused on three key areas: refining the data, enhancing our methods, and fine-tuning those methods. hereâ€™s how we approached it:

we started with multiple linear regression (mlr) for simple, linear relationships. however, when dealing with more complex, nonlinear data, we introduced features like \(x^2\) and \(\sin(x)\) to capture the patterns. this approach evolved into polynomial regression. initially, adding more features improved the model, but over time, it started to perform poorly. to address this, we kept only the most significant features based on their p-values.

we also experimented with feature selection techniques, such as forward and backward selection. the main challenge was avoiding overfittingâ€”where the model performs well on training data but poorly on new data. instead of using separate models for different types of relationships, we considered combining them into a single model. random forest emerged as a strong candidate for this approach.

next, we explored both parametric and non-parametric methods, along with delta analysis. we also briefly introduced neural networks and deep learning, which use multiple layers to uncover complex patterns in data. finally, we shifted our focus to classification, comparing traditional regression with logistic regression. we discussed the role of weights and introduced the sigmoid function, which is central to logistic regression due to its s-shaped curve.",-0.048708804,-0.06888638,0.058662914,0.104331024,0.09107186,0.051082168,-0.059011392,-0.063912995,-0.035153747,-0.048773333,-0.036261987,-0.0018625056,-0.04100719,0.07022238,0.017280543,0.06592208,0.032189332,0.03451823,-0.061715543,-0.014389581,0.014663697,0.013710947,-0.015074393,0.010154767,-0.017016511,-0.01009585,0.03204267,-0.02431794,-0.08274569,-0.022708734,0.028021628,0.017505893,-0.03748151,-0.026927818,-0.1194652,0.018538913,-0.049179595,0.098202795,0.004732765,-0.045699097,0.031403664,-0.08746914,0.0074323067,0.014988514,0.10074269,0.02944574,0.005086871,-0.02628294,0.03363989,0.039185088,-0.06783503,-0.017794207,-0.04684696,0.04386006,-0.0039244858,-0.040098213,0.03004393,0.028493075,0.022083024,-0.03350003,0.0948205,0.009014601,-0.05677309,0.002737467,-0.026021345,0.023294019,-0.014158657,0.059404593,-0.008744819,0.04143172,0.02076539,0.117099956,-0.09118359,0.07945573,0.0066809966,0.029704355,0.023971397,0.062994905,0.04691626,-0.054557294,0.00501669,0.02670438,-0.037788697,0.0019189197,0.09118929,0.027028112,-0.047738664,-0.0032091513,-0.09045103,0.03893014,0.033764575,-0.0005743414,0.058028016,-0.0057635163,-0.03804323,0.05781989,-0.0101829395,-0.14165469,0.0023713561,0.06414522,-0.03752829,0.09999697,0.0031279454,-0.053390365,0.03987102,0.008119021,0.005340895,-0.010809759,0.06672744,-0.06968882,0.017505988,0.034651432,-0.102358684,-0.049200132,0.09996231,-0.0011213192,0.0016177627,0.06901824,0.008797243,0.14180125,-0.045866497,0.040076196,-0.00975805,0.0028461197,0.06506614,0.0061429,-0.09862483,3.8460052e-33,0.037758105,-0.00013806697,0.00758931,-0.0052247834,0.032643642,-0.06507475,0.027851155,0.022517331,0.0018999535,0.10729796,-0.06325637,0.013367149,-0.043950062,0.052358393,0.066268966,-0.008717629,-0.08039061,0.043775495,0.020814592,0.019190602,-0.013848913,-0.04547257,-0.02536967,-0.031059394,0.0025727754,0.0588112,0.024636243,0.016468126,-0.03521577,0.008860725,-0.028706266,0.052420434,0.0019883178,0.017717537,0.008267413,-0.036326315,-0.046468187,-0.08368897,0.027423864,0.02627947,-0.041967694,-0.0133357765,-0.011612504,-0.0017034438,-0.0027145713,0.04177151,-6.447096e-05,-0.10947587,-0.097546674,0.023495015,0.0074818362,0.00019103703,-0.071815856,0.011975439,-0.11673142,0.026992546,-0.03037056,0.02542766,-0.036154363,0.032993056,0.0029966435,-0.003957299,-0.04812628,-0.015677001,-0.027675623,-0.025219718,0.030135853,0.012870397,0.03060763,-0.04839529,-0.0020311063,-0.025608245,-0.011339233,-0.08113271,0.0599996,0.06347522,0.07536998,-0.045097053,0.013353788,-0.023339745,-0.06111504,0.04023956,0.07271307,-0.047210056,-0.051043205,0.020240419,0.02394785,-0.07195136,-0.016099289,0.01212973,-0.1457432,0.07940572,0.0058095744,0.015693696,0.0425025,-2.7588739e-33,-0.036962565,0.053081572,0.027931709,-0.0073326663,-0.033259653,0.043914292,-0.0232953,-0.024972187,0.0010430729,-0.07614153,0.03926792,-0.039276067,0.10842575,-0.02145119,0.030636966,0.027405027,-0.00089245173,0.008291348,0.0036844,0.06376545,-0.040682495,0.12359641,-0.10596504,0.04710911,-0.072514646,0.0031379778,-0.017572325,0.024299866,-0.0028504175,-0.01309929,-0.06702537,0.028086519,0.04513925,-0.08553732,-0.02142858,0.03758172,-0.01595423,-0.08536842,0.031149058,0.046930756,0.07069099,0.04174295,-0.008222346,0.012285584,-0.041570287,-0.07067693,-0.048515107,0.024216855,0.060485456,0.07879802,-0.013413408,-0.03570319,-0.12593776,-0.00634905,-0.016633837,-0.010220261,0.01724587,-0.0017542536,0.056050446,0.0692996,-0.12438508,0.01666525,0.085653126,0.034719918,0.015881136,-0.053004533,0.07509639,0.012009525,-0.009977219,-0.011238098,-0.00796775,-0.051800273,0.0009557101,0.027320538,-0.07429522,-0.09612836,0.002375523,-0.085893825,-0.0786845,0.049470734,0.06198881,0.00010608861,-0.015018996,0.039935518,0.050135028,0.084617116,0.032853603,0.026441297,0.07123389,-0.04875195,-0.08180074,0.07444763,-0.06677484,-0.0057725944,0.025291152,-5.583752e-08,-0.032398462,0.08141709,0.0028007969,0.03693724,0.019914102,-0.058790285,0.017458372,0.18554053,-0.10941675,0.012896677,0.053384487,0.010593426,-0.015534748,0.046258755,0.077495284,0.008650914,-0.004113296,0.010780643,-0.01876896,-0.02104767,0.07515035,-0.03254814,0.030088792,-0.023664517,0.121620074,-0.10629947,-0.026206652,0.032665823,0.056757096,0.03217391,-0.03468853,-0.009055926,0.021626586,-0.0030084495,0.03691192,0.0756545,0.069329284,-0.05151212,-0.037008896,0.027308509,-0.023851708,0.059980724,-0.028548928,0.012008983,-0.021217223,-0.012349484,0.07313467,-0.042014007,0.018757071,-0.013190407,0.021679759,-0.0046171364,0.015375685,-0.023626255,0.06789813,0.089173354,-0.060231328,0.019461127,-0.012786033,0.027327945,0.04776312,-0.14350006,-0.0430429,-0.006639362,5,0.624246,-11.77684,9
287,"in today's session, we first see that we can improve the quality of results in 3 ways as: we can improve the sample by increasing the quantity of sample and size of the sample, improve the method by using multiple methods and select the best one, fine tune or properly use the method. then we see that in linear regression, outcome is expressed as a linear combination of independent variables. then we see that if the error plot is not random or follows a pattern, forcing a line to model to the data gives improper results. hence, we need non-linear independent variables so that mlr will predict the desired non-linear y. for ex, to get model for sin(x1) we take first independent variable to be x1, the other can be introduce as x2=x1*x1 and further x3=x1*x1*x1 and so on. the resulting method is polynomial regression, introducing such new x to improve models in ml is called as feature engineering. you can add features but based on f-statistic and adjusted r^2 you can cancel some features which are needed to be eliminated for better regression model. then we saw the comparison of backward v/s forward feature engineering. then we applied different models on the same model, compare them and check which of the model best fits the data. then we at last started learning about classification, seeing about logistic regression in which outcome is a classifier.",-0.08367886,-0.03422704,0.0007495346,0.029292056,0.05622362,-0.008337023,-0.0726209,-0.030088704,0.031052597,-0.018568063,0.034213215,0.048376765,0.033434078,0.006624723,0.06391247,-0.004183647,0.030743148,0.076056704,-0.0722416,-0.018538468,0.03095267,-0.04315655,-0.068767875,0.018934326,-0.02484681,-0.0027528598,-0.025869433,-0.03489879,0.009025556,-0.07598543,0.054415327,0.106486194,0.005051834,-0.047739964,-0.09824149,-0.02171083,-0.092444725,0.06847454,-0.019499023,0.04014193,-0.049333163,-0.06493351,0.04861606,-0.047343787,0.039056256,-0.04260032,0.029964486,-0.077272944,-0.01323787,-0.024282066,-0.023835095,0.032656454,-0.07766073,-0.053785812,0.021648945,-0.06952342,-0.020088764,0.008068729,-0.014414894,0.009326939,0.03662919,0.015458938,-0.06402858,0.012322272,0.008889326,-0.10212294,0.034737505,0.029100103,-0.00064720155,0.08902094,-0.03800714,0.033620708,-0.09714489,-0.032347973,-0.009475828,-0.004081264,0.027762499,0.102620706,0.040620755,-0.021852393,0.023555411,0.0344902,-0.0081461035,0.01927253,-0.003155829,0.01385928,-0.07955403,-0.041046523,-0.01136806,0.039329227,0.043413952,0.051458552,-0.107208766,0.05397924,0.03979419,0.002776754,0.057524007,-0.026062876,0.088957645,0.10202079,0.03218204,0.06422799,0.06340939,-0.011027001,0.014940158,0.00025305134,0.07119145,0.04721155,0.064766705,0.008233403,0.0035025936,0.04842306,-0.07782994,-0.027048418,0.11904153,-0.051583476,0.011605132,0.029888406,0.04308091,0.021712217,0.028050894,-0.031980406,0.05278048,0.057210375,0.03450628,0.024512537,-0.10208159,8.724365e-33,-0.04805532,0.07274518,-0.037622232,-0.041341864,-0.03364607,-0.011886471,0.0021522653,0.030788815,0.12381302,0.038977616,-0.04272536,-0.017041264,0.028656296,0.040489346,0.082557194,-0.031779926,0.007458994,0.009224835,-0.0041112993,0.04259397,0.025495725,-0.09747756,0.090766124,-0.058818627,-0.018339036,0.04090376,0.05138816,-0.0089667095,-0.11027364,-0.008032529,0.057112455,0.022127382,0.0062979506,0.03762356,0.0028490673,-0.017105797,-0.0046995203,-0.08766388,0.004276129,0.04599491,0.0047654076,0.0072374474,0.07636999,-0.010425139,0.017191282,0.030733576,-0.029364703,-0.0056737484,-0.032757923,0.030120363,-0.03231204,0.008913346,-0.018566897,-0.02091083,-0.028596334,0.028645229,-0.08269835,-0.063965835,-0.08679928,0.05924848,-0.1317427,-0.042851888,0.007208437,-0.06926555,0.0074059623,0.040708143,0.05262894,-0.057786725,0.016315408,-0.04707135,-0.03712067,-0.020612946,-0.020935668,-0.050305516,0.09230151,-0.06590273,-0.02394047,0.0005296057,0.060042616,-0.048602916,-0.051021475,0.0698959,-0.020077052,-0.051689077,-0.05380824,0.0437991,0.06826576,-0.032160636,0.050658867,-0.019684967,-0.10638554,0.039636824,-0.0013017087,0.077673115,0.09472071,-9.07491e-33,-0.048022754,0.06034849,0.024257267,0.0052807587,-0.031091647,0.028293006,0.023621522,-0.06388985,0.057732247,-0.055371933,-0.036729727,-0.03821327,0.029585477,0.025119709,0.027014446,0.046609923,-0.02258338,-0.06723763,-0.07130825,0.03450236,0.0019575893,0.13857521,-0.07566755,-0.029102178,-0.035012543,-0.010087301,-0.13290875,0.029786192,0.0015950063,0.0041742325,-0.01610589,0.035919257,-0.020150485,-0.059041362,0.022378659,0.08310153,-0.0061111767,0.013528325,0.00817163,0.005121171,0.064251795,0.013780122,0.03423897,-0.04251596,-0.005944386,-0.021749545,0.018270226,0.04518857,0.065058835,0.07060293,0.009348487,-0.020633418,-0.0790013,0.026037818,-0.051912215,-0.059348054,-0.034068663,-0.02875047,-0.031890728,0.025538225,-0.074019656,-0.04613774,0.0071374523,0.03993721,0.0050551845,-0.0049179783,0.021195415,0.007329368,0.06601935,0.03334441,-0.10696857,-0.05033976,0.0025753684,0.013566229,0.01900323,-0.06495456,-0.020357518,-0.103191756,-0.12698117,-0.0048035816,0.050554283,-0.0073330994,-0.0028417674,-0.014316016,-0.03598091,-0.018269097,0.04334162,-0.0012373684,-0.0037168236,-0.08776043,-0.02985541,0.025057308,-0.04223041,0.04849549,0.05717307,-7.600306e-08,-0.090653665,-0.026762567,-0.0011688862,0.042444468,0.024273569,-0.0053428095,-0.048546713,0.078600824,-0.030006554,-0.033724286,-0.015803691,0.06688678,0.016355865,-0.020955289,0.070138015,0.04505327,-0.04205508,0.08138428,-0.018472148,-0.08698447,0.08065087,-0.026803004,-0.025034826,-0.091968164,0.09056371,-0.040627,-0.015535927,0.021772766,-0.010902332,0.037632,0.02279239,0.044002596,0.032766208,0.09716379,0.10910374,0.019457435,0.09803768,-0.067210816,-0.019219168,-0.017128266,-0.06782624,0.17048088,-0.0715031,-0.003441655,0.09378168,-0.055937566,0.016873196,-0.10431368,-0.017531183,-0.02250634,0.028592069,0.02908966,0.021391233,-0.0296847,0.039788745,0.054253366,-0.037828963,0.042682383,-0.026197892,0.07523854,-0.06413104,-0.027390227,0.03453667,-0.12948826,5,-2.9739106,-9.29515,9
294,"to improve the results you can either improve the sample or improve the method used. improving the sample could mean increasing the quality of the sample or the quantity. when it comes to improving the method, you can either fine-tune or better understand the method or use multiple and select the best one based on your knowledge about the metrics. one technique used to improve the method is call grid search where you form an n-dimensional grid with all the possible value combinations of the n parameters of the method, and then evaluate which one is the best.
polynomial regression is a type of linear regression where we add additional features derived from the given features, and they are respectively, the features raised to an exponent, eg.- x1 -> x1^2, x1^3, etc. more generally, we can use domain knowledge and insights gained from exploratory data analysis to engineer even better features.
there are two extremes to the process of feature selection. in forward feature selection, we start with an empty set of selected features, and then based on our knowledge, keep on adding features to the dataset in the hopes of improving the results. in backward feature selection, we start with all the features we can think of and then start reducing them by some metric (eg. p-value).
in parametric methods (such as linear regression), we can perform delta analysis, which is answering questions like how much does the output change with a slight change in a particular feature. this can be somehow achieved in non-parametric models also, but is much easier in parametric ones.
in real life after spending 80% of the time doing something with the data that does not include 'fitting a model', we actually need to fit many models and then decide which amongst them is the best based on various factors, some of which are: interpretability (whether we can make sense of what the model is doing), maintainability (the model will have to be recreated when 'data drift' {some change(s) to the trend in the data that occurs with time} is observed, hence if one model can fit the whole data, it is better), and of course the understanding and feel for numbers and error metrics.
a neural network is a model where we have hidden layers that are connected to input and output and amongst themselves using links that have an associated weights. the output function can still be expressed as w_i*x_i. more layers (called deep neural networks) add more flexibility / provide more degrees of freedom to the neural network, but in turn it becomes extremely 'data hungry'. the latest neural network models such as chatgpt have billions of parameters and are trained on internet-scale data, but the task that it performs is just: given a few characters / words, what is the most likely next character / word.
logistic regression is classification period.
need to predict the boundaries that separate the different classes. in case of overlap, we look for minimizing the number of mis-classifications. define an equation for the boundary and then based on the output, assign a class label. one such function that can do this is the sigmoid function (s(a) = 1/(1+e^(-a))).",0.030691396,0.01773586,0.006837201,0.037291393,0.013744664,0.024823567,-0.050739992,-0.0010696627,-0.0032456513,0.019028926,-0.057440285,0.040707182,0.0161347,0.018982647,0.026375445,0.07579301,0.07097718,0.08240077,-0.019984841,-0.07426205,0.011284737,-0.079327404,-0.026328165,-0.021383975,0.054756247,0.00063983386,-0.05384936,0.028112873,0.00538475,0.04525172,-0.025421688,0.06298336,0.038870774,-0.059205104,-0.07268883,0.088636115,-0.033810217,0.123616606,-0.004055973,0.007692655,0.041063026,-0.05917259,-0.010870596,0.02571054,0.095686205,0.01417286,0.0018193296,-0.025798015,0.0166257,-0.038777903,-0.03747127,0.026837243,-0.019960253,-0.013486544,-0.0057586557,-0.07564845,0.029721946,0.08510262,0.018336693,-0.06530921,0.06797602,-0.03730946,-0.074319,0.025470523,0.04068652,-0.05602372,0.024272226,-0.02528192,-0.032671556,0.11785025,0.04523926,0.122951,-0.05091954,0.011052641,0.0037335719,0.01821301,0.016658776,-0.028322969,-0.0016200115,0.03812223,0.066182315,0.07368362,0.008979762,0.018278256,0.034024738,-0.009573386,-0.008860492,-0.08479914,-0.057796564,0.0543079,0.035900466,-0.01988737,-0.091304064,-0.004280444,-0.020519463,0.016683051,0.03170568,-0.11920653,0.049579643,0.07710603,0.034313682,0.051197167,0.018014109,-0.06659901,0.029126404,0.006814605,0.021576168,0.065214075,-0.0039775693,-0.0010331332,-0.00056613004,-0.03138594,-0.07188077,-0.026954932,0.093115985,0.049505576,0.026328128,0.05671275,-0.03111,0.0058592255,-0.025389146,-0.018908566,0.05356738,0.0013783572,0.06470707,0.0375815,-0.09307513,4.543598e-33,-0.006822316,0.0116823,0.026087767,-0.07575272,-0.033506226,0.014934564,0.059724376,0.028600032,0.070636936,0.07554364,-0.0015306812,-0.009460784,0.04151483,0.024625104,0.066350766,-0.016395418,-0.03615439,0.034365438,0.0016261083,0.09127244,0.033485208,-0.15081015,0.041403968,-0.053784106,-0.013425608,0.034024674,0.049340703,0.057224963,-0.09536022,-0.02145827,0.039465234,0.008728482,-0.07020044,0.029809384,-0.007943156,-0.035489447,0.018225824,-0.098075524,0.065364815,0.018277667,0.018301416,-0.0015096386,0.086652555,0.0038012257,0.0070801987,0.06348991,-0.020732755,-0.11784395,-0.030980986,0.069209926,0.030957468,0.013085355,-0.05539159,-0.023853423,-0.09601995,0.008384373,-0.06196431,-0.044407293,-0.010920642,0.03556703,-0.06441752,-0.0829394,-0.01143782,-0.06578638,0.0058770743,-0.00430716,0.044232067,-0.05292985,0.006964877,-0.026095992,-0.042869035,-0.09362792,0.01912708,-0.03892901,0.09155345,0.0012538536,0.012286007,-0.028486108,0.0531593,-0.112834826,-0.051208723,0.067877874,-0.0074674427,-0.08775914,0.02604381,0.0061818785,-0.030672532,-0.036721673,0.030941373,-0.03242407,-0.15176117,0.06842321,-0.052244186,0.06683136,-0.007861986,-4.9768517e-33,0.0063127438,-0.010163289,0.016636597,0.054111194,-0.056468982,0.025374297,-0.03581168,-0.07312577,-0.017362487,-0.10116782,-0.010269369,0.013721842,0.037314042,0.01065799,0.009544753,0.064843774,-0.061803795,-0.049392637,-0.0042788223,-0.0026367062,-0.058518708,0.10486178,-0.0018710224,-0.0015292643,-0.042711712,-0.019182717,-0.015644507,-0.076488614,0.059453774,-0.021116955,-0.052451007,-0.0035187707,-0.04542381,0.0250297,0.040786233,0.025218258,-0.039263994,-0.09099756,0.030277785,0.10978006,0.035020877,0.038795285,-0.0149447145,-0.060090113,0.009077848,-0.04818961,-0.053177148,0.01615148,0.10590283,0.0692055,0.041162096,0.0071027856,-0.098258294,0.023251945,-0.036751684,-0.049889132,0.014748744,0.0011565702,0.061889052,0.009048044,-0.07579951,0.0003945473,0.02971158,0.08010294,0.023847207,0.016907733,-0.03471632,0.024494627,0.0075134905,0.011680772,-0.1093655,-0.0767392,-0.033434328,0.0019045537,-0.05901274,-0.059239924,0.013683681,-0.028334657,-0.036499355,0.036996275,0.09182177,0.06168441,0.020850094,0.0061078784,-0.01610612,0.062256332,0.07496821,0.043069836,-0.019076131,-0.06802718,-0.046078097,0.012744733,-0.040080443,0.03556813,0.018388765,-6.2865766e-08,-0.09248443,-0.00014399941,0.061888497,0.025006035,-0.029132653,-0.06220306,-0.06948442,0.15194136,-0.06350363,-0.049899448,0.024130546,0.0044534677,-0.034773145,0.05364181,0.117376894,0.018630989,0.009837939,0.031728994,-0.06947563,-0.099051215,-0.012382413,-0.03131304,-0.08799696,-0.05901679,0.103454866,-0.051960442,0.026787842,0.017987685,-0.005649103,0.026115727,-0.0010017384,0.010664188,-0.0014587739,0.01584719,0.09820552,0.064699985,0.016640708,-0.08870675,0.033001535,0.050051242,-0.060334153,0.079116866,0.01514444,-0.0077673416,-0.054836653,-0.0075313672,0.0719886,-0.03828687,-0.08260006,0.047989815,0.0785807,-0.0004957403,-0.03104561,-0.06647788,-0.012995498,0.079023115,-0.05461023,-0.008387012,-0.022940625,0.04119527,-0.040002815,-0.049350765,0.034902476,-0.05176998,5,-0.8168438,-8.58167,9
301,"we began by reading through the summaries and looking at the trends in the data. from the graph, we observed that as the sessions progressed, fewer people were submitting summaries, but the average word count in each summary was increasing. then, we looked at a case where the function was like a sine curve. we started with a single feature, x1. then, we produced additional features like x1^2, x1^3, x1^4 with polynomial regression. this allowed us to calculate a p-value. to enhance the model, we added another feature, sin(x1), and found that the new p-value calculated was smaller than before, so this sine-based feature was adding to some extent. although addition of too many features beyond a certain point may reduce the model's effectiveness, this is reflected in a drop in the adjusted r^2 value. as long as a single model can capture the data well, it is always better than using multiple models unnecessarily. we also covered some significant parametric methods, particularly neural networks. these models have an input layer that accepts features and computational layers that transform the data. when these computations are spread across several layers, it is known as deep learning. the input layer is connected to the computational layers through links, which represent the model's degrees of freedom. however, increasing these degrees of freedom too much can lead to overfitting, making the model less generalizable to new data.",-0.015065129,-0.025814677,0.0013417088,0.07630232,0.07784509,0.09563412,-0.08132334,0.084036075,0.1101674,-0.019173652,-0.02788897,0.055826396,-0.008608289,0.10552679,0.071113884,-0.0019497038,0.0014667382,0.048905328,-0.060784712,-0.08034687,0.029991008,-0.032172788,0.023181653,-0.0058268,0.039885677,0.03761941,-0.051192034,-0.024590516,-0.012997282,-0.025959102,0.0050137267,0.12167689,0.01872722,-0.011864264,-0.103829525,-0.049388934,-0.096822,0.104908265,0.036201887,-0.009115832,0.013366571,-0.059460636,0.013464733,-0.0219198,0.070684746,-0.014467732,-0.017306315,-0.012103289,0.019352468,-0.050261274,0.019747676,0.028358191,-0.06397882,0.01425588,-0.0027581619,-0.03813119,-0.031130755,-0.021166729,-0.012124472,-0.046387833,-0.0108621605,-0.02288164,-0.032977037,0.03924275,0.042822417,-0.0008957109,0.034503013,-0.018430777,-0.026054814,0.11467892,0.023360716,0.11270938,-0.11216041,-0.02736965,-0.005240614,-0.01674874,0.020662704,0.057114232,0.024466114,-0.054925416,0.05259592,-0.02336053,-0.034085725,-0.0498392,0.066545434,0.024664557,-0.027564943,-0.05790267,-0.07057207,0.01636713,0.046699546,0.012229077,-0.013436883,-0.003644937,-0.013044188,0.015420765,-0.009481255,-0.019039841,0.008553799,0.07764954,-0.011171395,0.040724408,0.03406634,-0.07320998,0.050183002,-0.018912992,0.037229005,-0.00016557625,0.008116415,-0.023659939,-0.014982918,0.072509706,-0.06678154,0.012881195,0.08035717,0.009670368,-0.0012362171,0.035789005,0.044817634,0.10913645,-0.028815135,0.001123587,0.0366614,0.07484721,-0.030213231,0.018607646,-0.1110259,4.2736765e-33,0.014644906,0.04790134,-0.011972698,-0.0019376058,0.0129882675,-0.0023437003,-0.041276064,0.049807083,0.05092889,0.034806263,-0.07139738,0.05068832,0.0037602047,0.010177726,0.07418293,-0.024112841,-0.044679977,0.07885294,-0.032633126,0.034352563,0.052339792,-0.14716715,0.07281128,0.02858238,-0.0668016,0.062388208,0.02791708,0.04970614,-0.08691298,-0.010658733,-0.017225696,0.03645533,0.013383922,0.014115931,-0.0088812355,-0.05760062,0.028894162,-0.11236649,0.054435268,-0.016958846,-0.05351387,0.0019336414,0.024992213,-0.0637527,-0.05774382,0.038048964,-0.02726927,-0.009312853,-0.08962749,0.08268363,-0.015791524,0.032141697,0.007936822,0.046575606,-0.08606065,-0.018793156,-0.02266365,-0.095351644,-0.056549706,0.001680109,0.023996284,0.017812772,-0.0032012719,-0.09237136,0.01996847,0.10555136,-0.0016841926,0.04873299,0.060747866,0.03005189,0.01274729,-0.015985837,-0.046071753,-0.07965573,0.113752246,-0.049365323,0.012739394,-0.03485009,0.035026554,0.012493547,-0.013525531,0.0023670595,0.062117387,-0.05276889,0.016121127,-0.00073431747,0.0685914,-0.058473386,0.004912931,-0.006087564,-0.05550102,0.027873136,0.028308833,0.047032796,0.037465494,-4.1074928e-33,-0.11305062,0.05622122,-0.03263467,0.04147127,-0.007348529,0.010724505,0.03493509,-0.02387569,-0.009371302,-0.06471835,0.013383192,-0.048562475,0.052353077,-0.030305214,0.10753203,-0.020940445,-0.01959772,-0.105881535,0.084958315,0.039555725,-0.019499907,0.059436187,-0.06704036,0.004005186,-0.061282888,0.0121728955,-0.10980539,-0.094152905,0.027182333,0.0056106895,-0.077594556,0.06443777,-0.04297203,-0.029319162,-0.04813251,0.04664502,-0.007744836,-0.03098638,0.0027669773,0.08336224,0.062357344,0.020161122,-0.018031519,0.01866389,-0.026241021,-0.0056345025,-0.035926424,0.023406276,0.037682764,0.07671417,-0.02183249,0.012693812,-0.07342228,0.0013193629,-0.08559866,-0.06682628,-0.066008195,-0.03576005,-0.033204406,-0.030788107,-0.116603315,-0.053442433,-0.04054293,0.014763543,0.02318077,0.016143013,0.061294217,-0.03150794,-0.0031927642,0.019694915,-0.06265354,-0.06747443,0.03526899,0.007443066,-0.037399873,-0.03078227,-0.05963248,-0.08217626,-0.11433919,0.010448239,-0.050660413,-0.018282102,-0.029492378,-0.047554415,-0.051434223,-0.0075608343,-0.01777459,0.05145833,-0.013356938,0.016378582,-0.031123409,0.061260942,-0.09634562,0.044791184,0.013430887,-6.4185464e-08,-0.042134304,0.03217217,-0.02776258,0.037079256,0.083262846,-0.11219792,0.009681588,0.106142506,-0.036567673,0.007821322,0.01604964,-0.010068226,0.010010848,0.021377582,0.03537331,0.102403834,-0.03939606,0.04463404,0.0009948201,-0.088126905,0.10222964,0.016205795,-0.042641282,-0.06916457,0.07956848,0.0042211516,-0.037664585,0.081804596,-0.011900961,-0.0006328042,0.0630771,0.0061563076,-0.021650717,0.07099317,0.06485743,0.05458179,0.06891195,-0.012827022,0.01764944,0.06329628,-0.043889195,0.1081409,-0.024873832,0.086620346,0.066437095,-0.0055041974,-0.009662636,-0.0494897,0.03159659,-0.038545337,0.025248514,0.08452896,-0.050878078,-0.033382967,0.050516743,0.06830904,-0.011750191,0.010106815,-0.009466282,0.0016259059,-0.04724027,-0.13327764,-0.03319671,-0.06502495,5,2.5473537,-5.1403723,9
307,"we examined the summaries and saw that the average word count was rising over time, despite a decline in submissions. next, we modified a sine-like function for polynomial regression by adding characteristics like x1â², x1â³, and x1â´. the p-value decreased when a sine characteristic was added, demonstrating its importance. a single, high-performing model is better, though, as adding too many characteristics without making the model better might reduce the adjusted r2 score.

we also talked about neural networks: deep learning represents numerous levels, an input layer links to one or more layers of computation, and overfitting can occur when the number of degrees of freedom is increased.",-0.04138068,-0.076073825,0.009836207,0.084039874,0.02182534,0.111892484,-0.054476574,0.042724192,0.082726076,-0.030955272,-0.034819223,0.04330196,0.011780739,0.026779678,-0.013580782,0.010847601,-0.026728267,0.052318722,-0.0918548,-0.09402151,0.07663577,0.02162648,0.05345357,-0.06837733,0.07819008,0.016360436,-0.09218228,-0.028376501,-0.011753349,-0.0009713167,0.0107920645,0.08453913,0.0614272,0.048477687,-0.07451681,-0.008985811,-0.07101189,0.06970454,0.007550303,-0.025968136,0.017930385,-0.0679604,0.015886793,0.038090385,0.13885288,-0.00027217923,0.0026927344,-0.00076189317,-0.00072364643,0.008506671,-0.00870859,0.006052546,-0.03355845,0.09277459,-0.039156456,-0.0026853147,-0.03380502,0.060376715,-0.048041735,-0.06267912,-0.018565703,-0.057667967,-0.029565511,0.010091058,0.021062708,-0.0074833394,-0.006475701,0.03889669,-0.06437096,0.11189387,0.084844075,0.17758587,-0.08479214,0.022435058,0.023927692,-0.004379905,0.008861262,-0.033451345,0.052521985,-8.129168e-05,0.041004658,0.008020687,0.014099838,-0.03462574,0.09933829,-0.02626936,0.028440334,-0.029217837,-0.08121251,-0.0053458796,0.0266995,-0.052526813,0.0328957,-0.020109229,-0.055750515,-0.019407082,-0.020298718,-0.07203723,-0.055149708,0.074814305,-0.03769307,0.03490639,0.020354308,-0.068421856,0.0040332745,0.030008337,0.059951168,-0.01782189,0.023431826,-0.07822419,-0.010414649,0.056634456,-0.0044911657,0.010569214,0.06458382,-0.010678986,-0.0022051183,0.023806939,0.037883155,0.06442234,-0.09160611,0.05293873,-0.012084418,0.01819714,-0.041331425,-0.019524194,-0.057967566,4.7331445e-33,0.019846521,0.058591567,-0.05391061,0.009139662,0.06624216,-0.047506634,0.009512113,0.05170282,0.014486152,0.04476105,-0.061922994,0.052858938,-0.06016203,0.08070439,0.07337973,-0.004946848,0.029415037,0.042497333,0.014964966,0.0024148894,0.0037504733,-0.08762666,0.03689801,-0.0152664175,-0.06705299,0.031626668,0.034849163,0.006529452,-0.078375235,-0.019136846,0.0006994279,-0.032666042,-0.0010914796,0.042210646,0.017142076,-0.029480426,0.003550384,-0.0090220105,0.09586076,-0.009778814,-0.006837739,0.042964034,0.060328703,-0.033533785,-0.032382302,0.01403343,0.0002725114,-0.0825205,-0.08339655,0.023593422,0.019191492,-0.0064548836,-0.028743595,0.07607352,-0.019369494,0.016657336,-0.006781331,-0.0048467717,-0.06539543,-0.0020551663,0.02916093,0.062657155,0.014761186,-0.095530406,0.016272154,0.07606437,-0.04895117,0.024039514,0.06415874,0.046483945,0.028347636,-0.003164683,-0.06052131,-0.041807137,0.038454823,0.041227117,0.019542173,-0.15501782,-0.0064957775,0.04482235,-0.0109073315,0.016332753,0.05859036,-0.10429601,-0.069396146,-0.026847629,0.062317118,-0.103448585,0.03196787,-0.011800885,-0.083055474,-0.01915285,0.027062627,-0.017921867,-0.011050413,-3.635109e-33,-0.1601884,-0.003342107,-0.052971423,0.079176396,-0.06314662,0.020860482,-0.000644461,0.055600904,-0.0049270215,-0.014801614,0.050052978,-0.0143315755,0.073963895,-0.032642707,0.11310037,-0.05854075,-0.015769307,0.0035627985,0.054096058,0.007920487,-0.006062748,0.11189601,-0.043387,0.027362604,-0.013367973,-0.0014166725,-0.065493666,-0.027992383,0.039262313,-0.024005817,-0.069255896,-0.008504543,-0.031931173,0.04002605,0.006140551,0.022180308,0.022298805,-0.079975955,-0.01513902,0.06407209,0.057920065,0.004557518,-0.013581925,0.012516423,-0.039927445,-0.03961572,-0.06556425,-0.020407658,0.0930567,0.08061997,-0.03537375,0.022891378,-0.0955821,0.05596012,-0.080611914,-0.058710195,-0.050759666,-0.0419527,0.007938037,0.011904897,-0.09629912,-0.042003293,0.016467115,-0.023859346,0.035363846,-0.043416865,-0.036554813,0.045390174,-0.0067031463,0.027321216,0.039449852,-0.04943768,-0.007048152,0.05541026,-0.0931264,-0.07615797,-0.03398975,-0.016759995,-0.113363355,-0.039324068,0.0016040279,0.025870858,-0.038951527,0.0046067443,0.031000236,0.026981926,0.037093014,0.024584338,-0.00085887813,0.014553906,0.0068895104,0.04905242,-0.0358045,0.01705492,-0.08040921,-5.3932688e-08,-0.058867756,0.0041426644,0.010762817,0.05057267,0.06139897,-0.10407247,-0.0057473443,0.073660016,-0.05944772,0.010654425,0.050423093,0.0052547012,-0.036960766,-0.03623223,-0.024817662,0.10664457,0.004259052,0.07870195,0.024445277,-0.13537835,0.100149795,0.070394285,-0.05508237,-0.033803575,0.047381904,-0.026429374,-0.033383522,0.05387432,-0.023424858,0.039671548,0.06341262,0.010420058,-0.026280092,-0.0150613645,0.073378645,0.12694433,0.03136983,-0.05102254,-0.004720658,0.002102792,-0.030152306,0.090831675,0.021641064,0.05617914,0.07208465,-0.04165361,0.049692065,-0.09530962,0.017230367,-0.05517722,0.07524201,0.06271219,0.00015716911,-0.035889685,0.033633154,0.08120707,0.0063028354,-0.003213227,-0.055367395,0.05585611,0.0025298162,-0.0425231,-0.0048236037,-0.029757515,5,3.5600214,-5.6591706,9
317,"we explored three ways to enhance model performance: improving the sample, choosing better methods, and fine-tuning models. polynomial regression and feature engineering were introduced, covering forward and backward selection for optimizing features.

the modeling workflow was outlined: data collection, preprocessing, model selection, and evaluation. among linear regression, svm regression, and random forest, random forest performed best but was noted as unsuitable for parametric tasks like delta analysis.

neural networks were introduced as parametric models with input, hidden, and output layers, transitioning into deep learning as layers increase. model evaluation used râ² and mse to detect overfitting.

finally, we discussed classification, distinguishing supervised learning (where both input and output data are known) and logistic regression, which defines decision boundaries using the sigmoid function.",0.02535386,-0.034590874,0.009379046,0.050803743,0.069128186,-0.011916077,-0.050502665,-0.012623699,-0.07112094,-0.017190797,-0.08165195,-0.007661174,-0.042536523,-0.0038504037,-0.0043461705,-0.022076063,0.054494668,0.076797664,-0.08660872,-0.08867741,0.0462989,0.020559132,-0.03847033,-0.03649175,0.015206565,-0.026574975,-0.02372116,-0.0018043742,-0.06739174,-0.004460502,-0.011138399,-0.04386559,0.007288801,0.0080752,-0.100849114,0.029399445,-0.01883123,0.059968468,-0.031454943,-0.03734868,0.013585498,-0.09221999,0.023089897,0.010826697,0.048426658,0.017257713,-0.0038047542,-0.03412897,0.007649106,0.042410225,-0.054500252,-0.037308265,-0.02931941,0.025854833,-0.06950203,-0.026443915,-0.0059601027,0.033730786,-0.030269865,-0.03342078,-0.027410518,-0.055397343,-0.040964987,0.032854684,-0.00096761074,-0.039379086,-0.02345823,0.017408498,0.05384842,-0.00089604914,0.041974306,0.14765674,-0.029366724,-0.0009935317,0.01974649,-0.013628853,0.07442466,0.062017087,0.057684544,-0.07325564,0.020774262,0.028423185,-0.020207953,-0.01747011,0.14105019,0.042842645,-0.032671005,0.014703174,-0.042739917,0.008302735,0.055620328,0.010247794,0.017607747,-0.048016407,-0.07384983,0.07581916,-0.04619122,-0.13847135,-0.06188521,0.05188608,-0.02751648,0.09303067,0.08758326,-0.042130638,0.06512365,-0.017952278,0.08175342,0.01626881,0.041583095,-0.089840755,0.044169772,0.06149098,-0.018596994,-0.017990207,0.0817458,0.03500384,0.019391663,0.04365858,-0.051990148,0.10704508,-0.09849361,0.09836253,-0.011956702,-0.026780492,0.027158981,-0.026883129,-0.08525984,5.1401205e-33,0.02279366,0.01753839,-0.009488562,-0.026073243,-0.008796008,-0.051528033,0.019511864,0.051983222,0.023008903,0.10130815,-0.07071493,-0.046231143,-0.062303033,0.12498749,0.09444789,0.030727012,-0.061124004,0.089853674,-0.0040571033,0.013285377,0.03229259,-0.085581586,0.0088722175,-0.012352971,0.022250498,0.08074906,0.039076004,0.021761285,-0.09543499,0.028439729,-0.036763467,-0.01007132,-0.012622628,0.009739814,0.029106146,0.02834389,-0.030394142,-0.007012272,0.060441017,0.0062031006,-0.037708253,-0.029801426,0.0022971358,0.053923227,-0.06831734,0.0049426123,-0.006401623,-0.094984904,-0.03452042,0.042839374,0.0041026217,-0.029734887,0.0016574727,-0.02090325,-0.06517939,0.045908656,0.050646104,0.037532523,-0.050530277,0.07035212,-0.031758558,0.010593182,-0.027591709,0.0239608,-0.014493726,-0.046020456,0.047829308,0.045257136,0.0655963,-0.046367295,-0.045576334,-0.067925125,0.039201472,-0.015107378,0.054295763,0.04734587,0.08048189,-0.06940886,0.028154105,-0.037012428,-0.03942375,0.08952282,0.0027132796,-0.08328252,0.04965457,-0.037522525,-0.0008668862,-0.05183311,-0.016258458,0.013685712,-0.17053089,0.1269764,-0.028873418,0.02182087,-0.025995484,-4.2019232e-33,-0.04777918,0.0519484,-0.022869082,0.061980534,-0.071828134,0.0060870056,-0.065342866,-0.004943475,-0.038482424,-0.07835822,0.009907379,-0.016404776,0.08494092,-0.020490076,0.017882915,0.042108238,-0.028372489,0.012751728,0.021659331,0.011327612,-0.0567036,0.13236555,-0.06152701,0.033636764,-0.10943048,-0.017986326,-0.05068392,0.03026014,0.03036775,-0.043879382,-0.031797815,-0.025137246,0.0055645164,-0.021820456,0.052006595,0.00067842915,0.015566866,-0.06927333,0.05781286,0.07569091,0.021541182,0.058465786,-0.04981484,-0.014395723,-0.025891552,-0.06121483,-0.024561493,-0.020031292,0.02280623,0.042601433,-0.012692098,-0.03306699,-0.039405696,-0.0006624235,-0.04874797,-0.025295017,0.02106171,-0.05053979,0.049780108,0.054782443,-0.0794444,0.002714521,0.087500796,0.03786766,-0.00028610212,-0.031219965,0.031254284,0.06569245,0.0154664805,0.021594802,-0.04188408,-0.023451317,0.019306391,0.06293469,-0.084476896,-0.09886266,0.010686371,-0.068572916,-0.0332235,0.029768178,0.01909046,0.006926707,-0.023178529,0.07296722,0.062097065,0.101162225,0.0313978,0.03857506,0.07500751,-0.05851855,-0.055236407,0.09727026,-0.042664643,0.057543587,-0.016828913,-5.0063683e-08,-0.0135462275,0.059979767,0.05802736,0.0271514,-0.018846933,-0.0043304046,-0.041252885,0.15591234,-0.047564726,-0.032685466,0.018081706,0.02837773,-0.02824777,0.01745717,0.08391098,0.035415627,0.05423309,0.03353499,-0.033138957,-0.034594703,0.07745382,-0.010264404,-0.015992705,-0.029145977,0.10704138,-0.12716115,-0.06390109,0.051540837,-0.004564626,0.03212114,-0.022379383,0.04576213,0.02977089,0.027309608,0.045337718,0.06941195,0.09175639,-0.06872324,-0.031742644,0.06920502,-0.008611356,0.08692391,-0.06354218,-0.0044354773,-0.06163295,-0.031968657,0.09883468,-0.079556964,0.0033793882,-0.01864022,0.023113899,0.011468661,0.007196483,0.0045859865,0.009658986,0.04348393,-0.023806997,-0.025372345,-0.011139761,0.06217124,0.022874543,-0.062796816,-0.008306224,0.007865967,5,1.5208677,-12.328152,9
340,"we started off with a bit of revision of previous class and talked more about statistics and then we entered the fun part. sir showed us his backend of this forms. we were shown the heat maps and a bit about how the observation how it is made and how to inferre stuff. and more more point, anything can be data. 
after that we were told about feature engineering, which is basically calculation of features based on existing features by performing certain operations. we also had a small discussion on solvers and gradient descent .",-0.07778351,-0.02437879,0.053752996,0.030009221,-0.0077123125,-0.041974235,0.0017913327,0.016657226,-0.14206967,0.04688627,-0.05643029,0.018475182,0.03725807,0.013241439,-0.0016202367,-0.00014971885,0.023165299,0.011386839,-0.05810834,-0.103972726,-0.0021862772,0.0034304706,-0.01656836,-0.002942293,0.023365855,0.07847975,0.027171547,-0.014119997,0.00012071074,-0.004355416,-0.02029731,0.077281974,0.012202028,-0.02630981,-0.067119166,0.011958583,0.045887552,0.03210231,-0.064092256,0.023727784,-0.09737414,-0.083209686,0.047643792,0.054491173,0.09304681,0.036235444,0.0028536082,-0.12186263,-0.0037942303,0.021239223,-0.07374279,0.0025055555,-0.09606701,-0.045652363,0.01725912,0.044638682,0.033564616,-0.02871305,-0.029847821,-0.042085942,0.014584728,-0.049599603,-0.042241704,0.034598902,0.009749713,-0.077108234,-0.043081798,0.023623837,0.09750048,-0.06468387,-0.031261764,0.022964517,-0.030348442,-0.0030281718,0.049513783,-0.03329856,-0.02708377,0.03613419,0.028741779,0.008075089,0.022052398,-0.017061435,0.012836193,0.058036443,0.023405043,0.0133107435,-0.008210558,-0.03091688,-0.11645456,-0.036882255,0.052139472,-0.03713996,-0.050401524,0.0078260675,0.01052537,0.043871466,-0.037437543,-0.089424066,0.053583123,0.005416061,-0.05526163,-0.01475837,-0.016582021,0.002162762,-0.020594079,-0.05752313,0.038726646,0.013521922,0.092056215,-0.02243114,-0.043551933,-0.00431898,-0.091651775,-0.0444976,0.048947208,-0.07165944,-0.028971408,0.018040603,-0.03223666,0.04358558,0.02568272,0.053627122,-0.010125742,0.009727528,0.03585342,0.037099756,-0.06015188,6.859459e-34,0.024590729,0.037998483,-0.044252973,0.1364123,-0.009877138,0.012791611,-0.011162937,0.038685836,0.10163124,0.039655045,0.056231342,0.091899164,-0.027173406,0.09803305,0.08259116,-0.030101929,-0.085491344,0.020702813,-0.0090545155,-0.03521049,-0.0077849175,0.030346807,0.126775,-0.0042074146,0.03800119,0.09466804,-0.01976065,-0.034356613,-0.02227909,0.017816054,-0.013158235,0.020476945,-0.11653968,0.021359872,-0.033237275,0.017355228,0.008625032,-0.11754543,0.056764327,0.012621676,0.020310476,-0.023156315,-0.031219322,-0.03661975,-0.05311182,0.030367855,0.040507577,0.01573027,0.05043953,-0.046751864,-0.10822315,-0.07367813,0.010613031,-0.03756033,0.007618551,0.07527677,-0.016456615,-0.050484475,-0.085224606,0.06289366,-0.045138296,0.08528119,0.0049554645,-0.09373113,-0.12732907,0.02045304,-0.012812614,0.02221319,0.055566218,-0.032582037,-0.04352831,0.03962378,-0.062044967,-0.062257946,0.06185074,0.034609087,0.03102532,-0.032339722,-0.03926769,0.017530823,-0.009890223,-0.069825605,-0.0068079345,-0.0513844,0.011492154,0.04799315,0.049263608,-0.057819508,-0.006706963,-0.0106593305,-0.07930037,-0.03681168,-0.012841886,0.08426232,0.021237403,-3.8051127e-33,-0.1393591,0.034658227,-0.05284448,0.049227677,-0.009821761,-0.039021842,-0.074135244,-0.035866894,0.034216356,-0.005684209,-0.031592842,0.004395382,0.004707679,-0.055016078,-0.058739368,0.0031978518,-0.06849466,-0.10237611,-0.04600896,0.03946544,-0.076081544,0.10139083,-0.1008979,-0.08715392,-0.03566151,-0.0034297488,-0.0010893992,-0.047750622,0.02535737,0.028239846,-0.024419064,-0.069877155,-0.018104177,9.595198e-05,-0.042588606,0.021354588,0.09366611,-0.041771863,0.032450445,0.00542497,0.08283128,-0.03236622,0.088947594,-0.07403283,0.045816917,0.0010129242,0.0050964095,0.08085529,0.015298642,-0.01849135,0.12304713,0.022330767,0.022083126,-0.032514207,-0.008548405,0.012456321,0.048660316,-0.04702765,0.04590039,0.08266321,-0.04908461,-0.008541214,0.041099776,0.07357249,-0.030824242,0.028893905,-0.0441922,-0.019657241,-0.04484529,-0.018792408,-0.02277819,0.01739029,-0.011994446,0.00206243,0.014682118,-0.059617363,-0.032539874,-0.04435657,-0.041287705,-0.0072290204,0.05440608,-0.057660934,0.026804043,0.04268932,0.06442344,0.022931542,0.039592534,-0.033822346,0.029968826,-0.070199236,-0.07387905,0.062152997,-0.09413748,0.057478677,-0.0152757,-5.5277695e-08,-0.082220964,-0.040834382,0.10839407,-0.007877018,0.077546135,0.02197394,-0.03404378,0.0963563,-0.01905894,0.03412888,-0.014105508,0.066059485,-0.009304946,0.04027234,0.115533404,0.023285722,0.07539169,0.062091768,0.00034123895,-0.05368873,0.082715176,-0.036681112,0.06951912,-0.0006390055,0.04094895,-0.03798333,0.06327056,0.028527532,0.09417007,0.027169626,-0.06781946,0.038113642,0.037719678,-0.007014407,0.08625381,-0.006147089,0.089399666,-0.04715463,-0.027901674,0.018976035,-0.08546228,0.06610877,0.0149818845,0.061293796,0.07297691,0.068590045,-0.030825663,-0.0553033,0.020202154,0.08076344,-0.020121234,0.017073702,-0.016319662,0.034711313,-0.0025797265,0.07161378,-0.0041969637,-0.05178942,-0.02585275,-0.0027425652,-0.016028693,0.08627026,-0.02217685,-0.0034412323,1,7.917591,-8.17819,9
368,"linear regression is expressed as a sum of linearly independent variables. experimented with sin(x) is a variable that results in the error being distributed evenly about zero, although not normally. an effort to add more variable could result in overfitting. a comparision of different models performance on a certain nonlinear data was demonstrated. xgboost, random forest and ann performed better than slr and mlr judging from the fit and the error distribution. slr and mlr are parametric models while the former three are non-parametric models. introduction to logistic regression as a classifier. the distance from the separatin plane is passed through the sigmoid function to obtain value 'a' between 0 and 1.",-0.061517637,-0.08865573,-0.06792766,-0.016571231,0.08871662,-0.031442545,-0.047133893,-0.04918525,-0.01943039,0.011995466,0.08548832,-0.0023558198,0.007964559,-0.013215349,0.028837869,-0.03089244,-0.0064615235,-0.033160437,-0.08943759,-0.010480084,0.036270183,0.030979687,-0.080897056,-0.0007351689,0.047198527,-0.041781962,-0.018253885,-0.0059880754,-0.075966865,-0.021592068,0.019382022,0.019127004,0.017218579,-0.010673234,-0.104247466,-0.10339154,-0.07163934,0.018854972,-0.024280872,0.022025663,-0.034520518,-0.071654506,0.03632625,-0.02262525,0.03646534,-0.008998375,-0.02526853,-0.047076732,-0.0043371706,0.016557414,-0.044571146,0.01681518,-0.042256653,0.012980545,-0.05227928,-0.054374743,-0.059989624,-0.058239814,0.027210334,0.00038700254,-0.011685423,0.03957054,0.021206565,0.025383772,0.034039393,-0.062624134,-0.048596654,-0.0044680303,0.003294389,0.018682811,-0.031728197,0.01858792,-0.043939788,0.06016795,-0.023381414,-0.02316119,0.041056685,0.10215056,0.040215448,0.0057664425,-0.05816067,0.07329733,0.051757924,0.035523515,0.10412661,0.017484324,-0.0022937106,0.063022435,-0.05337296,0.036129177,0.034464926,0.055856925,-0.013270186,0.0039965943,-0.07871764,-0.0338938,0.022743396,-0.07826027,-0.021972423,0.054697357,-0.121100724,0.058533918,0.059905943,-0.022061951,0.045328982,-0.033344712,0.06271563,-0.009022389,0.09103118,-0.0053755967,-0.019987278,0.059455924,-0.07959382,0.015404231,0.09367724,-0.05794267,0.0158269,0.04761781,-0.003949939,0.07389577,0.02116445,0.028562328,-0.009694972,0.04447535,0.0026079174,-0.043620326,-0.102396555,7.3010895e-33,-0.056980588,0.05877101,-0.014775894,-0.07856596,-0.058513157,-0.026608456,-0.06315206,0.08002298,0.043277327,0.09611819,-0.10965847,-0.056760512,-0.014081744,0.10043202,0.100948706,0.06568173,-0.009237612,0.06143442,-0.06138431,-0.015355296,0.009608992,-0.120478004,0.03200029,-0.08615527,-0.0036381984,0.08616961,0.015358267,-0.013747711,-0.049108434,0.019794464,0.05616597,0.014209554,0.016705176,-0.011538882,0.041826606,-0.006522562,-0.03450054,0.00013128744,0.007965905,0.043034807,-0.07382233,0.014784357,0.05680122,-0.016894914,0.005884875,0.05646901,0.06462939,-0.06282988,-0.0100235725,0.024223076,-0.07580185,-0.047088858,-0.014002436,0.064110965,-0.03080436,0.06503385,0.014922956,-0.035978094,-0.13229948,0.016814582,-0.069375776,0.006195983,0.02167782,-0.08069927,0.019578923,-0.0005441292,0.061799448,-0.039369628,0.06387712,0.005919958,0.019317202,-0.031166278,-0.0048951553,-0.010252476,0.01444045,-0.006522211,0.026800556,-0.029062489,0.04512702,-0.007246894,-0.0016232195,0.041055184,0.011037994,-0.112928696,-0.068045944,0.06464421,0.041689724,-0.012772971,-0.04161422,0.015785318,-0.11395051,0.073634885,-0.0485898,0.0019069427,0.00014666367,-7.5057614e-33,-0.075340785,0.042914543,-0.0018514786,0.039431944,-0.04618501,0.07382266,-0.0168307,0.008875657,-0.006000098,0.031757884,0.023240305,-0.022005854,0.046832196,0.022234887,0.09530309,0.05397574,-0.013957526,0.00660879,-0.044668067,0.02997322,0.025469812,0.086922646,-0.0589419,-0.060498364,-0.06335261,-0.015502277,-0.077008754,0.066313006,-0.043332852,-0.06398821,-0.008201124,0.020252395,-0.059562936,-0.105391644,0.07108799,0.01331702,-0.013670247,0.02742385,-0.0024654812,0.004803049,0.023985922,0.046213627,0.010594895,0.034081105,0.0091629205,-0.065155566,0.07544223,-0.0056021456,0.0670874,0.090845935,-0.019927468,-0.047784373,-0.045847867,0.088478,-0.032672632,0.010974288,-0.08516642,-0.029174816,-0.08209788,-0.011830148,-0.059991322,0.029637046,0.044734683,0.09482171,-0.026727524,-0.047917906,0.062166378,0.031753335,0.010917811,-0.05416119,-0.010277861,-0.020321535,0.051047456,0.022427132,-0.099231176,-0.08498434,-0.029962175,-0.097614795,-0.074575365,-0.0037859757,0.045557395,-0.012158443,0.005153271,0.00844476,0.027038615,0.025903186,0.072922245,0.006638674,0.019842213,-0.046798743,-0.018560117,0.14377187,-0.013147024,0.00486214,-0.029154427,-5.9067133e-08,-0.031169394,0.07382436,-0.010313222,0.0032630204,0.008278906,0.018329121,0.04019278,0.04807923,-0.07468858,0.010796623,0.05653568,0.027751971,-0.026484838,0.015467345,0.005327036,-0.013414398,0.019794174,0.09735357,0.032209765,0.019362798,0.10811116,-0.014384684,-0.029771997,-0.068134025,0.08295514,-0.05052184,-0.08009823,0.076308325,0.00025401608,0.07987193,0.06378491,0.072795704,0.045820877,0.04462648,0.057839055,0.08168166,0.08053048,-0.028507145,-0.043925934,-0.0033532137,-0.0069094696,0.051703207,-0.05917946,0.028497204,0.060958814,-0.021102548,0.074298725,-0.16701193,0.050060816,-0.055629093,0.05969174,0.016151184,-0.009778551,-0.00058320956,0.069628954,0.018671138,-0.015536298,-0.015187088,-0.018714538,0.07865026,-0.031826288,-0.11059325,-0.041822333,-0.019537622,5,-0.09298359,-13.631729,9
380,"in today's lecture first we started by answering the question how to improve the quality of results? there are three ways to do so:
1-  to increase the quality  and size of sample|
2- improve the method like by using multiple methods and selecting the best one
3- fine tuning or properly using the methods
then we learnt that in linear regression outcome is expressed as linear combination of independent variables and there is nothing else linear in that.
when we have four features which are like x , x^2,x^3,x^4 and sin x , then
eventually your model will try to minimise coefficients of all the x1 to x4 and the coefficient of sin x will resemble the most that is will be the maximum so many of the features have to be eliminated otherwise the model will become unfavourable.
then we learn the basic difference between forward feature engineering and backward feature engineering. forward feature engineering is that we start with one feature and then keep adding until the performance becomes better and backward feature engineering is just the reverse of it that is we start with all the features and eliminate 1 by 1 based on the performance.
so naturally what we do is that we get the data we perform exploratory data analysis then we preprocesses and then we see multiple methods and then we compare them using the matrix and then select the best one.
after that we learnt about parametric like neural networks like which has the weights involved and if there is more than one layer in a neural network it is called as deep learning network.
in artificial neural network the data should be large otherwise overfitting will happen.
if we have to compare just the model within itself then we compare mean squared error with the value of y bar to get percentage and check whether the model is good within itself or not.
can we started with classification which is supervised learning in which why denotes the class and we have x and y both are available it is used for distinguishing between the discrete values.
then we learnt about regression and in which we learnt about the basic of logistic regression in which outcome is a classifier and our main purpose is to draw the boundaries between different types of classes.",0.047575187,0.03229978,0.035453558,0.04790841,0.03697353,0.01652373,-0.08601923,-0.008221718,-0.027642924,-0.019242618,-0.003018052,0.05382371,0.03882661,-0.025323821,-0.016378874,0.026785618,0.04945644,0.08576254,-0.10383178,-0.05232254,0.057728402,-0.06464302,-0.03956412,-0.008284468,-0.037823115,0.018278724,-0.035479855,-0.06114651,0.04588802,-0.047067776,-0.015253109,0.065187804,0.014436863,-0.05793846,-0.088179134,0.03987586,-0.031113332,0.014471394,-0.040914975,0.022032006,-0.007403172,-0.0025135358,-0.012989609,-0.017636469,0.087733164,-0.01746915,0.0470107,-0.05454036,-0.0007332035,-0.07599379,-0.007027584,0.07461038,-0.08056872,-0.041205756,-0.0059728227,-0.0766536,-0.04698651,0.03449726,0.0099849915,0.04222423,0.069662295,-0.010422188,-0.094521135,-0.011229534,0.04556973,-0.12505108,0.062047947,-0.032849327,-0.014773223,0.09019524,-0.011745634,0.043037202,-0.0421942,-0.010215107,-0.0035484468,0.04334664,0.03416515,0.07207231,0.035053767,-0.0034964313,0.024358472,0.02899729,0.03674275,-0.02084805,0.02717984,0.0208069,-0.05515933,-0.06350682,-0.06863111,0.07659381,0.04382022,0.0005321715,-0.078944676,0.0021514099,0.09062881,0.02978645,0.031632807,-0.09016398,0.09832559,0.066889495,0.025312249,0.048491348,0.07146003,-0.026475765,0.038912553,-0.0072515667,0.061854955,0.054768067,0.010920832,-0.0073497165,-0.009756103,0.018733347,-0.03273459,-0.028814556,0.09631698,-0.011638096,-0.021777024,0.034776676,0.042663373,-0.008779942,0.007528082,-0.010305458,0.0663289,0.046158727,0.07138243,-0.0032354647,-0.05448674,8.183717e-33,0.02368062,0.042991742,-0.04146549,-0.016678104,-0.06700636,-0.026582612,0.021780029,0.05363368,0.0684649,0.10604059,-0.00866219,-0.025357988,0.0075852,0.0708602,0.05379868,-0.06266623,-0.04586372,0.048187833,-0.014245276,0.010768978,0.043475408,-0.0871909,0.077771455,-0.06612676,-0.004276886,0.020870207,0.027606864,-0.009367203,-0.1716821,0.020123273,0.0005021158,0.043610204,-0.048058346,0.02957098,-0.038870096,-0.022558603,-0.007532447,-0.08757992,0.060973484,0.052310612,-0.0012933766,0.014708585,-0.027862787,-0.020093285,0.042604353,0.07733035,0.063550524,-0.06080647,0.003859434,0.014703532,0.006494322,-0.009911757,0.014627782,-0.020938532,-0.06010356,0.010726374,-0.032506663,-0.11893766,-0.04877958,0.061569404,-0.09874361,-0.020462262,-0.05072079,-0.03194855,-0.009094555,-0.006044536,0.07767517,-0.036591947,-0.0032190576,-0.07539886,-0.06783429,-0.06674238,-0.01122565,-0.09215131,0.08245034,-0.04341451,0.020047385,-0.000105850326,0.04167085,-0.048537582,-0.09327829,0.10204726,-0.014430371,-0.09134133,0.057176583,0.05161679,0.051759757,-0.07961109,0.0780806,-0.021872045,-0.07936929,0.05801848,0.037218697,0.08979411,0.062245935,-8.591852e-33,-0.036523633,0.052492697,-0.038305674,0.00072047516,-0.07317311,0.0029451207,-0.038201652,-0.014264959,0.0048807478,-0.10421189,-0.0035528827,-0.061033238,0.035102777,-0.050241362,-0.021163888,0.059201993,0.005975362,-0.106725685,-0.031807907,0.03166813,-0.02543051,0.091418974,-0.07049747,-0.010742511,-0.053661942,-0.046319358,-0.034873575,-0.008781313,0.055777796,-0.023052717,-0.0143843815,-0.0046477,-0.038255233,0.04040483,-0.031661116,0.023394143,-0.058922682,-0.0038710206,0.061396442,0.04923091,0.108355574,0.024613373,-0.03031092,-0.035107117,0.011117448,-0.044801492,0.004469964,0.0127598345,0.040035017,0.01817088,0.111615874,-0.010096121,-0.013403256,0.011195982,-0.040846806,-0.06140153,0.044274323,-0.036550082,-2.774137e-05,0.051224995,-0.071960576,-0.038671993,0.04115928,0.06875704,0.0036399402,0.015679535,-0.0066493205,0.034475718,-0.018226096,0.019261163,-0.1544475,-0.06742426,-0.019787887,-0.01941967,0.0016354945,-0.038675647,-0.038270388,-0.121062696,-0.109541595,0.018218251,0.034544617,0.045114562,-0.03353669,-0.0063519925,-0.03006823,0.026055608,0.035325766,0.053046957,0.0076450286,-0.012317473,-0.025837487,-0.0005405447,-0.013203287,0.010277021,0.07613649,-6.8655986e-08,-0.089983255,0.011741838,0.023067543,0.008376916,-0.05087038,-0.024914484,-0.03305308,0.17619176,-0.034317862,-0.06271683,-0.013736392,0.047217015,-0.018016191,0.01783301,0.10901472,0.06449344,-0.07520391,0.028736588,-0.00919033,-0.076815456,0.05557025,-0.032304958,0.0036573769,-0.08431527,0.05990058,-0.032686986,0.056322172,0.012069176,0.016384728,0.07113525,-0.0057655447,0.03661074,-0.009674784,0.054165192,0.09002656,0.01668966,0.082963414,-0.07881255,0.0047239936,0.011075256,-0.058254104,0.15698929,-0.028072415,-0.0009067757,-0.025401859,-0.05524619,-0.0007245221,-0.036601458,-0.006598028,-0.011088178,0.04027059,0.029722119,-0.02607275,0.015879117,0.024756823,0.047566146,-0.04064325,0.026782298,-0.008655671,0.042076536,-0.058112446,0.011417091,0.025292046,-0.0804754,5,-2.010544,-9.068815,9
395,"we learnt as per the problem handed to us we should use different models, we extended our knowledge to polynomial regression this time. then we studied feature selection in two ways: forward and backward . the latter we did in last class by eliminating the variables according to p-values as they are not important. non parametric models like random forest, k-nn. got introduced to the world of neural networks and deep learning. after spending some time on regression for few weeks, we started classification problem. we dealt with function of sigmoid in the end.",-0.043970622,-0.024138385,0.04785541,0.047646824,0.07013847,0.04010057,-0.021602998,-0.041494068,-0.023444064,0.05651921,0.005434063,0.02437272,-0.061485447,0.02522897,0.00047279111,0.057912607,0.00051714457,-0.007502999,0.022029383,-0.015249802,-0.031246435,-0.010020012,-0.0118285585,0.011551869,0.040374305,0.004565892,0.061015118,0.049631413,-0.043223288,-0.0007204362,-0.06724049,0.059019424,0.0008320912,-0.0048783803,-0.0871061,0.055735197,-0.07422473,0.025566675,-0.018546551,0.013148871,0.051320545,-0.11131838,-0.06756253,0.0013312917,0.12926394,0.029689126,-0.046068795,0.006498598,-0.01810668,-0.040244937,-0.027071772,-0.032542493,-0.08580255,0.03985375,-0.041294288,-0.099307306,0.029632742,0.045064215,0.00236523,-0.024711097,0.05997817,-0.045019582,-0.052212454,0.030142145,0.0610829,0.055435304,0.01391623,-0.028434569,0.057586074,0.05349625,0.049527004,0.116786495,-0.06622064,-0.008155465,0.009062413,-0.046698596,0.105809696,0.008261452,-0.025324818,-0.032754768,0.02944842,0.05552973,-0.0410999,0.020884162,0.08701352,0.019637212,-0.08276387,0.023765702,-0.082996264,0.0103322035,0.0036057497,-0.015786448,-0.0020018418,-0.035561074,-0.03437892,0.010115066,0.023007177,-0.10042256,0.00055602525,0.06218217,-0.04710954,0.014927717,0.06947469,0.032749746,0.039126515,0.043825567,0.022758342,-0.071289904,0.04018811,-0.08332504,-0.040078264,-0.00096775993,-0.05537499,-0.032495454,0.036153466,0.00024374668,0.0659434,0.060786273,-0.04574362,0.044384066,-0.09074436,0.015150215,-0.034950133,0.026551375,0.016984936,0.020400675,-0.061922606,3.99448e-33,0.031767108,-0.07731164,0.0031540324,-0.12323147,-0.0014679602,0.045098796,0.06802929,0.019644717,0.053893458,0.0128430715,-0.043091938,-0.04643658,0.014257607,0.047389947,0.085696384,-0.018380424,-0.020325415,0.0903483,0.01143291,0.028358728,0.020776477,-0.07725772,0.0021295443,-0.036700036,-0.08090841,0.0777266,0.024865896,-0.035805617,-0.048074223,0.009187881,0.017499633,0.04355678,-0.0039491626,0.026111612,-0.016042802,0.01350393,-0.03301703,-0.008250952,0.079862155,-0.031736944,0.006133369,0.008062779,0.058944006,-0.029563028,-0.00996745,-0.01821313,0.055122044,-0.071474485,-0.05322611,-0.014492241,0.00096621737,-0.033448752,-0.09774966,-0.063445084,-0.039455026,-0.012511173,-0.041444898,-0.0011542113,-0.09789673,0.034812387,0.032814994,-0.017359572,-0.012741059,-0.0653302,-0.022564491,-0.016958516,-0.010130294,-0.021116493,0.020875368,-0.05513549,0.01031793,-0.017823309,-0.014924077,-0.02117108,0.07722197,0.06128947,0.057147287,-0.035576627,0.029144684,-0.066267215,-0.059520137,-0.00068753056,-0.025662772,-0.07056166,0.027783196,0.050653324,0.08312622,-0.08613551,0.018493436,-0.007263111,-0.11482469,0.09375664,-0.029845567,0.09702697,0.057116657,-5.0954848e-33,-0.08253811,0.06499468,-0.029080968,0.012325792,-0.07935826,0.06002344,-0.06870526,-0.0029247839,-0.049054228,-0.005841417,0.052060552,-0.019707212,0.059450254,-0.040666215,0.020630328,0.044174,-0.04307752,-0.07167929,0.0050089415,0.036933724,-0.0172355,0.08681975,-0.078921795,0.024223518,-0.13471374,-0.01576311,-0.043314826,0.022204144,0.00078474166,0.008629164,-0.07558277,0.023100238,-0.07302273,-0.009549853,0.04452484,0.036175862,-0.08043934,-0.00761354,0.03486995,0.07146903,0.03332888,0.037080918,-0.022957237,0.022664111,0.020394862,-0.114635214,-0.0023441461,0.010485068,0.09644703,0.037506502,0.038960148,0.0149930585,-0.040245894,-0.033405576,-0.10049973,0.032994114,-0.015224234,-0.032917824,0.051314894,0.042478602,-0.020156594,-0.019662622,0.10754139,0.060787745,-0.01685492,-0.0070272004,0.043971524,0.108249605,-0.0051729237,-0.011508599,0.026243556,0.010496897,0.022982571,0.019661088,-0.0971744,-0.114208385,-0.06633244,0.027499784,-0.0169255,0.038001645,0.074343875,0.034471907,-0.012809253,0.031124251,-0.018329298,0.017893108,0.100904725,-0.0051955804,0.050407905,-0.08052635,-0.01910591,0.06648075,-0.025729988,0.035366446,0.045923416,-5.276835e-08,-0.021166379,-0.017257087,0.058213837,0.033485763,0.058516484,-0.040017024,-0.021067666,0.16002801,-0.055372577,0.04415732,-0.033788487,0.07202873,-0.030233864,0.05852716,0.06617587,0.043709844,0.10873382,0.035147216,-0.047567256,-0.100667655,0.03906399,-0.055032156,-0.042076256,-0.03567664,0.082576506,-0.09523158,-0.035166815,0.015860168,0.04081965,0.09218169,-0.038940027,0.039599508,-0.0427097,-0.041651156,0.068888046,0.12603742,0.05894376,-0.07741193,-0.071405336,0.035919193,-0.05308554,0.0010393427,-0.01585379,-0.010283619,-0.06362182,-0.0048218677,0.07382654,-0.124311365,0.005474284,0.03976269,0.0878775,-0.009511482,-0.013792289,-0.018613864,0.029656185,0.08249886,-0.012505843,-0.04311907,-0.03882924,-0.012898242,-0.0018834531,-0.035644457,-0.0029469447,0.005001137,5,0.9869993,-9.89726,9
413,"first, we looked at all the summaries. we had a pattern where the number of people submitting summaries decreased as sessions went on while the average words per summary were increasing.

the second case that we looked into was when the function was looking more like a sine curve. in this case, we only used one feature, x1 but used more polynomial features like x1^2,x1^3, x1^4, using polynomial regression to get a good fit. this model gave us a p-value. to further improve the model, we added a new feature, sin x1. in this second case, the resulting p-value was lower than in the first case. this means that the sine feature was statistically significant and improved the model's performance. however, adding too many features beyond a certain point does not necessarily improve the estimation and can lead to a decline in adjusted râ² values. in general, if we can fit the data using a single model, it should be preferred over the fit using multiple models. we then discussed parametric methods, including neural networks.

in these models, an input layer takes in features, which are passed through computational layers. when computations happen across more than one layer, it is called deep learning. the input layer is connected to computational layers in a way representing the degrees of freedom. while increasing these degrees of freedom may improve prediction it also means that there is an increased chance of overfitting, which is the model overly fits the training data and, therefore underfits new data.",-0.009536387,-0.0027597235,0.014132554,0.058482092,0.060870856,0.07329118,-0.022731028,0.07722766,0.10306476,-0.049434416,-0.017557474,0.03907285,0.025730725,0.037722003,0.03008072,-0.0007464019,0.036603235,0.045272365,-0.06681469,-0.081997596,0.08613744,-0.023334363,0.009286581,-0.01385626,0.015835885,0.04141183,-0.0402753,0.0063913343,-0.01455542,-0.01841579,-0.0118241245,0.09964607,0.041239507,0.0022409318,-0.069805905,-0.032049358,-0.12352976,0.091301605,-0.020800164,-0.01818485,0.0407412,-0.06914515,0.02861487,-0.022987364,0.08279249,-0.01906298,-0.009426099,-0.012087672,0.011310421,-0.063794725,0.025241075,0.044065993,-0.058538314,0.009617491,-0.032839395,-0.04157607,-0.028270954,-0.0049215704,0.01986531,-0.060790177,0.004673418,-0.0036908318,-0.024661373,0.03111888,0.06347892,-0.023913959,0.05329544,-0.016712794,-0.033407856,0.11240177,0.021102028,0.121890925,-0.089831896,-0.020332772,-0.020119147,-0.040051147,0.008172712,0.05305987,0.027236316,-0.046947148,0.03337189,-0.0385725,0.0045224833,-0.056224916,0.060552143,0.031147027,-0.027049141,-0.043191332,-0.0646985,0.035058863,0.030895198,0.0038058627,-0.019156467,-0.0056381244,0.0033808805,0.02007281,0.0112551935,-0.03928513,0.010487879,0.110118136,-0.0045165266,0.07504284,0.0310064,-0.08381102,0.055396296,-0.008003442,0.023174291,0.0011999432,-0.005169659,-0.00520295,-0.038458064,0.066511475,-0.065142915,-0.011143876,0.08785942,0.031735674,-0.0037723968,0.05050831,0.015643196,0.070709765,-0.03066965,-0.007040301,0.0058106896,0.085091665,-0.009150603,-0.015499935,-0.1034259,7.05238e-33,0.006907928,0.070603795,-0.045541573,-0.0218927,0.023906045,-0.0026483787,-0.028722653,0.056418013,0.057053,0.04296079,-0.08062467,0.01523052,0.023182316,0.012020573,0.06496555,-0.016720718,-0.018187732,0.059743777,-0.06090047,0.027047535,0.050656613,-0.13966894,0.07827143,0.0022109663,-0.0945892,0.052130647,0.0079332255,0.032117583,-0.08947221,-0.008984648,-0.02298969,0.003031868,0.011569629,-0.010622539,0.0171125,-0.0520765,0.021244759,-0.09153639,0.032025,0.001084564,-0.072690345,0.026478536,0.04806115,-0.0635106,-0.033298995,0.039340127,-0.007941516,-0.026576307,-0.06583464,0.09315237,-0.026819538,0.020535825,0.032362252,0.0548178,-0.06873665,-0.020214643,-0.0018662838,-0.08908432,-0.09039629,0.011991343,0.04917586,-0.017974162,-0.017418103,-0.08020765,0.020680921,0.07620457,-0.01984667,0.019449187,0.08107037,-0.00021057602,0.01724594,-0.022222469,-0.02592825,-0.052818526,0.085115775,-0.037253108,0.032156225,0.0049893125,0.0018830599,-0.0023624552,-0.02979898,0.0077163037,0.078409016,-0.09494094,-0.0061902604,-0.011715934,0.077904806,-0.06876066,0.000815996,-0.01726541,-0.045387533,0.07525914,0.02774287,0.05245803,0.023233658,-5.7314438e-33,-0.13301232,0.087892205,-0.016014751,0.027364267,-0.019601565,0.040737957,0.016665557,0.011139377,0.0109860785,-0.07039981,0.020139221,-0.048316322,0.07955959,-0.068207525,0.08201854,-0.0037360261,0.0038920601,-0.07720858,0.08498471,0.06088114,-0.034278933,0.09595294,-0.053428676,0.0033225808,-0.06598034,0.001206817,-0.074595004,-0.09392414,0.039078392,-0.0071075493,-0.07673077,0.077167116,-0.0869093,0.00488468,-0.038191203,0.0028095657,-0.024073355,0.0027489117,-0.002328435,0.097837575,0.085537784,0.023007108,-0.025782537,0.02277569,-0.042460855,-0.009904958,-0.017799187,-0.0034977035,0.050518364,0.08307908,-0.01924513,-0.023147855,-0.079753,0.027591394,-0.0868049,-0.048885144,-0.0711066,-0.050512686,-0.014957862,-0.03368889,-0.100345425,-0.03776726,-0.025179384,0.033816967,0.0412705,0.028427668,0.027275177,-0.0056063663,-0.018459227,0.030693797,-0.083764315,-0.062215704,0.043729626,0.020768436,-0.042676352,-0.05175707,-0.056829117,-0.061914805,-0.11330572,-0.010283831,-0.022614108,0.04282486,-0.05446165,-0.048563294,-0.051787835,0.02024135,0.0076436126,0.053746656,-0.038203564,0.010938322,-0.0030849974,0.08974957,-0.058618266,0.04563368,0.007694604,-6.4448095e-08,-0.09556833,0.04352248,-0.07065031,0.04517819,0.067143455,-0.09096825,0.017137608,0.083307266,-0.03883878,0.0058886916,-0.018945396,0.018327506,0.01924792,-0.0109800175,0.022913743,0.120533936,-0.07483355,0.044272374,-0.018211521,-0.093577184,0.13788185,0.01741555,-0.0653053,-0.067473814,0.06567683,0.005911516,-0.04142236,0.07791589,-0.014630009,0.0088311415,0.056706857,0.005679793,-0.027245494,0.04508638,0.052582834,0.06656531,0.056053955,-0.045073994,0.041440528,0.074200496,-0.034588836,0.10285891,0.009151902,0.055785272,0.0911115,-0.01925718,0.016805539,-0.08015207,0.038582277,-0.033979353,0.039244186,0.08780369,-0.059414394,-0.052840367,0.059962176,0.04729564,0.0014792208,0.021789307,0.003914964,0.011597048,-0.051583335,-0.08265875,-0.036858175,-0.062268667,5,2.6105134,-5.109686,9
417,"multiple linear regression: 
linear regression = outcome is expressed as linear combination of independent variables. 
taylor series ==> apply to mlr to capture the nonlinearity in the datset
this is just polynomial regression. the original was nonlinear so we engineer more features and then try to fit the model, the feature that fits best (like the sinusoid the in class example) will have more weight. so p value decreases and adjacent r2 value will decreases if you keep adding lot of feature more than required. 

forward feature engineering: put all possibl features and eliminate them one by one
backward feature engineering: start minimal set of feature and keep adding 1 by 1 based on your domain knowledge

usual methodology:
get data ==> preprocess & eda ==> good data ==> multiple ml methods ==> compare metrics ==> select best (or a combination of multiple methods with a weighted mean)

for a given data (the one in class): is it better to fit 1 model and 2? i think 1 because it would take into account for the variation in unknown data to be tested or applied later on. but 2 models might overfit.
then we got an overview of non parameter models like knn and random forest.

moving on to neural nets: input features ==> multiple layers ==> weighted average to get output 
we get a lot of tunable params which help us fit the model better to the data",-0.038397364,-0.04104693,0.036486205,0.010131522,0.023494963,-0.006868344,-0.09892441,0.0013185729,-0.0198973,-0.021550162,-0.029580023,-0.03308965,-0.004720081,0.009881517,0.04502521,0.03216361,-0.0070530353,0.0692855,-0.054653328,-0.032614537,0.08998996,-0.0341857,-0.06513069,0.055274047,-0.042901974,0.09914735,0.0015573271,-0.0019037464,-0.037662845,-0.020109007,0.037830714,0.023961969,-0.07335164,-0.008583979,-0.059968118,-0.012580076,-0.07664013,0.087338105,-0.0379727,0.0069918437,-0.040109143,-0.019174378,0.047177795,-0.06308053,0.07405897,-0.042761337,-0.032991827,-0.05198895,0.024877246,-0.07648964,0.02320947,-0.009630382,-0.05350517,-0.034189742,-0.04197177,-0.06533745,0.016837599,0.021445556,0.017486012,-0.0008436178,0.038748056,0.02349544,-0.026103234,0.034533292,0.008781644,0.0028158834,-0.024793703,0.04434752,-0.015820479,0.08099371,-0.020015737,0.014956462,-0.03526821,0.0044418727,-0.009600138,-0.0145597365,0.037549462,0.06560785,-0.0093674315,-0.0063890074,0.024193104,0.0599581,-0.042912893,-0.033571158,0.057285465,-0.040615723,-0.024454271,-0.043319702,-0.050423104,0.07574866,0.07798978,-0.0008288871,-0.0056290086,0.063796304,0.05267498,0.055458203,0.004646145,-0.089254566,0.073701546,0.054787934,-0.027883723,0.022814121,0.06927185,-0.010420762,0.0110099735,-0.004974663,0.009314201,-0.03519578,0.06391275,-0.018556828,0.018292904,0.013566212,-0.08398207,-0.028753323,0.10075652,0.03533626,-0.0014446735,0.038868934,-0.03232873,0.03170868,-0.062238444,-0.051014602,0.094548345,-0.0071850717,0.09080795,-0.01945789,-0.14705093,1.0731311e-32,-0.051444072,0.006112201,-0.076190524,-0.046214957,-0.014244264,0.009601957,0.03290414,0.08969988,0.09297051,0.019535514,0.014238752,-0.0070359115,-0.039057404,0.017591408,0.055420414,-0.04155699,0.025981747,0.052846845,0.048605897,0.045851123,-0.012976255,-0.094226494,0.06303931,-0.029052421,0.025408015,0.011038853,0.016829692,-0.004399462,-0.11990256,-0.005977237,0.08433959,0.0088690985,-0.059020337,0.00091294147,-0.022751626,-0.06709691,-0.019230489,-0.046132907,0.04372625,-0.010058226,-0.0044151037,0.0016569496,-0.01632441,0.03854431,0.05931929,0.06164473,0.018292617,-0.04771841,-0.087944806,0.04922083,-0.026419269,-0.073783755,-0.056611545,-0.013970025,-0.10834935,-0.0552809,-0.08848038,-0.072633155,-0.011095928,0.018203085,-0.053812474,-0.04302464,0.03179243,-0.10321364,0.02697,-0.029715126,0.02396501,-0.054826897,0.042274296,-0.01908098,0.0012157415,-0.086358815,-0.008069157,-0.009275753,0.08768901,-0.008385047,0.07611837,0.013538089,0.022655662,0.0114252595,-0.07732165,0.107269816,0.024866747,-0.032979902,0.038052734,0.046048388,0.018685948,-0.06717167,-0.032162294,0.028276216,-0.09288397,0.08672374,-0.04163418,0.0757206,0.0545633,-1.0036583e-32,0.015962634,0.09871516,0.05080212,-0.027486429,0.007855369,0.041777544,-0.045449357,-0.019571157,0.061394338,-0.07425447,0.014300073,-0.06484822,0.08134979,-0.022902606,0.033286642,0.028702226,0.014456356,-0.040099926,0.049310584,0.09898318,-0.018820712,0.06584403,-0.089486554,0.026781227,-0.07762998,-0.009924319,-0.09318783,-0.041141782,-0.016186683,-0.0028270914,-0.030480605,0.027293233,0.05832377,-0.012272438,-0.035288416,0.066896625,-0.015482818,-0.028756913,0.016656399,0.10984642,0.10348907,0.05415602,0.028007679,-0.010597729,-0.047495473,-0.065533206,0.0074666063,0.033838637,0.08445439,0.046033185,0.043196898,-0.016719494,-0.035371635,0.0028107925,-0.004136576,-0.075060636,0.017055947,-0.035993997,-0.017578663,-0.006581535,-0.04617461,-0.03553993,0.030782912,0.09526616,0.019698512,0.026922323,0.071656935,0.035962515,-0.10592221,0.047366016,-0.12044236,-0.029290585,0.076620236,-0.02806338,-0.0048990487,-0.09149375,-0.043289933,-0.06782436,-0.04503041,0.043643083,0.019470537,0.013374207,-0.057663314,-0.009368384,-0.039898787,0.026741829,0.06258152,-0.04526087,0.017729027,-0.061089322,-0.014531697,0.05863108,-0.030059054,0.022938548,0.092961475,-7.1884585e-08,-0.0207007,-0.083959304,0.035892293,0.017975677,0.01547003,-0.049470577,-0.06492925,0.11530104,-0.04784298,-0.012546819,0.0015288736,0.03932234,0.0560057,0.057970986,0.06725299,0.042237908,-0.056609184,0.04253945,-0.039482072,-0.047642887,0.07688346,-0.09620811,0.013437325,-0.053270474,0.09032409,-0.09442392,0.023694692,-0.030471606,0.016431516,0.062844686,0.029777098,0.020419164,-0.009370189,0.020821925,0.107616656,0.017413387,0.04704628,-0.03700572,-0.037353802,0.102644905,-0.05033535,0.11555395,-0.06376741,0.004046109,0.096707195,-0.031909507,0.0054404656,-0.088451274,-0.008854719,0.016372371,0.071413614,0.034660004,-0.015074774,0.03178405,-0.021406159,0.054672208,-0.06315636,0.10007894,-0.023845866,0.0017672407,-0.057958525,-0.11390674,-0.01292444,-0.031030076,5,-1.3514335,-6.584532,9
428,"it was discussed that how to improve the results: 
by increasing the quality of the sample 
or by increasing the size .
to improve the method we use multiple methods and then select the best one .
fine tune and properly use that method.
linear regression doesn't mean to fit a line but rather that the output is represented in a form of linear combination of independent variables.
in real life we get the data then preprocess it , from which we get good data then we apply different ml techniques(i.e form matrices ) , compare them and then select the best one.
lr and similar techniques are parametric methods.
then neural networks were discussed briefly , how different paths are given different weights.
after that we moved to nominal and ordinal types of data in the function y=f(x). where we classify the data into different categories. 
logistic regression was talked about , how our function focuses on giving us the boundaries between different classes.
",-0.04070433,-0.076321974,0.011966925,-0.025796743,-9.055586e-05,-0.025668213,-0.08395862,0.039919104,-0.036918525,0.0094487965,-0.00647565,0.11730256,-0.014632827,0.027285926,0.0021817272,0.027436191,0.0045952895,0.050659064,-0.1171623,0.0027315253,0.0694756,-0.0157603,-0.03654358,0.023873433,0.01701025,-0.05771105,-0.038213886,0.02516497,-0.06749259,-0.044371385,-0.008842002,0.06108809,-0.0073399963,0.02589023,-0.14008139,-0.0210621,-0.030576564,0.06853969,0.04973363,0.0069049587,-0.06716513,-0.06442193,0.052121382,-0.03637083,0.13958451,0.010167592,-0.012775908,-0.06042402,-0.029354844,0.029779147,-0.08722882,0.06573502,-0.0170536,-0.0059048524,-0.0519602,-0.030622846,0.004321627,0.019928299,-0.008217301,0.010768501,-0.041494142,0.0003104221,-0.028804705,0.02181125,0.011529491,-0.05519987,-0.035556294,0.032015465,0.051924787,0.05666194,0.031920414,0.028050818,-0.035988256,-0.0083564045,0.05456385,-0.0007218426,0.067720786,0.05997145,-0.027052395,-0.0059045167,0.03754067,0.07652079,-0.0029340268,0.04985067,0.07445842,-0.021055462,-0.016862297,-0.051137723,-0.057068016,0.020018088,0.039362285,0.067624755,-0.039016504,0.016673189,-0.02896045,0.0062564146,-0.014922262,-0.069554724,0.09606223,0.036854524,0.0022436378,0.052544866,0.0364415,-0.040848367,0.062128358,-0.019868389,0.1228679,0.058058474,0.049469937,-0.068196855,-0.0052306917,0.042335168,-0.12233671,-0.0048832432,0.079277925,-0.048482418,0.004888921,0.02546619,-0.0027677247,0.08895416,-0.024080794,0.003429764,0.035043873,-0.020995293,0.06906387,0.07327179,-0.08685354,4.8692536e-33,-0.068885036,0.0029019425,0.026714591,-0.037112538,0.011035139,-0.03391463,-0.043122727,-0.011066993,0.08194334,0.066935815,-0.05811395,-0.015548311,0.029939422,0.12315498,0.0983065,-0.02216072,-0.05135831,0.006813257,-0.07739269,0.027648237,-0.04021932,-0.041700102,0.1314528,-0.115326434,0.010226961,0.0047647497,0.052620213,0.03142561,-0.105815485,-0.008163471,0.03498607,-0.015673053,-0.02459827,0.04390433,0.03756853,0.031699028,0.0073224404,-0.008970544,0.029247453,0.023507282,-0.05333514,0.05667595,0.034804504,0.02366646,0.029115856,0.09625864,-0.039525326,0.007586856,-0.060638368,0.0008706213,-0.017636051,-0.0536349,0.005925071,-0.015815109,0.014088451,0.0036415318,-0.040789537,-0.030197559,-0.04015075,0.047757715,-0.0023045393,-0.028865423,0.02866176,-0.009986072,0.038206276,-0.028809605,0.066604905,-0.04811605,0.06144966,-0.047455434,-0.0035268299,-0.009622085,0.038225856,-0.036778428,0.066527486,0.010142594,-0.0276801,-0.03925037,-0.009507408,-0.032696944,0.0069642314,0.10391684,0.005445796,-0.08114281,-0.022575479,0.03072642,-0.010420993,-0.052783202,0.06016946,0.0063589015,-0.1420517,0.062315933,-0.011534141,0.0010872486,0.03707482,-5.5714847e-33,-0.013910883,0.0890352,0.047397684,0.055478126,-0.06484896,-0.007208526,-0.08013692,-0.07283973,0.07498532,-0.008992784,0.024264665,-0.005392961,0.06454854,0.026050974,0.061350834,0.020127812,-0.07362365,-0.031212404,-0.011814065,-0.022090204,0.041135736,0.079779305,-0.050483525,-0.045419954,-0.056566954,-0.040107027,-0.07379241,0.052762203,-0.0076970053,-0.06443057,-0.012616455,-0.045831267,-0.017399345,-0.054748736,0.06790708,0.09112942,0.010090785,-0.0023060765,0.021525724,0.013869168,-0.020811612,0.0068679485,0.033258475,-0.18363628,-0.037817076,-0.04534283,-0.013763546,0.018866405,0.0042569158,0.07547874,0.028903132,-0.01557448,-0.07991021,0.006178525,0.010589668,-0.0050986223,-0.05063996,-0.07350926,-0.015084963,-0.016820323,-0.04638426,-0.015971925,-0.023528043,0.06834658,-0.06372703,-0.08334702,0.0024889954,0.016566502,0.002789133,-0.0022294815,-0.13617525,-0.058131356,0.043501057,0.09178428,-0.07310008,-0.03928322,-0.005560111,-0.068453595,-0.052948434,0.05437181,0.062664345,-0.062289484,-0.010578439,0.024963846,-0.0027827495,0.08309587,0.06632715,-0.0011224354,0.038848963,-0.06553773,-0.001927848,0.031488188,-0.027258722,-0.00066437124,-0.06284564,-6.4865475e-08,-0.028859206,-0.049192034,0.07173833,0.058837716,0.03206807,0.028741013,-0.03662434,0.16398905,-0.059688378,0.0022211128,-0.021293385,0.12540774,-0.06553288,-0.01824486,0.0677643,-0.08113625,0.0048018233,0.018814268,-0.028471576,-0.01004961,0.04504325,-0.0035518275,-0.041405626,-0.045902602,0.116639264,-0.06330193,-0.008900069,0.044574555,-0.06753613,0.04024165,0.035372708,0.082397334,0.014063354,-0.0011339228,0.09327342,0.04505738,0.05936262,-0.038186274,-0.06851403,0.003095757,-0.022941623,0.06748727,-0.0048846137,-0.010825772,0.06967357,-0.06698083,0.044263184,-0.065634534,-0.028245194,0.0041406294,0.011760375,0.025650434,-0.021850595,-0.025021419,0.07795154,0.030467974,-0.01390306,-0.058319103,-0.014697836,0.08528465,-0.029905286,-0.00091152306,0.049378276,-0.032591823,5,-2.2168953,-13.715608,9
439,"as the sessions went forward the number of submissions reduced but the qualtity increased.
we analysed a function using a sin wave and it reduced the p value hence it was deemed usefull. including too many features increased the p value. 
we then studied neural networks where the features pass through multiple layers ",-0.015655296,-0.017558599,0.023499351,0.018576704,-0.0005990879,0.09700283,0.026519712,-0.047371287,0.02428724,-0.06310489,-0.031015247,0.03587027,-0.040049255,0.04296514,0.016751524,-0.027765406,-0.015416574,0.04782628,-0.032675296,-0.05624104,0.041612353,-0.009368533,-0.024978042,-0.03264101,0.0033954005,0.021589147,-0.006839451,-0.03112946,0.07491637,-0.025752967,0.031033376,0.087991,-0.02781258,0.0032487942,-0.07748471,0.014525117,-0.1295562,0.041741706,0.008939773,-0.044662885,0.00029424168,0.0038249907,-0.027615864,0.0126180565,0.10539781,0.054019965,-0.034401312,0.007212315,-0.031919155,-0.03643543,0.023392446,0.044861484,-0.027488418,0.035526026,-0.033228584,-0.044674803,-0.027459012,-0.0061136163,-0.059663314,0.029728426,-0.017887969,0.003725316,-0.036569465,0.00557157,0.11080577,0.05066599,-0.023310963,0.014374765,0.024517799,0.04918125,0.049251232,0.040474404,-0.09694325,-0.019828122,0.0377438,-0.038091153,-0.033107553,-0.011535642,-0.01704073,-0.028567359,-0.009312584,-0.04535428,-0.0453838,-0.07851635,0.08769012,-0.017642254,-0.07624597,-0.016910316,-0.0772712,0.024931643,-0.018610828,0.065119125,0.020661758,-0.021126676,-0.009889411,-0.022059726,-0.005773874,0.01792386,-0.009487578,0.09575559,-0.02745462,0.03233496,0.014864721,-0.034428008,0.037751425,0.014733298,0.02294395,-0.0415095,0.059413716,-0.10737501,-0.076987274,0.08354622,-0.027599854,-0.05645214,0.09264386,-0.0003679316,-0.0016437139,0.030255433,0.002979019,0.109098956,-0.05031971,-0.0077749738,-0.04500142,0.069567725,-0.02605542,-0.013073084,-0.07447909,1.870094e-33,0.01772944,-0.015469253,-0.05118537,-0.041696005,0.05072148,0.063589714,0.033767417,-0.028313447,0.06920725,-0.024414906,-0.17737035,0.04578111,0.0048273117,0.07666541,0.019263972,-0.03795485,-0.0002220929,0.059656885,-0.013985301,-0.09090513,0.02331655,-0.10360327,0.05944356,0.014954206,-0.0359765,0.0206849,0.01341989,0.01917055,-0.08646962,0.022391062,0.018105144,0.05034963,-0.054950546,0.03420353,0.042084444,0.011530423,0.04585342,0.0030361156,0.08593126,0.0218803,-0.104441576,-0.049551595,-0.040407732,0.023836048,-0.08737632,-0.017269643,-0.025143683,-0.0044950284,-0.049852405,-0.013532143,-0.011502349,0.008584375,-0.0005102024,0.009448887,0.002934824,-0.016397716,0.0041436963,-0.0022730706,-0.08935683,0.069089144,0.043607377,0.018538106,-0.069137245,-0.053240255,-0.04052759,0.06809193,-0.023325998,0.014305482,0.08672963,-0.013356656,-0.038595438,0.068420276,-0.009912568,-0.13078964,0.033220254,-0.010695086,-0.031327546,0.012121911,0.0041366476,0.08299269,-0.010647,-0.013357181,0.024945574,-0.027589342,0.021150267,0.020829404,0.10043292,-0.10613811,-0.0330054,0.003644083,-0.109868914,0.035496905,0.109403215,0.012913235,0.05626944,-1.7663542e-33,-0.13965198,0.106353246,-0.10383235,0.062035345,-0.021980882,0.025563138,0.011651492,-0.019558733,-0.045250263,0.059995238,0.0428737,-0.05691739,0.03945189,-0.06878009,0.006221701,0.0036062873,-0.065235555,0.009237258,0.08678854,0.019288471,-0.029410658,0.08288716,-0.014678653,0.0068440693,-0.031293925,-0.049968433,-0.057400934,-0.039947905,0.09432597,-0.026818503,-0.059762497,0.022198183,-0.064757735,0.03783345,0.055585444,0.04774312,0.07589184,0.032646578,0.017020361,0.032111652,0.07081795,-0.017635377,0.0005368972,0.018642414,-0.03871051,0.0001341431,-0.09035927,0.04700715,-0.030209556,0.08222569,0.010863855,-0.019794496,-0.039391037,-0.0019359242,-0.03869583,0.017558977,-0.0034997282,-0.005811291,0.06184902,0.013253375,-0.10170584,-0.108416975,0.0447859,-0.023117393,0.08343179,0.015168345,0.015802575,0.10744875,0.033027895,0.0034827874,-0.0102445865,-0.0031891083,0.060676023,0.035976544,-0.02611606,-0.06854327,-0.06589279,-0.074056774,-0.0642512,0.023334134,-0.003423163,0.03529411,-0.046962045,0.006990145,0.02902755,0.046941016,0.013138802,-0.0040086103,0.019894872,0.0022716247,0.035629682,0.11698053,-0.0012342043,-0.019502478,-0.008415671,-4.3041325e-08,-0.06644287,0.0051855026,0.024866149,0.030383766,0.11354776,-0.07094823,0.07566263,0.06584029,-0.047738392,-0.03282984,0.095181614,-0.006274661,0.03615678,-0.07060982,0.023255972,-0.02424582,-0.056018412,0.059416287,0.027100774,-0.07524877,0.16370508,0.015615267,-0.015355457,0.028827557,0.0405583,-0.008882216,-0.037514627,0.06699456,-0.0048008235,0.010696081,0.04362395,-0.018195035,0.01601469,0.02199903,0.08174357,0.12081472,0.009296084,-0.051329155,-0.076220006,0.06204196,-0.09390794,0.025670387,0.061937254,0.06272111,0.0673113,-0.03208206,0.025784368,-0.084669456,0.098374814,-0.03546124,0.021236066,0.077731185,-0.016264345,0.012206555,0.039947182,0.031825107,0.00036059105,-0.08166291,-0.016636407,0.015014104,-0.040719327,0.047218047,-0.07207953,-0.039536994,5,4.253,-5.045408,9
448,"improvement in results can be done in two ways, either model improvement or sample improvement i.e. either we collect high quality/more data or try many different models and select best model. one technique for model selection is grid search.
in regression methods, a more general method is polynomial regression where we use exponents of feature like x^2,x^3. feature selection has two methods i.e. forward selection: start with empty set of features and add them from knowledge , backward selection : start with complete set and remove features based on metrics like p-value.
in real life, interpretability and maintenance aspect of model is also important.
neural networks have hidden layers between input and output with weight parameters. more layers add more degree of freedom but requires huge amount of data to train like chatgpt.
in logistic regression, model needs to make boundaries to classify different classes, reduce misclassification.",0.039454445,-0.039586652,0.05472636,0.024386054,0.03254055,-0.043406967,-0.0295502,0.013625322,0.0045977226,-0.0049104583,-0.027562005,0.023688134,-0.083740555,0.033249248,0.05803002,-0.024320498,0.09084458,0.033502087,-0.05200042,-0.02262115,-0.00052563375,0.015317682,0.06116493,0.023008231,-0.0014396509,-0.06559311,-0.03483007,0.035201002,-0.02480044,0.014315616,-0.034499243,0.045996457,0.043582648,0.013598966,-0.14053634,0.018395176,-0.02134695,0.029039713,0.050946303,-0.020803558,-0.025893282,-0.08478591,-0.020380987,-0.04759237,0.13052124,-0.018108536,-0.028811036,0.020563642,-0.01749195,-0.0077417544,-0.09167932,0.08008158,0.033638798,0.004135586,-0.07739324,-0.010798309,0.0062500024,0.0012438396,-0.02329137,-0.09205033,-0.001953441,-0.0063440944,-0.03146076,-0.07191145,-0.006598332,-0.06418004,-0.013164909,0.039309923,0.10178137,0.04293682,0.10651649,0.0032967331,-0.046209596,-0.0121199945,0.011805592,0.06945157,0.11548651,0.0015508435,0.029872034,0.018087633,0.018908871,0.032023273,0.060654163,0.055040188,0.028448805,-0.017268062,-0.0014805062,-0.018371372,-0.09121855,0.008580913,0.020646397,0.033804704,-0.017176753,-0.04094723,-0.021307198,-0.017744087,-0.0478368,-0.04430865,0.014912776,0.019191373,-0.01561901,0.026092168,0.07123583,-0.03182811,0.053819466,0.016696626,0.05321256,0.04487179,-0.00033356668,-0.061359704,-0.023287399,0.05628299,-0.08979846,0.010768172,0.09146308,0.026276777,0.0034635293,-0.0030786525,-0.07277228,0.11381434,0.010622087,0.01192199,-0.006497819,-0.075944975,0.019901386,0.027155811,-0.08655147,1.0871081e-32,-0.0070230276,0.023109144,-0.004938184,-0.032596294,-0.028511751,0.055402745,0.0031497518,-0.0013623785,0.08436639,-0.00036054122,-0.10749278,-0.05085732,0.055756357,0.1077815,0.086384825,-0.073267855,-0.0906919,0.03180092,0.015775088,0.032507632,-0.019634742,-0.047923278,0.06858809,-0.067277215,0.045179497,0.024674911,0.05107053,-0.0017327964,-0.14015964,-0.0021041194,0.07530429,0.004825684,-0.00022802329,0.045995433,0.05819769,0.02678203,-0.040552717,-0.036924895,0.019343218,0.046237327,-0.075202376,-0.026859714,0.021546395,0.040703576,-0.01338948,0.038845897,-0.07740824,-0.07007885,-0.056550603,0.0006934224,0.0685633,0.015102477,-0.07171544,-0.0098075075,-0.025998354,-0.04156852,0.006080104,-0.009971603,0.020119252,0.04150254,-0.0010917762,-0.060079005,-0.0079076085,0.007833944,0.06802017,-0.06050887,0.03832209,-0.06989436,0.006032016,-0.09516889,-0.0025765179,-0.075316206,0.09032456,-0.00038766855,0.027906481,0.013815909,0.028032862,-0.03651866,0.022016926,-0.0064439685,-0.07861756,0.079881124,-0.017944094,-0.07464819,0.06683302,0.035872873,-0.029635003,0.0028520552,0.016130362,-0.011854411,-0.10517895,0.10054053,-0.031724606,0.01691413,0.042309295,-1.0195537e-32,0.038150717,0.053260576,-0.0057206387,0.04417133,-0.0713007,-0.046002205,-0.017709585,-0.07758611,0.0056622014,-0.109996475,0.03675494,-0.016307632,0.11572594,0.01570544,0.022676388,0.05848386,-0.11022289,-0.058485538,0.00820172,0.02577337,0.056738764,0.049756512,-0.07055218,-0.048215713,-0.056832816,-0.099243484,-0.04652432,0.015286233,0.05278289,-0.096210495,-0.036084365,0.015999632,-0.0672896,-0.015198557,0.1367728,0.0807693,0.044522084,-0.029630128,0.008650597,0.079935946,0.0007949687,0.0030038112,0.005560042,-0.044686258,-0.0043121614,-0.021366242,-0.041665748,0.006248844,0.034788985,0.062317114,0.061940644,-0.045646314,0.006358738,0.0054218923,-0.0011809526,-0.014647853,0.017700996,-0.049622755,-0.02287648,0.028222568,-0.062457874,0.00047533296,0.10973851,0.060207725,-0.0264957,-0.06472906,-0.0034920229,0.106671214,0.009567188,-0.056334157,-0.10044298,-0.071721286,0.08633027,0.06635487,-0.040949285,-0.030502489,-0.008190752,0.0048496127,0.007434408,-0.016131379,0.03666102,-0.018437799,-0.06144685,0.02348056,0.033135727,0.07783909,0.048533734,0.013360902,0.0039385515,-0.080808364,-0.018851802,-0.006642625,-0.004611343,0.021193527,-0.015124523,-6.46354e-08,-0.008922656,-0.071344934,0.07179481,0.04321524,0.04470158,-0.041633416,-0.059639256,0.13052891,-0.032253474,-0.008451292,-0.01504903,0.09521454,-0.049565773,-0.08183326,0.092140324,-0.010806917,-0.01284833,0.037350133,0.023688842,-0.060832847,0.050757416,-0.06310444,-0.013676836,0.016761295,0.107755765,-0.07646911,-0.030739358,-0.05290376,-0.06666676,0.034020446,0.04366624,0.005540863,0.033516493,0.039908174,0.13523303,0.036906164,0.008561843,-0.082683556,-0.09103654,0.033824667,1.0219351e-05,0.036852587,-0.031422943,-0.05649074,-0.023367982,-0.05727905,0.016223006,-0.07394932,-0.011784945,0.023651185,-0.0007788211,0.016054671,-0.016536273,-0.013210867,0.026424607,0.104223944,0.014201067,-0.04405204,0.016352285,0.06926561,-0.08652465,0.0029071893,0.0024771835,-0.040956303,5,-2.6537457,-13.902123,9
464,"we recapped some earlier topics like expectation algebra and clustering, then jumped into logistic regression, seeing how we can use the softmax function to classify things and how we find the best settings for our model. we talked about how to make our model better by tweaking it bit by bit (gradient descent) and how we measure how well it's doing (accuracy, precision, recall, and the f1-score). we also went over some common mistakes on the last assignment and how to make our reports clearer.
",0.03600459,-0.067861885,-0.043729197,0.050125282,0.053221893,-0.00989561,0.009760732,0.038379755,-0.112352006,-0.00014532232,-0.029668324,0.05452013,-0.01787083,0.06779326,-0.016240248,-0.015800707,0.02100759,0.06890884,-0.123764,-0.06428541,-0.03559383,0.0065217298,0.12209539,0.0049579106,-0.01724528,-0.0016312576,-0.0428086,0.016849248,-0.021103762,-0.079126336,-0.05624337,0.05312614,0.10451707,-0.013906977,-0.09529324,0.033197865,0.01427154,0.029310282,0.015787272,-0.014331164,-0.06741405,-0.08725564,0.08411882,-0.025908962,0.040253606,-0.060602654,0.010198602,-0.012336582,-0.02898248,0.011917146,-0.039062727,0.022424081,-0.06659107,-0.03289726,-0.04974621,0.038872838,0.014037288,-0.041944847,0.02222692,-0.069978036,0.022123676,-0.056103714,0.0098560415,0.016562628,-0.026852934,-0.026785254,-0.008656509,0.063119106,-0.005429831,0.026365474,-0.037632678,0.027724173,-0.07978815,0.047901306,0.057567526,0.029441154,0.087185115,0.018362029,0.01784009,0.032557145,-0.022241544,-0.052502204,0.013034546,0.05357114,0.12857278,-0.054511912,0.008117594,-0.0066473116,-0.07172563,-0.04019317,0.0026684608,-0.020188669,0.015702946,0.015034259,0.0634557,0.025440639,-0.020516822,-0.1155945,0.02185109,0.050992925,-0.05213235,0.10179837,0.013805625,-0.07915614,-0.0019032011,-0.0884585,0.06293979,0.08367208,0.024043486,-0.07149414,-0.010495369,0.109552324,-0.094461314,0.023771694,0.08647081,-0.051390644,0.05447728,0.044132844,-0.025474016,0.12140535,-0.04459181,0.070440084,-0.004678388,0.019564552,0.08711126,0.0072123758,-0.09975832,-8.957033e-34,0.012486243,0.015488306,-0.019942619,0.034493692,0.026871417,0.03268593,-0.043053877,0.057411473,0.05034272,-0.01342514,0.04073524,0.07572375,0.04166796,0.09267915,0.061916307,-0.059118327,-0.115632415,0.011229655,-0.042085562,-0.0051417784,0.015465755,-0.040990792,0.0195657,-0.087724455,-0.022309786,0.114602126,0.07082183,-0.020382514,0.0085741235,-0.0048192604,-0.023405991,0.028939381,-0.04993252,-0.013859229,0.039030578,0.06479206,0.0069729043,0.011229985,0.030271156,-0.061912164,-0.08884435,0.03758193,-0.011863294,-0.065312915,0.013257506,0.035325654,0.035696205,-0.043321792,0.04650231,-0.04539955,-0.038042985,-0.07396781,0.0035069126,0.029191276,-0.06141084,0.047845706,0.027189735,-0.062195357,-0.0440665,0.01757287,0.0515063,0.0044830753,-0.012177901,-0.08518672,-0.07115941,0.0298974,-0.01053029,0.07558139,0.0025416818,-0.004288518,-0.05771878,0.033418097,0.0011097648,-0.029204784,0.011882448,0.08825364,0.019092832,-0.063017204,-0.04669682,-0.06783588,0.045361686,0.021042718,-0.011672298,-0.08084704,-0.076674044,0.045290288,0.0101547735,-0.046307057,-0.0393462,0.027024215,-0.043021668,-0.0026457226,0.014753706,0.08001733,-0.00049550424,-1.8815907e-33,-0.1150859,0.007250986,-0.016069762,0.084352516,0.025978127,-0.052112605,-0.03650419,-0.0340942,0.05866942,0.0164653,-0.02283911,0.022620546,-0.022868168,0.046297092,-0.04145078,-0.0060776165,-0.031568367,-0.045299508,0.026914546,-0.01334083,0.003917044,0.11699317,-0.088655606,-0.051605184,0.008864361,0.050102357,-0.012656883,-0.018656427,-0.037875738,-0.0807341,-0.062014185,-0.04126656,-0.046039835,0.0025548448,0.030814564,0.057256583,0.03271441,-0.06700316,0.019031521,-0.0121237,0.06271382,0.023538457,-0.025664266,0.008115692,0.029478185,0.0026955032,0.043306403,0.017178763,0.0011969937,0.03759402,-0.024635946,-0.05124115,-0.026227493,-0.040553533,-0.03541971,0.039629746,-0.0028688174,-0.1248567,-0.0041348683,0.08558464,-0.1212604,0.055819485,-0.008723936,0.07119077,-0.10008338,-0.123084836,0.012417625,-0.05201514,-0.009510802,0.013177547,-0.06743823,-0.017113438,0.037688915,0.061288178,-0.012443914,0.0058554695,-0.034362223,-0.04031232,-0.09000743,-0.0077499393,0.06687961,-0.057547387,-0.05045922,0.005995013,0.019595724,0.038484666,0.08856198,0.050539505,0.049156252,-0.07809533,-0.103727944,0.02049156,-0.006180371,-0.008873778,-0.11560555,-5.5347638e-08,-0.02122695,0.01262969,0.042583425,0.041458037,0.026507288,0.0027229409,-0.0908891,0.10772213,-0.1003648,-0.0155326,0.0043647327,0.02680969,-0.13519639,-0.011922636,0.041544825,-0.014910911,0.06807496,0.06950591,-0.0026502244,-0.017935174,0.031441215,0.001130594,0.017559074,-0.029782804,0.07383415,-0.046176296,-0.007897286,0.05943918,-0.006266249,-0.028729962,-0.019963121,0.08532159,0.023821428,0.010736157,-0.022953603,0.047307923,0.059993804,-0.0102447625,-0.013889428,0.010660478,0.009899634,0.0343318,0.039322734,0.012816214,0.0068619857,0.029104706,-0.013070663,-0.028401205,0.038168132,-0.0360233,0.034566734,-0.0148325795,-0.01894431,0.05047742,0.09630493,0.07915425,0.014013358,-0.0948091,0.02829515,0.05194459,-0.03243699,0.086930655,-0.05726658,0.05992816,13,5.064314,-26.66472,9
468,"from the graph, we saw that for each sequence of sessions, less people produced summaries but more words.

we analyzed a model like a sine wave. by applying a feature y1, we produced multiple polynomial features such as y1^2, y1^3, and y1^4. a p value was produced. applying another feature sin(yâ‚) decreases the p value; thus, this feature is relevant. but when too many features are included, adjusted r^2 may decline. it is desirable to have just one model fitting well.

finally, we discussed neural networks, where input features pass through computational layers. stacking these layers forms deep learning, but excessive complexity that is increasing deg if freedom can cause overfitting.

",-0.022038879,-0.044586327,0.029756676,0.04467008,0.0068516736,0.048270177,-0.05675664,-0.0025484685,0.039473392,-0.08957793,-0.06611864,0.009325458,-0.03961437,0.046175823,-0.0046502384,0.016735105,0.0035207407,0.09441163,-0.06645176,-0.07661572,0.07741695,-0.006218558,-0.032749474,-0.020547532,0.037610505,0.058141246,-0.06323608,-0.026142424,0.020572623,-0.018179426,0.052914515,0.0844206,0.012333941,0.020039791,-0.04973885,0.036741283,-0.09163031,0.09495049,0.023248423,-0.04436629,0.012905045,0.011747653,0.01928762,0.06777931,0.1091146,0.0012182563,-0.05121934,-0.020464586,0.027960502,-0.008979767,-0.021995218,0.02422087,-0.007952247,0.07280588,-0.006002566,-0.011323165,-0.056682173,0.04756689,-0.038682494,0.00034427166,0.00318673,-0.057383757,-0.013309441,0.01477702,0.091614105,0.014471603,0.036249895,0.054011457,-0.013835688,0.08976012,0.01491616,0.09400728,-0.09433048,0.024982654,-0.022294082,0.008518955,-0.00564255,-0.012715163,0.019994613,-0.004103276,-0.009964555,-0.00038045683,0.037543654,-0.039348107,0.052436497,-0.01976907,0.0016161905,-0.062091537,-0.12738435,0.014357554,-0.066561244,-0.02001918,0.0193475,-0.05231108,-0.045511317,0.05109625,0.012238772,-0.03889139,-0.05081058,0.10603926,-0.030772569,0.035100628,0.08073133,0.0008878265,0.00064280635,0.0071771685,0.013218009,0.010379273,0.002096852,-0.10431885,0.015716543,0.07476467,-0.03241367,0.017176984,0.1231658,-0.020253865,0.017923081,0.007984219,-0.005225997,0.10329713,-0.08511347,0.043148287,-0.021749768,0.04545184,-0.057859287,-0.014380016,-0.0484829,7.2035425e-33,0.055964842,0.033534244,-0.02655556,0.023108574,0.05408602,-0.007617173,-0.038524497,0.034701712,0.05519195,0.054775145,-0.09404237,0.02604395,-0.034688514,0.09132984,0.045090396,-0.009566746,-0.027547564,0.07903645,-0.009218848,-0.025878789,-0.0073791426,-0.13104664,0.017467568,-0.018680507,-0.07917926,0.028639833,0.022009665,0.021782031,-0.061360773,-0.0054264325,-0.019222956,0.055484332,-0.016263084,0.03049738,0.063726746,-0.04916899,0.061932847,-0.028273616,0.10651602,-0.0016749685,-0.035397965,-0.0084923245,0.022675045,-0.053146083,-0.072366565,0.024757786,0.026640182,-0.084537454,-0.12273747,-0.009818576,-0.024203107,-0.015340348,-0.030442249,0.035569467,-0.00282883,-0.006903021,0.034158625,-0.013984641,-0.01998947,0.07015621,-0.0009772899,0.034438528,-0.037404545,-0.03204955,0.001841872,0.050481886,0.0110400515,0.032902144,0.07691173,-0.01622996,-0.027421651,0.019826708,-0.021742187,-0.087817155,0.071288325,-0.004453824,0.034283776,-0.063723445,-0.041577306,0.09048366,-0.026152737,0.013994292,0.03703492,-0.09311345,-0.022644542,-0.020673743,0.07487674,-0.09519381,-0.004253065,0.013194653,-0.13053946,0.030465266,0.08552375,-0.011747739,-0.014041403,-5.3115036e-33,-0.12955719,0.0500787,-0.05103663,0.03979275,-0.009909686,0.040438518,0.014212057,-0.014597489,-0.028946891,0.008220733,0.01786547,-0.0819709,0.08255139,-0.0493651,0.0492106,-0.003339537,-0.004958085,0.00399287,0.064319775,0.02700418,-0.017629115,0.0649878,-0.045080777,0.03912257,0.018395688,-0.0057830396,-0.087904006,-0.026738897,0.036722675,-0.00016956047,-0.07944329,-0.017598936,-0.066828914,-0.041377194,-0.016588794,0.050425407,-0.014309238,0.013025584,0.0059525054,0.05474698,0.067502014,-0.0006289834,-0.07020547,0.014543861,-0.04787294,-0.045598067,-0.08569464,0.0010830251,-0.00035829147,0.081575796,-0.023296036,0.00094897626,-0.053749565,-0.010957471,-0.08220814,-0.061408874,0.039170563,0.001433416,0.053130586,-0.015167682,-0.13395074,-0.09062797,-0.00071635755,-0.038964096,0.058896065,-0.01564715,0.004494173,0.082965165,0.016574174,0.007815696,0.029898351,-0.07482323,0.007112844,0.031365283,-0.004340215,-0.092985846,-0.03176758,-0.07701481,-0.08831993,-0.038581397,-0.041186206,0.04517939,-0.025996584,0.02665763,0.033077728,0.070160985,0.017845802,0.071206175,-0.008396056,0.03573264,-0.0065399315,0.08833569,-0.0790549,0.015642006,-0.043820713,-5.513619e-08,-0.09705026,0.018806182,0.02713985,0.017281631,0.090990655,-0.121895276,0.07559415,0.1199569,-0.04104904,0.012748505,0.060733806,0.02020328,-0.0010818237,0.0045197597,0.009296703,0.11116157,-0.0584709,0.07845969,0.00785494,-0.095635414,0.13499576,-0.0026283567,-0.03253285,0.043179445,0.06892749,-0.015595489,-0.04391084,0.062302545,0.0110047795,0.02768127,0.01925027,0.03178015,-0.020182116,-0.024215482,0.07562124,0.07462624,0.038242348,-0.048034087,-0.04955519,0.035857894,-0.109681405,0.10786727,0.013863667,0.0590994,0.05214254,-0.049932837,0.014014355,-0.09933047,0.05194101,-0.03478392,0.02716797,0.058889963,0.039922033,-0.014437386,0.02621155,0.059987698,0.0056701414,0.017047571,-0.0059618275,0.031478647,-0.0477744,0.009719487,-0.035839655,-0.0804342,5,3.8528166,-5.323793,9
495,"we saw there are two methods of improving quality of results : improve sample or improve method.. there is third one fine tune the methods.  then we start with understanding mlr for non linear cases. we again used feature engineering and have more terms involving x but in form of polynomials like xâ², and trigonometric function sin(x) : polynomial regression. we saw as we incorporate more terms adjusted râ² will gradually decrease. and at the end only significant term will remain due to p value . we saw backward and forward feature engineering and the issues like overfit. also we saw examples of non linear involving two things : trignometry and straight line.. if possible we want to have a single model to solve this instead of two models which can be ade by combination of two models. randomforest is a method which does the same thing. we saw parametric and non parametric method and delta analysis. we saw little bit definition of neural network and deep learning model which have more than one hidden layer. at the end we saw classification.  definition of regress and logistic regression . we started calling weights. we also saw the graphs and explanation for sigmoid function.",-0.09085472,-0.06111334,0.011772682,0.053729802,0.06565759,-0.07393812,-0.10151821,0.034560457,0.033825427,0.0052817827,0.037518363,0.034452282,0.020681465,0.0013304276,0.055274326,-0.026082255,0.022148285,0.08537998,-0.10090356,-0.010635915,0.06468569,-0.03692569,-0.046987817,-0.015865574,-0.03033374,-0.10187233,-0.014482073,-0.023531487,-0.0017910728,-0.03311916,0.07385588,0.036093052,-0.015615596,-0.013775381,-0.09363086,-0.0032144904,-0.08426479,0.039256547,-0.03249271,-0.008818291,-0.0030093342,-0.10347735,-0.02170856,-0.04699939,0.087106,-0.06433907,0.014100195,-0.09212786,0.004620869,-0.028663069,-0.09734219,0.016411804,-0.11176231,-0.0680488,0.005316538,-0.028744137,0.008264504,-0.0030717151,-0.016809229,-0.039869152,0.09771594,0.009393904,-0.09272564,0.010982406,0.059508733,-0.039692964,0.01184319,0.018281227,0.040090613,0.052848686,0.024354944,0.05754522,-0.054868862,-0.012449375,0.035305735,-0.0028916644,0.036240593,0.1060791,0.018960694,-0.0048029525,0.026349384,0.014234549,-0.02624068,-0.031547323,0.04684164,0.006769586,-0.0016547455,-0.07468829,-0.002166559,-0.04012599,0.04744423,0.042510636,-0.0802223,0.019202104,0.004069298,-0.019817146,0.10549789,-0.04463376,0.0258776,0.08262766,-0.043299865,0.052625965,0.02606072,-0.08669482,0.018124532,0.004323515,0.060496718,0.03140698,0.109043136,0.01298588,-0.0055691055,0.0596087,-0.032571986,0.0061855004,0.0896838,-0.03856256,0.026681287,-0.010881923,0.037799213,0.045673005,-0.05511903,-0.0065796934,0.041272704,-0.024070878,0.07750642,0.04980234,-0.06765035,7.367152e-33,-0.020816898,0.025172612,-0.007681988,-0.05789001,0.013020876,-0.017934958,-0.034387585,0.0077298773,0.10728853,0.05269452,-0.05070468,-0.026705358,0.02637764,0.06589154,0.124745056,-0.03758996,-0.014663262,0.01957083,0.049234215,0.06505137,0.016518883,-0.090207,0.025533028,-0.04236135,-0.005081733,0.04501015,0.07702739,0.0491552,-0.12238539,0.0350977,0.010221454,-0.001839211,0.00015896939,0.068650074,-0.055686977,0.016982723,-0.008864781,-0.04474854,-0.037099417,-0.05724255,-0.061731793,0.029490238,0.06492538,-0.045336787,0.01436922,-0.0019439561,0.011560923,-0.06361223,-0.08658389,-0.015479806,-0.0072261603,0.011405175,-0.068454966,0.011058488,-0.04640976,0.096987836,-0.019832166,-0.05350838,-0.037715025,0.08849547,-0.036180425,-0.005755187,-0.0035871104,-0.047686376,0.026672507,0.034705374,0.053142466,-0.06793277,0.031201039,-0.04800879,-0.029316984,-0.050647788,0.008205783,0.0034287444,0.077242896,-0.08828692,0.049751777,-0.024811113,0.052093297,-0.010886837,-0.06045939,0.09331435,0.04416548,-0.073207304,-0.038248897,0.024627876,-0.0040635765,0.0048900205,0.0021785414,-0.020176737,-0.10283085,0.08537727,-0.022345195,0.03238379,0.068347685,-6.684478e-33,-0.106876664,0.069733106,0.0057487385,0.016927961,-0.00822408,0.023060933,-0.045357976,-0.072346166,0.037910916,-0.08614019,-0.0027899044,-0.022178479,0.02314842,0.020340193,0.103549466,0.011173471,0.01920254,-0.06333583,-0.029771348,0.025859458,0.0467818,0.15371436,-0.10941422,-0.0062891915,-0.029800013,-0.03352088,-0.11444959,0.12385457,-0.0026989807,-0.02832651,-0.0009858788,0.044497162,-0.025573615,-0.04535411,-0.006817758,0.041853603,0.04024224,-0.0377708,0.025852911,-0.0034962988,0.039410323,0.0033613143,0.013946094,-0.018192252,0.023204485,-0.020968955,0.018928798,-0.055042714,0.042872168,0.035484098,0.053021252,-0.03152858,-0.10430389,0.0130774155,-0.051678095,-0.03579253,-0.020332769,-0.058156263,0.0201551,0.027103793,-0.029206583,-0.0314512,0.020825692,0.07378488,-0.018842947,0.025311416,0.01945632,0.052631676,-0.021363849,0.024013372,-0.07571303,-0.019451093,-0.008854644,0.067668624,-0.05449767,-0.0561067,-0.04757848,0.007983597,-0.044764437,-0.030802796,0.013147555,-0.026041398,0.03020818,-0.041063834,-0.00760263,-0.021324849,0.031367216,-0.02736163,-0.00065089384,-0.09310222,-0.06625335,0.039024077,-0.031945143,0.054515086,0.06430857,-6.957699e-08,-0.0153966565,0.003039736,0.049247254,0.017990503,-0.005661971,-0.014752069,-0.04530264,0.09947635,-0.03396318,-0.059212204,-0.049462445,0.06701803,-0.062199675,0.03130832,0.022977384,0.10370155,-0.043742087,0.073445976,-0.03244046,-0.056438033,0.06336386,0.0051736617,-0.040873922,-0.050159536,0.108749576,-0.020686211,-0.00019017623,0.029962866,0.005974704,0.04789377,0.0045218286,0.02608498,0.024530057,0.0826842,0.09387027,0.03504295,0.14076546,-0.052629676,-0.061625753,0.048696745,0.0037962757,0.1024832,-0.046763662,0.030182771,0.1093366,-0.010525991,0.066407345,-0.107005775,0.023356766,-0.020534104,0.03161841,0.06733216,-0.009927004,-0.03761185,0.009906618,0.036018137,-0.0136058945,-0.010191189,-0.048383716,0.10437909,0.01584667,-0.06775782,0.022909291,-0.01118603,5,-2.0499654,-11.06121,9
498,"polynomial regression extends linear regression by modeling the relationship between variables as a polynomial function, often using taylor expansion to express functions in powers of x for better approximation. some key points in feature engineering - simply adding more features doesnâ€™t always improve a model; adjusted râ² can start decreasing if the new features donâ€™t add real value. a lower p-value indicates a better model fit. feature engineering plays a key roleâ€”forward selection involves adding relevant features one by one using domain knowledge, while backward selection starts with multiple features and removes the irrelevant ones. exploratory data analysis (eda) helps determine which type of model may fit the data best. choosing the right model requires balancing complexity and performance, avoiding overfitting, using visualizations and key metrics. a good model should have error values within an acceptable range compared to the original variable range. logistic regression, often used for classification, finds boundaries between labeled observations, which may sometimes be non-linear.",-0.0032971387,-0.008370382,0.0154880425,0.047951818,0.049528696,0.008121857,-0.098882,0.02284724,-0.0049785697,0.036808454,0.0019376387,0.017850524,-0.050485555,0.037077826,0.025656477,0.053246092,0.036871992,0.015826663,-0.0068179592,-0.053153183,0.042606413,-0.05866506,-0.048079524,-0.036419522,0.066615924,0.0005070765,-0.032176454,0.06867251,-0.049470045,0.012323815,-0.0065680807,0.0433811,0.03094704,-0.048371956,-0.0952894,0.057505377,-0.00190499,0.070481434,-0.02972402,-0.0063149645,0.034408383,-0.061250534,0.036383778,-0.0076564797,0.11069186,-0.048806842,-0.011797401,-0.0151830735,-0.013513262,-0.035734512,0.028493902,0.0057831025,0.000814405,0.037541125,-0.036374312,-0.03510719,0.010218123,0.05360788,0.021682382,-0.014689499,0.014114927,-0.013987861,-0.03892997,0.051040098,0.042367116,-0.04082656,0.029435456,-0.04361946,-0.022158287,0.09741042,-0.0067303083,0.0919427,-0.09281369,-0.009730086,0.036538497,-0.028176554,0.061766278,-0.013418942,-0.008760695,0.029484829,0.039993856,0.12000738,-0.02148377,-0.0164873,0.06305844,-0.023372322,0.0042216848,-0.033527758,-0.039526004,0.08491783,0.04128574,-0.012481317,-0.025226422,0.020727748,-0.01943811,0.014905435,0.012006713,-0.14350536,-0.0181823,0.03502207,0.0034337165,0.062282342,0.03471773,-0.008045378,0.007249922,-0.02869475,0.039687295,-0.006010275,-0.02807037,0.003982188,-0.02189801,-0.0064850943,-0.018823648,-0.00144231,0.070649005,0.015531361,-0.033224102,0.01760877,-0.026953474,0.0067479694,-0.052261528,-0.0010807647,0.05861889,0.030208739,0.044192266,-0.042882565,-0.10301086,9.1504396e-33,0.0043733357,-0.008962527,-0.040546965,-0.045211125,0.03713627,0.026229389,0.03983364,0.05471976,0.028934509,0.081536174,-0.063681,0.07885383,0.013274633,0.088607974,0.11138615,0.027744284,-0.010796518,0.10767574,0.020840859,0.09493372,-0.007770111,-0.15457933,0.021108538,-0.026541902,0.021120023,0.047993638,0.03429425,0.047951918,-0.078539774,0.00044673265,-0.0085203415,-0.015359553,-0.023003602,-0.020649152,0.021435626,0.018248076,-0.011345906,-0.10514689,0.124393046,-0.012952367,0.026529534,0.029077597,0.051423606,0.019460429,0.0035391755,0.037984103,0.006219282,-0.1167489,-0.06427691,0.07533141,0.0027058371,-0.00532441,-0.05129221,-0.0013632016,-0.0837873,0.007604229,-0.0492197,-0.010308967,-0.05136305,0.012387998,-0.060653426,0.002894485,0.009105763,-0.105543174,-0.00033460706,0.010297214,0.030027263,-0.046201427,-0.004193792,-0.008870769,-0.04268556,-0.08237171,0.037901044,0.028897258,0.07230401,0.032221373,-0.01231549,-0.028666977,0.070144676,-0.030763693,-0.08387445,0.068557486,-0.00853692,-0.083567135,0.036814105,-0.0457229,-0.019855278,0.010505405,-0.0141403265,-0.04514972,-0.08924681,0.07755635,0.008867743,0.090682775,0.054552685,-8.5033065e-33,-0.014480896,0.027821817,0.036246844,0.027997771,-0.012651562,0.020333003,-0.062016636,-0.052121226,0.08263639,-0.07110396,0.008302126,0.0049628145,0.04150879,0.015255004,0.06743167,0.04249726,-0.061376326,-0.09566546,0.0028612958,-0.03848879,-0.001493707,0.06836556,-0.072012864,-0.027350593,-0.042025376,-0.032595854,-0.04518595,-0.07789152,0.0065072165,-0.09000954,-0.08758769,0.01389242,-0.056843717,0.0033124583,0.0068347375,0.03951122,-0.07306755,-0.08728731,0.043597072,0.13846974,0.0740525,0.059174594,0.020070557,-0.029794205,-0.021282721,-0.06689516,-0.009682497,0.032522358,0.13723956,0.04791159,0.026787303,0.061198115,-0.025686463,-0.0053058653,-0.06808183,-0.05358939,0.014221923,-0.017412676,-0.050026014,0.015529573,-0.044228345,-0.0013303688,0.08734265,0.076972745,-0.09180996,-0.00017544303,-0.013630985,0.021170832,-0.05385639,-0.016482435,-0.018654067,-0.08138412,-0.005363725,0.04655124,-0.104477204,-0.11974888,-0.023832243,-0.026161082,-0.01579424,0.025972737,0.053756826,0.04364432,0.015270262,0.021069,0.013917225,0.038400177,0.026581759,-0.023378834,-0.02981026,-0.09918801,-0.005977883,0.011700198,-0.056244552,0.07184086,0.027933808,-5.63179e-08,-0.05737133,-0.011014242,0.049724307,0.0007972525,-0.014257542,-0.085971415,-0.006682256,0.11215013,-0.058508743,-0.045051187,0.05832982,0.014340077,-0.015787289,0.05389373,0.098933965,0.07130895,0.069289826,0.04523886,-0.045061436,-0.07325156,-0.00052926445,-0.06477453,-0.11370048,-0.03847705,0.066966735,-0.082569405,-0.0065040337,0.07769338,-0.007278945,0.065073445,-0.002888064,0.058184985,-0.016183881,0.020195851,0.06982227,0.045448586,0.093429275,-0.11951033,-0.023965638,0.044274382,-0.018883528,0.076889895,-0.0029556477,-0.0047383215,-0.014996424,0.016708475,0.11099084,-0.1013638,-0.058155447,0.027539423,0.08732204,0.026110984,-0.0058560376,-0.05087972,-0.0048065693,0.10844038,-0.04018514,0.012316478,-0.031225696,0.04414094,-0.02945839,-0.06314136,0.05190039,-0.013155099,5,0.022449287,-7.94079,9
521,"today we started off with talking a bit more about linear regression and looked at an application of feature engineering like how higher powers of independent variables could be used and regressed. it is also called polynomial regression. and we discussed about how to select features, and a way to remove them is by excluding the ones with low p-value. we moved on to talk about models which can handle more complex data like random forest and neural networks. we were also given a look into how neural networks work. and lastly we saw a bit of classification the form of logistic regression.",-0.025034847,-0.0111925,0.010283818,0.08065485,0.084213376,-0.017402874,0.022673944,-0.030226685,-0.037571277,0.0374989,-0.016911486,0.07296699,-0.022038609,0.09344233,0.021483677,0.0337349,-0.008367683,0.048549905,-0.016376916,0.024115423,0.03408812,-0.0028102982,-0.04957297,0.037723515,0.079783544,0.0007087051,0.01178278,0.016859446,-0.069965616,0.004245252,-0.037066944,0.07272543,0.028158288,-0.038614545,-0.07499471,0.037910573,-0.008606387,0.074496895,-0.032680176,-0.035057027,-0.002777679,-0.12689406,-0.016892947,-0.014311099,0.117848314,0.021367218,-0.047158495,-0.060244493,-0.06527416,-0.033013295,-0.06317209,-0.022483584,-0.054647602,0.017601121,-0.0047781654,-0.103507765,0.009556455,0.014410625,0.010119002,-0.015758043,0.03535832,0.0005773051,-0.0024669976,0.025745254,0.023672212,0.027580516,-0.07078079,0.036172863,0.06345917,0.011331142,-0.021121442,0.05508366,-0.043869272,-0.018990103,0.054854006,-0.043948025,0.026516786,0.02363916,0.020250328,0.05892025,-0.0112364115,0.03348096,-0.0126426695,0.014364309,0.034376286,-0.011728884,-0.03854269,-0.007144956,-0.12006754,-0.008775372,-0.047547456,-0.032609724,0.07716792,0.022763036,-0.020217171,-0.00041286254,0.0021563456,-0.13197598,-0.0032546218,0.041754805,-0.051307205,0.036100976,0.026364243,0.0014930208,0.0068769637,0.009601703,0.019480312,-0.04402857,0.031629413,-0.020193497,-0.08407708,0.03532424,-0.09217015,-0.101221405,0.03862052,-0.030552298,0.007195445,0.07273608,-0.030293189,0.084473796,-0.03678242,-0.008799366,-0.021136388,-0.018940391,0.064233296,0.07022274,-0.09904684,2.1173893e-33,0.0041557094,-0.035911646,-0.056987558,-0.0625242,0.066877715,-0.04851813,-0.00960461,0.029667614,0.10948988,0.061743427,-0.021672025,0.0122698145,0.057497643,0.111861326,0.120929085,-0.0035008658,-0.035472486,0.0860264,0.06896763,-0.020051438,0.004704005,-0.011552716,0.04162189,-0.014509765,-0.006460756,0.016454162,-0.011957201,-0.0033488087,-0.044253834,0.016424667,0.07680844,0.06673499,-0.01948972,0.004602241,-0.0005767681,0.024619035,-0.03058426,-0.067556694,0.07070155,0.041891374,-0.053364415,-0.036361553,-0.025609782,-0.0017267009,0.0037182162,0.041065853,0.03871075,-0.035797346,-0.10206456,-0.04987067,-0.0037959022,-0.010253186,-0.052337788,-0.016058577,-0.048835367,0.015243892,-0.026661368,-0.07083726,-0.07723123,0.029110517,-0.01695201,0.029987913,0.0015171164,-0.1158551,-0.036537662,-0.025448442,0.080874115,0.014894275,0.014795513,-0.019731916,-0.031814273,0.0006606726,0.0075697917,-0.09356039,0.052276473,0.064523585,0.06314716,-0.028149864,0.010553351,0.006714513,-0.022588529,0.001477618,0.0060394886,-0.075075276,0.04111072,0.07026745,-0.026876647,-0.043302704,-0.037505303,-0.038375728,-0.10602373,0.035134435,-0.022531673,0.0015264553,0.06434276,-3.539249e-33,-0.071855746,0.016812025,-0.049846046,-0.0147359455,-0.011327066,0.00074993394,-0.054608226,-0.04826734,-0.025587237,-0.026630161,0.030538665,-0.027737666,0.082190566,-0.0064312704,0.005827433,0.014911737,-0.08440743,-0.047950517,-0.016199024,0.023783078,0.041107696,0.105621494,-0.09244588,-0.030588944,-0.04077934,-0.044917088,-0.11551972,-0.014854143,0.04558783,0.03256685,-0.0766195,0.0066208653,-0.04023382,-0.065854885,0.020671131,0.03615693,-0.011272169,-0.06643267,0.020150248,0.029065667,0.03868349,0.009756724,-0.010853365,-0.028331798,-0.013620266,-0.048757557,-0.0028017317,0.039898355,0.120105006,0.020941835,0.029341644,0.020230975,-0.022381814,-0.022325916,-0.041771494,-0.011624614,0.01085472,-0.022767445,0.027561238,0.05043865,-0.021626502,-0.0053586606,0.06924671,0.1398076,-0.0290321,-0.01945443,0.0077697677,0.047885515,-0.043020595,-0.07760149,0.0032109562,-0.0027883605,0.015347168,0.102714166,-0.09773639,-0.08846674,-0.016448991,-0.029268652,-0.09683862,0.060819857,0.10119473,-0.046989057,0.018744882,0.006069436,0.036532063,0.012575441,-0.030547766,-0.016479803,0.059056114,-0.099541135,-0.047524642,0.060110953,-0.05567038,0.05384603,-0.0060149846,-5.212588e-08,-0.037552714,-0.02665989,0.078085065,-0.015715173,0.058966067,-0.012927268,-0.017588798,0.15695511,-0.054322604,0.02299037,0.0008827242,0.06385905,-0.042343862,0.042564787,0.085160926,-0.014597191,0.0394416,0.06893453,0.0113341,-0.050768435,0.053252216,-0.062952295,-0.008464993,0.011640337,0.11382111,-0.14419338,-0.025753396,-0.0056055775,0.011875388,0.031022005,-0.011930595,0.064593144,0.030512942,-0.013436187,0.12898077,0.10628495,0.09579985,-0.07436213,-0.12354858,-0.0015245049,-0.048305172,0.021934798,-0.001489025,0.03811511,0.02390013,-0.01041419,0.076894365,-0.09738559,0.027624814,0.040981818,0.06244716,0.06669179,-0.0071709384,-0.02655336,0.031818334,0.118829325,-0.0060980525,-0.011947435,-0.054600183,0.025390232,0.021232894,-0.037057742,0.114112504,-0.023407394,5,0.8785109,-10.426704,9
528,"today we learned that there are three ways to improve the quality of results: improving the sample, improving the method, and fine-tuning the method. then, we moved on to understanding mlr for non-linear cases. we used feature engineering by adding more terms, like polynomials and trigonometric functions which is called polynomial regression. 
we explored backward and forward feature selection and the problem of overfitting. then, we looked at non-linear examples involving trigonometry and straight lines. instead of using two different models for this, we wanted a single model that could handle both, and random forest turned out to be a good method for this.  we also learned about parametric and non-parametric methods and delta analysis.  we talked about classification, the difference between regression and logistic regression, and started using the term ""weights."" we also saw graphs and explanations for the sigmoid function.",-0.06380682,-0.081581116,0.019633371,0.044481546,0.06636431,-0.090550624,-0.08148079,-0.024336314,-0.02516704,-0.0077151055,-0.050639592,0.009977864,-0.03828586,0.03297742,0.04774984,0.02095742,0.030819548,0.06048432,-0.03425101,-0.04165777,-0.019113101,0.0189349,-0.04044639,0.013235395,0.016248127,-0.10404568,-0.023825945,-0.0034181015,-0.047327127,-0.017806603,-0.027287602,0.036388382,-0.013614058,-0.009307661,-0.15896343,-0.012824353,-0.03282526,0.018755639,-0.011798482,-0.014134098,-0.045694172,-0.0710212,-0.005964522,-0.015655868,0.05888316,-0.016049184,0.0042699156,-0.07624035,0.010684868,0.023545885,-0.07741319,-0.008140393,-0.097355835,-0.04969625,-0.023607852,-0.02445743,0.03042661,0.010690831,0.040528,-0.016412044,0.08792205,0.0011639884,-0.05002011,0.01825042,-0.00837884,-0.05346799,-0.007890119,0.024492415,0.052195497,0.028468208,0.006318767,0.047619157,-0.04939808,0.032243144,0.018006327,0.007181731,0.044312686,0.11063258,0.033501383,0.045413427,-0.04169511,0.040457986,0.0092660785,0.035462454,0.0750812,0.022312637,-0.035027348,-0.006712832,-0.029969273,0.007211548,0.06593203,-0.0004496398,-0.05048536,-0.006383456,-0.07967947,0.026818054,0.02620444,-0.12879491,0.0017017508,0.04430871,-0.06204542,0.072197184,0.057142716,-0.08909565,0.028218193,-0.00088859827,0.07371629,0.027342303,0.099272214,-0.0033764779,0.046698112,0.07386697,-0.042293396,0.012466828,0.08911456,-0.041882634,0.014256022,0.029432021,0.0030819112,0.12865186,-0.03198221,2.179821e-05,-0.00023971709,-0.015667291,0.075004034,0.052341957,-0.055663224,3.900213e-34,0.009987272,0.037428167,0.017408732,-0.0129259415,0.0026143044,-0.06047989,-0.02093712,-0.0096509615,0.08623963,0.071878254,-0.053191543,-0.008498454,0.014729909,0.07270524,0.12759168,-0.005403828,-0.07227859,0.023357345,0.01531789,0.07820895,-0.020443393,-0.05332804,0.0005482859,-0.056193028,0.025217012,0.07821332,0.050610915,0.014638192,-0.095874175,0.02420624,0.010380817,0.018189423,0.010233737,0.06182481,0.0068313624,0.003819811,-0.01123142,-0.04635919,-0.012542374,0.010550278,-0.043886848,0.019490251,0.030686734,-0.021917868,0.033693604,0.045416936,0.025425274,-0.06003006,-0.09554829,-0.017673716,-0.04795036,-0.029000837,-0.035961207,0.018981706,-0.058981176,0.06042824,-0.03621836,-0.022370597,-0.07182615,0.0720126,-0.031682566,0.0019378398,-0.007952445,-0.06848303,-0.035123706,-0.0524411,0.04454547,-0.04950677,-0.016516196,-0.08505953,0.014701449,-0.00413914,-0.028087255,-0.03911752,0.057945136,-0.040606115,0.033616133,-0.00049983046,0.07630225,-0.020900575,0.018261487,0.06842183,0.02791435,-0.087864034,0.0010016608,0.05828724,-0.0029150597,-0.028193107,-0.040496998,0.008911905,-0.1168824,0.10450572,-0.023111794,0.038222145,0.050936308,-7.748628e-34,-0.07808022,-0.0022469177,0.046981413,0.06787803,-0.050878946,0.06260528,-0.073688254,-0.07451934,0.058165357,-0.07344457,-0.0061604884,-0.014441794,0.0118507,0.018720876,0.06135173,0.01135611,-0.030568363,-0.0076823477,-0.06629891,0.049590494,0.08323777,0.097520754,-0.1121323,-0.0048346287,-0.035722166,-0.011832446,-0.0974043,0.058556236,-0.013767382,-0.013276794,0.01935543,-0.00440124,0.0147406785,-0.14181682,0.0125317145,0.02783876,0.05472616,-0.014578125,0.054147042,0.018124145,0.05922183,0.051646993,0.062078748,-0.052683722,-0.025416076,-0.06370691,-0.007936481,-0.006874061,0.036255177,0.07880983,0.002490342,-0.03188558,-0.10805023,0.03991809,-0.033336915,-0.012155972,-0.016883666,-0.051305953,0.04156179,0.055803034,-0.11401024,0.017386327,0.027711513,0.073685035,-0.03201419,-0.050958022,0.063532986,0.041888814,-0.008251516,-0.017548138,-0.057461783,-0.033780746,-0.0052506346,0.0613131,-0.07479826,-0.04282067,0.041949753,-0.050903857,-0.09048454,-0.00045763212,0.08637939,-0.043009184,0.02659752,0.024283193,0.05629891,0.07014132,0.062435772,-0.027353112,0.027237862,-0.08352785,-0.045730405,0.027034767,-0.038251646,0.051492587,0.020485438,-5.7610592e-08,-0.058399107,0.03767665,0.02486626,0.012005809,-0.037723955,0.040494233,-0.007845332,0.1578895,-0.120113045,-0.048858985,0.030621601,0.05071086,-0.07775647,0.04579016,0.12359222,0.014470328,0.037764534,0.07519204,-0.030267403,-0.0198058,0.04605129,-0.036281984,0.028414255,-0.05241053,0.100496456,-0.024608739,0.0009885536,0.014033092,0.008982992,-0.0015843436,-0.022779806,0.08646204,0.02061129,0.078419805,0.068042204,0.047837444,0.083076246,-0.050065037,-0.064889565,0.042703107,-0.016095223,0.13061157,-0.07359867,0.016800601,0.07504509,-0.010884201,0.076207034,-0.06884486,-0.012183761,0.0038516326,0.056817073,-0.013509026,0.01619092,-0.0070223827,0.04940363,0.079578504,-0.0026781783,0.0060553425,-0.022360964,0.07248687,0.006935323,-0.07174463,-0.017226731,-0.016720816,5,-0.7046106,-12.068973,9
530,"we began by addressing a doubt from the previous lecture summaries. it was about the ways in which we can improve our results from the model apart from just increasing the sample size. so, we can also consider and try out different models before fixing one. we can compare the various metrics of these models and figure out the best one. if we want to stick at our original model, we can fine tune/ use it more appropriately to improve the quality of our results. one of the solutions suggested was that of grid search.
after having a brief discussion about the statistics from the summary submissions of the previous classes, we then moved on to understand the significance of feature engineering in improving our results from the model. by using feature engineering, we can create more features or destroy the already existing ones, to arrive at a set of most relevant and appropriate features that significantly describe the data. we can use something like polynomial regression. we already have one feature, x1 with us. we can raise it to higher and higher powers and introduce these as additional features in the mlr model. we can also have a combination of the polynomial and trigonometric functions like sin(x1). we observe that as we start adding more and more of these features to our model through feature engineering, the r2 value increases, however the adjusted r2 decreases. this is because we are adding more and more features to the model which do not significantly improve the results. hence, by using feature engineering and analyzing the scatter plots and histograms of errors, we can arrive at the best set of features for our model, which can explain much of the variation in the data.
all this is a part of exploratory data analysis (eda).
there are two types of feature engineering techniques - forward feature engineering and backward feature engineering. in forward feature engineering, we keep on adding more and more features in our model, by transforming the existing features. in backward feature engineering, we start eliminating these features one by one from the model, already having many features to arrive at the best set of features. the elimination is done on the basis of the p-values of the corresponding coefficients.
after this, we talked about multiple model creation. so, in real life scenarios we first have to get the data, then perform eda and preprocess it to get â€˜good dataâ€™. this good data can now be fed to multiple models. we can simultaneously compare the metrics of all the models and decide the best one from that. we can use multiple or single models to fit our data, depending on the variations in it. however, if a single model describes the data well, then it is always preferred over handling multiple models.
so, mainly there are two types of models- parametric and non-parametric. in parametric models, we can control the various parameters (regression coefficients) and we can also perform â€˜delta analysisâ€™ on it. this means we can find out how much the y value would change if the x value changes by an amount, say delta. slr and mlr are examples of such a model. however, in non-parametric models, like random forest, we cannot perform delta analysis, instead they give us better predictions than the parametric models. these models are also more flexible, i.e. they can adapt to different types of data well.
one of the models which we used on the data was the artificial neural networks (ann). ann consists of â€˜hidden layer(s)â€™ which have nodes. these nodes map/ link to each and every other node in the network to form â€˜linksâ€™. these links have certain weights associated with them. the y value is a function of these weights and the x values. these weights keep changing and are recalculated as more and more features are introduced. when there are more than 1 hidden layer in the network, we call it the â€˜deep learningâ€™ network.
more the layers, more capable the model becomes but at the same time we need to feed more data in the network. data (x values) is converted into information, which is represented by the weights.
we fitted many models on the same data set, and compared various metrics like r2, mse, etc. of these to determine the best model for the data. apart from comparing between models, we can also use the metrics, like mse within the model, to assess its validity.
we compared the relative difference between the metrics of the test and train data for a single model, to arrive at meaningful conclusions.
lastly, we started with classification models and discussed logistic regression in that. these classification models are used on discrete data like nominal and ordinal data to classify it into various classes. the y values are called â€˜labelsâ€™ and they determine a particular class. using logistic regression, we are trying to find out the line/ curve which separates or segregates the data into different clusters, such that misclassification despite of overlapping is minimized.
so, every point will have feature values x1,x2,... associated with it based on which it is given a y value, i.e. a label.
we concluded by discussing the sigmoid function and how it can be used to identify the equation of the line. further discussions to be continued in the next class.
",-0.07038595,0.0016571428,0.06967184,0.028498901,0.09716122,-0.0027940148,-0.069769025,-0.0032729616,-0.0039363652,-0.048024237,-0.057305697,0.046152513,0.028445724,-0.0198084,0.03355842,-0.010458965,0.057616297,0.04187723,-0.05774776,-0.04534455,0.0174086,-0.052825406,0.0011422177,-0.009042459,-0.009731692,0.02187242,-0.016502282,-0.010411327,0.046510976,-0.004163907,-0.0095486995,0.14566833,-0.012985557,0.0073060584,-0.01824678,0.019112188,-0.029225966,0.02845108,0.050529767,-0.008074355,0.021227442,-0.09878483,0.012477615,-0.0029751626,0.0700864,0.008570775,0.034382496,-0.08834722,0.008948004,-0.03137923,-0.09736288,0.030633915,-0.07708241,-0.06510238,0.0004890276,-0.048626237,0.03769795,-0.01718792,0.02618576,-0.063951544,0.051316362,-0.049779575,-0.05515578,0.05824184,0.011965913,-0.15157863,0.036038995,-4.874678e-05,0.04466807,0.032510545,0.025612302,0.07695566,-0.079509355,-0.00032360476,0.01292408,0.070497215,-0.013432245,0.04294536,0.043214664,0.033897772,-0.022679051,0.011193082,0.03330998,0.023063334,0.022475448,-0.01951472,0.009912505,-0.0994415,-0.026632631,-0.006285525,0.08216831,-0.009763735,-0.08042016,0.03240201,-0.01356218,0.07142421,0.044161353,-0.089642525,-0.0057931924,0.067318834,-0.041991252,0.06635044,0.06345814,-0.07777003,0.023704696,-0.01805417,0.015618302,0.100253895,0.066468485,-0.029803319,0.059082408,-0.02217535,-0.08359029,-0.02025141,0.09904348,-0.0183793,0.0043478906,0.012005921,0.01310609,0.019782336,-0.015509343,-0.05167178,-0.016251111,-0.022169096,0.05128293,0.086280175,-0.115439944,4.649798e-33,0.0004106693,0.035781424,-0.05478042,0.03012392,0.033183515,-0.0061698486,0.071776524,0.06138375,0.079332486,0.07597119,-0.081529394,0.051567998,-0.009429996,0.055749524,0.063179694,-0.04910691,-0.04973903,0.03958914,-0.03854901,-0.0035301025,0.083599605,-0.09546421,0.103894986,-0.06990431,-0.0001522756,0.021660049,-0.010054932,-0.034574784,-0.129671,-0.0035933915,-0.033026937,0.03609755,-0.009120614,0.103031,-0.06307301,-0.018953577,0.017872421,-0.10776912,0.044271376,-0.010109959,-0.039346173,-0.0026140488,-0.024369445,-0.027150199,0.010752137,0.037046093,-0.014940598,-0.090893105,-0.05061091,-0.016100593,0.038285285,0.0346456,-0.037879135,-0.026282158,-0.04267326,0.05056482,-0.05626082,-0.043420725,-0.017066643,0.04489961,-0.0662849,-0.019902071,0.04624024,-0.0072273044,0.092129596,-0.018829472,0.090378635,0.04065645,0.0026605774,-0.044457342,-0.0028595356,-0.08419259,-0.014652411,-0.07659877,0.11890433,-0.07104324,0.027900793,-0.008833392,0.029968506,-0.060664147,0.036237784,0.043752007,0.0036391846,-0.045907594,0.028407658,0.023213638,0.034602888,-0.0757166,-0.005168684,-0.053500928,-0.046474624,0.031016245,0.02522933,0.034272794,-0.008518293,-5.7509963e-33,-0.03888186,-0.012365695,0.010209903,-0.029129803,-0.041102543,-0.023718154,-0.021988422,-0.06519402,-0.04311318,-0.09726592,-0.0055377106,0.009314285,0.03900703,-0.02916209,-0.0833537,0.024122298,-0.016463552,-0.091316424,-0.04381808,0.055206206,0.008133703,0.13567051,-0.076914586,0.03747733,-0.050997347,-0.018013576,-0.0138602685,-0.023243742,0.041546833,0.011909179,-0.011776781,0.03385448,-0.04034937,-0.019137785,0.06555443,0.02218766,0.0208401,-0.054022107,0.05073206,0.13018888,0.10025075,-0.040571626,-0.013205205,-0.011149219,0.057375096,-0.046647288,0.04751069,0.036449905,0.057162683,-0.017201915,0.04688013,0.035451844,-0.06431949,0.037641007,-0.022882972,-0.008028557,0.021304348,-0.05612027,0.024299487,0.012291385,-0.12714303,-0.0063872673,-0.021749044,0.055427436,0.02136071,0.048560258,0.018718058,-0.032889184,-0.05159978,0.033198927,-0.106402375,-0.06692884,-0.050254475,-0.009804247,-0.03016234,-0.04644573,0.05387022,-0.047488973,-0.05448297,-0.010767899,-0.003747076,0.013254185,-0.004284033,-0.0014454819,0.045877125,-0.0010100229,0.054148056,0.045101795,-0.0049609966,-0.047090236,-0.07220883,-0.0018461071,-0.05711078,0.046749517,0.03968448,-7.9119744e-08,-0.12185366,0.09487779,0.02030668,-0.003368158,0.061771955,-0.045762498,-0.0677445,0.1502393,-0.07706346,-0.009163575,0.046028536,0.02786167,-0.03391213,0.091341734,0.13721837,0.045918964,-0.029791666,0.006565533,-0.049348306,-0.09010812,0.07578819,-0.039567977,0.05936672,-0.030577602,0.04764985,-0.0542014,-0.0068459357,0.0073597496,0.04063565,0.025562374,-0.016241532,0.03545518,0.04072604,0.0770098,0.10709743,0.031072646,0.07020394,-0.07370587,0.0015206577,0.01818578,-0.0457223,0.05681316,-0.04000158,0.023352997,-0.0068625873,0.020118454,0.017824259,-0.059169766,-0.019192519,0.035568845,0.0484728,0.018415434,-0.0104061635,-0.01261837,-0.013508142,0.08661017,-0.05476045,0.02583124,0.055944566,0.00024052346,-0.029461328,-0.017850546,-0.018853234,-0.07086765,5,-1.6142162,-9.671325,9
532,"in todays class (5/2/2025);
we started with discussion on doubt about how to improve the quality of data without increasing the data incorporating the principles of sample randomness. next we continued with the discussion about multiple linear regression recapping of previous classes.
then we moved on to the topic of polynomial regression where understood the inclusion of higher order variable creating a complex polynomial equation and performing regression analysis on the same. we then started our discussion on feature selection where we understood about forward(adding most relevant independent variables) and backward(eliminate variables with low significance) selection and their significance. 
we further dived into understanding the parametric and non parametric models which involves distinction on limiting on number of parameters to optimise the loss function. parametric methods are simples with limited parameters but hallucinate or find it difficult to cope up with complexities and tend to underfit. non parametric models like knn, random forest use the complete data capturing more intricate relationships with the rosk of overfitting. thus we need to analyse our data clearly to get the best optimisation possible.
lastly, a brief introduction on neural networks and classification problems was provided about how deep neurons are framed to carry out the complex tasks and role of activation functions to effectively achieve what we want.",-0.08157776,-0.05768316,0.0408305,0.07671899,0.07047928,0.030031716,-0.018720066,-0.029877981,-0.0004876065,0.0013915664,-0.058232736,0.019273816,-0.0007359183,0.01460589,-0.009364317,0.030390013,0.044721637,0.073288105,-0.111382864,-0.016965967,0.073208965,-0.034507398,-0.025430499,-0.03088633,0.032226387,-0.06334784,0.0089248195,-0.0018084759,-0.051580593,0.0220437,-0.04259587,0.016714808,-0.030652458,-0.0051803575,-0.08884511,0.048499353,-0.020311907,0.03817482,0.016936911,0.032419935,0.015747499,-0.03499628,-0.028852217,0.05309028,0.12527429,0.05248927,-0.025281962,-0.08006625,-0.02424993,0.026925784,-0.070944,0.02192123,-0.08261595,0.06746066,0.011586381,-0.051918734,0.00097332196,0.024333624,-0.0063521336,-0.0052982536,0.09009435,-0.101341814,-0.03804937,-0.004676954,0.029226732,-0.02539813,-0.007698472,0.0006762272,-0.0037792516,0.104020566,0.009102717,0.09713394,-0.07715383,0.002402464,0.010063663,-0.03576346,0.06617546,0.020262785,-0.06889805,-0.06664465,0.05842878,0.05348323,-0.0030064238,-0.021357846,0.07501918,0.021782428,-0.0049174866,-0.018229047,-0.04104804,0.06687311,-0.027389217,0.033167977,-0.037545368,-0.023624424,0.009241031,0.06863472,0.030283343,-0.12893061,0.023864051,0.0875889,0.025050623,0.02078336,0.05743922,-0.08716575,0.02849836,-0.03334898,0.04016447,-0.03756618,-0.027402362,-0.017528664,0.003927487,0.009551762,-0.028068637,0.007276699,0.07233892,0.014261486,0.0022223569,0.06677048,-0.01671316,0.061909974,-0.051393438,0.053261988,-0.0038016913,0.029733222,0.006421366,0.007425458,-0.05692859,7.724615e-33,-0.0061140927,0.0018980214,-0.027056385,-0.036432676,-0.009185216,-0.030697696,0.038200844,0.010752432,0.069986664,0.11342622,-0.09551185,-0.059084315,-0.05782819,0.021628717,0.0805402,-0.0028495917,-0.05846306,0.07502202,0.0042115157,0.036041074,0.045682266,-0.0677536,-0.035110068,-0.07533107,-0.010187225,0.03201758,0.04527604,-0.028912252,-0.10581405,-0.006408467,-0.011233576,0.014839897,-0.03779123,-0.0005037922,-0.011212382,-0.0044654813,-0.0055015073,-0.040815078,0.04799487,0.0055044577,-0.002848202,0.03752361,0.008501562,0.027084982,-0.008604959,-0.030696847,0.05202932,-0.0517854,-0.02908753,0.036608107,-0.04331817,-0.0381795,-0.041465927,-0.044005536,-0.08787266,0.0065925745,-0.044669155,0.0012440386,-0.064673826,0.09080718,-0.06399161,-0.07967248,-0.048344657,-0.027075486,-0.025610646,-0.00755001,-0.02143563,0.027152814,0.057564087,-0.07104906,-0.019775,-0.023777267,0.01098887,-0.10066065,0.10204754,0.07879046,0.048976425,-0.02815012,-0.0015509024,-0.066651255,-0.04344572,0.062101502,0.0033276856,-0.08473263,-0.012231618,0.02122941,0.04946349,-0.08153341,0.0030311004,-0.024418363,-0.12865482,0.085505754,-0.07545448,0.0016002582,0.048236247,-6.649707e-33,-0.06271269,0.010562083,0.04503393,0.06551045,-0.03387325,0.055874392,-0.07855324,-0.057883468,-0.020084532,-0.05929819,-0.007104579,-0.019701954,0.08397451,-0.007501417,0.022702659,0.020078445,-0.08060982,-0.0053521614,0.013734342,-0.0025006442,-0.010668689,0.16356894,-0.08192925,0.010100497,-0.070911385,0.043954857,-0.098695174,0.06375041,0.040261596,0.009437395,-0.05237808,-0.032641403,-0.0559042,0.027073823,0.02998257,0.029127259,-0.02019581,-0.03445622,-0.026656406,0.12921672,0.06923063,0.015691759,-0.06087898,-0.02476268,0.00012917937,-0.06740724,0.012969721,0.0143881515,0.08604476,0.0597835,0.056021225,0.019379795,-0.12712823,0.041727502,-0.032339975,-0.02249409,-0.025345907,-0.071218476,0.051372178,0.03198963,-0.071216345,0.004925076,0.040047538,0.059306607,-0.01815467,-0.029459914,0.03247413,-0.0008533026,0.027810697,0.014487053,-0.0813988,-0.052407183,0.057586633,-0.016377518,-0.0608939,-0.09517958,0.03672064,-0.005326495,-0.051426888,0.0546625,0.07010914,0.03334595,-0.022632057,0.032555856,0.049722675,0.08108191,0.065126434,0.044573035,0.042726852,-0.03977148,-0.0022800786,0.09688951,-0.04297046,0.052383848,0.03116099,-6.5316854e-08,0.005555098,0.0770347,0.012404674,0.041693766,0.046991166,-0.06264381,0.048811436,0.121907346,-0.08786583,0.022982469,0.07727513,-0.017042208,-0.0033720564,0.06207197,0.0718412,0.02754595,-0.007211795,0.02794966,-0.032138277,-0.044430345,0.05476035,-0.051066227,-0.08372169,-0.026339723,0.087992236,-0.08769132,-0.013765464,0.049526826,0.05818763,0.092281915,0.008473326,0.040872216,-0.011143972,-0.0006177155,0.012606712,0.090588234,0.094377495,-0.01909794,-0.03993545,0.029904038,-0.08018391,-0.00021841346,-0.04099139,0.0549545,0.026394613,-0.053702842,0.11054418,-0.0684877,0.015579354,0.009830084,0.034921676,-0.016396992,0.033538584,0.002139717,0.012858688,0.11342666,-0.07232808,0.023609726,-0.032089923,0.047276426,0.023096759,-0.04044032,-0.051012,-0.041451383,5,1.8305974,-10.527864,9
536,"we started our lecture with a doubt asked by a student. the question was how to improve the quality of results. we looked at 3 ways which can be used to do this. first is improving the sample. it can be done by either increasing the sample size or by increasing the quality of the sample. second was to improve the method in which we use multiple methods, compare them and select the best one. third one was an extension of the second point which was fine tuning the method which basically means using the method properly. we then began the theory by understanding the fact that multiple linear regression is not regression of linear parameters, but the linear combination of any parameter. we then looked at an example where the error was following some sort of sinusoidal pattern. we took multiple feature like x1 = x1, x2= x1^2, ....., x5 = sin(x1) and by intuition we know that the p values of x1,x2,x3,x4 will be high and that of x5 will be close to a zero. these feature making is called as feature engineering. there are two types of feature engineering - forward and backward. in forward feature engineering, we keep on adding feature one by one till we get a good model. in case of backward feature engineering, we add all the features once and remove irrelevant features one by one. we then started number crunching on excel and looked at models like slr, randomforest, xgboost, knn, neural networks. we looked at how the parametric models can be used for delta analysis and non parametric models like random forest can't. we then had a brief on neural networks where the dependent variable is a function of features and associated weights. neural network consists of nodes and links and they make up hidden layers. if there are more than one hidden layer, it is called as deep learning network. more layers with a small sample leads to overfit. we concluded our lecture with a small introduction to logistic regression, which involves finding boundries for classification.",-0.044934258,-0.00068034884,0.06323433,0.018343985,0.033584252,0.027698025,0.005856782,-0.0051066363,-0.008847259,0.0052904184,0.0067156493,0.070052415,0.033942793,-0.029783864,0.025487337,0.036935136,-0.01362949,0.09815322,-0.08317437,-0.0033821259,0.082144514,-0.07862471,-0.06268048,0.004061876,0.007911905,0.057450715,-0.07812671,-0.047685083,0.022618013,-0.016737366,-0.012343928,0.11918697,-0.07531371,-0.018874612,-0.0287782,-0.014797553,-0.06315371,0.0711518,-0.032942303,0.047997788,-0.013880419,-0.011162256,0.0104745515,0.014728929,0.11231952,-0.02432849,0.0019361676,-0.09420923,-0.0031199628,-0.09775089,-0.03345846,0.0363247,-0.068877846,-0.02831026,0.022929974,-0.08254911,0.0017923473,0.06073019,0.04798062,0.017352521,0.091861024,-0.002540247,-0.09211529,0.017313909,0.07512292,-0.046029393,-0.019550836,-0.071080714,-0.06900062,0.07692805,-0.05441155,0.031757474,-0.021925855,0.012381544,0.043135837,-0.0005507764,0.03406584,0.056526374,0.026870707,0.017564896,0.02859734,0.0054847775,-0.002729738,-0.075984426,0.06750284,0.024536,-0.08752022,-0.1559191,-0.049324647,0.046293806,0.059434544,-0.013483143,-0.12593699,0.05573522,0.011432387,0.010293815,0.007442818,-0.044340983,0.09269392,0.059965357,0.032256834,0.029751454,0.041750085,-0.039932813,0.047359847,-0.037603244,0.06770547,0.015158939,0.059463825,0.030320933,-0.07982243,0.011746413,-0.041099768,-0.0024080358,0.05987175,-0.028670173,-0.046542566,0.054171614,0.03686466,0.00022871673,0.024235498,-0.080488816,0.083924085,0.028773136,0.042660717,0.018803192,-0.06945141,6.0960068e-33,-0.008340325,0.010804671,-0.033503015,-0.027955607,-0.041163653,-0.022732722,-0.06033867,0.015176841,0.07596125,0.021659236,0.009500558,-0.028759588,0.0357605,0.08325673,0.06587519,0.03817617,-0.015727127,0.068820216,0.041443296,0.032186586,0.036297675,-0.06358311,0.09669757,-0.048050333,-0.064172864,-0.0150169,0.024298357,0.059547596,-0.1377566,0.01895248,0.0063616526,0.038486864,-0.057368632,0.071814224,-0.015038669,-0.027157174,0.045300476,-0.07979058,-0.010867704,0.061610214,-0.041281905,0.015156717,0.028581608,-0.0028129828,0.06850422,0.05186601,-0.01659641,0.02489567,-0.054014396,0.00023363672,-0.07611913,0.0012845262,0.023216123,-0.006861844,0.011379525,0.055222075,-0.09682154,-0.0836216,-0.025312273,0.04214906,-0.08698633,-0.0130649265,-0.05031836,-0.070080675,0.04106993,0.048721228,0.081930265,-0.002054336,0.053062674,-0.057975534,-0.014717446,-0.06130723,-0.052780397,-0.062244058,0.09082942,-0.09446356,0.010018355,0.044628337,0.03208723,-0.07495709,-0.05441217,0.07668497,-0.014698892,-0.013138363,-0.031345993,0.069172546,0.0044337176,-0.045829363,0.05741376,-0.038397197,-0.092534594,0.037578456,-0.023669833,0.01587555,0.030131683,-6.7021824e-33,-0.01628277,0.05494235,0.004300614,-0.00048783614,-0.01786029,-0.031950943,-0.032388095,-0.11809789,0.013521435,-0.058518004,-0.013200973,-0.056791168,-0.01549841,-0.033182163,-0.020354722,0.008671876,-0.024638949,-0.06594351,-0.012649902,-0.008101363,-0.01376758,0.06133377,-0.022006469,0.002701906,-0.068086155,-0.0069311066,-0.12358099,0.024166798,0.01329837,-0.016309405,0.0230874,0.016260898,0.03612255,0.026173586,0.024132732,0.101956025,-0.024132818,0.042601343,0.04435282,0.04088813,0.05476377,0.024888296,0.037856642,-0.10816237,0.014322398,0.009405203,0.039953716,0.010629979,0.01406718,0.028571859,0.06891937,-0.0018005709,-0.069452144,0.024221804,-0.03457897,-0.017278304,-0.023117548,-0.00089520524,0.049333546,-0.0061794757,-0.06510837,-0.011961001,-0.0061011035,0.039493892,0.039948385,0.04951316,0.015687803,-0.04554914,0.0077762706,0.014760551,-0.16716519,-0.060091354,-0.009247978,-0.0028714116,-0.029967263,-0.06723748,-0.003990659,-0.09548721,-0.114608,0.030735804,0.040886246,-0.043405082,0.014792172,-0.00063882256,-0.09479207,-0.010285625,0.034723587,0.040469844,0.051952563,-0.05902449,0.010074926,0.045975864,-0.048344936,-0.008400708,0.06722204,-7.379802e-08,-0.040711287,-0.0024530997,0.015220612,-0.008979547,-0.01977846,-0.03838912,-0.011822502,0.14687173,-0.025808418,-0.068144724,-0.02748672,0.09919911,0.011172277,0.032654416,0.067637436,-0.028043585,-0.105292335,0.033365577,-0.041916557,-0.0129758455,0.06526488,-0.030189743,-0.016206885,-0.079868995,0.094734035,-0.037854258,0.022234866,0.05519277,0.008558323,0.054238282,0.056898504,0.069284976,0.007581282,0.0381406,0.0765436,-0.00754753,0.08818434,-0.059632767,0.0156577,-0.012266586,-0.061384168,0.10562782,-0.020212533,0.055058304,0.115155086,-0.048175212,0.03279092,-0.08064169,-0.034933746,-0.039344314,0.04932111,0.045780968,0.009322266,0.014279881,-0.0043401746,0.017630259,-0.05264535,-0.022671197,-0.012703434,0.04210097,-0.04137284,-0.03546327,0.061190628,-0.06738532,5,-3.508868,-8.98955,9
543,"today in class, i learned about feature engineering for images,  in the context of convolutional neural networks (cnns). the preparatory steps are image processing and text processing. it was explained that simply converting an image into a long vector of pixel values is not a good approach because it ignores how pixels relate to each other. instead, we need to consider these spatial relationships when creating features for machine learning models.
i also learned about convolutions, which are mathematical operations used in image processing. they help with tasks like edge detection, feature extraction, and applying filters to highlight important parts of an image. ",-0.0061097043,-0.007108101,0.089242995,-0.02407748,0.028003003,-0.04849643,-0.01758621,-0.06789693,-0.03609398,-0.05030472,-0.05259471,0.018475063,0.0100206025,0.08372582,-0.05611408,0.010966259,-0.02222735,0.08406948,-0.0740001,-0.01916464,0.0020673533,-0.05577448,-0.06179255,-0.035901345,0.075517975,0.03780506,0.026163556,-0.09879041,0.061262783,-0.027145255,-0.010163107,0.029710349,0.017600816,0.00532612,-0.047838926,0.10689045,0.055886064,0.043018263,0.025666168,-0.047413263,0.042453207,-0.039573725,0.032807827,0.011133148,0.11297107,0.0431154,0.07324565,-0.072410166,0.037062906,-0.015962875,-0.09000746,0.024295822,-0.11636495,0.032555096,-0.0033072855,0.039245185,0.062189788,0.01823857,-0.003320133,0.07295012,0.0400296,0.007185724,-0.035476636,0.030579597,-0.0058831004,-0.12646508,-0.036716327,-0.0207592,0.034794424,-0.063476495,0.02183454,0.086464986,-0.022391478,-0.027452826,-0.03836566,0.04988737,0.011329395,0.055025306,0.060325246,-0.04518843,0.057465244,-0.0077991933,0.030307764,-0.01784901,0.11724331,0.04065898,-0.073749706,0.007396012,-0.057296395,-0.0067505995,0.040260237,-0.08025529,-0.03523715,-0.040959593,0.050379805,-0.0055105896,0.022714308,-0.12946819,-0.04491544,0.0077438387,-0.040855724,-0.06266195,0.033567343,-0.017924095,0.057715192,0.008705068,-0.0020504927,0.053598635,0.05971667,-0.061569914,0.010260085,-0.010797213,-0.02688732,-0.02575691,0.02845708,-0.03954038,-0.058665726,-0.01235689,0.11087738,-0.06490423,-0.08832395,-0.039623592,-0.086201735,-0.02125929,-0.019942094,-0.0051622037,-0.050911948,3.1057696e-33,-0.016085688,-0.00097293494,-0.03994496,0.040789254,-0.028340394,0.003847932,-0.03840719,0.01716207,0.0688327,0.0004291364,-0.02518406,-0.041230604,-0.011968869,0.14768569,0.067062676,-0.0129406545,-0.014455343,0.0011116721,0.022106882,0.0014849512,-0.035550315,-0.037264057,0.0039440487,0.087005846,-0.047242306,-0.042714924,-0.0906542,-0.010949654,-0.06266039,-0.015606235,-0.043503493,0.09177641,0.0067985724,0.08265736,-0.14502506,0.028619215,-0.012833615,-0.058489375,0.06310352,0.059351157,0.009865719,0.03977358,-0.035609305,-0.011168818,0.014476833,0.120288536,0.017179761,-0.06250423,-0.08166647,-0.0026344229,0.028081056,-0.002997913,-0.013407883,-0.07335368,0.019353498,0.035229135,0.038401876,-0.033098128,-0.0145366555,0.02137094,-0.026566248,0.044322412,-0.0037978683,0.05039783,-0.03305039,-0.07086626,0.074886404,0.07079223,-0.028436895,-0.013977322,-0.06802056,0.057134427,-0.061430268,-0.10853219,0.0049769795,-0.024171963,0.015533781,-0.04455915,0.0112329535,0.05816365,0.007910183,0.04967564,0.004274684,-0.07777729,0.011488747,0.09544709,-0.02066291,-0.11032308,0.009655353,-0.025882335,-0.04356795,0.010678096,-0.045010764,0.049034063,-0.0040208143,-4.7122935e-33,-0.030626977,0.042964388,-0.016022818,-0.002794037,-0.048219416,0.008606489,-0.0013036989,-0.026013693,0.019716615,-0.069283366,-0.08405093,0.017500501,-0.021356763,-0.06628199,-0.0985382,-0.11887089,-0.016461411,-0.10444371,-0.033606265,0.022833796,0.01739691,0.085587606,-0.003532902,-0.013102852,-0.06009405,0.006465032,-0.00080717675,-0.029101431,-0.007254227,0.030802766,-0.033746745,-0.044074703,0.011833992,-0.018489974,-0.0079481425,0.0031282792,0.024191806,-0.042645566,0.08686489,0.0050240485,0.10910012,-0.047307808,0.027142137,0.022717385,-0.038529884,-0.009115556,0.04968971,0.010892086,0.027513761,0.072361566,0.056023102,0.052833695,-0.050807737,-0.008073747,-0.0057727587,-0.040965535,-0.03461202,0.015001885,0.13206011,-0.014955885,-0.099028066,-0.09417751,-0.00064942794,0.002402838,-0.019614177,0.069721736,-0.04533847,0.07684404,-0.0146260215,0.048437424,-0.030636843,-0.041062,0.029472318,0.10615882,-0.051406942,-0.1140183,0.01057528,-0.0438224,-0.014914917,0.001582288,0.038562495,-0.010127456,-0.014600058,0.06797108,0.07815215,0.06364216,-0.0152446665,-0.04053866,0.11836755,0.011583875,-0.037763756,0.04108444,-0.030760502,-0.00908511,0.04692007,-5.275487e-08,-0.0023261274,0.00076961063,0.070354745,-0.0695917,0.007657595,-0.009098897,-0.013086553,0.15032743,0.0014152986,0.028687743,-0.018346632,0.01980441,-0.05000495,0.010532768,0.061199903,0.011801035,0.04168928,-0.0012588612,0.029170714,-0.019151159,0.0028428386,-0.034097046,0.00096985337,0.052534513,0.0094259735,-0.092592336,-0.040810578,0.003464665,0.040372036,-0.028453749,-0.018215185,0.028375853,0.04198522,0.054061417,0.12350782,0.0032170194,0.023820503,-0.09540065,-0.041379046,-0.063520566,-0.03379403,0.09317647,0.041281506,-0.027168188,-0.015739232,0.058317102,0.10122394,-0.028946035,-0.018479148,0.10741105,0.027062189,0.05523345,0.010620655,0.02585794,-0.023459941,-0.029779887,-0.01183379,-0.0049201054,0.081671424,0.04704127,-0.057535306,0.061866432,0.01848271,-0.061490934,5,6.14592,-10.17868,9
559,"we firstly learnt about polynomial regression, and learnt that it models relationships by extending linear regression with polynomial terms, while the p-value helps determine the significance of those terms in predicting the outcome. to a function similar to sin curve, and with one x1, we got the p value and stuff, and then we added sin(x1) to it, and in this case we got a different value. so it made a significant difference to the model. sometimes some features are more important than the other, it's not more important to have more features, you should have features which better explain the model. so adding new features only gives better results upto a point, then after that it's unnecessary. also, neural network was talked about in class today, and we learnt how it has layers and neurons and ties to mimic the human form of thinking. multiple layers and all are there in deep learning. so basically, a neural network is like a bunch of connected math equations that learn patterns from data, kind of like how our brain learns from experience. we also saw overfitting graphs with example, and how it looks when data is overfit. and how it's related to degress of freedom too, that more degrees means overfitting. ",-0.028028643,-0.05835434,0.021351147,0.049732104,0.07786074,0.11660235,-0.06702989,0.0269126,0.086421356,-0.032959145,-0.008487537,0.074075624,-0.033156242,-0.012282049,-0.014667148,0.0035485274,-0.009402294,0.049423892,-0.070973136,-0.03460831,0.05161954,-0.100989915,-0.029751379,-0.024654254,0.012272179,0.07704027,-0.04299502,0.012104581,-0.0030849224,0.012713492,0.057193104,0.023707304,-0.012664635,0.0044588004,-0.07959372,0.056629397,-0.042789236,0.09651508,-0.020060107,-0.041377485,0.077659026,-0.03653003,-0.021767829,0.05976472,0.11921811,0.018240055,-0.0013502778,-0.0313843,0.03578567,-0.09156135,-0.01387439,0.02657111,-0.04514983,0.011526221,0.04795728,0.02013781,-0.014703115,0.095566556,-0.1205804,-0.032847617,0.011427503,-0.0121840555,-0.009577273,0.041192167,0.11991925,-0.021233777,-0.01476151,-0.005936139,-0.005206216,0.035899736,0.07457031,0.102934204,-0.034021202,-0.03710785,0.025325323,-0.023986738,-0.0020278178,0.024196468,-0.04251245,-0.012356436,0.0711957,0.05014643,0.014017636,-0.045670606,0.045411352,-0.011078843,-0.011472713,-0.068010174,-0.10961003,0.010472883,0.049899984,-0.026503714,-0.11028326,-0.021356396,0.050345838,0.025171146,-0.051456057,-0.038209192,-0.025615266,0.019585907,-0.014590947,-0.029863711,0.06335058,-0.03038711,0.05737951,0.037262134,0.007111237,-0.036787838,0.04916262,-0.046363525,0.003376935,0.0042184517,-0.027139043,-0.06471111,0.07548598,0.0071763024,-0.043261208,0.04986268,-0.041737065,0.04974666,-0.10147242,-0.02913123,-0.03219815,0.014455139,0.008730378,0.026675023,-0.11596288,2.3324526e-33,-0.016209545,0.00015536771,-0.04055525,-0.036344305,0.049256615,-0.019396493,0.069851615,0.034814056,0.06382723,0.039587695,-0.07439945,-0.010277711,0.00059513055,0.0727717,0.03647928,-0.024249453,-0.031142063,-0.020371579,0.093149684,0.039056502,0.029098464,-0.0865064,0.035347704,0.00839649,-0.039489284,0.016972275,0.01684619,0.0823294,-0.14599283,-0.023457441,-0.021103397,0.058705688,-0.015680918,0.016086426,0.0007116154,-0.019991966,0.021797994,-0.10238026,0.074690856,-0.036561098,0.022819884,-0.041411113,-0.03246613,-0.0024761495,0.0071773487,0.029653478,-0.022758292,-0.10786066,-0.110153556,0.0028274907,-0.00813241,0.039834928,-0.040331062,-0.052216046,-0.07243058,0.017820675,-0.078601755,-0.037343785,-0.047680043,0.014917773,0.030198973,0.003600077,0.0052203313,-0.037668608,0.020569986,0.01931037,0.01526618,0.059251327,0.0427226,0.0056409477,-0.03897115,-0.015173073,-0.06711228,-0.046356045,0.076037616,0.039586123,-0.029894054,-0.07895222,0.017016254,0.02561361,-0.016700847,0.051105823,0.013400316,-0.036485896,-0.008833093,0.06454183,0.057920557,-0.024990281,0.10192808,-0.022414122,-0.034888264,0.009658252,0.006748564,0.008951608,0.039875988,-4.0458073e-33,-0.09585475,0.020335494,-0.08685414,-0.00815564,-0.058223885,0.02999331,-0.034045864,-0.025444048,-0.04356114,-0.020061912,0.04083829,-0.012959832,0.0006433207,-0.06376208,0.032994524,-0.0186744,-0.12903865,-0.01999358,0.06368461,-0.03910371,0.033390727,0.13799354,-0.109498374,0.0035008255,-0.04426613,-0.011738399,-0.090729624,-0.036668595,0.07390123,0.008416192,-0.025103645,0.010327176,-0.028007476,0.0421111,0.06779596,0.071543306,-0.0034162363,-0.10089484,0.04375723,0.07062464,0.012426396,-0.01674702,0.014528218,-0.011268525,0.04419705,-0.05308236,0.015145613,-0.01143798,0.06921243,0.061470553,0.08781975,0.011714926,-0.03484048,-0.05117185,-0.040844534,-0.056810193,-0.013580259,0.020194976,0.0623132,0.015290224,-0.054296162,-0.05990149,-0.018390667,0.06195534,-0.05152172,0.044524595,0.006420311,0.01838764,0.026582265,-0.018600257,-0.06269946,-0.03195197,0.003721298,0.04053018,-0.08934422,-0.06665386,-0.035324816,-0.0018203496,-0.06350325,0.048156057,-0.028601486,-0.035291914,-0.053943075,0.017020844,0.05203408,0.043888357,0.0052783675,0.042221893,-0.014243334,-0.047619462,-0.0022413405,0.056941565,-0.114859775,0.0391794,-0.04552083,-6.251533e-08,-0.05998596,0.023515662,0.035179008,0.015393651,0.07922531,-0.11913005,0.053636003,0.076695524,-0.05087621,0.03388772,0.032479957,0.010911156,-0.031111842,0.02435257,0.08414768,0.11403254,-0.0038371116,0.017910888,0.05625286,-0.03971344,0.06145229,-0.046069786,-0.035440676,-0.00084272085,0.10739978,-0.10616251,-0.021938132,0.07710204,-0.026613275,0.064477615,0.019676032,-0.01659776,0.005159095,0.05866894,0.111280315,0.07959391,0.057360996,-0.07848025,-0.0048471517,0.0031281833,-0.06375801,0.047692645,0.09316207,0.025904976,0.0058681797,-0.050839256,0.07066791,-0.03891349,-0.059555113,0.014196761,0.040290784,0.09713509,-0.007497195,-0.029868511,-0.022474883,0.052874766,0.0046341354,-0.045143124,-0.029774984,0.06303413,-0.099570684,-0.014744765,0.03386912,-0.07143111,5,2.866704,-6.895019,9
586,firstly we discuss the classes how much are left and and how much we have to attend the reamaining of the 10 classes to get the marks then we discuss about midsem answers and project work . then we discussed about feature engineering which is a crucial aspect of machine learning and data analysis where you transform raw data into meaningful features that can improve model performance. it involves creating new features from existing ones or selecting the most relevant features for your model.,-0.0100282105,0.0005121846,0.056791738,-0.010915005,0.07079659,-0.004502168,-0.005808514,0.026031312,-0.11316735,-0.038377225,-0.10958731,0.0011398498,-0.0020872103,-0.006483404,0.011739762,0.034227803,0.02565749,0.015130055,-0.07997278,-0.076834194,0.048912663,0.008694943,0.01536161,0.060350373,-0.05566405,0.025071636,0.081356265,-0.0044930014,-0.004468499,-0.02606844,-0.041603733,0.05701347,0.021314938,0.051541742,-0.09177128,0.031293023,0.096319824,-0.0074529997,0.023829365,-0.028695052,-0.056575235,-0.052338813,0.01700286,-0.0038613395,0.12415044,-0.013306107,-0.0041553094,-0.096431315,0.026914416,0.028749198,-0.08021712,-0.049978066,-0.13469738,-0.039750516,-0.036962826,0.033848874,0.03809824,-0.017509675,-0.020025542,-0.045872275,-0.018167984,-0.01514583,-0.05599081,0.0063549615,-0.011598817,-0.11192973,-0.028710445,0.102716856,0.031861097,0.046561677,0.017775428,0.051301077,-0.015876858,0.0018011128,0.00084015017,0.011992654,-0.008664249,0.04602204,0.048924115,-0.0004477102,-0.01447093,-0.03735543,0.013339871,0.007368237,0.07570708,-0.04347297,-0.0155389225,-0.018269366,-0.1425346,-0.047726903,0.014672884,-0.018050438,0.039044775,-0.039544716,0.015832214,0.09465241,-0.0097780675,-0.09396976,0.04773607,0.044630475,-0.08194595,0.075891316,0.007954739,-0.037076946,-0.035429515,0.010993379,0.03721867,0.03965745,0.032513034,-0.021797067,-0.0048732213,0.020381154,-0.073138505,-0.048997164,0.08827667,-0.03015969,0.016998893,0.00070667773,0.015391748,0.04074342,-0.091309026,0.006696805,0.025788411,-0.019490931,0.020039244,0.0041716797,-0.08182105,9.92475e-34,0.017831018,-0.029186904,-0.040314294,0.111694716,-0.029235793,-0.047785256,-0.034636192,0.08505589,0.054038152,-0.010055129,0.042254377,0.0108290985,0.010874328,0.08051406,0.09436955,-0.07733144,-0.02246533,0.019811612,-0.04343038,-0.019196764,-0.004985181,-0.009072846,0.116685286,0.032786056,0.028061813,0.035901148,0.023613855,-0.057802506,-0.07014374,0.02479645,0.0040033855,0.03806587,-0.06304617,0.0062704165,-0.058094848,0.04662463,0.01966574,-0.058114078,0.08599118,0.00029274492,-0.014884852,-0.03323421,-0.03286564,-0.05354919,-0.014225068,0.051086728,0.066905245,-0.04404045,0.055511113,-0.033626646,-0.04492,-0.075320296,-0.02041102,-0.043921657,-0.029191341,0.006194021,0.033271205,-0.09705933,-0.082670115,0.012805579,-0.033671558,0.019283969,0.013129434,-0.029143548,-0.0515665,0.005427004,0.035179608,0.06550718,0.06558671,-0.039528247,-0.0830695,-0.019980893,0.004118468,-0.056597933,0.031296767,0.042632967,0.0161714,-0.026262188,-0.008678158,0.06372354,0.021090768,0.019349918,0.0055105523,-0.12452481,0.06387609,0.05906733,0.06132488,-0.062246356,-0.03776232,0.012232609,-0.048269767,-0.036802113,-0.014934946,0.07305151,0.03155369,-4.046697e-33,-0.03962038,0.09209715,-0.07779874,0.08357796,0.038001567,-0.0076971073,-0.06789002,0.010923157,-0.05183253,0.021234203,-0.033360217,-0.032915473,0.058531456,0.007996991,-0.0883929,-0.011262938,-0.057847694,-0.13762265,-0.038219605,0.0462455,-0.015107357,0.1254155,-0.04112197,-0.02619966,0.029036306,-0.011880402,-0.068999596,-0.0051122727,0.056198243,-0.015394164,-0.012476368,-0.05074582,-0.020110855,-0.040342905,-0.05119571,-0.004916152,0.078035794,-0.10231958,0.04229877,0.1229182,0.14262195,-0.0618521,-0.03107171,-0.009044091,-0.016104797,-0.037168328,-0.019802898,0.06228669,-0.012696349,-0.03123608,0.05709029,-0.0008732115,0.042836268,-0.054429967,0.03235392,0.00079958484,0.036499888,-0.07289185,0.064719036,0.10043916,-0.07946027,0.03087268,0.045359015,0.032737188,0.008971813,-0.016609838,0.0018809008,0.017159931,-0.10277143,-0.042044085,-0.036823567,0.051460724,-0.02322353,0.052115545,-0.056396652,-0.05343699,0.00246189,-0.027940307,-0.0075990777,0.01016721,0.034053035,-0.04730494,-0.062045004,0.06450958,0.024163082,0.040691327,0.048850276,-0.0077426396,0.045629725,-0.089113444,-0.04804324,0.042304616,-0.03215863,0.014480251,0.0030089873,-5.0150007e-08,-0.058706466,-0.019362092,0.09560528,0.002096178,0.025912875,-0.020197824,-0.12455475,0.13225909,-0.00079808873,0.08313934,0.0027654998,0.011154694,-0.07677103,0.07557465,0.042585585,0.006741847,0.02460802,0.07032809,-0.018875057,-0.083030455,0.09211097,-0.040461015,0.047077388,0.022103814,0.04850092,-0.1006062,0.051859107,0.02495203,0.017898535,0.03586608,-0.06478554,0.015403822,-0.02494735,0.0011610717,0.110430226,0.04103944,0.075552836,-0.017635657,-0.02841781,0.027513666,-0.051160794,-0.011018937,-0.03062783,0.0654375,0.016844373,-0.011591086,-0.05218539,-0.015417928,-0.011290143,-0.0060275616,-0.048208833,0.0049072434,-0.022227827,0.055293422,-0.039821323,0.112027906,0.014711456,-0.070831485,0.008541641,-0.008459205,0.020624459,0.05848244,-0.047918603,0.03047328,1,7.889689,-8.221119,9
609,"1. we saw the methods of improving quality of results : improve the sample or method or fine tune the existing samples .
2.mlr non linear cases : using trigonometry and straight line
3. as we use more terms , adjusted r2 will decrease and at last only significant term will be there 
4.  backward and forward feature engineering 
5. we learnt that it is better to have one model rather then 2 subordinate models 
6. neural networks 
7. explanation for sigmoid function 
8. definition of logistic regression ",-0.0064994995,-0.044553723,0.08752248,0.0058938526,0.052743524,-0.034841023,-0.08788073,0.040769752,-0.07621428,0.0073037995,0.010476706,0.034113325,-0.010278661,-0.013309255,0.006868548,0.0047214455,0.012211751,0.121023715,-0.117632836,-0.03393023,0.004823675,0.02210387,0.052839518,0.04174645,-0.0006587339,-0.06194602,-0.039380655,0.0392936,-0.006923123,-0.022237012,-0.024391444,0.0807009,0.03468364,0.04704975,-0.08756446,-0.029976644,-0.057066686,-0.007464147,0.014448757,-0.003195158,-0.035378683,-0.07581807,0.021892238,-0.051206224,0.092771366,0.016043354,0.028375793,0.0035447315,-0.018277517,0.031045595,-0.1132306,0.00015220528,-0.03958421,-0.0467817,-0.055055477,-0.032174364,-0.029052103,-0.03011224,-0.044577222,-0.018460019,0.096227586,-0.013783721,-0.05704917,0.005569761,-0.011911156,-0.10012263,-0.042291578,-0.014141372,0.033807598,0.12099644,0.04456791,-0.0060915337,-0.05344813,-0.038149953,-0.014959394,-0.0015917046,0.057878383,0.041661907,0.04504445,-0.023324637,0.0123655405,0.040359028,0.025273347,0.031244615,0.026575908,-0.045394007,0.01945528,-0.08912697,-0.09188371,-0.009443338,0.03421413,0.023070477,-0.0028449465,-0.038605142,-0.0239376,-0.03235237,0.006664547,-0.08507498,0.0019515853,0.058618654,-0.028338393,0.09145267,0.04183574,-0.033231933,0.04473919,-3.107219e-06,0.10883403,0.06273148,0.04790357,-0.029037312,0.042996872,0.10604428,-0.115703836,-0.015346104,0.10793039,-0.018164841,-0.0098488815,0.020795455,-0.037561946,0.07643811,-0.04407328,-0.0041176477,-0.0064783227,-0.037408207,0.00890598,-0.010643437,-0.09494775,5.4899825e-33,-0.038972065,0.05226973,-0.0006869217,-0.033654906,-0.01258209,0.05661211,-0.08230164,-0.0057222364,0.061417144,0.02172332,-0.10519352,0.009824314,0.030624567,0.12510517,0.06791755,-0.069401704,-0.015367486,0.09633031,0.012147921,0.03318896,-0.04087355,-0.07047383,0.032290626,-0.12850198,0.025455771,0.08126264,0.07295239,0.04457555,-0.14164092,-0.002782609,0.04907663,0.025565913,-0.032760922,0.048947163,0.013187817,0.0011736257,0.01882319,-0.025654107,0.046750832,0.02384302,-0.075172186,0.063228555,0.00791061,0.019525819,0.02208883,-0.0068898494,0.0008158736,-0.051238388,-0.026502404,-0.022730777,-0.022025751,0.030231843,-0.014826175,-0.0184914,-0.02742383,0.03793194,0.024261914,-0.050383158,-0.04805891,0.012699156,0.00390932,-0.001922025,-0.0055351546,-0.027373752,0.029553575,0.046509404,0.043810636,-0.036489937,0.049142685,-0.06147544,-0.016842837,-0.019635607,0.018615562,0.02061788,0.0314949,-0.0012850306,0.0101647405,-0.03968047,0.09602038,-0.03480166,-0.011237063,0.083990745,-0.04865882,-0.068998404,0.010977168,0.050411306,0.01509623,-0.028511586,0.02038826,0.020878622,-0.092147455,0.046153437,-0.01490484,0.06580903,0.028971022,-6.56593e-33,-0.09449141,0.0615824,-0.036081847,0.0068230233,-0.071080446,0.012519566,-0.059710287,-0.08195176,0.03211814,-0.019006724,0.053672943,-0.05740277,0.018960837,-0.0043709264,0.044783946,0.014048302,-0.041897874,-0.027023558,0.015304506,0.0070680827,0.09091453,0.08108439,-0.0731116,-0.026354259,-0.040675793,-0.064049475,-0.080011986,0.052461814,-0.0011325045,-0.10811903,-0.0028717853,-0.048689295,-0.06560492,-0.014854421,0.15307462,0.06037889,0.017648274,-0.06667579,0.0036107325,0.015750665,0.04268024,0.014978545,0.058737237,-0.03173557,-0.0075015295,-0.06882665,0.02483089,-0.010483201,0.012836077,0.0835854,0.009462199,-0.068371765,-0.050922338,0.0005480365,-0.04059281,0.0256934,-0.008789192,-0.07608596,0.012289966,0.017590998,-0.052483186,-0.0139005445,0.010234015,0.052046236,-0.0032490501,-0.039075516,-0.007023429,0.089820795,0.021768266,-0.041892435,-0.08870067,-0.07467778,0.045937147,0.06657193,-0.03848775,-0.041816775,-0.05877859,-0.011578152,-0.007054236,-0.026764924,0.00937955,-0.0587209,-0.027449809,0.051588297,0.010944383,-0.012952845,0.062282678,-0.010000143,0.050671976,-0.023098571,-0.008788423,0.013507158,-0.04050741,0.02778249,-0.03964255,-5.2153275e-08,-0.017011205,-0.07359377,0.059486765,0.048547894,0.03588406,0.01622112,-0.114739604,0.12425029,-0.060561605,-0.087175585,-0.018809574,0.10025156,-0.079794705,-0.029415565,0.072460875,0.011697759,0.022992663,0.057657197,-0.0021604497,-0.033568412,0.034838926,0.0117109185,0.055065583,-0.08364242,0.04907648,0.0058839293,-0.05349171,0.033623144,-0.0953499,-0.031836,0.115399525,0.090172574,-0.018316953,0.030233135,0.07489177,0.060940277,0.076643765,-0.02435586,-0.063573815,0.014017189,-0.04617913,0.103759654,-0.04569804,0.046603598,0.0088293,-0.06828799,0.061133455,-0.08808395,-0.023473768,-0.04133814,-0.015135248,0.051205136,0.0023573865,0.027485006,0.06193921,0.052993607,0.04509071,-0.0422619,-0.03885899,0.11796803,-0.035875794,0.030705055,0.0038972674,-0.04739081,5,-2.1578608,-13.990003,9
627,"today's lecture begin with a discussion of how to increase the quality of results, there are some methods for it, one of them is by improving the sample (either by upgrading the quality of sample or by increasing the sample size). then we also came to know about what is forward(start with minimal number of features and then increase features based on requirement or according to further domain knowledge) and backward(start with large number of features and then eliminating those which are unwanted) feature engineering. also, it is not necessary that given a data set, only one model will be able to fit the data (i.e., one can fit multiple models for a given dataset based on requirement). after this, we discussed parametric(regression, classification) and non-parametric(random forest, k-nearest neighbor (knn)) models. both this models can predict values, but sometimes non-parametric model unable to find what will be the value at some delta(x). we also had a brief intro on neural networks (like what does it means, how does it works, etc. in short), and like what are links, nodes, weights, features, degrees of freedom, etc. for neural networking systems. the outcome in neural network is a function of weights and features (y_i = f (w_i, x_i)). then, we compared different models like linear regression, svm, random forest, xgboost, knn, neural network, etc.  representing same dataset with the help of r^2 values, rmse, etc., the model with higher r^2 value and lower rmse value is generally a better choice. then we moved on to classification of data (when data is of nominal or ordinal form, classification model is used; when its is of interval or ratio form, regression model is used).
 outcome in classification is called y = class. after this, we first discussed the definition of regression and then what is meant by logistic regression. in logistic regression, we are not interested in finding the best fit line or to predict values, but we are rather interested in finding the boundary between the group/clusters of values. outcome in logistic regression is known as classifiers or class labels. the number or distinct labels = number of boundary lines = number of classes one want to classify. at last, we also discussed the function used by logistic regression to classify the data (in 0/1 or left/right or up/down, etc.) which is called sigmoidal function where outcome is either zero or one based on which class your given value belongs. in order to find a sigmoidal function (s=1/(1-e^(-a))), we need to find weights(w; where a = w1x1 + w2x2 + ... + wnxn) just as we were findings regression coefficients for linear regression.",-0.046314575,-0.09539959,0.013113012,0.003165942,0.043849576,0.0072795413,-0.06255489,0.009323176,0.01716043,-0.06301718,0.0012073611,0.046331767,0.041574247,-0.037780102,-0.0025914884,-0.021818079,0.052384358,0.08283949,-0.12798999,-0.0813054,0.039173722,-0.011698559,-0.015556643,-0.005842898,-0.051369347,-0.06022746,0.005380154,-0.01996383,-0.03717539,0.02332223,-0.028197048,-0.007817356,-0.065295786,0.04427368,-0.012936532,0.015286809,-0.006995049,0.09205816,-0.0582552,0.022801327,-0.0039501167,-0.03026788,-0.016311515,-0.018058414,0.13582759,0.050392177,0.005109751,-0.039904702,-0.038732648,-0.014734417,-0.10425954,-0.0024130463,-0.09533622,0.022918262,0.031494193,-0.013672274,-0.032575276,0.027300889,-0.03839474,0.01703239,0.06895092,-0.07652919,-0.05709751,-0.008362443,0.06986793,-0.06406424,-0.04163907,0.038066864,0.039142303,0.00024535495,0.022305457,0.0084810145,0.007663528,0.027849166,0.0031033629,-0.005803794,0.122393765,0.13892461,0.016624339,-0.05482995,-0.022772139,0.018553022,0.008487742,-0.003118705,0.029914891,0.030902786,-0.055093985,-0.033856552,-0.07976441,-0.004350099,0.04736693,0.013174074,-0.09014334,-0.05680107,0.03930769,0.05473672,0.04840366,-0.096470475,-0.028089877,0.09574093,0.00186058,0.09814535,0.07260357,-0.06734192,0.10646946,0.023677235,0.017461067,0.0065235873,0.06866237,-0.032035474,0.014773126,0.041766904,-0.080247,-0.068505265,0.038062993,-0.056756683,-0.015409153,-0.044250514,-0.06705891,0.0370949,-0.14512728,-0.02113894,-0.021824738,-0.02905634,0.038119856,0.063165575,-0.08587872,2.65012e-33,0.013489315,0.010242563,-0.009145508,-0.032908775,-0.031582113,-0.04264066,-0.008506534,-0.0005474707,0.104936205,0.03712535,-0.053194918,0.01839114,-0.010445168,0.033124197,0.12268862,-0.0031714253,-0.06806973,0.0024099264,0.042164132,0.040637594,0.06500443,-0.09615406,0.022545405,0.011853242,-0.029758176,0.015083015,0.027766075,0.010367727,-0.116504945,0.012517248,-0.04302515,0.049964357,0.020144345,0.018299894,0.0047024232,-0.022508608,-0.029999984,-0.045175713,0.008671666,-0.010143312,-0.02700318,0.007987715,-0.013734598,0.038368177,-0.022442004,0.01146412,0.080220945,-0.09991162,-0.05249991,-0.05123417,0.0037459151,-0.033956405,-0.033672307,-0.05690325,-0.0425218,0.038822982,0.0056728628,-0.07264432,-0.0017065034,0.05461054,-0.026122913,-0.067944415,-0.030313347,0.029609578,0.05249812,0.003686964,0.005532282,0.031292323,0.072842434,-0.09738066,-0.018684397,-0.025593836,-0.050466806,-0.05392544,0.05178083,-0.028031938,0.0067755054,-0.005712255,0.020576239,0.04600569,-0.0264402,0.020949136,-0.0071455683,-0.049946044,0.05318275,0.03261325,0.014701812,-0.042612534,0.0259273,-0.074269235,-0.07512127,0.107159406,-0.07233391,0.0136979055,0.033519596,-3.240961e-33,-0.044091105,0.050641622,-0.066654384,0.08960243,0.0071368064,0.03429122,-0.082093924,-0.016164107,0.012900219,-0.091289535,-0.050529853,-0.039967425,0.17062126,-0.03982181,-0.030218799,0.029033188,-0.08199062,-0.029380824,0.009007621,0.035653565,3.8455153e-05,0.07173642,-0.10683685,0.000113548944,-0.05750813,0.031858895,-0.14402291,0.09762743,-0.029452888,0.002888204,0.06883242,-0.03274269,-0.00011045676,0.018958773,0.060840376,0.035744093,0.05827662,-0.0264095,-1.0760909e-05,0.05813993,0.042046066,0.010013344,-0.004489328,-0.05858425,-0.05079614,0.014400188,-0.022483597,0.002020053,-0.03861697,-0.0579891,0.10672381,-0.0031254904,-0.030855693,-0.017742379,0.0011321465,0.013925653,-0.05019752,0.015971038,0.13338676,0.056628056,-0.03481358,-0.039872896,-0.014928506,0.04780063,-0.011772507,0.007176793,0.03393748,0.05146688,-0.015183499,0.006983745,-0.097108915,-0.0125951525,0.06734963,-0.016957618,-0.08815234,-0.09932474,0.014055945,-0.04705178,-0.048376955,-0.045335293,-0.003438426,0.00601237,-0.028609836,0.05827887,0.07346341,0.10615433,0.080281466,0.04330792,0.020665333,-0.024204742,-0.012738386,0.058647975,-0.07282547,0.091323316,-0.008735274,-6.6469354e-08,-0.031646717,0.05533735,0.008493791,-0.010139135,0.015983481,-0.05496091,0.03315802,0.14220732,-0.039387155,-0.007700595,0.023701305,0.025312733,-0.04045737,0.06992882,0.08912803,0.028179994,0.021682605,0.014605291,-0.02521199,-0.011393099,0.05034016,-0.039390765,0.031146765,-0.02379603,0.12970024,-0.06610617,0.034617495,-0.0023736276,0.0025353595,0.010109641,0.0025567387,0.017314183,0.055867497,0.036853522,0.046127737,0.08034387,0.050638247,-0.02191676,-0.08380645,-0.021077745,-0.03318193,0.08297106,-0.045070406,0.05011712,-0.019573882,-0.072224826,0.10631473,-0.010159394,-0.009348661,0.02310876,0.028843436,0.03683835,-0.014012045,-0.0011105029,0.011667883,0.06695476,0.0049036597,0.0032455232,0.0031908855,0.035341546,0.0120107215,-0.039495148,-0.021293283,0.00074861496,5,3.5067985,-11.086738,9
638,"the class started with a recap of what had been done in the past few classes, specifically doubts regarding p-value. p-values can be used to reject those input variables that don't contribute significant enough to the output. then it was discussed how the summary data that we submit is being analyzed. a similarity heatmap was shown indicating how similar two submissions were. basically, every submission can be considered as a vector in a higher dimensional space. two similar submissions will be close to each other in that space and this idea was used to create that heatmap. this was really amazing to know. it was discussed that even if a student tries to slightly modify another submission, he leaves out trails of it and that can be identified easily using modern ai tools. a little discussion on feature engineering was done. it is making up new features from the existing features to reduce better model the function f. then we started discussing mlr. we discussed that all problems may not have closed form solutions and we use a method called gradient descent to numerically compute the optimal parameters. then it was discussed how quantities like mse, mae, f-score don't give much information about the model and are rather mostly used in comparing two models. finally it was shown how removing those independent variables with higher values of p, lead finally to a model with higher f-score and comparison metric.",-0.081338055,-0.007721724,0.028222792,-0.007202676,0.09102336,0.0046711517,-0.05614711,0.012148157,-0.026015155,0.0006727303,-0.029227143,0.047041148,0.043456547,0.0038707114,-0.029014423,-0.010615605,0.025834773,-0.00371697,-0.09364016,-0.04785578,0.020266393,0.022292664,-0.030551616,0.0616479,-0.023981916,0.07220324,0.056791864,-0.016963365,-0.027158085,-0.0358297,-0.014521544,0.05084719,-0.02236701,0.024080032,-0.054272667,0.039416745,-0.022211486,0.091754906,0.0013120265,-0.086201385,-0.07865958,-0.10783646,0.0091883,-0.041087665,0.09601763,0.036017377,-0.055573437,-0.1077014,-0.041093446,0.009991144,-0.103097394,-0.012587264,-0.10035742,-0.075440034,-0.04660932,-0.0041033747,0.05639628,-0.010088265,-0.012220521,-0.06690389,-0.05687522,-0.015641851,0.007574477,0.027393324,0.03809982,-0.050292715,-0.01130454,0.006514433,0.05660244,0.011396969,0.020329593,0.11165705,-0.041099515,-0.0105004255,0.022765683,0.022337746,-0.043677576,0.09370433,0.047319006,-0.010017331,0.050577823,-0.024503961,-0.018226445,0.016346766,0.026647633,-0.04948875,0.034190673,-0.06260422,0.030784454,-0.04146955,0.028741864,-0.03126887,-0.0065400815,0.024540668,0.067170285,0.012503189,-0.02452825,-0.063192174,0.041146033,0.054471895,-0.08473596,0.07811535,-0.013758949,-0.12417648,0.01612757,-0.0379244,0.05277758,-0.089380704,0.081812285,-0.06982078,-0.004243468,-0.012664796,-0.057133753,-0.033724397,0.11485298,-0.040385347,0.044994798,0.049020924,-0.045145787,0.0937248,0.0034572748,0.008331653,0.017995497,0.024370585,0.048786826,-0.0053315507,-0.15294723,6.392811e-33,0.06593847,0.055948634,-0.0441232,0.107889995,0.047294024,-0.0034105566,0.019315287,0.05125526,0.064272664,0.0075437473,-0.030689072,0.04720884,0.02162369,0.10086233,0.120351575,-0.010311391,-0.07678334,0.01614337,-0.08195818,0.0026377684,0.03698412,-0.005500599,0.080047294,-0.06854814,-0.0124809975,0.08258748,-0.026726982,-0.016751463,-0.034503076,-0.010948134,-0.029941382,-0.006128489,-0.040639736,0.056345087,-0.03181164,0.04488216,0.0054903417,-0.106026866,0.027049243,-0.026350362,0.015861511,-0.0069559165,-0.05589772,-0.0036252202,-0.030123577,0.052505452,0.00087034545,-0.059972018,-0.0407559,-0.076003,1.3054962e-05,0.00011287352,-0.04108441,-0.05805226,-0.053117972,-0.010828878,-0.037043877,-0.056142177,-0.05654977,0.014728968,0.045696523,0.07541551,-0.036127012,-0.014686731,-0.06567972,0.018566038,0.0442843,0.07497947,0.097205766,-0.053589094,-0.040277537,0.0067268093,-0.12401463,-0.13842395,0.053384718,-0.05512268,0.04750832,-0.06189489,0.03075767,-0.0085322345,0.0005608341,-0.070126295,-0.021855354,-0.063064694,-0.0137782395,-0.068033375,0.030882774,-0.005729568,0.047246747,0.026117658,-0.043467663,-0.009386588,-0.04481499,0.07331017,0.060342588,-7.451578e-33,-0.10640635,-0.0027027915,-0.079119526,0.09430395,-0.028708562,-0.043907627,-0.027756222,-0.05759951,-0.002757375,-0.0388776,-0.06264767,-0.07050516,0.004070898,-0.0581358,-0.028787978,-0.047206882,-0.030247493,0.0148219345,0.004983598,0.047119908,0.08276571,0.14235856,-0.045306053,-0.023775795,-0.040390294,0.0083643915,0.03032962,-0.032514922,0.007030147,0.00041751462,0.033578802,-0.026840478,-0.03389384,0.0049065156,-0.02761492,0.043578573,0.05099115,-0.0460692,0.021993775,0.13630602,0.07182453,-0.02857375,-0.029319856,-0.016814744,0.0102416165,-0.001059454,0.0005411395,0.016008032,0.098147586,-0.021795813,0.051952872,-0.02652868,-0.052494913,0.016288614,-0.027475195,-0.025672225,0.011781143,-0.07169442,0.018153755,0.13420525,-0.024240952,-0.030475307,0.043126468,0.035994016,0.008432753,0.019747216,0.0060598194,0.057329368,-0.047312763,0.06222418,-0.0031354788,-0.023610143,0.018634211,0.050373282,0.023053404,-0.0048749372,-0.015238561,-0.025599536,-0.04896743,-0.057936884,-0.00070656236,-0.07255702,-0.009297754,0.035349354,0.02519015,0.018775024,0.04606006,-0.007915559,0.024398128,-0.00055885443,-0.033250503,0.10134241,-0.0021500674,0.027266681,-0.056689117,-7.56425e-08,-0.06143237,-0.023813361,0.015737161,0.05121666,0.04591055,0.01725528,-0.059615295,0.05801054,-0.005808116,0.058279637,0.04684012,0.01146725,-0.027323669,-0.010068242,0.08855975,0.0364256,-0.0057036164,0.032097522,-0.020702211,-0.027928315,0.10071288,-0.044877365,-0.024995752,-0.033786185,0.021120533,-0.10202841,0.032870326,0.090687744,-0.032212883,0.028719636,-0.04642484,0.040034216,0.062169258,-0.0025154063,0.113801464,0.03967208,0.11109235,-0.010849817,-0.03552869,0.06306968,-0.064081736,0.042662695,-0.01304157,0.012953985,0.11830884,0.038739778,-0.024122147,-0.13293914,0.019926103,0.010260941,0.031073097,0.048611395,-0.024644934,0.07267246,0.044721045,0.06642753,-0.035509277,-0.026754208,-0.0066874474,-0.0144505985,0.016602695,0.034786038,-0.0025220267,0.04100575,1,7.224587,-7.432812,9
