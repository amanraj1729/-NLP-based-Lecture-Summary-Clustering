SerialNo,Session_Summary,BERT_Feature_0,BERT_Feature_1,BERT_Feature_2,BERT_Feature_3,BERT_Feature_4,BERT_Feature_5,BERT_Feature_6,BERT_Feature_7,BERT_Feature_8,BERT_Feature_9,BERT_Feature_10,BERT_Feature_11,BERT_Feature_12,BERT_Feature_13,BERT_Feature_14,BERT_Feature_15,BERT_Feature_16,BERT_Feature_17,BERT_Feature_18,BERT_Feature_19,BERT_Feature_20,BERT_Feature_21,BERT_Feature_22,BERT_Feature_23,BERT_Feature_24,BERT_Feature_25,BERT_Feature_26,BERT_Feature_27,BERT_Feature_28,BERT_Feature_29,BERT_Feature_30,BERT_Feature_31,BERT_Feature_32,BERT_Feature_33,BERT_Feature_34,BERT_Feature_35,BERT_Feature_36,BERT_Feature_37,BERT_Feature_38,BERT_Feature_39,BERT_Feature_40,BERT_Feature_41,BERT_Feature_42,BERT_Feature_43,BERT_Feature_44,BERT_Feature_45,BERT_Feature_46,BERT_Feature_47,BERT_Feature_48,BERT_Feature_49,BERT_Feature_50,BERT_Feature_51,BERT_Feature_52,BERT_Feature_53,BERT_Feature_54,BERT_Feature_55,BERT_Feature_56,BERT_Feature_57,BERT_Feature_58,BERT_Feature_59,BERT_Feature_60,BERT_Feature_61,BERT_Feature_62,BERT_Feature_63,BERT_Feature_64,BERT_Feature_65,BERT_Feature_66,BERT_Feature_67,BERT_Feature_68,BERT_Feature_69,BERT_Feature_70,BERT_Feature_71,BERT_Feature_72,BERT_Feature_73,BERT_Feature_74,BERT_Feature_75,BERT_Feature_76,BERT_Feature_77,BERT_Feature_78,BERT_Feature_79,BERT_Feature_80,BERT_Feature_81,BERT_Feature_82,BERT_Feature_83,BERT_Feature_84,BERT_Feature_85,BERT_Feature_86,BERT_Feature_87,BERT_Feature_88,BERT_Feature_89,BERT_Feature_90,BERT_Feature_91,BERT_Feature_92,BERT_Feature_93,BERT_Feature_94,BERT_Feature_95,BERT_Feature_96,BERT_Feature_97,BERT_Feature_98,BERT_Feature_99,BERT_Feature_100,BERT_Feature_101,BERT_Feature_102,BERT_Feature_103,BERT_Feature_104,BERT_Feature_105,BERT_Feature_106,BERT_Feature_107,BERT_Feature_108,BERT_Feature_109,BERT_Feature_110,BERT_Feature_111,BERT_Feature_112,BERT_Feature_113,BERT_Feature_114,BERT_Feature_115,BERT_Feature_116,BERT_Feature_117,BERT_Feature_118,BERT_Feature_119,BERT_Feature_120,BERT_Feature_121,BERT_Feature_122,BERT_Feature_123,BERT_Feature_124,BERT_Feature_125,BERT_Feature_126,BERT_Feature_127,BERT_Feature_128,BERT_Feature_129,BERT_Feature_130,BERT_Feature_131,BERT_Feature_132,BERT_Feature_133,BERT_Feature_134,BERT_Feature_135,BERT_Feature_136,BERT_Feature_137,BERT_Feature_138,BERT_Feature_139,BERT_Feature_140,BERT_Feature_141,BERT_Feature_142,BERT_Feature_143,BERT_Feature_144,BERT_Feature_145,BERT_Feature_146,BERT_Feature_147,BERT_Feature_148,BERT_Feature_149,BERT_Feature_150,BERT_Feature_151,BERT_Feature_152,BERT_Feature_153,BERT_Feature_154,BERT_Feature_155,BERT_Feature_156,BERT_Feature_157,BERT_Feature_158,BERT_Feature_159,BERT_Feature_160,BERT_Feature_161,BERT_Feature_162,BERT_Feature_163,BERT_Feature_164,BERT_Feature_165,BERT_Feature_166,BERT_Feature_167,BERT_Feature_168,BERT_Feature_169,BERT_Feature_170,BERT_Feature_171,BERT_Feature_172,BERT_Feature_173,BERT_Feature_174,BERT_Feature_175,BERT_Feature_176,BERT_Feature_177,BERT_Feature_178,BERT_Feature_179,BERT_Feature_180,BERT_Feature_181,BERT_Feature_182,BERT_Feature_183,BERT_Feature_184,BERT_Feature_185,BERT_Feature_186,BERT_Feature_187,BERT_Feature_188,BERT_Feature_189,BERT_Feature_190,BERT_Feature_191,BERT_Feature_192,BERT_Feature_193,BERT_Feature_194,BERT_Feature_195,BERT_Feature_196,BERT_Feature_197,BERT_Feature_198,BERT_Feature_199,BERT_Feature_200,BERT_Feature_201,BERT_Feature_202,BERT_Feature_203,BERT_Feature_204,BERT_Feature_205,BERT_Feature_206,BERT_Feature_207,BERT_Feature_208,BERT_Feature_209,BERT_Feature_210,BERT_Feature_211,BERT_Feature_212,BERT_Feature_213,BERT_Feature_214,BERT_Feature_215,BERT_Feature_216,BERT_Feature_217,BERT_Feature_218,BERT_Feature_219,BERT_Feature_220,BERT_Feature_221,BERT_Feature_222,BERT_Feature_223,BERT_Feature_224,BERT_Feature_225,BERT_Feature_226,BERT_Feature_227,BERT_Feature_228,BERT_Feature_229,BERT_Feature_230,BERT_Feature_231,BERT_Feature_232,BERT_Feature_233,BERT_Feature_234,BERT_Feature_235,BERT_Feature_236,BERT_Feature_237,BERT_Feature_238,BERT_Feature_239,BERT_Feature_240,BERT_Feature_241,BERT_Feature_242,BERT_Feature_243,BERT_Feature_244,BERT_Feature_245,BERT_Feature_246,BERT_Feature_247,BERT_Feature_248,BERT_Feature_249,BERT_Feature_250,BERT_Feature_251,BERT_Feature_252,BERT_Feature_253,BERT_Feature_254,BERT_Feature_255,BERT_Feature_256,BERT_Feature_257,BERT_Feature_258,BERT_Feature_259,BERT_Feature_260,BERT_Feature_261,BERT_Feature_262,BERT_Feature_263,BERT_Feature_264,BERT_Feature_265,BERT_Feature_266,BERT_Feature_267,BERT_Feature_268,BERT_Feature_269,BERT_Feature_270,BERT_Feature_271,BERT_Feature_272,BERT_Feature_273,BERT_Feature_274,BERT_Feature_275,BERT_Feature_276,BERT_Feature_277,BERT_Feature_278,BERT_Feature_279,BERT_Feature_280,BERT_Feature_281,BERT_Feature_282,BERT_Feature_283,BERT_Feature_284,BERT_Feature_285,BERT_Feature_286,BERT_Feature_287,BERT_Feature_288,BERT_Feature_289,BERT_Feature_290,BERT_Feature_291,BERT_Feature_292,BERT_Feature_293,BERT_Feature_294,BERT_Feature_295,BERT_Feature_296,BERT_Feature_297,BERT_Feature_298,BERT_Feature_299,BERT_Feature_300,BERT_Feature_301,BERT_Feature_302,BERT_Feature_303,BERT_Feature_304,BERT_Feature_305,BERT_Feature_306,BERT_Feature_307,BERT_Feature_308,BERT_Feature_309,BERT_Feature_310,BERT_Feature_311,BERT_Feature_312,BERT_Feature_313,BERT_Feature_314,BERT_Feature_315,BERT_Feature_316,BERT_Feature_317,BERT_Feature_318,BERT_Feature_319,BERT_Feature_320,BERT_Feature_321,BERT_Feature_322,BERT_Feature_323,BERT_Feature_324,BERT_Feature_325,BERT_Feature_326,BERT_Feature_327,BERT_Feature_328,BERT_Feature_329,BERT_Feature_330,BERT_Feature_331,BERT_Feature_332,BERT_Feature_333,BERT_Feature_334,BERT_Feature_335,BERT_Feature_336,BERT_Feature_337,BERT_Feature_338,BERT_Feature_339,BERT_Feature_340,BERT_Feature_341,BERT_Feature_342,BERT_Feature_343,BERT_Feature_344,BERT_Feature_345,BERT_Feature_346,BERT_Feature_347,BERT_Feature_348,BERT_Feature_349,BERT_Feature_350,BERT_Feature_351,BERT_Feature_352,BERT_Feature_353,BERT_Feature_354,BERT_Feature_355,BERT_Feature_356,BERT_Feature_357,BERT_Feature_358,BERT_Feature_359,BERT_Feature_360,BERT_Feature_361,BERT_Feature_362,BERT_Feature_363,BERT_Feature_364,BERT_Feature_365,BERT_Feature_366,BERT_Feature_367,BERT_Feature_368,BERT_Feature_369,BERT_Feature_370,BERT_Feature_371,BERT_Feature_372,BERT_Feature_373,BERT_Feature_374,BERT_Feature_375,BERT_Feature_376,BERT_Feature_377,BERT_Feature_378,BERT_Feature_379,BERT_Feature_380,BERT_Feature_381,BERT_Feature_382,BERT_Feature_383,kmeans_cluster,TSNE_1,TSNE_2,agglo_cluster
73,"we learnt about the closed form solution in multiple linear regression but found that it is not feasible as we have to deal with matrix inversion and multi collinearity. also during selection of features if p value is greater than 0.05 we can ignore that feature as it is not impacting much. then we learnt about train and test data today . normally we use 80-20 %. we are advised to never ever use 100% test data as that will not benefit.we also saw if râ² values of training and testing matrixes are close enough then overfit can occur. we also saw what is adjusted râ², multiple r and why is there only n-1 degree of freedom in total sum of squares when there are n observations because the formula is such that one degree of freedom is already used.",-0.004306506,0.016209539,-0.023553228,0.08259245,0.14334711,0.05814395,-0.054854788,0.034595717,0.027395584,0.09356728,0.005396312,0.10461949,-0.08176874,0.026446544,-0.008789167,0.021888653,-0.019030519,-0.0038473306,-0.051758856,-0.013470103,-0.028238581,-0.08189935,-0.02048697,0.020066077,-0.011894769,0.01497412,0.013865237,-0.041637883,-0.0012562455,0.030027853,0.035842538,0.063894205,0.00047691312,-0.06644788,-0.09838618,-0.024902746,-0.01257057,0.03140326,-0.01237,0.0495732,0.029639829,-0.030018426,0.010737364,0.050569557,0.1102365,0.0006488621,-0.04987418,-0.06145095,0.062321823,-0.05516107,-0.022699248,0.018008478,-0.013704554,-0.036661927,-0.02397851,-0.191591,-0.06437735,-0.03551129,0.0037010834,0.023243127,0.036530823,-0.057283018,-0.07836497,-0.04102602,0.06405452,0.04029794,-0.09153578,-0.030748675,-0.053912237,0.06322763,-0.09794964,0.086051635,-0.024778021,-0.04652497,0.03639879,0.024400482,0.0038309903,-0.042762283,0.02396954,0.07675538,0.08793502,0.021358483,-0.024324393,-0.09022328,0.03458078,0.048700128,0.020972235,-0.017148023,-0.003967753,-0.0009312349,0.058690708,-0.007456097,-0.020412333,0.02786839,-0.052035414,0.021109661,-0.014374049,0.033818528,-0.016314229,0.007525445,0.039108295,0.00653243,0.043995384,0.050186418,-0.042905495,0.009923952,0.07207791,-0.039159305,0.07880599,-0.019200476,-0.022738889,-0.032701522,-0.019458443,0.0240902,-0.0140337385,0.018907532,-0.091466635,0.036346585,0.014126044,0.02583047,-0.0073524527,0.00861363,0.073804565,-0.01437016,0.061057035,0.015267333,-0.12984587,1.0189759e-32,0.012402503,0.047042344,-0.008324833,0.058672342,-0.009387241,-0.026980836,-0.01264679,0.028001918,0.08542153,0.056729726,0.04551265,-0.04745021,0.004018238,0.017321737,0.034130543,0.07176756,0.01854112,0.111517124,0.030092705,0.017592812,0.093141876,-0.09649758,0.02603931,-0.078370415,-0.039206345,-0.044000495,0.010988208,0.018832102,-0.030966908,0.0035661187,-0.023712398,0.08990196,-0.06101773,0.060495634,-0.020418325,0.015140911,-0.050697025,-0.036619112,0.022165133,0.03951647,-0.020446891,0.04511908,0.060460776,-0.03508476,0.076106325,0.0072784415,0.0099312,-0.03935772,-0.113364756,0.028280327,-0.08446457,0.024132414,-0.13823934,-0.015972791,0.0077076093,0.102519624,-0.076248236,-0.0757357,0.004308509,0.050940245,-0.082197316,-0.06284754,-0.010806271,-0.06895345,-0.04215432,0.021844743,-0.0028252543,-0.034160726,0.042704567,0.10701974,-0.031175219,-0.07628392,-0.07611771,-0.034611158,0.07147607,-0.03171014,0.16362835,0.054172914,0.08442281,-0.06216547,0.012453726,0.04333049,0.063552044,-0.099314176,0.046803348,-0.06762454,-0.030185217,-0.09521364,-0.04339729,-0.0056915483,-0.061825715,0.013218379,-0.07461211,-0.056425,0.009048949,-8.919164e-33,-0.054150045,0.037137065,-0.06676922,-0.0013044289,-0.042943746,-0.058068447,0.039348796,-0.031106748,-0.029589714,-0.07625784,0.07805639,-0.070771225,0.039090402,0.0019562652,0.05287407,0.0054980256,-0.034972038,0.0011272694,0.0067088855,-0.03080573,-0.08409719,0.06770163,0.076548085,0.02342439,-0.067729145,-0.047683127,-0.14973094,-0.0066273515,0.038248755,0.019247027,-0.045813892,0.0047144787,-0.046302203,0.005779297,-0.02369298,0.00300225,-0.036237158,-0.003418228,0.054480985,0.092181236,0.06546435,0.051748388,-0.061525375,-0.010549094,-0.009274167,-0.0018746536,0.04952412,-0.054926213,0.090579525,-0.040089425,-0.0077427723,-0.05691264,0.033430837,0.015230054,0.029008036,-0.006848535,0.017980162,0.05966053,0.0043203356,-0.02195458,0.018605698,0.061481945,-0.026820818,0.08213099,0.040677786,0.058323856,0.02858456,0.074355535,-0.020299897,0.05608973,-0.07330756,-0.06523718,0.03458733,0.009997836,-0.03025508,0.025844784,-0.04192134,-0.050421886,-0.032951817,0.05138313,-0.04279882,0.0074047134,-0.020105908,-0.0029927546,-0.010136613,0.030664224,0.07271085,0.020919949,-0.0050265742,-0.027997714,-0.025243808,0.050560474,-0.031893253,0.006203763,0.06593735,-6.294575e-08,-0.12314695,-0.015072657,0.035994016,-0.07124425,-0.0122452965,-0.048140272,-0.009350114,0.07908215,-0.044963576,0.07669533,-0.03852451,0.013757674,-0.07101517,0.03384146,-0.019657413,0.051516436,-0.046246424,0.050979737,-0.0064712334,0.028437579,0.031145096,-0.020844508,0.0064208587,-0.028062144,0.047869068,0.009357129,0.050489552,0.04569451,0.04840724,0.06804218,0.030563382,-0.07269351,0.04724907,0.010766491,0.06579963,0.056629002,0.048213016,-0.042083673,-0.0069771735,0.004995811,-0.028322175,0.052756492,0.03141096,0.011731154,0.056244925,0.058798168,-0.017418869,0.010313167,-0.09093091,-0.093509674,0.008727412,0.008824781,-0.011140378,0.03345031,0.08664533,0.030006472,-0.057383794,0.0053629293,-0.032338217,-0.007759863,-0.03481866,-0.03924826,-0.022623118,-0.042945046,8,-1.5672781,3.5670815,5
87,"in this session, we learned about multiple linear regression (mlr) and how it works in real-world scenarios. while there is a mathematical formula to solve mlr, it involves complex calculations and isnâ€™t always practical. a key challenge in mlr is multi-collinearity, where some input variables are too closely related, which can make predictions less reliable.

to build a good model, we donâ€™t use the entire dataset for training. instead, we split it into 80% for training and 20% for testing. this helps us check if the model can make accurate predictions on new, unseen data. we also talked about overfitting, which happens when a model performs well on training data but fails to predict new data correctly.

we explored key performance measures like multiple r (which shows how strongly the inputs and output are related) and adjusted r2 (which checks if adding more variables actually improves the model). we also discussed why, in statistics, we divide by n-1 instead of n when calculating standard deviation.

linear regression is a parametric method, meaning it works based on fixed formulas and p-values. in contrast, non-parametric methods rely on different performance measures like r2 and mean squared error (mse). we also touched on data drift, which happens when data patterns change over time, making old models less effective.

finally, we implemented mlr in python using the ordinary least squares (ols) method, giving us hands-on experience with how regression models are built and analyzed.

",-0.06710755,-0.09124668,-0.02190821,0.05610021,0.10140239,-0.025543679,-0.0850305,-0.026780901,0.029993933,-0.014298729,-0.032677174,0.052817464,-0.0048308447,0.017929602,-0.0054851435,-0.018252337,-0.009137903,0.07767441,-0.1278306,-0.08741001,0.027520249,-0.03465511,-0.010108603,0.06355855,0.0064195613,-0.014552086,-0.050120607,-0.08208141,-0.0844979,-0.019198935,0.08267883,-0.03722432,-0.01591746,-0.0052776253,-0.07187077,-0.032379977,0.0069380025,0.06829904,0.06993862,0.017373648,-0.054757256,-0.066176444,0.05969478,0.006858902,0.050559014,-0.0135956695,-0.00056346133,-0.08900706,0.011583552,0.026514063,-0.12122891,-0.049171716,0.009452088,-0.022456625,0.0027570117,-0.0479883,-0.049248643,0.046303026,0.030145684,-0.026439926,-0.011175124,-0.041602172,-0.0261044,-0.0057941964,-0.008880832,-0.010250332,-0.07520449,0.00961898,-0.07304748,0.084201686,-0.031932056,0.06882094,-0.07337887,-0.03684741,0.040480535,0.072928645,0.0123830885,0.028583353,0.017235287,0.08013879,0.04379594,0.05454031,-0.010558777,-0.057757128,0.087739095,-0.009981783,0.009176279,-0.057098195,0.0034143555,-0.011026481,0.048429128,-0.017108202,-0.015423353,0.028067725,-0.0013213851,0.071833834,0.026195124,-0.052983318,0.024524359,0.041969106,-0.01740891,0.085723676,0.023330152,-0.039546087,0.08623167,-0.014014434,0.072621316,-0.027081368,0.038288776,-0.062466525,0.057115436,-0.005130254,-0.046931837,-0.0017293843,0.058564015,-0.026830863,-0.052341275,0.039079797,-8.9033936e-05,0.10271807,-0.041331965,0.038331646,0.10771271,0.002701552,0.08673629,8.010915e-06,-0.12777478,7.793573e-33,-0.060719877,0.055613372,0.010924141,0.06796206,-0.03819037,-0.08850579,-0.026665036,0.041216873,0.07758083,-0.019672018,0.004668995,-0.03997028,-0.043127365,0.07566328,0.08153635,0.094452985,-0.03224988,-0.020442111,0.015957164,0.040295307,0.025565209,-0.10808218,-0.022767974,-0.08978633,-0.0010841045,0.03424348,0.07107297,0.014597135,-0.077938825,0.00061094266,-0.008140642,0.054889392,0.008883486,0.120504685,-0.012584682,-0.047469396,-0.024821892,-0.042154737,0.045337092,0.06769608,-0.068128884,0.01967351,0.007996631,0.012357637,0.011784156,0.0058586253,-0.00819714,-0.023031758,-0.11231227,0.0030553385,-0.036200695,0.016974233,-0.06955056,0.026379585,-0.09238477,0.08015407,-0.053017892,-0.07588724,-0.014044077,0.039580233,-0.04985538,-0.03787744,-0.0072088246,-0.0007930741,-0.044256672,-0.014754298,0.031004256,0.055003855,0.066961944,0.00830809,-0.022520142,-0.07884628,-0.020475365,-0.039342478,0.117125295,-0.025556404,0.081730895,-0.014115481,0.054205626,-0.0012185962,0.027918646,0.08334652,0.06838523,-0.071066566,-0.049289607,-0.019879099,-0.017520322,-0.057772256,-0.027122762,0.011364419,-0.12129164,-0.0050967783,-0.06754488,-0.034131773,0.022906536,-6.5028024e-33,-0.010331004,0.009558018,0.034952402,0.09498521,-0.043053858,-0.090751715,-0.017015804,-0.10836269,0.0039602453,-0.08222384,0.020713331,-0.03226584,0.08076867,0.06589945,0.024108786,-0.048595227,-0.0033041877,0.041161727,-0.004831486,-0.010117898,-0.00023361188,0.022536404,-0.028614927,-0.022063764,-0.05998498,0.03288462,-0.10309737,0.043623306,-0.032988448,-0.038007718,0.0027958083,0.011136323,0.048661467,-0.03858533,0.02088753,0.06940401,0.0108481,-0.02577878,0.03574483,0.08809737,0.08619194,0.0068565947,-0.015446823,-0.036438458,-0.0004583846,0.00396901,0.004963151,-0.0070476914,0.037892874,0.03729705,0.007492439,-0.005869588,-0.06920091,0.027599426,0.0466648,-0.04330158,-0.055936404,-0.04622127,0.03304777,0.019239528,-0.034724437,0.033474877,0.0006759524,0.12343905,0.001824698,-0.014849699,-0.018190656,-0.0018378533,0.06552988,0.08072927,-0.030522218,-0.042328123,0.039455857,0.0508757,-0.045714933,0.005938152,-0.032327704,-0.09616321,-0.060351428,0.049784213,0.0018450782,-0.053787705,-0.03014429,-0.05484118,0.02041705,0.08121334,0.094138935,0.027179515,-0.014136714,-0.08107254,-0.06006801,0.0473633,-0.05634021,0.034805924,-0.019315025,-6.603112e-08,0.007202948,0.055343382,0.01889233,-0.038484495,-0.022343611,-0.069842495,-0.009196115,0.08211626,-0.045656044,0.07403447,0.09358747,-0.044764474,-0.041784775,0.011276501,0.05799232,0.035192754,-0.0018088354,0.05343967,-0.03111015,0.019536927,0.029293777,0.0013391135,-0.00031556492,-0.037167847,0.13035785,-0.04306992,0.028083319,0.109401725,0.0108081205,-0.012152985,0.012562976,0.011986139,0.071343996,0.016956363,0.024903916,0.035039846,0.09499879,-0.029259544,-0.00026570854,0.018156549,-0.027809378,0.09014757,-0.06757801,0.037862208,0.08801485,0.008342835,0.058814332,0.011790586,-0.058543995,-0.08452707,0.02935394,0.027944995,0.02129423,0.043305732,0.0934143,0.027089158,-0.027378738,0.07696498,-0.020948453,0.059622888,-0.040438972,-0.10189064,-0.038703192,-0.020521743,8,-5.138781,1.3829433,5
96,"for a mlr, in theory, closed form solution exists but in practicality, the closed form solution is not taken into consideration due to two reasons 1. finding inverse of large matrices is difficult and 2. there can be multi-collinearity. from a population we take sample, but in order to train ml model, entire sample should not be used. after cleaning and examining the sample, it should be divided in 80:20 ratio; 80% sample should be used to train the model (this sample data is known as training data) and the rest 20% sample should be used for testing purpose to check how well the model can predict the data (this is known as testing data or unseen data). then we discussed what is meant by overfit situation : (case1: râ²_trn=0.95 & râ²_tst=0.75, in this case the model is able to fit the training data, but is unable to predict the test data well; case2: râ²_trn=0.95 & râ²_tst=0.88, in this case the model is able to fit the training data and is able to predict the test data as well), in the above example, case1 is overfit situation, and in case2, ml model is good. after this, there was a discussion on different râ² values, i.e., like what is meant by multiple râ², râ², adjusted râ², etc. and what are their significance. also, we came to know that linear regression do not always mean fitting a straight line (linear is not for straight line), it is called linear regression because it uses linear combination of independent variables. then we discussed what are parametric models(slr & mlr which uses p-value for its operations and prediction) and non-parametric models(decision tree and random forest which rely on râ², rmse, mse ,etc. values for predicting outcomes).then we jumped into python discussed its two library 1. sklearnt and 2. statsmodel.api ; we also discussed q-q plot (quantile-quantile plot) in order to judge the that are the residuals normally distributed or not. 'sklearnt' do not give p-values, f-values, etc. so feature selection and dropping cannot happen in it. where as 'statsmodel.api' provides all the things just like excel and some even more. at last we learned about ols(ordinary least square), aic & bic (lower their values, better is the model), omnibus statistic(a number arrived from some formulation of skewness & kurtosis; lower its value, more the residual plot is near the normality), omnibus p-value(high p-value suggests that residual follows normal distribution), jarque-bera test(high p-value suggests that residual follows normal distribution) & durbin-watson test(it was something about auto-correlation between the error terms).",-0.103762634,-0.07244229,-0.00565659,0.06147931,0.10670159,-0.08796824,-0.04928207,0.05511303,-0.033454508,0.033381764,0.04137764,0.04090789,-0.04557855,-0.06992298,-0.015310252,-0.004211793,0.008656104,-0.024488198,-0.08644672,-0.045854487,0.03097685,0.018543493,-0.0783749,0.058142297,-0.08767948,-0.012078636,0.0049931128,-0.033149496,-0.03487943,0.0031031554,0.09839886,0.02782147,-0.01816355,-0.040181004,-0.07680949,0.015401476,-0.040812895,0.0502385,0.02496864,0.036289405,0.021541337,0.016496835,0.013454795,0.025660403,0.07227697,-0.0041907676,0.0056548873,-0.10003872,0.0045137485,0.06825329,-0.11754335,0.014559715,-0.014168352,-0.05313564,-0.020075481,-0.13576382,-0.030788066,-0.05824262,-0.038609307,-0.026659658,-0.027589062,-0.04162676,-0.084743395,-0.07157354,0.051090345,0.013089322,-0.07148607,-0.032020837,-0.0031445078,0.012223362,-0.027314348,0.040684585,-0.051587313,0.011479817,0.016217466,0.029559115,-0.034872923,0.009748295,0.028597303,0.052566934,0.097665,-0.0017459899,0.05924449,-0.09111665,0.045493364,-0.0033305106,0.047154546,-0.002692789,0.10320763,-0.018138306,0.039861236,-0.014357661,-0.04662184,0.027243404,0.04228009,0.03028098,0.048843265,0.03632204,0.034421396,0.03373915,-0.0053839684,0.05654136,0.014693168,0.00021098241,-0.028206926,-0.0144118145,0.050331622,0.0069739823,0.09811642,-0.07292365,0.06517619,-0.05537403,0.012954576,-0.018843347,0.042496145,-0.015367914,-0.018325705,0.049221408,-0.060077757,0.054248337,-0.023907123,0.029236134,0.0616416,-0.030354848,0.060142122,0.022400962,-0.1159734,8.884479e-33,-0.10130872,0.039067715,0.023599666,0.068955876,0.0006435942,-0.031959515,0.0098952055,0.02109821,0.03127929,0.07040344,0.006064525,-0.050969906,-0.035593003,0.042769022,-0.018650822,0.044317693,-0.022860272,0.056197345,-0.06469029,-0.027949534,0.07792248,-0.024594538,-0.014697316,-0.08073641,-0.00743551,-0.05069733,0.052364994,0.022607975,-0.022125168,-0.0070437025,0.006616435,0.010333754,0.002299808,0.13187878,-0.028513052,0.034439657,0.024083888,-0.021605128,0.036947004,0.037489664,-0.015274323,0.036770027,0.055260755,0.019732261,0.036933865,-0.04016015,0.022792252,-0.0473159,-0.08731072,0.0016352916,-0.03845425,-0.013113937,-0.13980204,-0.037980873,-0.0719799,0.14068939,-0.008169742,-0.00453246,0.015002059,0.09254623,-0.0007487978,-0.09424854,-0.003330308,8.059646e-05,-0.02906538,-0.054060653,0.009562551,-0.10550663,0.10075001,0.05570618,0.026025817,-0.056538682,-0.061511464,0.0042897104,0.01312455,-0.043988753,0.0756424,0.06453928,0.035313383,-0.036289465,0.012168257,0.045062874,0.006582813,-0.05684388,-0.07014632,-0.046577837,-0.042453315,-0.0652221,-0.0706083,-0.03521265,-0.10932394,0.0022463908,-0.09046457,-0.029507583,0.0037292945,-6.9626e-33,0.0044325492,0.0071844547,0.0343579,0.048130155,-0.06527606,-0.10021369,0.039735526,-0.049701743,0.01616102,-0.049060974,0.06306799,-0.06793634,0.1182175,0.011014384,-0.011010328,-0.011500424,0.045191374,0.035485636,-0.0341594,-0.023218747,-0.056044992,0.040074807,0.033540674,0.00747914,-0.04905144,0.0102450885,-0.10032374,0.04215223,0.075711794,0.017124629,-0.032591343,-0.04006159,0.054846615,0.02687429,-0.0063219885,0.06545738,0.021084812,-0.008969731,0.03733051,0.027429208,0.057872377,0.04414714,-0.11094557,-0.017992377,0.01083287,-0.033311,0.013042933,-0.019304967,0.09839241,-0.07029508,-0.01679223,-0.034646377,0.043310184,0.1239086,-0.0029018358,0.02459614,-0.006159832,-0.007517962,-0.025037572,-0.017190687,0.07629772,0.06192112,0.054250363,-0.00032265804,0.038560882,0.031806216,0.0038249672,0.059428856,0.06795278,0.03669435,-0.040952817,-0.064610556,0.06355252,0.05310394,0.06122807,0.027431086,-0.015841868,0.008230212,0.008315244,-0.04269286,-0.092955776,-0.05660015,-0.040923607,0.056396708,0.078592055,0.022417817,0.09731395,0.008414886,-0.027663194,-0.026905771,-0.041474145,0.02564565,0.007843299,0.04468483,-0.0010479373,-6.9687026e-08,-0.08886579,-0.041403767,0.02058421,-0.060223736,0.024641072,-0.0071758595,0.01226027,0.042913653,-0.011902156,0.06737287,0.008881636,-0.0115529625,-0.07231684,-0.004464204,-0.0057311133,0.06402974,-0.050631814,0.04628066,-0.024413778,0.062754326,-0.014682239,-0.03552472,0.0023386586,0.027663436,0.101463325,0.045371316,-0.06143707,0.05126976,-0.0112914,0.04411172,0.0040791836,-0.05061113,0.07004864,0.055846892,0.057668447,0.03998902,0.0762368,-0.046441585,-0.04246398,0.040618762,-0.020968705,0.05948762,-0.04567019,0.011101685,0.15596552,0.0035846478,0.012202247,-0.027759233,-0.10957847,-0.035924863,0.002790141,0.031356443,0.05804025,0.07060994,0.03328312,-0.019608935,-0.055598814,0.11546093,-0.06769623,0.092977494,0.016466951,-0.022600321,-0.045668285,-0.021594994,8,-2.0724876,2.0026946,5
117,"we started today lecture with a small recap of previous lecture and looking at the problem in finding solution that are multicollinearity and matrix inversion. we then started with the splitting of sample which we do in 80-20 manner using the 80% dataset as training data and other 20% as test data. the split is done randomly. our split should be such that the r squared value of training data is nearly same as that of the test data. if r squared value of training data is considerably higher than that of test data, the model is said to be overfitted. we then discussed about multiple regression parameters which is given by excel. multiple r is root of r squared, adjusted r squared gives an idea of how much variance is captured per variable. we then moved on to the python part. we had a brief discussion on the different libraries in python like pandas, scikit learn, statsmodels etc. we concluded our lecture with a small discussion on aic and bic values, jarque bera test.",-0.062320236,-0.05000502,-0.054948993,0.04625964,0.02644143,-0.056713507,-0.06136001,0.034513652,-0.008675059,0.03708954,-0.0325536,0.06719481,-0.053171717,0.0011274014,0.010122945,0.013611167,0.042699028,0.051079642,-0.11688091,-0.061733197,0.0074217417,-0.04270457,0.005529646,0.046556056,0.060301434,-0.056210008,-0.010271557,-0.02566362,-0.090416305,0.007085203,0.10552542,0.034317072,0.0024860792,-0.034882054,-0.021700948,-0.040907264,0.047775593,0.017492611,0.0707068,0.06306901,-0.038708296,0.0015723566,0.045891248,-0.031118246,0.021833627,-0.008028225,-0.05610988,-0.06295671,0.05192792,0.031822134,-0.032104533,0.03209408,-0.09841633,-0.004878205,-0.0020237244,-0.08803589,-0.05257348,-0.052771248,0.059374355,0.053766508,0.05900098,0.018287152,-0.019681334,0.024917379,-0.021013146,0.020192511,-0.05999047,-0.0035636676,0.0031827027,-0.0129711265,-0.112484775,0.04596747,-0.053348575,0.0149530545,0.12529998,0.025830511,-0.027217088,-0.08255812,-0.010321791,-0.031463206,0.026680898,0.0031431576,-0.011446549,-0.018502424,-0.023801044,-0.04966988,0.07828208,0.046315543,0.009293837,-0.0024525814,0.10877585,-0.00968047,0.021768872,-0.0048500407,0.015785417,0.071244046,0.033007946,-0.0064026136,0.05737222,0.06681133,0.06636056,0.010486146,0.058858458,-0.0063193976,-0.036354363,-0.023125328,0.14188586,-0.0693265,0.09239445,-0.023741934,0.018455407,-0.00088513416,-0.07903402,0.044670008,0.026926469,0.06869501,-0.0038781578,0.044822037,-0.061448127,0.062023178,-0.07740469,0.008535456,0.11959399,0.06496807,0.07532831,0.053055074,-0.13945593,8.7475984e-33,-0.028070644,0.035038035,0.10684113,0.05785842,-0.024032883,-0.028289184,0.0232866,-0.031464852,0.0466463,-0.015558998,-0.007068941,0.022897301,-0.00918397,0.0029208339,-0.03576523,0.04624107,-0.032662723,0.0065537947,-0.008811239,0.06175385,0.07627853,-0.044234153,0.0126201,-0.02290277,-0.0011894474,-0.122555666,-0.030855395,0.02524293,-0.028709559,0.055265356,-0.07387569,0.040694244,-0.035877768,0.08144117,-0.0030752106,-0.03645425,0.009408714,-0.0055352366,0.030437283,0.0287057,-0.07197805,0.04137144,0.05948273,-0.040382568,-0.04746792,-0.040275924,0.007157472,0.0724199,-0.006849986,0.052824125,-0.057681073,0.0066735377,-0.022413189,0.011337951,-0.054804783,0.09100285,-0.05130388,-0.030252924,0.06260408,0.06550153,-0.0759036,0.029278938,-0.03929431,-0.016044559,-0.014021038,0.028182162,0.007692308,-0.038223304,0.017988335,0.08909574,-0.024389513,-0.11903239,-0.06563908,0.0075132637,0.012069233,0.0065111993,0.036359977,0.060134254,0.008991152,-0.113690004,-0.0017608138,0.045198858,0.06100799,-0.12485109,-0.04678572,-0.02889178,-0.022161199,-0.106802724,-0.10604397,-0.0007598498,-0.033672217,0.03046555,-0.043743346,-0.07015409,0.011350194,-7.983992e-33,0.039180923,0.021875482,-0.07643519,0.105317645,0.02958511,-0.07030588,0.013074319,-0.00088620576,-0.01561371,-0.062306643,0.0641181,-0.073419094,0.048911266,0.042251367,0.0056411545,-0.027570276,0.043630432,0.02908606,0.018644573,-0.013133772,-0.0755142,0.036673278,0.103829764,-0.06735913,-0.10362436,-0.0034969742,-0.07804821,0.0055426103,0.022787653,0.0053743487,-0.1045645,0.08134727,-0.056614794,0.06880578,-0.019423638,0.02050685,-0.016276618,-0.056931645,0.023898516,0.04984378,0.065805994,0.07528143,-0.11772034,-0.004774251,0.015851375,-0.008364986,0.0007232135,0.089592256,0.075088084,-0.05538614,-0.050451815,-0.016730655,0.029359646,0.0895522,-0.031808123,0.019704612,0.013027613,0.033689663,-0.055258043,0.049179487,-0.045028433,0.045365766,-0.03183205,-0.011097025,0.004082885,0.035523117,-0.01061289,0.025107652,0.035885457,0.033234168,0.023963997,-0.12048041,0.05943286,-0.040571116,-0.023699222,0.00027332042,-0.038684767,-0.08885316,-0.04678603,0.05846659,-0.052648176,-0.04570338,0.01399254,0.016115569,-0.041095935,0.053223487,0.045876622,0.041813757,-0.02710732,-0.036089476,-0.018952886,0.056453712,0.0067253932,0.011406857,0.08939194,-6.299187e-08,-0.031041741,0.057617698,0.015635464,0.034024987,-0.01954464,0.00520384,-0.039039854,0.057446007,-0.030028654,0.04966518,0.054550655,-0.03648448,-0.09236269,0.03405445,-0.05213048,0.04031639,-0.02383898,0.094491996,-0.034976315,0.039761133,0.0129780425,-0.037911516,0.060852654,-0.0090877265,0.004662495,0.020256748,0.026665386,0.060464177,0.011891069,0.038271576,-0.01122547,-0.08188936,-0.031731773,-0.06827957,0.026488818,0.034239404,0.015848838,-0.010949583,0.014012853,0.05267815,-0.08501426,0.036164522,-0.07981742,0.016743345,0.113200024,0.0072689415,-0.005602875,0.065438844,-0.021522041,-0.099799894,0.015977086,-0.024385817,0.012445549,0.012124708,0.06534585,0.025848113,-0.098259926,0.05736133,-0.042092923,-0.002801097,0.011943125,-0.013522881,-0.061050374,-0.059854083,8,-3.4646373,1.561174,5
136,"in today's class we learn more about the regression through python modules. the very basic theory is that do not use the entire sample for creating the model. we split the data in the ratio of 80:20 to training and testing samples. this splitting helps model to perform well as compared to using the whole sample. and we done this process by randomly splitting.
from training matrics and testing matrics we get r_2. if the value of r_2 is differing much from each other then it performs well on training and testing data but not on test data.
this is called overfitting. another term is bias variance tradeoff. multiple r which is square root of r_2 which indicates correlation between y and rest of the x. the adjusted r_2 value will drop if we are adding the variables and it tells how effective the addition of new variables is. slr and mlr are parametric method of model creation as they use parametrs.
example of non-parametric model is decision tree. we also learnt about different python modules such as pandas, numpy and scikit - learn. next we see the quantile-quantile plot of residuals which tells that for good model, there should be less deviation in plot of residuals. scikit learn modules doesn't give values such as statistic values.",-0.06496474,-0.085260086,-0.04202659,0.065397784,0.12502387,-0.08635074,-0.076542646,0.05910956,0.013029822,-0.009679703,-0.039092723,0.1166119,-0.013807015,-0.016379641,0.076958686,-0.060101643,-0.008043431,-0.0024152638,-0.1101766,-0.10192998,0.09849644,-0.0705284,-0.034425803,0.058318593,0.0010431953,-0.04383275,-0.050973333,-0.037398763,-0.04008278,-0.011428798,0.06804097,0.005398519,-0.052027807,-0.0013222906,-0.06377328,-0.05934637,0.038852476,0.04849158,0.011199079,0.029870138,-0.04412785,-0.0037878104,0.013726726,-0.012652888,0.054135285,0.0034460495,-0.02933109,-0.08826095,-0.02177814,0.008935119,-0.08604732,-0.009864082,-0.05223722,0.0022664343,0.011487035,-0.06000996,-0.027527902,0.009912789,-0.0145279765,-0.047226366,-0.033477645,-0.038078446,-0.021928601,-0.010683106,-0.06612499,-0.11122326,-0.035434414,0.0066298493,0.03584842,0.040217064,-0.059084333,0.06906817,-0.02896277,0.021839319,0.07879021,-0.053096604,0.041547347,0.009942487,0.023569714,-0.013168151,0.025730314,0.05127517,0.0508429,-0.029685332,0.0100489035,-0.024054818,0.04450672,-0.0042016567,-0.03930065,-0.017494934,0.11901187,0.007774835,-0.078580365,0.030214712,-0.006153571,0.060442783,0.0245831,-0.014399424,-0.0009474311,0.0467433,0.01350659,0.034196563,0.038410716,-0.0057804408,0.08460712,-0.041735988,0.034141652,-0.12647519,0.06465453,0.02112569,0.027344115,-0.006377834,-0.056717593,0.008960973,0.018327693,0.009364261,-0.06605824,0.07316341,-0.102838375,0.09881824,0.0022375425,0.009748835,-0.014195735,0.023735214,0.045275364,0.03016201,-0.17126012,7.561234e-33,-0.05424528,-0.03296783,-0.0048548607,0.073372476,-0.009353122,-0.08461076,-0.0066358587,0.092539825,0.043045957,0.03824446,-0.036035072,-0.03361081,-0.020187858,0.028816435,0.103809886,0.019953476,-0.077141985,0.011588727,0.061298463,0.10246995,0.042123504,-0.06960677,0.022132682,-0.01543706,-0.012322551,-0.027471866,-0.0062334496,0.058827944,-0.10468562,0.029044187,-0.048608698,-0.0350776,0.028432617,0.11085233,0.025829904,-0.031598076,-0.016305717,-0.015447653,0.0145055335,0.020167915,-0.06030838,0.050615467,0.026003271,0.02040926,0.032571305,-0.050307862,0.03665559,-0.0497918,-0.05979726,0.050973624,-0.051331695,-0.0147736445,-0.028881917,0.0314449,-0.06910993,0.09660098,0.016351005,-0.076726004,-0.036877897,0.004439434,-0.07485137,-0.05318629,0.009478739,-0.009081685,-0.0013113589,0.040792666,0.015309877,-0.024080947,0.02295727,0.09407388,-0.035813585,-0.038618248,-0.067241855,0.02382502,0.05238831,-0.0130737685,0.055423856,0.013685079,0.062341604,-0.027019173,-0.0021008416,0.06867794,0.05220291,-0.11643286,-0.1034825,-0.038475573,-0.019014066,-0.03324652,0.022556853,-0.02800137,-0.0897789,-0.025069036,-0.10128318,-0.0650038,0.050951313,-8.345484e-33,0.001764198,0.03345675,-0.0002967969,0.09454778,-0.049021807,-0.024807451,0.037782986,-0.038530514,0.036968213,-0.059313964,0.039037727,-0.0043464317,0.107912675,0.11449979,0.026920347,-0.022297215,0.013363695,0.032577645,-0.012052509,-0.042275734,-0.014907001,0.059841286,-0.0026044524,-0.08089712,-0.073387675,-0.01808827,-0.13634284,0.042149387,0.049079627,-0.023908027,0.015699888,0.04465137,0.022844957,0.008972968,0.04907346,0.0517289,-0.0059653562,0.022041015,0.026009291,0.076648705,0.07359434,0.07558334,0.0027275325,0.030723006,0.026726648,-0.00674504,-0.012022185,0.019447217,0.053502433,-0.05702849,-0.0020190866,-0.040174168,-0.016303042,0.07265809,-0.024217065,-0.062357318,-0.060041863,0.044920858,-0.018687446,0.05440928,0.051343452,0.045722794,-0.056761812,0.05015116,-0.08312273,-0.001470633,-0.032835886,0.029788023,0.092309326,0.047061328,-0.02880831,-0.04725083,0.008705135,-0.012055681,-0.087625906,0.04454698,-0.084271275,-0.05196959,-0.054097254,0.011950818,-0.031401195,-0.032005157,-0.0448027,-0.016295541,-0.03106614,0.042856447,0.032141265,0.123075545,-0.021352168,-0.035840243,-0.028575324,0.020233026,-0.011037649,0.027143154,-0.0063463068,-7.062836e-08,-0.017070653,0.0050381906,0.08249103,0.0024715138,-0.029071392,-0.04513267,0.04050154,0.056262895,-0.022176713,0.10119522,0.021423198,0.005693961,-0.08584127,2.751833e-05,0.051362563,0.10911932,0.0084704645,0.094477765,0.008321981,0.016499294,0.042150244,-0.062103268,-0.030554892,-0.008790433,0.07061824,-0.018109865,0.0005346612,0.04977288,-0.00052584114,-0.030339573,0.047575187,-0.012257725,0.05392715,0.0024713313,0.07118747,0.058398455,0.044343073,-0.023495452,0.02094877,0.05246002,-0.047273863,0.030322248,-0.045000926,0.011642441,0.07443342,0.0037384147,0.032001127,-0.0045100986,-0.081918016,-0.12175939,0.069713265,0.013113381,-0.0125898635,-0.05214703,-0.013091445,0.041191477,-0.05170245,0.079706386,-0.064443596,0.01727114,-0.05593348,-0.03226193,-0.004341018,-0.027737994,8,-5.0938883,2.9409053,5
142,"the lecture introduced multiple linear regression, emphasizing the importance of multiple independent variables and the 80-20% training-test split to prevent overfitting. overfitting was illustrated with a graph, showing how high training accuracy with low test accuracy indicates poor generalization.

key regression statistics were covered in excel, including multiple r, adjusted 
ð‘…^2, and variance formulas (explaining why the denominator is nâˆ’1 due to degrees of freedom). it was clarified that linear regression does not always imply a straight line, as polynomial models can also fit within this framework.

in the python segment, q-q plots were introduced to check if data follows a particular distribution.
",-0.024541046,-0.018315833,0.0005930327,0.05897842,0.07811167,0.01523277,-0.0494715,-0.012668292,-0.0038751406,0.038263686,0.06057149,0.061224587,-0.0067765308,0.058342062,0.02363819,-0.024741005,-0.06821144,-0.024868025,-0.06491847,-0.047273107,0.05445208,-0.062199853,-0.023790961,-0.025456654,0.11941023,-0.017353823,-0.0405531,-0.008806623,-0.07827734,0.021360764,0.044770475,0.060033794,0.04871144,0.025827665,-0.0110406615,-0.059776787,-0.018035019,0.06283122,0.049805433,0.11595782,0.011824904,-0.076541916,0.032292865,0.02733271,0.09438583,0.007258868,-0.08632827,-0.03240027,0.040635478,0.016126584,-0.05903221,-0.034352772,-0.04146501,0.08775794,-0.0038688222,-0.11831914,-0.03202913,0.015318356,0.054384973,0.010099416,0.0228677,-0.012259078,-0.054869603,0.07529105,0.039263908,0.014053673,-0.081986256,0.015067003,-0.0834374,0.04338585,-0.06536514,0.04221828,-0.083986886,-0.019200768,0.04743875,-0.03634902,0.0022762357,-0.00690984,-0.08749506,-0.014597257,0.01285549,0.03596602,0.008925518,-0.039918274,0.05650487,0.00880473,0.0013223585,-0.023500858,-0.02917412,0.054707672,0.041764673,0.008530588,-0.012283992,0.06386302,-0.07759211,0.070460655,0.02686132,-0.11642843,0.016213829,0.04596353,0.041352443,-0.038661025,0.058400292,0.06022562,0.036666706,-0.048769813,0.115444876,-0.058653016,0.04196479,-0.058049534,-0.036852516,0.00476657,-0.021647038,0.02316086,0.027860265,0.008637162,-0.018561965,0.116905764,-0.059258483,0.037939087,-0.053945538,0.008169623,0.03866402,0.018501965,0.04471719,0.009189477,-0.11644065,6.8448154e-33,-0.0016974967,0.011356802,0.0158422,-0.018118048,0.07461233,-0.054785687,-0.022542609,0.015044327,0.057005137,0.07509627,-0.0031663901,-0.025534833,-0.02504397,0.07113834,0.017335674,0.13198076,-0.051501416,0.051480796,0.024419433,0.101115614,-0.0166039,-0.11454481,0.00743466,-0.026410598,0.0041890927,0.015643347,0.026960513,0.054218225,-0.074899666,0.01751545,-0.05879518,0.07096946,-0.05170583,-0.01315617,-0.0022965774,-0.029084524,-0.07595615,-0.03954461,0.044327706,0.029066917,-0.056396473,-0.010261198,0.078804046,-0.046898924,0.015571439,0.02917444,-0.059361454,-0.018526599,-0.033802163,0.04864652,-0.09494951,0.0037896824,-0.030503176,-0.013093666,-0.052823376,0.090495676,-0.055596158,0.018613745,-0.07843369,-0.024715098,-0.04446218,0.006980423,-0.05454948,-0.06981768,-0.05600047,0.022561165,-0.0031896874,0.0087939985,0.08501531,0.15325806,-0.040165767,-0.059073735,-0.06616944,-0.027227279,0.08038932,0.053938616,0.009660365,0.0006934553,0.07210525,-0.11052076,-0.01196722,0.029329948,0.054264996,-0.09407328,-0.06745986,-0.03655849,0.04165983,-0.055911604,-0.034584388,-0.008151635,-0.12282857,-0.003853924,-0.0589785,0.0078102625,-0.02273956,-6.294205e-33,-0.047928844,0.123916306,-0.002574999,0.033363227,-0.051564798,0.028465025,-0.015879627,0.002120274,0.045263577,0.03550172,0.054425627,-0.00945404,0.01747827,0.032709617,0.034642544,0.012643626,-0.015775736,0.010491164,-0.034908146,-0.07353003,-0.051496316,0.034853246,0.0053794524,0.010564618,-0.0502885,0.0037653379,-0.09812618,0.00035171455,-0.0453997,-0.033149954,-0.05303832,-0.030816253,-0.04222443,-0.03731238,0.016049406,0.052014533,-0.04369227,-0.00831098,0.055104055,0.097880974,0.050578237,0.0075588957,-0.02390279,-0.013642504,-0.030213855,0.03688339,0.06827074,0.020439995,0.055142835,0.024184216,-0.06600221,0.09322296,0.00018181739,0.056715526,-0.056592733,-0.063694425,-0.031399675,0.043478828,-0.071684286,-0.067024216,-0.031788763,0.016611136,0.0008503077,0.066439874,-0.013212718,-0.042668078,-0.019119231,0.014181514,0.056021772,0.03601036,-0.0008798763,-0.038768664,0.029295133,0.0084783435,-0.090279676,-0.01862239,-0.07553444,-0.09237159,-0.052967306,0.090406194,0.060564235,-0.06610766,-0.007806664,0.056751706,-0.011339504,0.048339,0.06118669,0.04833647,0.06374336,-0.05268631,0.015956841,0.10407132,-0.091216035,0.010287717,0.0101606315,-5.423977e-08,-0.021678278,0.029943338,0.0009677119,-0.03275534,0.0050237956,0.004405593,0.038954552,0.093260676,-0.05057934,0.06244346,0.026726034,0.016643833,-0.0873187,0.05268335,0.00062356243,-0.008414451,-0.027695542,0.110957555,-0.014078718,-0.0026173312,0.021536466,-0.039866056,-0.05415519,-0.0064473785,0.058638297,-0.015912551,0.047828812,0.11432976,-0.0271746,0.052400343,0.05456787,-0.030321324,-0.010625592,-0.042210814,0.014699434,0.11099195,0.036952425,-0.043758035,0.07898978,0.0349866,-0.07932525,0.009591802,-0.0035459243,0.058518264,0.05420071,0.018161112,0.06357054,-0.046164747,-0.06424706,-0.078527495,0.05870452,-0.026358753,0.0031721,-0.029357653,-0.007189005,0.03542296,-0.02684861,0.02265107,-0.07364874,-0.0045439214,-0.009839809,-0.03898431,0.060594123,-0.06978903,8,-6.672185,1.2313997,5
170,"we began by looking at a closed-form solution in multiple linear regression, which proved to be impractical because of complexities of inversion of matrices and multicollinearity. at feature selection, we introduced removal of features where the p-value is greater than 0.05, meaning they contribute minimally to the model. we then looked into data training and testing, which for example, should be split 80-20 after exploratory data analysis. we also evaluated the râ² values of both the training and testing datasets in order to observe potential overfitting, and the model's generalization towards the population was assured. also discussed was the concept of adjusted râ², multiple r, and why it only has (n-1) degrees of freedom in tss.",-0.024556786,0.01129283,0.037180852,0.12396193,0.12387448,0.03556675,-0.040027037,0.018095665,-0.005064388,0.066925064,0.012573999,0.09982132,-0.05912206,0.029031157,-0.010453484,-0.018333174,0.021993041,0.008905573,-0.033951417,-0.053876735,-0.009470909,-0.06157572,-0.019136617,0.037058115,0.020740647,-0.0061178287,0.007853038,-0.013861181,-0.020638397,0.040887237,0.0235783,0.10058114,0.02217219,-0.08742148,-0.07053752,-0.030424653,0.0040124743,0.07525984,-0.031408,0.030079786,0.042615462,-0.06029374,0.0146883335,0.035877693,0.085623756,-0.0009463294,-0.049922112,-0.05304329,0.01526326,-0.04725271,-0.020230517,0.041824404,-0.079040766,0.008279087,-0.04634135,-0.15566942,-0.07936541,-0.016100207,0.024014216,0.02023922,0.036634598,-0.038819652,-0.049403813,-0.00095814036,0.100977965,0.07368978,-0.066244744,-0.030411268,-0.042480953,0.059877712,-0.08548668,0.067503005,-0.076945186,0.011589762,0.03473804,0.0056393784,0.023013134,-0.039545108,0.029390516,0.048725873,0.06422329,-0.0075926115,-0.038093742,-0.051666833,0.058586918,0.013904354,-0.01851197,-0.050681856,-0.043963473,0.029013017,0.060330633,0.005818252,0.034956876,-0.007953874,-0.054442875,0.025413819,-0.016640032,-0.055178106,0.023968492,0.054701805,0.00017342273,0.032117013,0.059367668,0.048198506,-0.058516003,-0.026975403,0.063442975,-0.0585193,0.06907421,-0.030700328,-0.022748323,0.028076766,-0.051869523,0.0287147,-0.0064239367,-0.0048591793,-0.04113359,0.066150054,0.023766058,0.066540435,-0.010817903,0.015951267,0.05169988,0.01945995,0.072769776,0.0527767,-0.11360119,7.019958e-33,-0.028956031,0.058593135,0.018540502,0.054254677,0.022487745,-0.028418645,-0.042748284,-0.0059226784,0.048210826,0.09405551,0.006980873,-0.014830724,0.02677781,0.013483207,0.052878186,0.07398889,-0.034930278,0.1088155,0.0146796815,0.044647723,0.0796422,-0.066017106,-0.0070274947,-0.043387577,-0.04120532,-0.033191085,-0.05073297,0.01270994,-0.03403277,0.0012562147,-0.019893121,0.10303153,-0.078590356,0.08203324,-0.04388761,-0.010868982,-0.059414335,-0.069821395,0.0822648,0.08815715,-0.039314568,0.020864164,0.003157011,0.008782774,0.041595083,0.045085203,0.0630966,-0.032877095,-0.115653805,0.041508984,-0.043555073,0.05178038,-0.091330424,-0.014771742,-0.0008803453,0.057544447,-0.046107423,-0.057000905,-0.019121539,0.03369299,-0.042013116,0.011911938,-0.01892392,-0.11747196,-0.02760871,0.01313975,0.009677247,-0.03501855,0.057145815,0.05918637,-0.029573552,-0.08473865,-0.031827506,-0.011356741,0.06101152,-0.0033628447,0.15647274,0.037559878,0.06431655,-0.05651923,-0.018785805,0.05073515,0.048722263,-0.071766235,0.057264134,-0.09710362,-0.030335233,-0.118034564,-0.016271777,-0.03323789,-0.023095988,0.05671063,-0.09153021,-0.02501095,0.013235529,-5.0738528e-33,-0.043392763,0.03927942,-0.05193095,0.044369865,-0.065916136,-0.07797242,0.080432504,-0.015521714,-0.015656883,-0.10123713,0.097503796,-0.083940126,0.055626605,-0.016226508,0.039135043,0.016294653,-0.004073683,-0.033151764,0.019269932,-0.034725506,-0.034776147,0.10561421,0.058834724,0.04518262,-0.08730856,-0.0034421273,-0.08848904,-0.029560994,0.032669473,-0.010667153,-0.11008331,0.04321778,-0.034578323,-0.032029673,-0.024913926,0.040650044,-0.050392542,0.032237772,0.021281034,0.14393678,0.0583243,0.07055749,-0.06517132,0.019387072,-0.030698275,-0.012406022,0.03811042,0.0002021652,0.08008939,-0.052291766,-0.0268676,-0.03689331,0.020821476,0.013999396,-0.0042112083,-0.048027176,0.0641,-0.008498819,0.034114692,0.03180867,0.00028072845,0.031479966,-0.0076938407,0.07948454,0.041174218,0.018810635,0.069051296,0.08273925,-0.040870428,-0.02196736,-0.035699513,-0.13383108,0.072573975,-0.015152581,-0.03508812,-0.040884502,-0.03199662,-0.014706015,-0.04279234,0.009193348,-0.049500287,0.007164601,-0.010816368,-0.017981533,-0.03167311,0.026879644,0.02256233,-0.00048674364,0.019095182,-0.030514125,-0.061907265,0.037216138,-0.033655282,0.0257216,0.06655897,-6.163087e-08,-0.07604131,0.043446995,-0.0060976576,-0.019719481,0.010879615,-0.015510843,-0.01882026,0.13300842,-0.010455981,0.071309656,-0.021431414,0.039760962,-0.04433854,0.041755237,0.035716087,0.035319842,-0.061454497,0.06499499,-0.015919318,0.017505893,0.07598185,-0.010971997,-0.008905513,-0.056293882,0.089774854,-0.015224915,-0.00924196,0.04864958,0.037472848,0.046868145,0.0018620314,-0.05419637,0.060548767,0.009638763,0.07386323,0.049342744,0.015055324,-0.023642102,-0.04107692,-0.007066658,-0.08023647,0.098020054,-0.03829035,0.031746935,0.022840878,0.035181426,0.019096732,0.014988944,-0.034450393,-0.096503444,0.039553188,-0.0053534093,-0.037775908,0.046888445,0.065965235,0.017786229,-0.018571373,0.038401473,-0.050307497,-0.03856364,-0.0066122054,-0.0065431916,-0.06089859,-0.0814859,8,-1.3285226,3.7932315,5
185,"in multiple linear regression, a direct mathematical solution exists but is not practical due to difficulties in handling large matrices and multicollinearity. to check how well a model works, data is split into 80% for training and 20% for testing. overfitting happens when a model performs well on training data but poorly on new data. linear regression means using a linear combination of variables, not always a straight-line fit. parametric models (slr, mlr) use p-values, while non-parametric models (decision trees, random forest) rely on metrics like râ² and rmse. we also discussed python libraries like sklearn and statsmodels.api",-0.042826578,-0.09755814,-0.082497016,0.03351003,0.09164896,-0.06156038,-0.09719605,-0.03379943,-0.016308066,0.024538552,-0.006935315,0.038208466,-0.026452214,0.05293639,0.030190684,0.015472557,-0.05104984,0.0791877,-0.047188956,-0.10703706,0.04169389,-0.018819373,-0.06941616,-0.00784207,0.01730683,-0.04246349,-0.017714972,-0.054361816,-0.14310934,0.03958508,-0.032850817,-0.019326732,-0.0343412,0.00039526957,-0.07570407,-0.044477023,0.034578536,0.036017027,-0.016767493,0.038382284,-0.03586584,-0.024937728,0.039064456,-0.022050595,0.07353449,-0.039857093,-0.09115266,-0.08900618,0.07411301,0.023003204,-0.05888141,-0.09687763,-0.023043007,-0.007722284,0.03554629,-0.117332,-0.060553305,-0.021289853,0.047414944,-0.059667964,0.03398088,-0.018848287,-0.03247284,0.00854648,-0.005654618,0.09022709,-0.07372483,0.045394015,-0.056243602,0.022618262,-0.112551555,-0.016699841,-0.028679993,0.037419625,0.02193324,-0.010780444,0.035075564,0.02479738,0.019790558,0.05203414,-0.012424125,0.05600522,-0.04512901,-0.016155913,0.10794831,0.00297804,0.014938915,0.05382046,0.023432512,-0.0049631028,0.042473193,0.011391756,0.0075964513,0.027356442,-0.110838294,0.04757045,0.02438784,-0.08403747,0.0076041548,0.059780292,-0.02898836,-0.014582755,0.039099608,0.017130688,0.058972392,0.028658824,0.073120385,-0.02745067,0.08354876,-0.036591664,0.014969948,-0.014682421,-0.06102826,0.027029837,0.08181696,0.035809454,-0.032043528,0.008698352,0.03753252,0.06481126,-0.05614937,0.034067065,0.122028746,0.022479065,0.08319871,0.045272004,-0.13462213,9.7963664e-33,-0.0702395,0.057291772,0.057507075,0.03887093,-0.011518156,-0.059762094,0.0052141254,0.028774185,0.06730803,0.0037785426,0.015180563,0.03625314,-0.05939155,0.042834334,0.06428326,0.07289947,-0.007891975,0.033784296,0.038583085,0.07953277,0.0038488852,-0.07572805,-0.022970298,0.0068499283,0.031081898,-0.03781538,0.016301269,0.011571419,-0.031793647,0.029911887,0.023861453,0.016480865,-0.0053057973,0.03449723,-0.00030730775,-0.076523066,-0.10990238,-0.015496767,0.034088355,0.023725385,-0.06363309,-0.0006847435,-0.007734141,-0.034685235,-0.027292553,0.072893776,0.013970229,0.033321403,-0.007870582,-0.020513779,-0.012839609,-0.012607963,-0.06324994,0.064324625,-0.12969989,0.074055396,-0.043773763,-0.058949597,0.0027494794,0.0161274,-0.04441033,-0.05591892,0.010445788,-0.09938856,-0.01799409,-0.011264165,-0.010036557,0.025893738,0.061355013,0.08382529,-0.041807566,-0.058114927,-0.045811668,-0.022083478,0.03547557,-0.04983205,0.09281908,0.011162767,0.005301701,0.0032399725,0.041700236,0.06319791,0.07190935,-0.118696205,0.00691366,-0.033532206,-0.032667566,-0.092340134,-0.08310098,0.0006498547,-0.12448441,0.09767433,-0.058745172,-0.024361983,0.017764559,-7.915938e-33,0.007955368,0.024062008,0.044707704,0.067838,-0.012589586,-0.050104853,-0.06780789,-0.042112343,0.04271176,-0.03639859,0.07468546,-0.066172786,0.07511748,0.03719298,0.08569878,-0.0010243685,-0.004842792,-0.0010094229,0.02551524,-0.022803247,-0.049292658,0.06541567,0.032667857,-0.06711306,-0.0961681,-0.022907978,-0.115626365,0.0155906305,-0.01229675,-0.00810251,-0.06683974,0.042399086,0.0065529,-0.011482574,-0.025372706,0.015816998,-0.004660369,0.030063326,0.028386258,0.048151243,0.06080201,0.04070694,-0.032208353,-0.013304067,-0.03373235,0.047063112,0.01224353,0.043323778,0.015514543,-0.021542085,0.014069623,-0.0137817655,-0.011808167,0.015516983,-0.0061554634,-0.04514281,-0.034745276,-0.015140684,0.0010252283,-0.040165324,-0.07966117,-0.0167158,-0.0129007455,0.12364631,-0.0040216506,-0.010351511,0.007147726,0.0009940205,-0.043185826,0.08602928,-0.0015439643,-0.030626956,0.030008323,0.049532495,-0.07528676,-0.07159599,-0.042197168,-0.056519363,-0.071180604,0.07432402,0.041025378,-0.003540884,0.0035957505,0.0038432973,-0.00074742606,0.076119825,0.12538145,0.042549897,0.034796868,-0.03214893,-0.057251077,0.12446794,-0.011045909,0.0053319177,0.033572655,-5.3961625e-08,-0.0022637262,0.08099452,0.036928967,-0.039543085,-0.05621665,0.005589394,0.03483185,0.14696606,0.0053324723,0.078764,0.005336829,-0.0238567,-0.040657558,0.009768146,-0.030977048,0.03989685,-0.045495298,0.070227034,0.013890725,0.07132151,0.021019347,0.0017914987,-0.0028051129,0.040648427,0.070410386,-0.026941212,0.046075772,0.061224293,0.032654922,0.036220822,0.034690514,-0.027867123,0.08024475,-0.04952987,0.053699117,0.08254313,0.025087446,-0.0035595347,0.01604333,0.0347646,-0.021237232,0.09117809,-0.0615077,0.009291742,0.11789147,0.0090789925,0.06606447,-0.011209243,0.00013765974,-0.057667386,0.06392754,-0.018250257,-0.028406277,0.013710595,0.0072466135,-0.012029977,-0.07027287,0.046100598,0.034600485,0.0037749074,0.009095358,-0.10638196,-0.015077941,0.016715392,8,-5.59905,0.44050506,5
190,"the lecture started with some recap about the closed form solutions that we discussed during the multiple linear regression which can't be use in actual world as it deal with the inversion of matrix and also the fact that it also includes the multi collinearity.  if p> 0.05 we remove that feature . then we discussed about train and test data today, then we saw r2 values of training. we need our model to learn and represent population. we also discussed that why there is n-1 and not n in the formula of tss",-0.08586346,-0.04593603,9.616345e-06,0.09493465,0.07521908,0.058162212,-0.006237722,-0.030114412,0.026858993,0.09294794,0.06061316,0.084950715,-0.07383583,0.012943779,-0.029872237,0.005994239,-0.052462954,-0.061766323,0.03581941,-0.016744558,-0.01023277,-0.011667993,-0.0032114584,0.057181217,-0.02994767,0.026875932,0.012347105,-0.0033914505,0.008751287,0.015526553,-5.9613558e-05,0.09741851,0.05117788,-0.046209108,-0.03885651,-0.026279643,0.04145889,0.06704709,-0.031951763,0.07959541,0.00685939,-0.00932039,0.010233653,0.036461174,0.12020879,-0.048148684,-0.09157598,-0.06828564,0.012658001,-0.024134917,-0.049919207,-0.020311873,-0.0515888,-0.026182333,0.07062459,-0.14731848,-0.117458574,-0.044995457,-0.040538866,0.0523901,0.07050553,-0.020319946,-0.055776596,-0.026223585,0.1517289,0.047019675,-0.14325887,-0.054664038,-0.05653322,0.062428694,-0.14112869,-0.0014576251,-0.038308617,-0.08741746,0.019741183,-0.047105815,0.008926327,-0.023812205,0.038660973,0.06910183,0.071842335,-0.005096143,-0.040989455,-0.06429566,0.0005927543,0.05874141,-0.030908918,-0.060334623,0.034491405,-0.038171224,0.0572631,-0.006797282,0.023537463,0.067281015,-0.037863463,-0.029068626,0.0032641508,0.088333726,0.055184595,0.04129246,0.031932477,0.0061450438,0.053438056,0.058786932,-0.13577668,0.015095212,0.033225775,-0.016291806,0.047347758,-0.03400631,-0.07056754,-0.026143955,-0.037629295,0.053445157,-0.055950575,-0.067475215,0.00020565867,0.026609672,0.04622113,0.024648862,-0.022661848,-0.014430041,0.016435442,0.013662332,0.0012206332,0.027365305,-0.0625901,1.7932198e-33,0.01018311,0.034026533,0.06518365,0.054208484,0.022517156,-0.027439324,-0.02520974,-0.06120938,0.07632938,0.010082459,-0.04731046,-0.01609351,0.04996731,0.04602051,-0.03270391,0.03903966,-0.026545255,0.07914645,0.047328327,-0.00974381,0.10749822,0.022599947,-0.004049271,-0.10914238,-0.014723854,-0.0049940604,0.010998644,-0.008315367,-0.0047406093,-0.0064949724,0.0029393218,0.07452165,-0.059950117,0.050119124,-0.034468368,0.007756686,0.02881968,-0.038588032,0.03790816,-0.02304135,-0.002531426,0.06377493,0.02033602,-0.0029504928,0.016301908,-0.024229795,0.050770823,0.03604462,-0.03789701,0.049236532,-0.07653054,0.023878837,-0.11827629,-0.07032218,0.051839903,0.12047596,-0.06575334,-0.0026493738,-0.0219427,0.02054049,-0.019939546,-0.007836257,0.009421783,-0.13473733,-0.021889424,0.043985315,-0.0033545366,-0.058014598,0.125801,0.03214151,-0.042909674,-0.022007912,-0.07361952,-0.046618804,0.11238309,-0.05996007,0.08216347,0.05412417,0.068014346,-0.020769669,-0.008340419,-0.011673692,-0.028861,-0.00067962153,0.05152718,-0.03397029,-0.030640936,-0.11497487,-0.0017382002,0.018597638,-0.015338529,-0.049342312,-0.11274965,-0.04550072,0.093257114,-3.2638163e-33,-0.04224199,0.008290895,-0.116280854,-0.0120290965,-0.07381149,-0.12007588,0.041218884,-0.05711656,-0.031848937,-0.039264843,0.0632482,-0.010237663,0.047908723,0.019886045,0.051226128,-0.02416541,-0.0034548042,0.0070912284,0.020040283,-0.07580311,-0.020854171,0.08576831,-0.00380299,-0.04460094,-0.08646211,0.008085491,-0.058189042,0.01848624,0.01898046,0.008566313,-0.0026370361,-0.040946975,-0.024040857,0.013480864,-0.014924341,0.08026862,-0.025107631,5.9112488e-05,-0.028768687,0.05205949,0.007493522,-0.0071728644,0.019219905,-0.012199158,-0.013118886,0.004775567,0.03978248,0.0064170165,0.07212355,-0.059455626,-0.034398858,-0.027290026,0.0040906984,0.0073084435,0.008531602,0.062033337,-0.016629936,0.014962792,0.06981832,-0.008886869,0.0533546,0.032473233,-0.041110855,0.047517728,0.055436198,-0.0128294015,-0.0177239,0.02529803,0.015556863,-0.006249006,-0.032218583,-0.038593005,0.08810852,-0.011527359,-0.05745979,-0.004031805,-0.07791801,-0.0101019265,-0.028513923,-0.023615692,-0.031945776,-0.0194803,0.042297453,0.023981081,-0.00702426,-0.03614499,0.14886315,-0.0027270215,-0.009819429,-0.027026847,0.013347594,0.0034042494,-0.055255875,0.023834536,0.047879092,-5.6893732e-08,-0.07960719,0.025481768,0.008284276,-0.073438734,0.0116806235,-0.025045473,0.010012657,0.0586559,0.004180144,0.11320411,-0.023734871,0.04508681,-0.04363633,0.013486864,0.015085297,0.041707452,-0.032726582,0.058181483,-0.03742807,-0.0024410265,0.040511653,-0.011258698,-0.01705887,-0.041150123,0.096546665,0.020367898,0.0033858914,0.016238647,0.034785394,0.0046300944,0.032072105,-0.017624559,0.10344855,-0.01632758,0.052134775,0.04285685,0.0047415975,0.0063618636,-0.0013782453,-0.057205614,-0.07840106,0.03237962,-0.015570964,0.05685257,0.11955572,0.083742075,-0.08224674,-0.016003808,-0.1293724,-0.06527163,-0.0075662346,-0.018099798,-0.042000018,0.07331508,0.06278968,-0.02603218,-0.039215494,0.011172274,-0.0063342284,0.007823867,-0.04265556,0.039806675,0.015336413,-0.031417903,8,-0.036314946,2.9511793,5
216,"we started by going over the closed-form solution in multiple linear regression, which isnâ€™t very practical because it requires matrix inversion and also runs into issues with multicollinearity. then we talked about feature selection, where we remove features with a p-value greater than 0.05 because they donâ€™t really add value to the model. after that, we got into training and testing data. usually, we split the data 80-20%, doing it randomly after exploratory data analysis. we also checked whether the r-squared values for both the training and testing datasets are close to each other, since if they arenâ€™t, we might be dealing with overfitting. the goal is for the model to generalize well and represent the population. finally, we covered adjusted r-squared, multiple r, and why thereâ€™s only n-1 degrees of freedom in the total sum of squares (tss).",0.016502896,-0.03831286,0.081605814,0.10242412,0.13903016,0.02004382,-0.071698286,0.0070720324,-0.013365575,0.04569246,0.01198112,0.037858393,-0.0864758,-0.0015542532,-0.0067932573,0.006321526,0.028734319,-0.034862675,-0.032213647,-0.046994545,-0.022544093,-0.06827863,0.041087225,0.06112837,-0.027787717,0.0061289,0.021736585,-0.012541303,-0.03956613,0.019614965,0.06494189,0.06748838,0.009383108,-0.10517715,-0.09094945,0.0034085857,-0.048297152,0.068976834,0.015385615,0.03639453,0.016949782,-0.038856737,-0.038359143,0.052669946,0.10326164,-0.013266265,-0.08631358,-0.0023183005,0.06765673,-0.01619642,-0.010556798,0.028147327,-0.015004258,-0.02610643,-0.028506445,-0.17547628,-0.05711317,-0.038349647,0.0038888368,-0.035253875,0.053011674,-0.034131933,-0.054464895,-0.008487588,0.049788304,0.032046746,-0.054032777,-0.016026717,-0.008306154,0.012439662,-0.04450963,0.07155582,-0.030957526,0.05808009,0.02053337,0.020608833,0.013586661,-0.027167706,0.04272952,0.060909517,0.11061364,-0.0026812572,-0.051120386,-0.035202526,0.027972527,0.05136124,0.023997074,-0.06389412,-0.07939141,0.007983017,0.08576523,0.014584909,0.04579198,0.012107748,-0.026996206,0.03749533,-0.009343167,-0.013652648,0.019505503,0.05839289,0.0063307527,0.023236368,0.08908134,0.02803213,-0.028330535,-0.043933146,0.029619679,-0.04342221,0.061709154,-0.066206455,0.027770173,-0.02952407,-0.059997484,0.08646565,-0.008206227,0.020875944,-0.0054609887,0.05237498,-0.040753704,0.114434496,-0.028598767,0.03241559,0.072269835,0.03170795,0.088287085,0.036539316,-0.12770553,8.4555175e-33,0.0025060768,0.02113083,0.011318161,0.08245616,-0.0049263276,-0.023197094,0.02351469,0.030996876,0.046677668,0.08062333,0.005250946,-0.055488154,0.007380839,0.022905195,0.046918415,0.05604045,-0.06521294,0.0999008,-0.009563037,0.051845312,0.06633201,-0.07232997,-0.009403527,-0.059918847,-0.024883855,-0.0106159765,-0.032912925,0.017930921,-0.013551685,-0.0128435,-0.02405924,0.095811434,-0.01536883,0.10974527,-0.018471813,-0.020785846,0.033844605,-0.07027121,0.06862059,0.03740312,-0.05322063,0.019149926,-0.006176557,0.0064433906,0.040868014,0.013600876,0.010175598,-0.02082246,-0.11973535,0.07964095,0.00077148253,0.055558868,-0.1202474,0.0319432,-0.078543894,0.054424074,-0.0731759,-0.08156457,0.04531609,0.021506822,-0.03158059,-0.02097709,0.002310658,-0.08529123,-0.022846336,-0.004738259,0.039875314,-0.037520234,0.037696328,0.0630371,-0.012847864,-0.10262265,-0.043841094,-0.0067790844,0.063049465,-0.010613059,0.15177426,0.06864868,0.09110982,-0.061442256,0.038828727,0.052044794,-0.018568363,-0.08080316,0.053804614,-0.06168419,-0.011503947,-0.090987116,-0.051808387,0.021225886,-0.009430794,0.029229412,-0.060715746,-0.017169354,0.035499383,-6.10124e-33,-0.024051778,0.044767775,-0.046050213,0.022987235,-0.044196483,-0.091471285,0.03378648,-0.058420487,-0.0284446,-0.09024419,0.05729876,-0.069416836,0.0922247,-0.009021747,0.006658338,-0.0026027956,0.035015386,-0.005679537,0.054743998,-0.0074487412,-0.06497152,0.042008992,0.055320475,0.02349805,-0.072423115,0.033502683,-0.11578887,-0.023788188,0.07720815,-0.04299963,-0.07816891,0.022007465,-0.034947407,-0.03829758,-0.018311922,0.06741397,-0.06209251,0.012420246,0.0802138,0.13511896,0.025744604,0.03378687,-0.08853851,0.0044657835,0.0071164677,-0.010146575,-0.009215564,-0.031553686,0.061755225,-0.051453937,-0.044483244,-0.014790959,-0.011994151,0.047001276,-0.021164024,-0.0016870623,0.026312612,0.028071368,-0.0068467217,0.039196577,-0.0056850947,0.031090125,-0.046936356,0.06229851,0.022785349,-0.0108606955,0.10988609,0.083728544,-0.030061683,0.022837952,-0.09317983,-0.08038261,0.090446755,-0.013505723,-0.0035254655,-0.017343795,-0.044881523,-0.03124418,0.020585034,0.020222466,-0.03937462,-0.011768178,-0.025433611,-0.01195149,-0.012232186,0.045229018,0.034693267,0.0040402934,-0.016886171,-0.018060636,-0.03572574,0.0075627184,-0.003917085,0.005329775,0.052008018,-6.967287e-08,-0.025027405,0.04291274,0.010074746,-0.027084967,-0.0056819585,-0.08948708,-0.056663383,0.109788395,-0.008856178,0.024657665,-0.016349308,0.020627359,-0.070837036,0.035000626,0.0199349,0.030805996,-0.06780572,0.10082767,-0.010785188,0.030601159,0.020918244,-0.054371387,-0.003173525,-0.063135125,0.085175924,-0.012289581,0.029866293,0.053799376,-0.0015810715,0.048545934,-0.024576603,-0.053589944,0.027544526,0.051959705,0.070555136,0.037689954,-0.01442619,-0.022458164,-0.0058049713,0.013964445,-0.062445797,0.0633027,-0.019948116,0.0117786005,0.03866036,0.009020466,-0.005564233,0.02312776,0.015816808,-0.06159044,0.057382446,-0.035595253,-0.04827047,0.06439779,0.087190464,0.04916257,-0.05198363,0.0742998,0.0011136786,-0.025078416,-0.077575184,-0.057035875,-0.102742195,-0.08188248,8,-1.2828623,3.7660735,5
233,"1. we learned closed form solution in multiple linear regression .
2. in feature selection we remove the feature if it's  p value is greater than 0.05  
3. we use 80-20 random split b/w test and train data . 
4. overfit can happen if r2 values of test and train data are not close enough .
5. we learned about adjusted r2 and want a model to represent population and also about 
n-1 degree of freedom in tss",-0.060220007,-0.0095871175,0.103757285,0.1256634,0.14291,0.062650494,0.0083177285,0.057879888,-0.07456852,0.075987145,-0.0037574198,0.02189238,-0.0761666,-0.011284806,0.043135237,0.018094849,0.028557964,-0.053620674,-0.031509884,-0.044600468,0.046187874,-0.031351753,-0.0015213746,0.051306903,0.0041996096,-0.047532056,0.044072058,0.030048905,0.008531583,0.0029997171,-0.021107558,0.06324282,-0.037586357,-0.09502667,-0.040735777,-0.057486173,-0.03823715,0.06040942,-0.028569078,0.037918434,0.0150344325,-0.037133757,-0.03267803,0.028661657,0.08874564,-0.003206083,-0.08708884,-0.018353764,-0.00400125,0.037818734,-0.05485344,0.013899142,-0.03641077,0.017510984,0.0072465925,-0.18903787,-0.08408579,-0.03592682,-0.03281816,-0.017163761,0.024579909,-0.013858601,-0.06276618,-0.023969794,0.10168563,0.07591008,-0.11705209,0.039135735,0.00083368464,-0.023013508,-0.0680648,0.04138556,-0.082530625,0.01672056,0.0384048,-0.044771418,0.0065815025,-0.03193013,0.016810458,0.08026608,0.008139758,0.0031158193,-0.021926941,-0.06306453,0.0152916275,-0.020820258,0.0151722245,-0.050057817,-0.016330704,0.023231147,0.06993026,0.008416872,0.05155102,0.044963323,-0.053178515,0.02098797,-0.033416238,-0.01722538,0.0223261,0.021613184,-0.028003382,0.020295229,0.05055736,0.06307516,-0.010251477,-0.016119123,0.01903002,-0.061096825,0.105404705,-0.07405983,0.00960922,0.019166501,-0.036229163,0.056936003,0.0035388828,-0.010932298,-0.075357296,0.04487771,-0.011651783,0.060082804,-0.013985136,0.014292288,0.017448127,0.017162863,-0.0034440293,0.03787067,-0.07206241,6.902597e-33,-0.03660248,-0.019154346,-0.03567961,0.038090374,0.049172267,-0.0020662528,-0.040829837,0.016808072,-0.001805647,0.06378298,-0.03331011,-0.08171328,-0.008701474,-0.008021105,0.087906204,-0.04785793,-0.009986602,0.122999564,0.039187912,0.06779913,0.077009745,-0.029800322,-0.0036954416,-0.07756943,-0.0034840673,-0.041320506,-0.055856746,0.013753236,-0.039206937,0.0032163004,-0.00069725176,0.067915976,-0.07517029,0.086880274,-0.05321934,-0.0005043338,-0.05214118,-0.0480761,0.021721087,0.057632048,0.0074555664,-0.0055907583,0.0068021445,0.040651757,0.047287088,0.002454782,0.026336683,-0.07960571,-0.08383051,0.037456613,-0.04247582,0.04053854,-0.11934987,-0.022426844,-0.009212284,0.08754506,-0.04178004,-0.061917275,-0.0038015146,0.050068446,-0.04998809,-0.020540783,0.0377294,-0.10525302,0.0076695825,0.010696541,0.01622631,-0.078002505,0.079842396,0.008212733,-0.054550342,-0.06898324,-0.017951543,-0.02776222,-0.0027884382,-0.0033238896,0.16071674,0.065442644,0.08382511,-0.09499121,0.004786031,0.01502484,0.00668515,-0.061270546,0.10845823,-0.05425546,-0.053219862,-0.089830615,-0.0426609,-0.057066556,-0.044571977,0.04002011,-0.08808981,-0.022339735,0.027660416,-5.6817556e-33,-0.055888046,0.061651804,0.02527483,-0.0033360664,-0.086078875,-0.07031881,0.04548373,-0.033768326,0.015803862,-0.023605809,0.0517038,-0.053361446,0.08827485,-0.024126101,-0.007842766,-0.037348814,0.063155,-0.034199037,0.047261283,0.0061435783,-0.056988318,0.030602345,-0.009482782,0.0634066,-0.03696076,0.020182626,-0.14347838,0.002896378,0.06425472,-0.02502792,-0.04973561,0.0052145417,0.011445599,-0.016784338,0.005182939,0.017801799,-0.028293263,0.03071447,0.057691984,0.16955447,0.065811805,0.039014798,-0.07326299,0.03064847,-0.0055603147,0.024947347,0.0115229115,-0.03364075,0.07104084,-0.036775827,-0.012489955,-0.015587355,0.016653595,0.037949458,-0.049101315,-0.03081992,0.037194233,-0.004283464,-0.00618491,-0.008405052,0.07047746,0.019132968,0.021787072,0.065251276,0.088082634,-0.018035853,0.039888863,0.047448367,-0.0114207575,-0.032101862,-0.04818341,-0.059758097,0.12176205,-0.0060677007,0.02792461,-0.060068972,-0.02903132,-0.0033491175,0.037497982,0.018678643,-0.08338434,-0.023539826,-0.015550993,-0.028820321,0.00031263914,0.018361729,0.03598487,0.043105964,0.059281677,-0.051271684,-0.03226129,0.03638958,0.009454269,-0.010748182,0.04004354,-5.4028646e-08,-0.05600198,-0.012879661,0.03251078,0.013877906,0.040815167,-0.01987684,-0.06588202,0.08355,-0.023318015,0.028020125,-0.052282743,0.028634872,-0.052518696,0.05723106,0.0758615,0.0033504246,-0.048614554,0.1313991,-0.0046348153,0.012424939,0.061511286,-0.038068924,0.012039567,-0.0025196918,0.084237374,-0.026356088,-0.042586777,0.031180982,0.028864574,0.058019035,0.006965269,-0.027932897,0.04585788,0.040913384,0.12269912,0.052490517,0.00047506348,-0.020729778,-0.033346247,0.075170204,-0.047631975,0.041705698,1.4174714e-05,0.028819095,0.036215413,0.0062418412,0.03558765,-0.054130137,-0.052075323,-0.067422815,0.055129062,-0.0059425435,-0.018035674,0.013056926,0.058570083,0.07866587,-0.040063765,0.071856365,-0.075196765,-0.063030146,-0.056910705,-0.02331139,-0.07113286,-0.039522424,8,-1.0976871,4.533042,5
235,"sir talked about closed form solutions in mlr. 
this is not feasible due to two reasons
(1) multi colinearity
(2) matrix inversion
if p value is bigger than 0.05 then values are strategically similar to 0 and don't help much so these feature values are deleted.

then sir said that we should never use the entire data for taining our model. after cleaning our data (which would be most of the task that we would do in data science) we should split the the data into two parts - training data and testing data. now the question arises what should be the ratio of volume of the two parts? it should be 80-20. we can keep 50-50 but that won't be enough to train our model and this much data won't be enough to represent the entire population.
how to split data? there are functions in python which do this. sir also said that people who are not familiar with python should start working on it.
then sir talked about overfitting of data and gave us example of the worst case of over fiting.

if r-square values are near then over fitting issue won't occur. i am talking about the r-square values of training and testing metrics. sir also gave a formula for adjusted r which is mentioned in notes which has degree of freedom and tss and rss. i don't know much about the notation because in the last semester (in ai/ds) the notation was a bit different so of you are reading this please check notation.
sir also talked about multiple r = square root of r-square.",-0.035004918,-0.013406626,0.037930675,-0.029105332,0.026241735,-0.0967239,-0.12023684,-0.007361273,-0.06793584,-0.006536966,-0.006220681,-0.012341094,-0.015686411,0.049286366,0.03774277,0.046087537,-0.0378042,0.011277903,-0.11152504,-0.013532413,0.009664035,0.010547103,-0.054934904,0.10471449,-0.03858198,0.009850229,0.04277196,-0.031375214,-0.0086751385,-0.020668538,0.16070393,0.03562954,0.031030355,-0.06475918,-0.06845721,-0.046364427,-0.03874024,0.05758408,0.010531435,0.043801934,0.045789648,-0.018050445,-0.043619886,-0.0004099165,0.007946536,0.029141502,-0.04436871,-0.016039982,0.096137345,0.03560279,-0.06363469,0.05843575,-0.05660838,0.06919428,-0.015673101,-0.11939895,-0.0019483161,-0.09644418,-0.06417464,-0.025677923,-0.024864266,0.06944744,-0.017680332,-0.034082912,0.018184366,-0.023832515,-0.042554375,0.0024399958,0.0077233766,0.05251894,-0.0678664,0.025507933,-0.03752137,-0.011531058,0.0030848943,-0.026783435,0.0075398893,-0.020909172,0.012449517,0.01645402,0.057035927,-0.0055031027,-0.050606474,-0.020955913,-0.09785965,-0.060373373,-0.0013917958,0.09563564,-0.02519615,-0.054823115,0.04048411,0.028213441,0.046574168,-0.002608853,0.043499254,0.05088936,-0.030436898,0.011888485,0.081197396,0.051597007,-0.012189924,-0.00091290165,0.056546345,-0.01583736,-0.028691707,-0.09455923,0.007462354,0.020017453,0.107799426,0.006831052,0.07796742,-0.0034047137,-0.056485865,0.026099604,0.1013037,-0.032623187,0.004853177,0.038533174,-0.040408768,0.10588052,-0.017737158,-0.031964876,0.023705889,0.04601171,0.115200326,-0.05478396,-0.10564261,6.4091824e-33,-0.1011302,0.011330823,0.045700513,0.10268224,0.016353972,0.038967576,0.02414204,0.03589147,0.067549676,0.04308995,-0.03487861,-0.0005749247,-0.0136667965,-0.016269246,0.02552718,-0.038091306,0.011305423,0.11090039,-0.10722595,-0.09005695,0.032256942,-0.04300845,0.030763622,0.0106298365,0.013776792,-0.119313315,-0.034503095,-0.04673533,-0.02704932,-0.0060105515,-0.06594386,0.020619206,-0.017973328,0.06983085,-0.010213766,-0.00885429,-0.0067980364,-0.024554314,0.02013474,-0.031286206,-0.019503225,0.03077854,0.060109705,-0.025943035,-0.024019768,0.007215773,0.023141755,0.0042980355,-0.07642376,0.042321894,0.022700971,0.007807615,-0.06175229,-0.03858073,-0.06898716,0.037468955,-0.06108223,-0.051698696,0.01474536,-0.01804127,-0.038411107,-0.020410124,0.04358996,0.006180953,-0.012973403,-0.025660103,0.022229103,-0.023481153,0.040003195,0.063279845,-0.050581776,-0.075244516,-0.045446668,-0.03787261,-0.057108313,-0.025818015,0.118230656,0.022127122,0.05110189,-0.03308541,0.02373552,0.06926629,0.026771082,-0.056567,-0.0052421167,-0.021066785,0.021757048,-0.003970726,-0.06967342,-0.013025548,-0.09496282,-0.041478932,-0.04593899,-0.08043643,0.04534957,-6.8500104e-33,-0.021861814,0.050826225,-0.023808822,0.0062757963,-0.0024953182,-0.043994922,0.06474598,0.06708248,0.026865823,-0.024243934,0.007891758,-0.031414103,0.14052716,-0.04137677,-0.07549457,-0.00765237,0.050312355,0.037508275,0.063061364,0.043394204,-0.09065105,0.024606796,0.0039214906,-0.0048618065,-0.076792374,0.019730221,-0.06460338,-0.047470067,0.095463164,0.016645385,-0.096078485,-0.06361384,0.036047198,-0.049290676,-0.04589317,-0.027255418,-0.0053426353,0.007901958,0.08303196,0.048523966,0.05004089,0.047510732,-0.11609315,0.018141318,0.0051080557,-0.07400905,0.064470164,-0.039947994,0.0063553746,-0.05067452,-0.007600762,0.026977785,0.0014739351,0.018518256,0.012428495,0.025295258,0.023468766,0.02653853,-0.03808371,-0.06217162,-0.064210825,0.036614876,-0.030773394,0.044049375,-0.0057912203,-0.0014963249,0.07656437,0.034645002,-0.016351258,0.03368092,-0.029316574,-0.028237602,0.18512149,0.006611584,0.022049244,-0.0039405967,-0.076137766,-0.07031859,-0.009021207,0.05841653,-0.06657983,-0.079763696,0.009450652,0.02313024,-0.014752422,-0.055563718,0.10135299,-0.014723331,-0.03463497,-0.06650237,-0.050044972,0.039475523,0.07287093,0.025026014,0.028245775,-6.6550385e-08,-0.01869782,0.030668747,0.012217928,0.020483026,0.014743581,-0.035661586,-0.067044884,0.09449328,0.037255406,0.110675365,0.03858068,-0.024649901,-0.092309974,0.021660492,-0.0067711184,0.04579427,0.00675447,0.038433723,-0.026964478,0.011879235,0.020093255,0.0029860518,0.0062370272,0.005660544,0.07990842,0.016783545,-0.005543649,0.06718095,-0.009829932,-0.0642148,-0.002029592,-0.060247097,-0.014959646,0.10155168,0.0906937,0.013063516,-0.015008426,0.054472726,-0.022546105,0.02601128,-0.036466643,0.030398086,-0.041866843,0.019946767,0.13424127,0.048703235,-0.020873949,0.051888797,-0.028147196,-0.012536391,0.05205803,0.058360312,0.011818716,0.04151743,0.084991075,0.01597395,-0.066468485,0.100985624,-0.06056543,-0.029167417,-0.02915893,-0.021989392,-0.10795115,-0.07891625,8,-0.8644628,1.532764,5
275,"today we learned about multiple linear regression on independent variables. we learned about training, test data and what are the conditions for feeding data in machine learning model. we discussed that 80 % of data should be used to train and rest should be used to test the model.
we then learned about confidence interval and training matrix.
then we learned about importance of r^2 and square root of it and how do we see the values for them as goot fit or bad fir
then we moved to python where sir used many libraries  such as skewness and kurtosis statistics.",-0.024335489,-0.074752286,-0.09399631,0.043195743,0.07432955,0.00515868,-0.013184225,0.021378718,-0.021157408,0.0067329034,0.021902543,0.13473967,-0.015985716,0.035833467,0.027182428,-0.025351562,-0.052338112,0.03833344,-0.07839047,-0.104828954,0.026177043,-0.047115006,-0.0067036417,-0.008780357,0.072100356,0.010372546,9.1616545e-05,-0.01987886,-0.05499753,0.011417729,0.037572727,-0.022921678,0.016648319,-0.011769663,-0.070379384,-0.069618694,0.0698618,0.055242896,0.04167997,0.023396062,-0.015812222,-0.06415348,0.039559156,-0.03609878,0.056590967,-0.034658026,-0.07782465,-0.0973213,0.04299862,0.004610643,-0.07969889,-0.10889035,-0.012990808,-0.04086477,0.030335035,-0.0728652,-0.029100623,0.018517705,0.012814215,0.016344808,0.051379874,-0.050853703,-0.07051179,0.037386667,-0.008331674,0.015792357,-0.06527135,0.046200108,-0.03655684,-0.013761185,-0.121368475,0.003513676,-0.074667454,0.0114113195,0.066660814,-0.026699105,0.001907833,-0.0672791,-0.010023645,0.033269163,-0.004990203,0.11125787,0.023438888,0.026803957,0.01626343,-0.01071727,0.03482741,0.050957207,-0.057635114,-0.018127276,0.070789084,-0.02400818,-0.0002805456,0.047583867,-0.018463634,0.06274453,-0.0011022693,-0.07147849,0.03368422,0.046747852,0.017395653,-0.034561247,0.036216795,0.090752274,-0.00869494,-0.017249856,0.03639853,-0.020593677,0.1046365,-0.025741601,-0.054875553,0.06696696,-0.10141731,0.036996726,0.04612522,-0.02094151,-0.039514862,0.019214742,-0.044267576,0.035287846,-0.04435084,0.05294192,0.1265678,0.017748594,0.052185174,-0.0146033745,-0.13927639,4.9641207e-33,0.009957408,0.018199537,0.0037210505,0.014161807,-0.017046006,-0.06700746,-0.011871685,0.012045556,0.09020098,-0.022845019,0.04356859,0.059061114,-0.038907778,0.036047723,-0.018316133,0.08013695,-0.032686006,0.07280772,0.032023378,0.077539414,0.05268695,-0.050822224,0.075204104,-0.00906594,0.019267596,-0.0030975621,0.015264267,0.03855785,0.0044074547,0.026884343,0.014642183,0.043179035,-0.08650657,0.009692503,0.023011362,-0.020575412,-0.07451223,-0.03625877,0.0023660655,0.041200377,-0.0017354591,0.001393552,0.086930715,-0.06264704,-0.0071950783,0.050549865,-0.0008656348,-0.015134592,0.038067147,-0.049125224,-0.06547244,-0.018794576,-0.046236675,-0.007126958,-0.029954705,0.13234428,-0.072841056,-0.05077355,-0.030163506,-0.03624554,-0.060517292,-0.026324486,0.04849078,-0.07736699,-0.035528865,0.06344064,-0.02776685,0.001955283,0.036484834,0.060488485,-0.03409453,-0.08241606,-0.046853017,-0.043666244,0.07478475,-0.003465872,0.021969058,-0.04254711,0.011029628,-0.028220378,0.06950142,-0.0054089385,-0.001366273,-0.1197181,-0.051941287,-0.012464413,-0.034790877,-0.111467674,0.01057734,0.02676416,-0.09573274,0.0070773694,-0.07897001,-0.005525338,-0.042544723,-6.621907e-33,-0.038329512,0.0571684,-0.036336664,0.070828155,-0.020305462,0.021523362,-0.018093018,-0.008194277,0.0033693176,0.0021960824,0.0054592253,-0.031409014,0.01968289,0.0634036,0.038980115,0.039251145,0.015106272,-0.06278775,-0.017586047,-0.060596745,-0.058970347,0.057403687,-0.016788399,-0.04798557,-0.04508276,-0.014235677,-0.1089775,0.012439656,-0.07703059,0.013580497,-0.02570375,0.015879149,-0.015484878,-0.032207586,-0.005932545,0.050134297,0.026497765,-0.0072023827,-0.0106398435,0.044489495,0.06620476,0.08583703,-0.014710836,-0.03023631,-0.04396599,-0.002923906,0.029649895,0.016687732,0.04763396,-0.044484936,0.008142546,0.023188593,-0.002605609,-0.054321,-0.036151182,-0.025510399,0.016353639,0.033305924,0.032368463,0.004320318,-0.120508485,0.008480225,-0.05665731,0.090500794,-0.05924347,-0.008582338,-0.061494116,0.012635527,0.0039696535,-0.043010227,-0.046225026,-0.026043633,0.06984615,0.014041974,-0.08697557,-0.031829115,-0.05819707,-0.08172539,-0.10382742,0.10590797,0.021235002,-0.04610232,0.0014934005,0.022354694,-0.023436671,0.10728682,0.1298483,0.03607811,0.031110162,-0.10856264,-0.054708496,0.08556374,-0.08470747,0.00044452332,0.049607046,-5.1761504e-08,-0.031726103,0.018020708,0.124484986,0.01714605,-0.02535379,0.0042437185,-0.026241021,0.15981951,-0.06825905,0.08791582,-0.0074570165,0.002085014,-0.07194057,0.005232477,0.06956251,0.005000099,0.041013476,0.06833235,-0.026827862,0.005194919,0.092317246,-0.037606366,0.04518681,-0.006044998,0.06587013,-0.029212179,0.08022574,0.084399715,-0.013562868,0.07736778,-0.013006536,0.020840954,0.024344971,-0.053785793,0.07068706,0.053431515,0.06380757,-0.054684028,0.0436259,0.029914936,-0.09017551,0.05592378,-0.021406984,0.03866838,0.030847868,0.055484656,-0.016539134,-0.006838244,-0.027475215,-0.056710716,0.08188904,-0.034890674,0.0038255304,0.06842608,0.08247374,0.014013247,-0.032156404,-0.0122596305,-0.06809313,0.013729185,-0.025439495,0.0026529238,0.014494605,-0.037574723,8,-6.4336886,0.82441753,5
309,"we have learned why not to use whole sample data for training itself so basically if we have a large dataset we can use 80,20% for smaller datasets can use 90,10% in a randomized way from these 2 sets we get training matrix, testing matrix upon comparing the r^2 value if they are close to each other it is a general model and a fairly good data if separated by each other then it is a overfit situation.we also learned that with increase no. of variables there is more scope for the variance to be captured and improve r^2 then we saw why n-1 term in sse in unbiased as we know mean  total unknowns drops by 1then we have a handsome experience of libraries like sklearn on jupyter notebook.",-0.006223384,-0.060317937,-0.036705688,0.069238074,0.118842475,0.038016506,-0.04301863,0.06490374,0.03534091,0.006817878,-0.09134319,0.11292109,-0.014968138,-0.017131235,0.021373456,-0.072917216,0.055571783,0.0012881969,-0.07676364,-0.07442148,-0.014722659,-0.03138472,0.027251465,-0.027348708,0.06395681,-0.06465836,-0.03916712,-0.020326452,-0.034128346,0.03689559,0.11641931,0.04889167,0.004010579,-0.05424467,-0.0918764,-0.071068525,0.043637507,0.022115352,0.039535902,0.072963,0.017867882,0.072328314,-0.035616655,-0.023496373,-0.00480383,-0.06327617,-0.005959453,-0.10496461,0.030440362,0.0002115874,-0.14846052,-0.06310369,-0.0073318444,-0.038495433,0.016620927,-0.071423754,-0.0403218,0.0056035086,-0.03641516,-0.012113825,0.055807833,-0.12095727,-0.042353354,-0.04083423,0.022583388,-0.0996044,0.005011895,0.027937647,0.007965463,-0.0073833433,-0.021060206,0.09809424,-0.08096964,0.046724062,0.034593042,0.022469161,-0.003925036,-0.008795858,0.07189822,0.005661561,0.043020297,0.020909617,0.03002763,0.013486191,0.0099938605,0.030549973,0.04588937,-0.008925531,0.0033148525,0.0284489,0.10776493,-0.029919509,0.0014628937,-0.023755383,-0.0045467927,0.03326425,-0.014804388,0.023989886,-0.012953225,0.046248384,0.028105145,0.03974903,0.057087835,0.03359103,6.689464e-05,-0.0028196012,0.11478403,-0.091679305,0.06741158,-0.07698107,0.0069270707,0.018303627,-0.039315213,0.058550075,-0.028832495,-0.01626942,-0.013812488,0.035914596,-0.047217213,0.05848576,-0.0734246,0.01631583,0.019094542,0.061581265,0.028555522,0.06830948,-0.11288599,8.933138e-33,0.012157592,0.051717736,0.02239426,0.0015409292,-0.03421004,-0.04381068,-0.017385611,-0.030619659,0.06971556,0.059292503,0.0135932425,0.0656674,0.017950404,-0.017407414,0.057127256,-0.022701437,-0.099099986,0.057892147,0.034202106,0.014169864,0.10389714,-0.076925956,0.03925307,-0.003960083,-0.017916715,-0.086614855,0.053955153,0.059755705,-0.020246292,-0.00480323,-0.044458017,0.013978893,0.008015629,0.06510853,0.027385738,-0.08418738,0.020985365,0.02819064,0.036266785,-0.01192415,0.010486212,0.08470533,0.08372956,0.011357944,-0.03140252,-0.08965362,0.03922495,-0.014816018,0.0044604647,0.041845985,-0.08039407,0.0064672725,-0.029697292,0.034226105,-0.019950781,0.14007334,0.044229593,-0.0016733407,0.0351071,0.024272317,-0.12138079,-0.06486287,0.028703988,-0.03641924,0.030107044,0.09499173,-0.03185922,-0.023250312,-0.0045335945,0.015803797,-0.0006814834,-0.08284104,-0.07147414,-0.029743709,0.089504555,-0.034887966,0.05688925,0.05180839,0.07383734,-0.061424274,-0.06691354,0.06321789,-0.0385834,-0.10626019,-0.069602884,-0.03925688,-0.0024772948,-0.09610964,-0.0134716565,0.030506091,0.0032925115,-0.053996224,-0.04144212,-0.063201465,-0.028473238,-8.348398e-33,-0.0796284,0.018881803,-0.008225487,0.09413358,-0.017610759,-0.022541394,-0.025218707,-0.030280024,-0.029115194,-0.04298976,-0.018013464,0.0109619405,0.061458003,0.032310102,0.017795922,-0.014589825,0.059192944,0.0040598577,0.03764144,-0.05620172,0.042184427,0.068272345,0.004266851,0.017644156,0.029940654,-0.0093252305,-0.12387893,0.04340812,0.04269939,-0.019217135,-0.021538392,-0.01813681,-0.009896637,-0.02519951,-0.022247866,0.035691883,0.04382667,0.042046614,0.020144198,0.052385718,0.033412848,0.017325107,-0.060353402,0.039967302,0.01471874,-0.027817054,-0.04412782,0.035632234,0.035090953,-0.049090922,-0.015770635,0.006562942,-0.037675276,0.008068931,0.014782302,-0.06524063,-0.01832522,0.057242937,-0.055795647,0.03097822,0.02302272,0.058472898,-0.0852226,0.024446547,-0.059993364,0.0070800222,-0.09662094,0.0859049,0.059873704,-0.012771654,-0.057896566,-0.040530197,0.040033616,-0.025829885,-0.07913185,0.040308338,-0.01854904,-0.027137944,0.0041056187,0.008627074,-0.082884334,0.013759604,-0.030710299,-0.06874753,0.054706316,0.14413917,0.07486529,0.0023828982,-0.017020876,-0.044574115,-0.049580015,-0.048890993,-0.0056947377,0.0679092,-0.025566317,-6.041599e-08,0.024308197,-0.006756598,0.08081124,-0.009330538,0.030222006,-0.11293229,-0.060555425,0.07792555,-0.02968168,0.06679135,0.003023886,-0.03544777,-0.11009789,0.0113914795,0.04884792,0.10872762,-0.046034943,0.05193607,-0.031147717,0.08953232,0.062187042,-0.02613347,-0.009849825,-0.016056003,0.08530235,0.024540355,-0.0109329065,0.025997812,0.022068508,-0.03612544,0.02313386,-0.06847272,0.045313735,0.02048836,0.09499822,0.0101331165,-0.022479057,0.0065746093,0.003854223,-0.026583677,-0.04929991,0.015368884,0.03440165,0.011716068,-0.01334898,0.047407154,-0.045409393,0.08044171,-0.011884378,-0.10475509,-0.013085121,-0.07182528,-0.07406146,0.008442135,0.021454116,0.05526257,-0.020240461,0.02073862,-0.0570685,0.04970044,-0.028546859,-0.04140613,-0.10087644,-0.033070035,8,-4.163121,4.4692655,5
311,in today's lecture we started transitioning from excel to python. we came across a python library called sklearn or scikit-learn which provides us a lot of machine learning algorithms on our dataset efficiently. we also learnt that error scores on it's own are not that good indicator for example if we just take squared errors it is not a good identifier of the fit of the model as more the data points more will be the error hence we should use mean of squared errors or infact root of mean of squared errors which will be a better indicator. also that r2 score is also not perfect every time and we need to incorporate number of features and number of data points by which we calculate the adjusted r2 score which is a better indicator. after that we fit a linear regression model on a dataset resulting plot of which didn't showed up as straight line meaning linear regression doesn't just mean a line it can be a polynomial function too. we also touched the topic of train and test data that whenever working on a dataset we should always split it into training and test data preferably in 80:20 ratio. we also discussed that the model efficiency also depends upon the dataset we have like it is possible that on a problem today a linear model might be a good model but after some time when the data points change completely some other model say decision trees might outperform.,-0.051154196,0.0071935426,-0.054998867,0.118141964,0.078543715,-0.016980588,-0.10832895,0.043671485,0.0097019905,-0.041806266,-0.074059695,0.06667861,0.06980821,0.029638736,0.019573022,-0.03165827,0.03174711,0.07531382,0.017091326,-0.049641717,0.018477725,-0.039214056,-0.021539545,0.034129336,0.09644056,-0.030153658,-0.026982104,0.029822499,-0.08454603,0.03465688,0.0497431,-0.032691658,0.004499988,0.001158006,-0.07586081,-0.055099662,0.09736681,0.025487775,0.024981024,0.0006263767,0.020802572,-0.04233761,0.050463345,0.022129107,0.022702096,-0.00857933,-0.021387808,-0.11650629,-0.035092473,0.025180796,-0.045134477,0.00609824,-0.038299218,-0.019250501,0.05372062,0.06179325,-0.031030415,0.023664117,0.06663708,-0.020874493,0.037973538,-0.07624594,-0.0063047297,0.03042273,-0.05513839,-0.055289384,-0.018271936,-0.007908562,0.02756217,0.03502334,-0.055720545,0.07470722,-0.0023095768,-0.014363746,0.06836315,0.0048590982,-0.012004538,0.007083952,-0.057370458,0.04317247,0.0049533485,0.022206586,0.007479463,0.017478624,0.016323907,-0.044148825,0.06779359,-0.023481691,-0.083214074,0.016680714,0.095150925,0.0019703624,0.021604154,0.02138551,0.010492742,0.07101228,0.022856008,-0.059931893,0.008416753,0.04455896,-0.016728228,0.04945271,-0.008555524,-0.048740532,0.08521484,-0.009286325,0.022291686,-0.039043915,0.11047245,-0.04744685,0.031817768,-0.0018684617,-0.085817836,0.047262833,0.084625475,0.015877161,-0.029579822,0.042489763,-0.027273469,0.086411744,-0.0604307,0.003795107,0.06707846,-0.010137553,0.060705103,-0.0010595861,-0.10635168,7.268162e-33,-0.047099404,0.020787043,0.022382062,-0.03632372,-0.00042103798,-0.11068008,-0.021890145,0.0018744101,0.08554748,0.025323285,-0.046934832,0.017413262,-0.0052836565,0.063228205,0.1115968,0.04924462,-0.005275224,-0.028917966,-0.0076702014,0.08921341,0.03190736,-0.1064375,0.066429004,-0.06130078,0.05240543,0.006081415,0.023602875,0.038736153,-0.036868967,0.012615848,0.009215265,-0.014379367,-0.006044647,0.03471871,-0.026374882,-0.094074324,-0.018523892,-0.02269765,0.052128952,-0.0044061644,0.022272624,0.05240289,0.029384324,-0.019525176,0.004806985,0.059717048,0.01756872,0.034042634,0.04861505,0.10340121,-0.03156665,-0.029104065,0.043511078,0.04408073,-0.033121478,0.04969419,-0.06482083,0.0028533232,-0.017081954,-0.03226391,-0.03542315,-0.05263861,0.052893266,-0.059592765,-0.03403284,0.066196516,0.05178012,0.02387466,-0.026054142,-0.002003168,-0.04380685,-0.042246003,-0.046700202,-0.014976868,0.117246546,-0.0796987,0.05133829,-0.07215468,0.021913383,-0.038370773,-0.011177813,0.013698565,-0.001329153,-0.1715956,-0.010434951,-0.041447222,-0.017754635,-0.058929354,-0.06531175,0.021290563,-0.10394699,0.0076168715,-0.07375975,0.01291428,-0.031842526,-7.26744e-33,0.043637406,-0.0031480584,-0.07163663,0.07793841,0.01905271,-0.007301738,-0.004040916,-0.034269962,0.01517074,-0.033483714,0.026494369,0.003883784,0.017177485,0.079068,0.03349465,0.018867737,-0.07260483,-0.014360855,0.0029067197,-0.07598722,-0.008065631,0.06662932,0.008187037,-0.027036311,-0.047328476,0.007204738,-0.09837588,-0.01105934,-0.07598949,-0.033164162,-0.011659394,0.044481162,-0.061575998,-0.04140826,0.02826657,0.0076650735,0.037337076,-0.09974099,-0.045680054,0.14948899,0.094136894,0.1613714,-0.106246,0.007842576,0.009414592,0.015604412,0.031373307,0.011248017,0.052252598,-0.020484826,-0.037291907,0.057277516,-0.059195705,0.029401805,-0.07073989,-0.011768172,-0.04024125,0.040767666,-0.039170463,0.032948922,-0.07246659,-0.03474147,-0.10144577,0.07404151,-0.047122974,0.040499996,-0.023839029,0.031377412,0.03940506,0.04729541,-0.058542084,-0.08677068,0.008746379,0.018598182,-0.08871153,0.033899114,0.014847025,-0.067722864,-0.08443593,0.01562587,0.03413405,-0.006965737,0.002370325,0.04384434,-0.031111445,0.03889557,0.055760335,0.048262864,-0.036193594,-0.04592066,-0.03260857,0.046051595,-0.036903504,0.03772817,0.017070876,-6.246158e-08,-0.07952172,0.024421299,0.08873041,0.014175484,-0.013071402,0.022351196,-0.082420625,0.1153623,-0.07928575,0.0034764642,0.0022905534,-0.035818275,-0.1227394,-0.019144133,0.027033407,0.09056564,0.047795787,0.07576652,-0.038076658,0.0372276,0.04597065,-0.034155663,-0.026955618,0.0063157026,-0.0052150474,-0.058858078,0.025629986,0.054648407,-0.05271862,0.0456476,-0.014919269,-0.042128295,0.053001944,0.0049314364,0.06692363,0.043597076,0.0028595144,-0.041236766,0.013991596,0.061118305,-0.05267125,0.090873376,-0.032027356,-0.0019806025,0.035107944,-0.025441818,0.0038824629,0.016733192,-0.04085489,-0.11265536,0.07953754,-0.025665827,-0.112145215,-0.013320525,0.05567482,0.010144755,-0.09897703,-0.00064176094,-0.08516974,0.0029199093,0.013359203,0.030108105,0.028455498,-0.051900316,8,-6.315488,2.82884,5
327,"we revisited closed form solution of linear regression and how it was impractical.we also learned like if pvalue for that variable(feature) is more than 0.05 we drop that variable as it is not contributing more to the model.sir also discussed train and test split and how 80,20 split is done generally and if r_squared value is too close then overfitting might be present and sir also told that sst should have n-1 degree of freedom and also introduced adjusted r_squared.",-0.053363223,0.03872134,0.035402447,0.10175682,0.10357581,0.1105447,-0.03851918,0.103295326,-0.08492822,0.007629889,0.029702732,0.085319825,-0.06212782,0.062218793,0.033832364,-0.011164807,-0.0108238915,-0.0029299445,-0.023399042,-0.044680912,0.047381222,-0.018627688,-0.0705876,0.041225772,0.078287676,0.00060351193,0.046417035,-0.071871884,-0.049027577,0.02959398,0.023044635,0.054200046,-0.05921147,-0.049833994,-0.06734926,-0.064926796,0.015889766,0.0095912125,-0.025314081,0.0042921086,0.0008659089,-0.062761225,-0.025734147,0.024437683,0.09458537,-0.02355423,-0.046141826,-0.06694559,-0.039275102,-0.014967358,-0.06912048,-0.04573308,0.025284082,-0.012926804,0.051178187,-0.12521356,-0.042711087,-0.01917069,0.009749053,0.019422464,0.016300853,-0.029949462,-0.11023748,-0.02558867,0.018482033,0.00016398789,-0.09664909,-0.037167285,-0.007443153,-0.00063210883,-0.03531383,0.07074239,-0.09233485,-0.01917603,0.024731148,-0.011169668,0.028065177,-0.008206064,0.031511188,0.069050886,0.029884096,0.030446166,-0.03639177,-0.048384015,0.02622618,-0.009258457,0.06455918,-0.010333488,-0.059383072,0.01757774,0.0818633,-0.018851213,-0.016635815,0.05058584,-0.020987283,0.027804855,-0.04029489,-0.041704223,-0.009877467,0.020078449,0.006585924,0.10029907,-0.025279105,0.007877431,0.012353167,0.016336912,0.14097928,-0.04391772,0.0572862,-0.0391893,-0.0025177437,-0.026439989,-0.05959942,0.009513858,0.008863445,0.020526558,-0.05575871,0.045459077,-0.061690103,0.01319504,-0.029612543,0.027566232,0.010194337,0.022741066,0.028883671,0.0390659,-0.11899552,9.5869675e-33,-0.03446751,0.04957915,-0.028630396,-0.026884696,0.049270395,-0.043288447,-0.037599582,0.052859936,0.07064145,0.014791927,-0.019969473,-0.11437803,-0.050165795,0.0022148283,0.022973672,-0.01659413,-0.027806446,0.108205095,0.020406168,0.020974133,0.0016075888,-0.033338573,0.027394526,-0.049054243,-0.01725143,-0.071841225,0.016067969,-0.04226578,-0.020472858,0.016706854,-0.03292683,0.03651628,-0.017221073,0.07773162,-0.0230078,-0.049168933,0.015498138,-0.0454643,0.07205865,0.012054517,0.02346744,0.027683794,0.031302143,0.028031973,0.07906967,0.026161803,0.048521243,-0.017785339,-0.07032453,0.10258363,-0.1192702,0.09764474,-0.118794024,0.00012137905,-0.060604263,0.089040264,-0.034751896,-0.019173741,-0.024931248,0.04700719,-0.037669454,0.0040437193,0.028650425,-0.07796566,-0.032311022,0.013182396,0.018364267,-0.054088984,0.021310335,0.038760953,-0.08213146,-0.055296764,-0.026053347,-0.057954904,0.07202497,-0.05305337,0.12126764,0.03954312,0.10853154,-0.14869456,-0.0033133999,0.05586758,0.0006652639,-0.118396215,0.029935302,-0.048486903,-0.029855601,-0.040146083,-0.050272528,-0.015597958,-0.053758428,-0.03548726,-0.04384491,0.04094041,0.049442075,-7.990352e-33,-0.065156616,0.08657102,-0.010976163,-0.0029725577,-0.0057759676,-0.06187733,0.021198843,-0.008425018,0.01646022,0.029851742,0.0973507,-0.03408793,0.0054652984,0.042183723,0.006297264,-0.01312444,0.035905395,-0.037918486,0.030201003,0.01900237,-0.043643754,0.07077278,0.017800549,0.07111393,-0.0041937022,0.03575168,-0.11571076,0.05378803,0.01204771,-0.026123498,-0.00019653921,0.031458843,-0.026966568,-0.061204527,-0.020492613,0.0336557,0.007609255,0.024043337,0.042223625,0.105785154,0.090960905,0.026947202,-0.09920531,0.0021090975,0.013542907,0.00908023,0.04588092,-0.034715574,0.06162126,-0.027682748,-0.014041481,-0.044771444,0.019321686,0.043311097,-0.012705994,-0.009776729,-0.008144933,0.028627718,-0.04515204,0.0074553303,0.050525296,0.050351694,0.013793821,0.009056213,0.07187988,-0.052813552,0.020233138,0.061163403,0.040947028,0.0063770576,-0.0671426,-0.035006925,0.070623346,0.024207955,-0.050031547,-0.046687704,-0.05199776,-0.050020587,-0.010071576,0.037074834,0.010275727,0.031190997,-0.011415902,-0.09030898,0.022289556,0.021723228,0.040583834,0.0056133075,-0.0020492084,0.009437044,-0.10632181,0.06688543,-0.0008507086,0.05345521,0.0174843,-5.8688475e-08,-0.031639196,0.011855486,0.018029304,0.03097601,0.03046027,-0.07278683,-0.0069524706,0.084839836,-0.04135465,0.096720785,-0.048755802,-0.035574436,-0.061204866,0.061763715,-0.012579526,-0.043607213,-0.04760505,0.10747483,-0.040896088,-0.0031092844,0.14187023,-0.04215905,-0.014406051,0.008290419,0.04228219,-0.056140754,-0.009855191,0.11239146,0.015006344,0.011344177,0.0276987,0.023250537,0.013880188,0.051653378,0.061490756,0.06941751,0.07832415,0.00056927186,0.03013414,0.070906274,-0.069322325,0.02951175,-0.0244197,0.030755749,0.037785772,-0.009596367,0.040810112,-0.06180435,-0.010645797,-0.116767645,0.05849688,0.008755702,-0.026523286,0.00992828,0.07970716,0.06637116,-0.06282983,0.07894868,-0.09825372,-0.009746721,-0.093003996,-0.06935515,-0.040601075,0.0021399441,8,-1.4480753,5.0898585,5
329,"the closed-form solution in multiple lr was discussed with the formula for it(matrix one) .we do not select the features if the p-value is higher than about 0.05 of characteristics. data splitting is usually done by taking an 80-20 random partition between train and test. significant differences in r2 for test and train data indicate overfitting. in building a model to represent the population, we have also looked at modified r2, and talked about the idea of (n-1) degrees of freedom in total sum of squares .",-0.034219913,-0.014163105,0.030060308,0.04844988,0.124700405,0.08632451,-0.101749256,0.064743795,-0.07509429,-0.028883738,0.044971906,0.017397478,-0.044788588,0.009414102,0.015385073,0.028171375,-0.06959245,-0.034632027,0.008603829,-0.029568179,0.02808142,-0.062603615,-0.049693614,0.026125513,0.029273514,0.0045895707,0.05557485,-0.017342407,-0.0060092197,-0.0142633,0.10614036,0.06956326,0.011546418,-0.111582085,-0.083242685,-0.05769447,-0.05693216,0.060411144,0.026663901,0.08560154,0.015428682,-0.08283753,-0.022147153,-0.0016188453,0.0854721,0.02924433,-0.098754965,-0.014889193,0.00042703195,0.009295242,0.008845793,0.039875116,-0.040313393,0.025810357,-0.0129857985,-0.1980215,-0.11330704,-0.029939292,-0.046861053,0.08851043,0.008353477,0.051041424,-0.042042136,-0.026992226,0.08844471,0.03589552,-0.083249114,-0.0012147162,0.037807375,-0.025225587,-0.050075438,0.042119395,-0.04742045,-0.020186909,0.007730241,-0.024839249,-0.0006605364,0.015758662,-0.00842533,0.03540108,0.007901364,-0.0046370807,-0.07524241,-0.092319146,0.008705848,0.0020352057,0.035103228,-0.019303259,-0.03787204,-0.036176123,0.037008364,0.016945774,-0.02788466,0.049384426,-0.0705017,0.042312555,0.005802096,0.014711525,0.08265942,0.014720187,0.036864247,-0.014492675,0.0718737,-0.01524466,-0.033237804,-0.027075188,0.08417604,-0.059672873,0.07894217,-0.060194038,0.0799094,0.0024850187,-0.028109903,0.05325214,0.00808957,-0.031031448,-0.10175452,0.049701266,0.06333392,0.063535064,0.023487769,-0.09976907,0.08302033,-0.0030555793,0.08718409,-0.035795353,-0.088309005,4.9198703e-33,-0.055445418,0.01400516,-0.029181298,0.037330076,0.018578477,-0.023460511,-0.015936814,0.023689387,0.012583897,0.07568482,-0.059226327,-0.0846139,-0.024062654,0.006061864,0.054096416,0.007057346,-0.0952046,0.064752385,0.019754328,0.02262384,0.09965748,0.0073619965,0.012937498,0.0013651575,0.012579651,-0.022229237,0.0013805776,-0.07043852,-0.033704113,0.005812816,0.01751801,0.029075019,-0.05622992,0.110023364,-0.03891312,0.013491116,-0.028038401,-0.007999447,0.056932736,-0.032897204,0.062535636,-0.028730113,0.048149485,-0.0021609042,-0.008871542,0.021878978,0.009117933,-0.032965563,-0.08627002,0.031719632,-0.0209104,-0.01937574,-0.12056781,0.015900845,0.0065702004,0.065422624,-0.09137272,0.050999757,0.0040672347,0.085532255,-0.049109146,0.051983338,0.027116701,-0.04417757,0.044162236,-0.04413111,0.0078722285,-0.09822489,0.049717024,0.06564429,-0.041419517,-0.09518051,-0.0050626593,-0.06406526,0.020407822,-0.060110185,0.07016012,0.05483508,0.058448397,-0.043192685,-0.025255164,0.071798176,-0.016426822,-0.04608459,0.040583365,0.0101643605,-0.034513824,-0.092205904,-0.03638058,-0.03911364,0.009793817,0.020970685,-0.048348997,-0.031147012,0.045680206,-4.153823e-33,-0.026942728,0.05086928,0.069668524,-0.0007529741,0.0056452733,-0.07971824,0.07604981,-0.021434683,0.028445888,0.06821685,0.09199317,-0.0808541,0.07074627,-0.0017117329,0.04429112,-0.01146349,0.009259375,0.039652098,0.021337083,0.050022744,-0.06380269,-0.00542087,-0.028852094,0.013417745,-0.046933927,0.017237809,-0.0399841,-0.030744975,0.08916034,-0.028480276,-0.036035523,-0.05260702,-0.014534112,-0.05232612,-0.033137307,-0.022908255,-0.009838095,-0.0023125964,0.0885632,0.14872974,0.05096474,0.03735617,-0.029566232,0.017163234,0.0028498282,-0.026519539,0.018781833,-0.05777326,0.036183976,-0.023555085,0.012882792,-0.036948524,-0.050570026,0.00793408,0.017010618,0.011585429,-0.01474799,0.028643284,-0.032212023,-0.026289402,-0.004484082,-0.02732516,0.0022330384,0.06412673,0.009378916,-0.06079652,0.043896504,0.012683504,-0.038526192,0.059014,0.017502207,-0.01762523,0.11988888,0.029971188,0.020522153,-0.08779121,-0.08077139,-0.023625081,-0.026056658,0.006894335,-0.06717966,-0.053139754,-0.0009430518,-0.026782354,0.00242693,-0.010306914,0.030353934,0.016019722,0.009678344,-0.09241531,0.04459953,0.10416075,0.06554425,0.018268866,-0.02011641,-5.9341488e-08,-0.03308013,-0.03421068,-0.00914222,-0.019866139,0.027709177,-0.015627956,-0.03266373,0.08139375,-0.047313303,0.07003027,0.034150776,0.0022431314,-0.06013907,2.3083507e-05,0.060741793,0.04251388,-0.029860705,0.14693758,0.0101204,-0.00034135795,0.10450653,-0.035754997,-0.001197628,0.0004610678,0.015972512,0.00967116,-0.03396691,0.050591316,0.047809694,0.002382861,0.06271804,-0.008215645,0.080229215,0.019070681,0.10924804,0.062408734,0.042358182,0.022375228,-0.006457116,0.038483378,-0.025090981,0.076255575,-0.07149012,0.033613913,0.09880591,0.014985694,-0.015528134,-0.09287556,-0.030085972,-0.06403334,0.009575585,0.0022025139,0.0026823783,0.006386238,0.06753213,0.042702507,-0.07656505,0.0657859,-0.008291001,-0.11391576,-0.08327686,-0.0048045386,-0.041130766,-0.014135409,8,-0.103227,4.233721,5
367,"
the lecture began with an recap of multiple linear regression did in last lecture , where we have multiple independent variables (x_1, x_2, x_3, ... x_n). although multicollinearity is an important consideration, it was noted that this would be discussed later.  

next, we covered the concept of training and test data, emphasizing that the entire sample should not be used to train the machine learning model. a typical 80-20% split (80% training, 20% testing) was recommended, as a 50-50% split might lead to an unrepresentative training sample.  

we then discussed two outcome sets: training matrix and test matrix. for the training matrix, we introduced the concept of the confidence interval. this led to a discussion on overfitting, which occurs when training accuracy is much higher than test accuracy. a graphical representation was used to illustrate this issue.  

after this, we performed regression statistics in excel. several key terms were defined:  
- multiple r: the square root of r^2 (coefficient of determination), which provides a measure of the nonlinear correlation between y and the independent variables.  
- adjusted r^2 the formula for adjusted r^2 was explained.  
- the denominator in variance formulas: we examined why it is n-1 instead of (n)due to degrees of freedom being reduced when using the mean of x.  

we clarified that linear regression does not necessarily imply a straight line, but rather a linear relationship between yand x 

towards the end, we transitioned to python programming for linear regression. several statistical tests were covered, including:  
- omnibus test and its p-value criteria
- skewness and kurtosis statistics
- durbin-watson test for autocorrelation  

additionally, we discussed quantile-quantile (q-q) plots, which help assess whether a dataset follows a particular distribution by dividing the sample distribution curve into sections.  ",-0.029627595,-0.055718184,-0.06276389,0.057147298,0.0646371,0.040745616,-0.049874473,-0.0039887587,0.046852224,-0.0073233745,-0.004137776,0.08725566,-0.019387072,-0.009087176,0.015338682,-0.019608585,-0.035732884,0.037302244,-0.10406561,-0.032985847,0.09519728,-0.06562445,-8.963249e-05,0.049378317,0.012709873,-0.020126814,-0.05842593,-0.02177538,-0.09600521,0.05271904,0.04934423,-0.0155844325,0.0017345404,-0.015737081,-0.054514807,-0.050588373,0.051276963,0.083987765,0.0253663,0.072391875,-0.043381963,-0.030611254,0.06855066,0.051469706,0.12250861,-0.015473104,-0.06519778,-0.0800952,-0.023967598,0.03030839,-0.055680603,-0.04403626,-0.022409944,0.012512357,-0.029577339,-0.05701406,-0.076817095,0.046199396,0.016504904,0.0587601,0.01685436,-0.039729845,-0.05509297,0.031023944,0.021641718,-0.013217508,-0.11523858,-0.015445195,-0.07246063,0.0059802216,-0.094556585,0.033497076,-0.052261215,-0.023239154,0.03525921,0.03571126,-0.04346901,0.01898799,-0.012590317,0.01934635,0.023292698,0.106232345,0.03456626,-0.03989384,0.04462887,-0.021760391,0.04464417,-0.009443176,-0.06989189,0.017902683,0.0956256,-0.018229762,-0.03735531,0.050763816,-0.03416681,0.042097475,-0.013191217,-0.03109995,0.03780097,0.04977866,0.0061668563,0.0011081656,0.038085118,0.036027554,-0.029514847,-0.030437643,0.04210527,-0.02681473,0.0792039,-0.045355704,-0.015317749,0.03385105,-0.11950501,0.058518782,0.034167357,0.07718214,-0.067558914,0.038936626,0.016319321,0.055740733,-0.046845917,0.024957294,0.11433853,0.0320268,0.04711938,-0.010849004,-0.11049827,5.0870267e-33,0.0031542743,0.036885597,0.028281301,0.066975035,0.015461443,-0.0872545,-0.003425296,0.018273601,0.0812734,0.039124794,0.030980673,-0.023014072,-0.010702037,0.069915995,-0.015159102,0.13260609,0.0013654283,0.027231764,0.03202898,0.075392164,0.029370867,-0.075524114,-0.0033971898,-0.042370692,-0.0041174074,-0.020212604,0.015983097,0.055978063,-0.054822147,0.017776709,-0.019416649,0.068607874,-0.047482423,0.056037232,0.013081234,-0.05169821,-0.061147112,0.010040465,0.05236129,0.076075055,-0.0346973,-0.014907893,0.086431876,-0.0618717,0.055654332,0.012422096,-0.027689084,-0.026304025,-0.073783986,-0.0020732267,-0.07208307,0.056756977,-0.01862101,-0.029580766,-0.089235015,0.12414784,-0.082235314,-0.060206372,-0.04128022,0.027141891,-0.085591145,0.009252164,-0.008595152,-0.042579133,-0.08079617,0.034570478,-0.0119880745,-0.020734323,0.087667555,0.09243223,-0.035976395,-0.12274292,-0.049574263,-0.045484543,0.08377489,-0.007860007,0.033023335,0.040716294,0.03950376,-0.07213095,0.029897427,0.03678434,0.041027855,-0.07691053,-0.07966201,0.01313087,-0.043713845,-0.089196905,0.013778008,0.011947975,-0.044735357,0.015856082,-0.07780513,-0.061043806,4.0814695e-05,-6.239341e-33,0.041064337,0.02423255,-0.029914038,0.056376882,-0.011416093,0.003298976,0.04442159,-0.020363968,-0.0060807765,-0.037366048,0.07613092,-0.042222846,-0.016144898,0.057917506,0.00648467,-0.006133531,-0.01149337,0.013742989,0.023609273,-0.040773068,0.005315123,0.0036985297,0.08575193,-0.053255208,-0.04336132,-0.03263492,-0.057974532,0.03366506,-0.009649883,-0.040438678,-0.0093891835,-0.025648465,-0.005250916,-0.0028237146,0.024947518,0.03127003,-0.037790306,-0.05147458,0.028231248,0.11130757,0.07766535,0.005477715,-0.039722256,0.007032673,-0.014112192,-0.0016897118,0.044096015,-0.001982448,0.041407626,0.013500406,-0.046097215,0.007483236,-0.017494842,0.012601455,-0.01002435,-0.028530264,-0.027924469,0.0055270554,-0.0068295375,0.023843594,-0.04723894,0.06536156,-0.065939486,0.068197854,0.075357765,-0.029714232,-0.029317267,0.043665636,0.064447254,0.026006699,-0.051288012,-0.050292555,0.03709437,-0.03564086,-0.05830655,-0.0067714443,-0.049239267,-0.1322822,-0.106031075,0.070783,-0.0519057,-0.071534775,-0.04563573,0.016405473,0.012971951,0.03839129,0.08685722,0.04639956,-0.018763034,-0.0975887,-0.003575385,0.046844333,-0.06279344,-0.017803859,-0.0103669735,-6.4541375e-08,-0.04600573,0.07297962,0.0057226256,-0.021799963,0.016961463,-0.03810377,0.019877065,0.09179482,-0.0628653,0.05427324,0.099727884,-0.01687356,-0.073638454,0.030118523,-0.015684161,-0.010735473,-0.023864126,0.08484268,-0.0022582428,0.041278124,0.048783224,-0.03944553,-0.005527674,-0.041437734,0.09843078,-0.044632364,0.028182197,0.15557067,-0.04997132,0.01042209,-0.0032124256,-0.028696803,0.029272402,-0.006728453,0.038577504,0.057025295,0.035139043,-0.05255209,0.04523697,-0.028864207,-0.082003206,0.059745014,-4.503663e-05,0.05255707,0.13195072,-0.016995568,0.0486028,0.013265777,-0.044806737,-0.1181807,0.018098604,0.0451688,0.012377712,0.051673003,0.077536695,0.034797683,-0.037437018,-0.002646262,-0.090051584,0.052959003,-0.049188677,-0.029581867,-0.013879974,-0.10037624,8,-5.8836927,1.2593849,5
372,we recapped in the starting about the closed form solution in multiple linear regression which is impractical as we have to deal with matrix inversion and also one more issue is presence of multi collinearity. also during feature selection if p value is greater than 0.05 we remove that feature as it is not contributing much. then we learnt about train and test data today . typically use 80-20 %. randomly split after doing exploratory data analysis. also we saw if r2 values of training and testing matrixes are close enough otherwise issue of overfit can happen. we want model to learn and represent population. we also in detail saw what is adjusted r2 multiple r and why there is only n-1 degree of freedom in tss. we saw quartile plots and importance of errors being normally distributed or not. we saw python lib called as pandas and see training testing and fitting data. also we saw the case where sometikes test stats is inside ci and sometimes not basically hypothesis testing.,-0.032920286,-0.04264193,-0.0038520151,0.09825123,0.08311201,-0.0063132336,-0.07572183,0.05840168,0.00091030425,0.08517311,-0.03655613,0.051320273,-0.07216327,0.04944371,0.04421424,-0.055870827,-0.008706383,-0.0280249,-0.04986388,-0.055058096,-0.061571643,0.003218487,0.018628318,0.047429938,-0.022444764,-0.049591124,-0.007012155,-0.05356827,-0.009753502,-0.003380405,0.030408196,0.07288544,-0.00061304204,-0.056026276,-0.044826765,-0.060470745,0.0018176418,0.050584927,-0.012067447,0.05246338,0.0009550947,-0.026730297,-0.004479171,0.025054004,0.043545045,-0.030666625,-0.053434975,-0.06297826,0.039415926,-0.02056733,-0.04808779,0.04094942,-0.021421934,-0.0035770738,0.0076178894,-0.13578248,-0.067062445,-0.07873344,0.0004974158,0.0010851431,0.008912243,-0.013735179,-0.04652571,-0.0689477,0.07917349,0.063765846,-0.115579985,0.024840593,-0.01095658,0.01846852,-0.11715878,0.032216642,-0.055679016,-0.029675875,0.06015904,-0.0071584326,0.04935,-0.004019563,-0.0081563555,0.026313534,0.099098444,0.04310853,-0.028908346,-0.042982835,0.0068904515,0.012176952,0.01086322,-0.011666408,-0.021181641,-0.032209873,0.14464742,0.04638396,0.0069142436,0.028912183,0.003641796,0.059636544,0.011766406,-0.007837055,0.027863935,0.046362553,0.046717547,0.0004724042,0.08894134,0.0491499,-0.056014206,-0.012689832,0.06824549,-0.069237106,0.08293079,-0.021490213,0.0086961435,-0.0039864173,-0.06423553,0.039114103,-0.009017612,-0.031858627,-0.10106248,0.0583968,-0.021900926,0.09614509,-0.04912697,0.03815643,0.042223055,0.016206065,0.08698578,0.06363779,-0.12032643,1.6043467e-32,-0.017043892,0.03668281,-0.011924847,0.088046476,-0.017511247,-0.03438977,-0.032122787,-0.003473767,0.047335684,0.029907074,-0.014599401,-0.022014102,0.0009625923,0.014059004,0.049558204,0.02964613,-0.03954364,0.095560096,0.05105146,0.095045164,0.090084076,-0.056876764,0.022740742,-0.081683695,0.022932427,-0.05812117,-0.017072726,0.04910432,-0.067113526,0.019455278,-0.0479008,0.08606605,-0.07361271,0.060083553,-0.011944937,0.0010288953,-0.009602732,-0.010600703,0.014573809,0.00090882427,-0.059681967,0.055469863,0.024854606,0.032286394,0.02808567,0.02070742,0.011981645,-0.02842446,-0.10431212,0.04316037,-0.07873156,0.02398874,-0.076826006,-0.043978658,0.024755431,0.08840473,-0.05965164,-0.107425444,0.025957178,0.0046413597,-0.053860698,-0.04812754,-0.039519537,-0.08941588,0.018024784,0.06645093,0.012142446,-0.018776363,0.0496904,0.08612478,-0.036572486,-0.10599492,-0.034688532,-0.034291845,0.09993886,-0.022233436,0.1118521,0.051216166,0.06876982,-0.03308687,0.02609349,0.024910623,0.024137057,-0.08074117,0.078526184,-0.1301095,-0.019287866,-0.023936806,-0.052274827,-0.025517806,-0.03305745,0.004804128,-0.09024081,-0.002155373,0.0005601035,-1.2209511e-32,-0.02291369,0.06902872,-0.08451528,0.01898068,-0.04346981,-0.08328179,0.073534556,-0.0013904053,0.032962296,-0.06402795,0.05645539,-0.037857957,0.07910568,0.044799518,0.039271757,-0.02622361,0.015382687,-0.004780807,0.054924868,-0.07695019,-0.052346148,0.06515654,-0.004321434,-0.0217325,-0.05541118,0.016893703,-0.19038689,0.013805355,-0.0111444015,-0.015788026,-0.044521302,0.022716332,-0.025309239,0.005086623,0.010483053,0.0436526,-0.025594445,-0.014131383,0.0086665135,0.08574532,0.06207165,0.09051044,-0.04697987,0.020730136,-0.019307321,0.024001162,0.042548854,0.004776028,0.06123852,-0.0470313,-0.029830497,0.00065918104,0.02034885,0.007823026,0.0056334813,-0.0022738702,0.043336425,0.02582936,-0.0581702,-0.010029785,0.049390525,0.05755853,-0.040710207,0.0421813,0.029012384,-0.003984337,0.011625167,0.035334945,0.037737016,0.012071435,-0.054095443,-0.11076176,0.094422996,-0.06626538,0.000103480575,0.032216426,-0.05007289,-0.016873932,-0.024559611,0.06887634,-0.03986734,-0.021282969,0.03704077,0.048135925,-0.049447723,0.020142157,0.07907946,0.01176711,0.046305556,-0.057419803,-0.046827577,-0.010988038,-0.012349091,0.05679271,0.094967216,-7.883866e-08,-0.09635204,0.017853517,0.0118018165,-0.034480795,-0.030599931,-0.03508818,-0.05194755,0.090792015,-0.026389768,0.092724495,-0.0436691,0.002154531,-0.10582468,0.0048078676,0.012919583,0.0729759,-0.037932273,0.09308626,-0.031440195,0.021579698,0.0637717,-0.022169547,-0.006972179,-0.0012964496,0.054025516,-0.0066591636,0.02719262,-0.020760398,0.028929748,0.06391722,-0.009485052,-0.09665531,0.055260215,0.031584453,0.06256899,0.022604477,0.0112342965,-0.002336527,0.04425667,0.018495992,-0.05199806,0.06453628,-0.03925601,0.043810464,0.05233458,0.030271005,-0.032849625,0.03291715,-0.035796072,-0.10329313,0.03887039,-0.005549002,-0.023224633,0.06359483,0.07189441,0.054270122,-0.0479274,0.04386789,-0.059257552,-0.0135923885,-0.04503126,0.02344446,-0.019970704,-0.050636835,8,-1.8293957,3.495923,5
402,"after exploratory data analysis on our dataset, we split the data into two classes: 80% for training and 20% for testing. training data was used to create a linear regression model, and hence, we had two key sets of output metricsâ€”training results and testing results.

overfitting was discussed as a potential issue in model performance. this arises when the model performs well with the training data but poorly on the testing data, which gives an indication that the model learns specific patterns specific to the training data rather than generalizable trends.

we also discussed the idea of adjusted râ². while regular râ² increases with each additional predictor, adjusted râ² adjusts for the number of predictors in a model and provides a better measure of the fit of the model. this value gives us the amount of variance explained per degree of freedom, giving a more realistic view of model fit.

linear regression models: simple linear regression (slr) and multiple linear regression (mlr) were covered. these are parametric models that depend upon the use of p-values, coefficients like î²â‚ and other statistical measures in order to understand the relationships between the variables.

we then looked at how to determine if errors are normally distributed. a q-q plot, or quantile-quantile plot, is used for this purpose. if the error values fall along a straight line in the q-q plot, it suggests that the errors are normally distributed, which is an assumption of linear regression.

the session explained the basic setup of scikit-learn, or simply sklearn, the python library for building and evaluating ml models. in this session, we learned how to use this tool to successfully implement linear regression.

hypothesis testing: the session concluded with an introduction to hypothesis testing, including an overview of p-values. we touched on some common statistical tests, including the omnibus test, jarque-bera test, and durbin-watson test. these tests help assess the underlying assumptions and validity of the model, such as checking for autocorrelation, normality of residuals, and other model fit aspects.",-0.014070492,-0.030256456,-0.03976548,0.1318352,0.107958496,0.01134943,-0.07833512,0.05053995,-0.0039541493,-0.011524622,0.013710145,0.08661088,-0.012663796,0.010070896,-0.010758645,-0.038181838,0.023585845,0.057567775,-0.10970294,-0.04081353,0.041625317,-0.043920107,-0.012275273,0.056026347,0.037002143,-0.014676328,-0.039529227,-0.015166319,-0.043531403,0.020111801,0.05823162,-0.015205285,0.0051367213,0.011762345,-0.05490348,-0.07836656,0.03248303,0.09993242,0.009374499,0.0133455265,0.006792685,-0.08395369,0.0622625,0.029704677,0.044073705,-0.009824907,0.011876963,-0.099544585,-0.042035002,-0.015898779,-0.04563917,-0.03674325,-0.0159814,-0.029557751,-0.04308363,-0.021178307,-0.04478498,-0.0005997337,0.07839089,-0.011840136,-0.022109054,-0.087781295,-0.08600723,0.0008749915,-0.055615913,-0.036373775,0.009395812,-0.0052196053,-0.023929661,0.042609747,-0.07198488,0.1491014,-0.04515612,-0.03005859,0.052572172,0.039380517,0.00393897,0.063249424,0.01817282,0.0021310614,0.02646626,0.042778574,0.04955854,-0.057743616,0.05372292,0.005170103,0.071788035,-0.038647033,-0.03181594,0.01427717,0.011219331,-0.016749043,-0.0036814366,0.018953599,-0.02026664,0.06387552,0.053235933,-0.09230216,0.010423509,0.0559078,0.015238435,0.09670813,-0.00230295,0.0094573945,0.026131067,-0.008098566,0.03697069,-0.059130087,0.043099526,0.0063963877,0.015994493,0.0735486,-0.079936214,-0.0013924537,0.05559177,0.024957195,-0.08616932,0.022873614,-0.03355905,0.045405265,-0.025736514,0.05641043,0.05073372,0.024819491,0.07692989,-0.015186084,-0.1479831,9.3096015e-33,0.015029174,0.055333085,-0.023160607,0.062001668,0.010097984,-0.056574956,-0.060687337,0.047541067,0.05090064,0.060020123,0.0064780973,-0.0027084865,-0.013011467,0.08092498,0.08879022,0.11027454,-0.023244806,0.036418173,-0.004800812,0.057732742,0.042646986,-0.11547044,0.021717085,-0.02551633,-0.024982424,0.009095063,0.0012421658,0.05451867,-0.079051994,0.017296935,0.0031742314,0.029354542,0.043862082,0.030396441,0.01136577,-0.04721645,-0.031821113,0.00034503668,0.038984194,0.02235452,-0.04914967,0.027715404,0.04316983,-0.001555591,0.031226063,0.026274836,0.023384843,-0.07934268,-0.08096406,0.04245747,-0.05520011,0.061639823,-0.07329564,0.025164438,-0.041642893,0.042549364,0.0015557011,-0.04955239,-0.053382587,0.04293529,-0.04552924,-0.029299881,0.017880319,-0.022963297,-0.0046304185,-0.0102062,0.015129477,-0.004717902,0.012902054,0.077771,-0.04489301,-0.0707657,-0.0043085096,0.05030054,0.07413333,-0.039116994,0.12434943,-0.00364156,0.019453572,-0.04896701,-0.029278114,0.071235456,0.058748793,-0.1268107,-0.082474045,-0.109382786,-0.035429977,-0.0947446,0.019269189,-0.015366311,-0.075739995,0.030116173,-0.06105461,-0.050004553,-0.004715334,-8.08034e-33,0.02025672,0.0662402,-0.03883093,0.052638944,-0.022889502,-0.025162252,0.027844306,0.049500853,-0.011374802,-0.119464196,0.057646614,-0.028249426,0.041424513,0.07429144,0.047862396,-0.011749899,0.020928558,-0.039512694,0.007582183,-0.021648705,-0.0062986566,0.09244956,0.013848209,-0.016751183,-0.056166016,-0.012249687,-0.08143121,0.07713003,-0.027033703,-0.04130021,-0.075776726,0.07100542,-0.029679758,-0.0427835,-0.025555959,0.00534373,-0.0095822355,-0.014913086,0.011520864,0.14064553,0.07436537,0.08314173,-0.082734354,0.018197326,0.00023833908,-0.019949,0.037586015,-0.041729722,0.079213224,-0.04472594,0.007808375,-0.0531835,0.0041203564,0.006434856,0.0062363814,-0.10348542,-0.03534267,-0.036882192,-0.0218708,0.049701996,0.0033093425,0.058835976,-0.045343067,0.058717623,-0.054048695,0.017105054,-0.013855622,-0.009347245,0.049421933,0.053735845,-0.03956876,-0.09058467,-0.0012114426,0.047364283,-0.027935922,-0.017871486,-0.047462568,-0.06909175,-0.062459353,0.023622703,-0.038226917,-0.009211976,-0.02277175,-0.021949701,-0.009740219,0.09765234,0.018496735,0.08474133,-0.021780083,-0.068958074,-0.073576525,0.03422714,-0.07957027,0.08756202,0.026764438,-6.2995284e-08,-0.075301416,0.050431166,0.02184761,0.0036372598,-0.013016212,-0.009934867,-0.0017886547,0.106374465,-0.088324696,0.04666698,0.0022942408,0.0026100983,-0.0376276,0.0060389126,0.053214975,0.113828324,-0.016694263,0.01762415,-0.021693772,0.034898575,0.046001002,0.013470818,-0.009593899,-0.034844607,0.08130209,-0.059522074,-0.013943494,0.13784668,0.010136688,0.023034083,0.0621123,-0.0059373425,0.085366674,-0.049214914,-0.027175218,0.028273925,0.08235798,-0.09357053,0.041609425,0.037275035,-0.016660081,0.086294256,-0.012246781,0.030739669,0.00429279,0.0072173765,0.0601146,-0.017596167,-0.040037017,-0.17484829,0.011397299,0.011121723,-0.022908289,0.037006054,0.036615107,-0.0015786326,-0.044137042,0.0092636235,-0.061970744,-0.0041607404,-0.019309357,-0.066424504,-0.048617076,-0.033832036,8,-5.445428,2.5344458,5
437,"class 8,

explained how multicollinearity can be a problem and why we cant solve mlr by closed form solution. the exact form (xtx)^-1... type form.-> impractical, inverse matrix is very hard to compute for so many features and higher number of data. hence we need to shift to gradient descent for reduction of computation.

splitting of data: we split a sample. do not consider a whole sample for training purpose. around 80%-20% split? why? because we want to test the model whether it is really eefective om the unseen data or not.

why not 50-50? why 80-20? we want the data on which the model is going to be trained to be representative enough of the data for variance or complexity capturing.

two sets of outcomes: -training metric and test metrics. training metrics include sse,rmse, f-statistics, r^2. test metrics also include various operation like accuracy r^2.

overfitting: if the r^2 test is significantly lower than r^2 training. model is not generalized.

then we tried mlr on excel. showed result. did the analysis. various things we found out.  multiple r -> sqrt(r^2), correlation of y and all the (x1,x2..) taken all together.(non regression statistical result resemble)
adjusted r^2: gives you an idea. how effective the addition of new independent variable is.
1-[(sse/its respective dof)/(sst/its dof)]  we divide by degree of freedom because we want to get the adjusted equivalent r^2 if there was one independent feature. dof is n-1 for sst because considering the mean is known. similarly for sse its n-k-1.

moved to python -> did same data analysis.  residual plots-> how do we know this residual plot or e^2 plot is okay? take histogram check whether is normal distribution or not.

q-q plots if the errors lying on the histogram perfectly aligns with the normal curve or not.
these are hardcore statistical analysis not present in sklearn -> hence we use stats model.

stats model give a lot of other analysis: aic,bic omnibus statistic, omnibus p-value, jarque bera test, durbin watson test.",-0.06688086,-0.055279333,0.018781194,0.0022704776,0.10712238,-0.018054668,-0.08612975,0.08152108,0.025786621,0.041751932,-0.030690784,-0.0031084279,-0.053993616,0.012482712,0.014914357,-0.022296991,-0.024324534,-0.015795646,-0.1795843,-0.057550117,0.07021848,-0.041073993,-0.024387566,0.07428133,-0.0038096437,-0.041921623,-0.012417431,-0.07656347,-0.061733875,0.02661625,0.12632942,-0.029660558,-0.039338447,0.0037977132,-0.07007023,0.0042420374,0.026997024,0.03298569,0.005531399,0.07510914,-0.017579978,-0.06356245,0.02052305,-0.03010621,0.103006765,-0.051675137,-0.039362166,-0.120518684,0.051272467,0.029228168,-0.08498341,0.0210557,-0.08264428,0.014924316,-0.010962879,-0.07029849,-0.058326725,-0.04708035,-0.03583385,-0.0039042085,-0.025463121,-0.040805817,-0.025286207,-0.020539727,0.0064997165,-0.045658633,-0.030063223,-0.07031794,-0.015500105,0.02256831,-0.04634277,0.013344108,-0.042630404,0.02053273,0.045418918,0.022328926,0.01245673,0.026805462,0.05315079,-0.02651922,0.1029086,0.05121764,0.040708777,0.00291849,0.0035770615,-0.043628033,0.011561286,-0.023133446,0.014861361,-0.031170415,0.054241516,0.0057049603,-0.06023462,0.03152714,0.010662603,0.051875774,0.0054967157,0.023396857,0.04531449,0.06593877,-0.016130954,0.026498584,0.026984587,-0.01362325,-0.06359425,-0.08290227,0.11281471,0.03277064,0.077604294,-0.051813893,0.099831074,0.012526649,-0.054964308,0.02714036,0.042143308,-0.027032292,-0.057024658,0.010227736,0.024986684,0.13386714,-0.119167715,0.004697493,0.10123083,0.035184853,0.106932424,0.011298217,-0.1203848,6.391761e-33,-0.03498426,0.05842704,0.054816693,0.018388962,-0.009703604,-0.049947966,-0.009928496,0.08508512,0.029959938,0.052415557,0.009422284,-0.055964332,-0.027047696,0.049624167,0.045599945,0.052083496,-0.0096374145,-0.0042575574,-0.06436732,-0.026555076,0.08605741,-0.039051767,0.043311182,-0.04587158,-0.039452944,-0.032951612,0.00075869524,-0.0038331419,-0.032714717,0.012189297,-0.06987862,-0.006813886,-0.027874583,0.11543341,0.019380635,-0.03144892,0.027824072,0.006422221,0.038303923,-0.02432322,-0.025337487,0.02330535,0.08960086,-0.030596297,-0.022688812,0.0076773753,0.02088117,-0.03836572,-0.02802712,0.038786992,-0.0004462032,0.0153365955,-0.091122404,0.010545687,-0.07161548,0.14819956,-0.052277107,0.0065288073,0.089156725,0.07258811,-0.091269515,-0.0020345813,0.013265167,-0.018910175,-0.04716639,-0.039056163,-0.028785178,-0.061268203,0.031766366,0.060552876,0.0009831431,-0.078958765,-0.051121548,-0.07335585,0.10558927,-0.03207134,0.0857175,0.055842143,0.08321918,-0.03455998,0.025403453,0.07635148,0.03472511,-0.063220434,-0.048010927,-0.007629135,0.010393484,-0.07287378,-0.010243384,-0.02592895,-0.067980364,-0.01314452,-0.051741198,-0.044005718,0.012891059,-7.0376274e-33,0.003637049,0.022559006,-0.031132169,0.06578902,0.030917991,-0.06015861,-0.0038512205,-0.025999097,-0.016556868,-0.00069981825,0.10998448,-0.037641335,0.05874845,0.040905457,-0.051906385,-0.016163578,-0.024122804,0.020122683,0.04126471,0.004983751,-0.040985946,0.05741731,0.010306198,-0.014343094,-0.017469771,-0.0028756494,-0.08713308,0.037252612,0.062394504,-0.00052873354,-0.05588785,0.00502121,0.010101592,0.013882008,-0.023438776,0.010002888,-0.04164812,-0.01807555,0.057858475,0.09878173,0.037244633,-0.015388147,-0.1081516,-0.0035026208,0.01827357,0.010537558,0.006504307,0.025662433,0.08395881,0.046915956,0.0077196527,-0.039592315,-0.015613475,0.06949958,-0.016442534,0.023456132,-0.021952417,0.0030779366,-0.06301983,0.022754278,-0.010048307,0.06525496,-0.051370785,0.05657653,0.09127641,0.051405735,0.020000966,0.019960029,0.043416385,0.03377064,-0.064314626,-0.058441058,0.049600966,-0.017162839,0.023070786,0.033774484,-0.03129921,-0.09895651,0.0054264017,-0.02083632,-0.045447998,-0.07068937,0.00932744,-0.05031746,0.039912745,-0.0017482486,0.10058924,-0.01389316,-0.064550586,-0.05575613,0.0066446946,0.007296268,0.059806217,0.01619311,-0.032928985,-6.9774934e-08,-0.06702065,0.02631559,0.0058306167,-0.0531245,0.049819592,-0.06735833,-0.02683377,0.067572884,0.026820395,0.0835626,0.05666759,-0.09797753,-0.1691021,0.039169665,0.017258,-0.01166973,-0.032315176,0.0972366,-0.037626117,0.032695785,-0.0059660356,-0.037191015,0.03497501,-0.03272355,0.062952556,-0.029188335,0.02191832,0.10728812,-0.006401004,-0.06478998,-0.06760042,0.0044406313,0.0044857236,0.05451567,0.05072633,0.052635882,0.018038757,0.0056006187,-0.028488247,-0.013593472,-0.056026343,0.04870214,-0.049237207,-0.013290532,0.14029698,-0.006677345,0.0070102788,-0.007331642,-0.053208172,-0.03459299,0.01853554,0.034257542,0.028339261,0.047697984,0.033979595,-0.015396018,-0.039719775,-0.0048998953,-0.057542227,0.0005605014,-0.0007963973,-0.07727344,-0.07580066,-0.0757135,8,-2.3854146,1.1214498,5
489,"for multiple linear regression, the closed form solution for b exists but it might be impractical to calculate these as matrix inversions might not exist and etc. if we have a sample of data, we should not use the entire data for creating the ml model. we need to split the data into two parts in the ratio 80%-training data and 20%-testing data. this splitting has to be done randomly. two sets of outcomes that we have to derive and measure: training metrics and test metrics. some of metrics will only be relevant to the training data but may not have any meaning for test data. as we are building model using the training data there are some metrics which are suitable for this only. overfit  situation: when r square value of training data is much greater(generally a difference grater than 0.2) than r squared value of test data. the training errors maybe less but test errors will be too much in case of overfitting. this is a practical tradeoff how much error we are allowing for test data and training data. both these errors are important. as we add more variables, r square value increases. this doesnot mean that the data is better fit. so we introduce a adjusted r square value which is the part of variance captured by each independent variable. n-1 comes in the denominator of calculations involving variance and standard deviation. the calculation of variance involves mean of x. this is already calculated using all n variables. this reduces one degree of freedom and it becomes n-1. the outcome of a linear regression need not always be straight line. slr and mlr are called parametric methods of model creation. there are non parametric methods of model creation. data drift- if we create a model now, the data coming after one month maybe far away from this model. so we have to change our models also frequently according to the data. then we shifted on to python. sir explained how to do multiple linear regression in python. q-q plot tells us how much our data is similar to normal distribution. regression model can be imported from scipy or stats model libraries. sm.ols: sm-statsmodel, ols-optimised least square. stats model gives more statistical measures and parameter than scipy. low values of aic and bic are better. aic and bic are to be discussed later. omnibus statistics, omnibus p value, skewness. we cant use the old p value everywhere. we need to use different type of p-value. omnibus- normality of distribuition. jarque - bera test- check the normality of residuals. durbin watson test tries to asses based on value of error can we predict next error or kind of autocorrelation in the residuals of the models. ",-0.078298755,-0.0014987618,-0.013837089,0.051350985,0.05472061,0.057448957,-0.026695704,0.0041845073,-0.027192539,0.023598868,5.6086785e-05,0.012908345,-0.030753544,0.009489373,0.044769827,0.086831965,0.02904024,0.05378382,-0.118415974,0.007497909,0.018123424,-0.0066539845,0.014344209,0.0768987,-0.061185736,0.0024385604,-0.00263373,-0.051828563,-0.071460456,0.019784082,0.115764275,0.020012992,0.030446155,-0.015810799,-0.07116811,-0.060195997,0.00096459076,0.045271844,0.045316707,0.0339125,-0.049706664,0.004102067,0.057316862,0.05465147,0.05140164,-0.0315419,-0.013441836,-0.10704269,0.031237347,0.09148648,-0.045530602,-0.021187687,0.035253167,-0.022249952,0.006534326,-0.12992193,-0.035864864,-0.040281244,0.01653207,-0.004732715,0.0027169175,-0.036526445,-0.054292683,-0.0322072,-0.03372474,0.004862267,-0.0983985,0.0072473115,-0.052222624,0.07692158,-0.0753067,0.06563723,-0.04815374,-0.0049626986,-0.0007947042,0.05179503,-0.009103694,0.006351982,-0.016659034,0.0041898764,0.08669358,0.052970268,0.03380089,-0.04133352,0.004680654,-0.029623194,0.06821077,-0.0043326127,-0.0026393477,-0.026280262,0.046751812,-0.0067126234,-0.04915938,0.009590051,0.060200896,0.012188922,-0.0008503847,-0.036160808,0.05144632,0.039198667,-0.02344886,0.07847321,-0.02558517,0.059515897,-0.0012736412,0.006579511,0.012023502,-0.028563827,0.12395104,-0.09532196,0.10984379,-0.03978747,-0.06496712,0.020069793,0.013813732,0.0033604028,-0.00092442846,-0.024923172,-0.073725246,0.09825512,0.004645481,0.04648944,0.0990755,-0.0009100779,0.028644532,-0.0127329165,-0.14336845,1.2596036e-32,-0.014904921,0.053152498,-0.0216307,0.07088779,0.027356656,-0.03818159,0.031963155,0.010705469,0.04014245,0.010566864,0.005977269,-0.042764075,-0.020513764,0.008775042,0.013216084,0.09477055,0.011345147,0.055238374,0.015848022,0.014280398,0.024009276,-0.07665585,-0.045881126,-0.055638283,0.004296489,-0.11911659,0.08976828,0.0823392,-0.038821667,-0.02807649,0.034925792,0.015121942,0.0011271692,0.10137617,-0.05134148,-0.06427406,-0.012828775,0.009001745,0.023018703,0.027346859,0.0058012404,0.0016908705,-0.04241038,0.00410798,0.05426969,-0.016512478,-0.002887029,-0.036549926,-0.0060533592,-0.027776947,0.0154026775,0.038937442,-0.07478917,-0.025477696,-0.086490795,0.11318564,-0.07132797,-0.049952526,0.020251844,0.05049065,-0.054276623,-0.108662784,0.013342437,-0.04108817,-0.06728024,-0.021368202,0.06766327,-0.06346288,0.019023271,0.037114047,-0.02110305,-0.14982253,-0.040654797,-0.003907134,-0.008161341,0.0012155147,0.07380559,0.0491372,0.029235544,-0.05494466,0.02350851,0.05730276,0.09042397,-0.06755044,-0.03668677,-0.04979807,-0.030084955,-0.07953154,-0.05681935,-0.01326633,-0.15688355,0.010109643,-0.08733987,-0.07712741,0.009838441,-1.100236e-32,0.01202897,0.056120876,-0.04729758,0.037662532,-0.010744824,-0.05568755,0.10924538,-0.056312025,-0.00036019096,0.011467695,0.02453293,-0.050031114,0.068397656,0.012285924,-0.013119398,-0.008268086,0.02213763,0.0029701455,0.02490257,0.012351116,-0.04993205,0.06944928,0.10688016,-0.019835781,-0.042878106,0.010444725,-0.10641527,0.082303934,0.033479296,-0.014906049,-0.06114935,-0.008005778,0.06533004,-0.0011160312,-0.0062795426,0.08592339,0.04336344,-0.05487668,0.03582486,0.11609247,0.002237795,0.010229663,-0.093344264,0.022362456,0.04453785,-0.0041010124,-0.008051716,-0.06008152,0.034299217,-0.08943907,-0.054066546,0.02104913,-0.0032315354,0.036582485,-0.029915534,-0.070105344,-0.047131125,-0.010170258,0.005303566,-0.004768227,0.03759757,0.012936498,0.019801315,0.07043118,0.00043410133,0.06258532,0.016490327,0.024242403,0.039973125,0.09293858,-0.08071638,-0.059533305,0.072937466,0.07321714,0.04746116,0.027714625,-0.015813723,-0.09291889,0.004119117,0.011747055,-0.115090884,-0.022009835,-0.03803081,0.049959395,-0.0013103078,0.040488936,0.03924639,0.077851,0.030295655,-0.04929212,-0.053217344,0.013725989,0.0023785862,-0.010005313,0.04219196,-7.560509e-08,-0.04112313,-0.0076919105,0.059826434,-0.029145326,-0.014980544,-0.049066696,-0.03075305,0.01568477,0.003223156,0.023948837,-0.023937423,-0.00017811893,-0.07784656,0.01694542,-0.07299819,-0.0010177101,-0.0766164,0.03230826,-0.0063583646,0.11596668,0.008571312,0.05522914,0.05384169,0.023275414,0.11490936,-0.031150334,-0.0027008618,0.03914596,0.0040899725,0.022034716,0.034657553,-0.06451907,0.01538646,0.02376674,0.0561773,0.009125112,0.05608911,-0.021416899,-0.030273791,0.05966335,-0.022540947,0.013830558,-0.0024776682,-0.006028794,0.07154042,-0.042842858,-0.027432384,0.0064495336,-0.09680547,-0.10573777,0.08642881,0.0041196877,-0.009386829,0.071565196,0.023379495,-0.02846007,-0.10833658,0.06955751,-0.04598121,0.061422504,-0.019876378,-0.04586421,-0.042503603,-0.06914945,8,-2.888146,2.5875874,5
493,"1. we learnt solutions in multiple linear regression.
2. if the p - value is greater than 0.05 we reject it 
3. 80/20 data split is idea in test train dataset 
4. overfit if r2 value of test train not close enough
5. we learned about adjusted r2",-0.039727196,0.032936804,0.069208376,0.038639575,0.14060833,0.030231249,-0.010805949,0.083469786,-0.038014144,0.0559515,0.026330946,0.029082619,-0.06127665,0.013698009,0.032280546,0.019829731,-0.016714986,-0.02785867,-0.06293819,-0.005549586,0.06293582,-0.09759797,0.012704566,0.074115515,0.004037069,0.03432142,0.006047771,-0.008794392,-0.010634709,-0.017886756,0.039431263,0.032587484,-0.011978333,-0.009192274,-0.031964477,-0.06729762,-0.00755698,0.07267923,-0.0005361499,0.07407363,0.028223114,-0.056646924,0.016664108,-0.014511932,0.09203675,0.0115595395,-0.095244,-0.043261312,-0.021674998,-0.052132353,-0.0867523,-0.0106911985,-0.0031569144,0.0035027105,0.017249526,-0.110508025,-0.03522508,0.008217323,-0.004852095,0.012418786,0.0059532654,-0.0062274393,-0.0810197,0.013912395,0.07402848,0.034557417,-0.09106115,0.014812001,-0.05210798,0.030688092,-0.08557211,0.052069604,-0.07401631,-0.059731003,0.021919237,0.00083103403,-0.009634513,-0.035083614,-0.030350007,-0.022027157,0.033953063,-0.007536183,-0.040228035,-0.091277495,0.010079147,-0.038387753,0.01863097,7.586379e-05,-0.056765355,0.00034387287,0.079952314,-0.025341878,-0.06674491,0.13752526,0.011600312,0.047393046,-0.052356903,-0.03742434,0.057143323,0.008538261,-0.0007362551,0.050704386,-0.01621443,0.05005351,-0.043531343,0.000727944,0.04020543,-0.078472435,0.11623427,-0.014780597,-0.00762596,-0.023055797,-0.053642586,-0.00993019,0.038516875,-0.0076049245,-0.0201862,0.092158034,-0.059657805,0.003618803,-0.024452241,-0.013065201,0.042148955,0.011964868,0.008442717,-0.032151032,-0.12241882,7.732675e-33,-0.027238641,-0.056356255,-0.0039533447,0.0019198495,0.03725573,-0.004232386,-0.05554843,0.031658705,-0.0034230023,-0.010276281,0.017848419,-0.080396414,-0.02223176,-0.020658879,0.047994975,0.0317191,-0.03813845,0.07094988,-0.014847552,0.08190095,0.047296666,-0.07515239,-0.007592478,-0.03035055,0.017675823,-0.043695997,-0.028937615,0.04915903,-0.011184375,-0.018196408,-0.014345474,0.053717818,-0.034091912,0.0751963,-0.03625744,-0.008032729,-0.013790191,0.009324167,0.011673557,0.016718635,-0.013809108,-0.016690139,0.045775272,0.003079004,0.08443502,-0.02377317,-0.027657976,-0.008605248,-0.044834346,0.05306275,-0.037565976,0.03755287,-0.09227272,-0.001173901,-0.06964742,0.0590813,-0.076331206,-0.04585691,0.021478094,0.04147683,-0.011714422,-0.03464707,0.03146867,-0.06733368,-0.03229207,0.08420714,-0.0024454333,-0.08005623,0.08293127,0.112425,-0.082454786,-0.055230275,-0.030960355,0.0208205,0.05861078,-0.0266812,0.07381616,0.10775865,0.10974032,-0.09009364,0.004098386,0.008207367,0.06843687,-0.06127279,0.013620473,-0.04469174,-0.024419913,-0.07139932,0.0075303908,-0.019339886,-0.043484848,-0.023648426,-0.059159625,-0.022715336,0.07137744,-6.734354e-33,-0.0014825104,0.10954263,-0.014815838,0.024195053,-0.035793483,-0.0046676905,0.07589073,-0.046845715,0.02934796,0.038680546,0.07549544,-0.0459573,0.040090904,0.014066486,-0.048455164,-0.099124774,0.032246113,-0.0027323235,0.066514164,-0.026165389,-0.03767061,0.07189496,0.023555709,0.059213623,-0.04698186,0.051196076,-0.09841872,-0.0035799795,-0.00051724113,-0.06971292,-0.0085154725,0.018868152,-0.0007834938,-0.013471829,0.0005274806,0.02214147,-0.021158457,-0.00029908508,0.029212514,0.15926751,0.07170315,0.057527933,-0.07887276,-0.023839869,-0.04420139,-0.0038707098,0.043190364,-0.037981585,0.024626026,-0.010126745,0.006644464,0.010712105,0.025127925,0.041544832,-0.004877393,-0.06671408,-0.010983066,0.04080213,-0.026909322,-0.015511316,0.031588536,0.044320244,-0.018666865,0.1099586,0.04699453,0.004165597,0.005287383,0.050019555,0.05896185,0.034558285,-0.08829865,-0.04506259,0.14106518,0.006233415,-1.2063018e-06,0.0054911305,-0.08109386,-0.033132967,0.028436236,0.08649174,-0.059646327,-0.08554648,-0.005209554,0.022454122,-0.046747003,-0.023589129,0.0111463545,0.08366491,0.030980052,-0.069660746,-0.030876948,0.02433104,-0.02535115,-0.05285493,0.04400491,-4.5544134e-08,0.0030500747,-0.06879705,0.032406535,0.0017274594,0.022396758,0.0057823504,-0.07422073,0.11464753,-0.069571,0.039905984,0.0024681338,-0.0075588706,-0.06409831,0.006851411,0.0034407736,-0.035151105,-0.035099182,0.105923675,-0.0007063959,0.011177664,0.021983264,0.00057107606,-0.028782876,-0.0007797095,0.086181,0.018215341,-0.016816791,0.083644636,-0.07094985,-0.0055652526,0.086768694,-0.04456697,0.04483912,-0.009681945,0.102543406,0.091715805,0.049388308,0.037256733,-0.0041126134,0.050085533,-0.058725305,-0.015041889,-0.0039319596,0.0014809278,0.059463564,0.0057866187,-0.0013984989,-0.027546233,-0.07055178,-0.17869999,0.054670285,0.09604567,-0.015680881,-0.022269541,0.014535307,0.03474185,-0.09545385,0.08155042,-0.135412,-0.06620817,-0.042971034,-0.07915736,-0.017445762,-0.0030468276,8,-1.9399751,4.983979,5
497,"this lecture focuses on multiple linear regression (mlr) and its implementation in python.  while mlr has a closed-form solution, it's computationally expensive for large datasets. therefore, gradient descent is the preferred optimization method.

a crucial practice in machine learning is splitting the available data into training and testing sets.  a common split is 80% for training and 20% for testing, performed randomly.  this allows for evaluating the model's performance on unseen data.  two sets of evaluation metrics are generated: one for the training data and one for the test data. a good model exhibits strong performance on both sets.  if the model performs well on the training data but poorly on the test data, it's a sign of overfitting. conversely, poor performance on both sets indicates underfitting.

the ""multiple r"" metric is simply the square root of r-squared.  it represents the correlation between the multiple independent variables (x) and the dependent variable (y).  adjusted r-squared is a modified version of r-squared, calculated using the residual sum of squares (rss) and the total sum of squares (tss).  the formula incorporates  n-1 (degrees of freedom) because we lose a degree of freedom for each estimated parameter. adjusted r-squared penalizes the model if the rss doesn't decrease sufficiently relative to the increase in the number of predictors.  it helps to prevent overfitting by considering model complexity.  it's important to remember that mlr doesn't always result in a straight line; it can model more complex relationships.

the lecture then transitions to implementing mlr in python using the sklearn library.  sklearn is excellent for model building, predictions, and handling large datasets.  however, it can lack fine-grained control for researchers needing detailed model analysis.  to assess the model's fit, the distribution of residuals (errors) is examined using a histogram and a q-q plot.  ideally, the residuals should be normally distributed.

for more in-depth analysis, the statsmodels library is introduced.  statsmodels provides access to a wider range of statistical metrics.  one such metric is the ""omnibus"" test, which combines skewness and kurtosis to assess how closely the residuals approximate a normal distribution.  the jarque-bera test is another normality test for residuals.  a desirable outcome is a non-significant p-value (typically greater than 0.05) and a test statistic below a certain threshold (e.g., 2), indicating that the residuals are likely normally distributed.",-0.048198804,-0.066170126,-0.016196081,0.059255537,0.0796281,-0.046264663,-0.0955262,0.017767366,0.009409693,-0.044768583,-0.08444301,0.07182291,-0.0005462068,-0.0088396715,0.010592507,-0.0134491725,-0.002572764,0.018177595,-0.08723066,-0.091818824,0.028421596,-0.014513856,-0.006787931,0.06503594,0.03594634,-0.07851191,-0.07721396,-0.01509866,-0.042373005,0.018642418,0.12510926,-0.018734723,-0.03666252,0.037804257,-0.08796706,0.008526194,-0.005696208,0.03457541,0.049750526,0.056762736,-0.049753416,-0.057374883,0.045936584,-0.01443522,0.0424368,0.0298296,-0.059360504,-0.06850489,0.040810905,-0.04574425,-0.051748056,-0.026321054,-0.03910364,-0.00446276,-0.017078394,-0.04039721,-0.030974291,0.016322494,-0.008096194,-0.03026234,-0.005638107,-0.10258313,-0.031316184,0.023037517,0.004596521,-0.0630559,-0.027659286,-0.031655915,-0.0008574891,-0.0043973,-0.021857262,0.04259291,-0.04871982,0.019399937,0.027292907,-0.009652378,0.09687622,0.0173141,-0.019062666,0.029192116,0.06607964,0.079497196,0.009331611,-0.029542334,0.07044535,-0.09244988,0.043743793,-0.074476756,0.0064093433,0.018629607,0.112699896,0.00091339694,-0.07475875,0.036850918,-0.05116995,0.020816732,0.0073386785,-0.02362918,0.01767451,0.056788556,0.013328248,0.03161903,0.026593007,0.0025963928,0.041858286,-0.030321298,0.08271517,-0.048506085,0.11281388,-0.028591836,0.05162101,-0.011214229,-0.02070578,0.08522786,-0.018386168,0.0007799275,-0.056447893,0.042043936,-0.01572925,0.12501831,-0.079121396,-0.030551128,0.107469685,-0.04553394,0.047693647,-0.02031855,-0.15339494,6.0356364e-33,-0.04045555,0.008076043,0.00287122,-0.00817274,0.009767393,-0.050072096,-0.026552685,0.030089878,-0.00080898526,0.049151838,-0.033450477,-0.029566646,-0.017056663,0.06831573,0.05741972,0.05545429,-0.027267812,-0.00784336,0.037217338,0.08738369,0.04901424,-0.043882042,-0.0015028423,-0.025468362,-0.04241345,-0.012716004,0.08554167,0.080816686,-0.06975404,0.020541815,-0.05243367,0.0304751,0.0053582154,0.10183738,0.052930385,-0.027857449,-0.009262131,-0.021019978,0.029266845,0.030581271,-0.07320086,0.036574963,0.035353884,-0.030745065,-0.024318216,0.027777066,-0.080399945,0.018795315,-0.027148463,0.027322983,-0.04905798,0.008039003,-0.03771594,0.025297284,-0.024318423,0.067904316,-0.07651076,-0.016797526,0.012300786,-0.0035706286,-0.03857901,-0.033623833,0.02453888,-0.000625065,-0.0071302955,0.0048043123,0.080393925,0.0008484695,0.042878415,0.09321648,0.004065235,-0.06390663,0.027250504,-0.07270884,0.08562614,-0.042408545,0.08433613,-0.01914657,0.077591754,0.0051053553,-0.032423906,0.17059056,0.032006837,-0.08436898,-0.09268153,-0.037620507,-0.017184282,-0.05234388,-0.009961142,0.01468918,-0.13011537,0.008452564,-0.05318358,-0.039808553,-0.02284404,-5.3788623e-33,0.007601214,0.023332871,-0.013178886,0.064468026,-0.023278626,-0.04105921,-0.0022502837,-0.044456605,0.020550853,-0.019246481,0.06984577,-0.049256116,0.032876715,0.082277365,0.08753941,0.006238786,0.017560508,0.05669552,0.005206811,-0.0466168,-0.02152777,0.071480356,0.022141289,-0.016365271,-0.06803272,-0.011189013,-0.0971493,0.01111046,-0.02251531,-0.03738455,0.012230419,0.07316984,-0.036467988,0.031800836,0.023734841,0.10106063,0.0025576395,-0.04517572,0.039705575,0.08660789,0.06136168,0.09512102,-0.0065730126,-0.023002226,0.0116215395,0.021578863,-0.032648623,-0.017168364,0.06611058,-0.016260166,-0.032547865,-0.028839005,-0.040881548,0.06347047,-0.04684959,-0.09178569,-0.06643887,0.033361882,-0.022298252,-0.0016450112,-0.0075885053,0.0010604057,-0.085379876,0.1646355,-0.04103165,0.042843726,0.048247986,-0.013110867,0.06323124,0.10499214,-0.05254852,-0.02442255,0.064369805,0.024131697,-0.060676936,0.030390887,-0.0821061,-0.09383208,-0.08688514,-0.008107246,0.006062198,-0.05115332,-0.043287225,0.013187463,-0.042559586,0.03407895,0.024734266,0.11794339,0.0025800653,-0.08334813,0.028584193,0.0025226697,-0.007228853,-0.017996397,-0.019348899,-6.8808816e-08,0.03260733,-0.008821929,0.026425894,-0.06741421,-0.07298164,-0.033305507,-0.012968545,0.09162226,-0.032229617,0.045566738,0.06426929,-0.01402989,-0.10496467,-0.01486295,0.022154763,0.057947557,-0.033795808,0.086116344,0.0031846876,0.07842289,0.0017882723,-0.012522096,0.017903747,-0.037623096,0.0329438,-0.03094814,0.061358526,0.07406159,-0.024499403,-0.057841647,-0.0035832166,0.008425262,0.06565234,0.015999824,0.038053483,0.0681318,0.04066404,-0.00470765,0.0319302,0.06892935,-0.068442,0.029578758,-0.035417072,0.0149109755,0.084169544,0.004715958,-0.0007415907,0.036542583,-0.08944493,-0.08830002,0.057331905,0.033507477,-0.0067128004,0.0259304,0.055955093,-0.00581123,-0.081392616,0.023328891,-0.02293732,0.033592273,-0.048689928,-0.015377904,-0.0047102566,-0.11257442,8,-5.090676,1.9739283,5
553,"in today's class, we revised some points, such as the p-value being less than 0.05 for the regression coefficient beta1 to be significant when zero is inside the confidence interval. then we saw that we did not use the model completely, we split it into 80% and 20% random parts, the 80% part serves as our training data, while the 20% part is used by us as test data. we first develop the model using training data and then apply the model to testing data. if the result of the training data is not as accurate as the testing data, the data is called overfit data. doesn't matter how good the results or graphs of training data are; if the testing graphs or results are not good, that model is not best suited for the data. multiple r, the term we got during data analysis using regression in excel, is the square root of r-squared. then we saw about adjusted r^2 which tells about how effective the adding of a new variable is. then we saw that during the calculation of the variance of the sample, we used (n-1) in the denominator instead of n, as we are using the mean already which has n observations in it already, so we have only n-1 unknowns. then we learned about the quantile-quantile plot which is plotted by separating the distribution of the data(errors) in some quantile. then we perform regression analysis using python in which we use python libraries like numpy, pandas, and sci-kit-learn. at last, we see the analysis of errors using various tests like omnibus which tells about the normality of tests, skewness, kurtosis, and jarque-bera test.",-0.041858394,0.03702724,0.017633466,0.120154716,0.08678621,-0.028272642,-0.04118323,0.10958303,0.046733856,0.031827174,0.02031764,0.14950472,-0.029811025,-0.008383244,0.033471417,-0.04283553,0.08395673,-0.022922358,-0.062939875,-0.041945223,0.051950835,-0.039438978,0.036123328,0.061687972,0.028660439,-0.002294849,-0.09684383,0.010022077,0.032128166,0.0020215209,0.065654635,0.010574941,-0.025598887,0.00995998,-0.088417314,-0.056283556,0.031361256,0.06259873,0.06845854,0.06367684,0.0049093,-0.021513727,0.02399216,0.0777211,0.0946085,-0.002732329,-0.026068868,-0.021065395,-0.04268876,0.07304426,-0.10700602,-0.018178606,-0.00542567,-0.0648655,0.008650702,-0.040478583,-0.010828814,-0.020213034,0.038876623,0.007057559,0.036800865,-0.02414856,-0.049032696,0.013109892,-0.0029067507,-0.061419483,-0.05782314,-0.033538878,0.0009223792,0.0034905311,-0.025427338,0.10631582,-0.052591205,-0.019547312,0.04947147,0.014926099,-0.08573628,0.051146112,-0.034555323,-0.008896647,0.104077056,0.0045986213,0.04468213,-0.05439136,-0.0075058625,0.030598303,0.08809225,-0.05124427,-0.023684226,-0.015971461,0.025352443,-0.037155256,-0.08897258,0.10100685,-0.013312547,0.08186981,0.006855401,0.003004869,0.023889141,0.049942814,0.0017972053,0.07566012,-0.0077473526,-0.010779493,-0.036823705,0.010644729,-0.002037198,-0.056390766,0.054910414,0.0055791135,-0.012518661,0.017282689,-0.09191541,0.012176887,0.06572641,-0.05418323,-0.09368301,-0.003720604,-0.107368045,0.0843934,-0.002961842,0.042683642,0.01901275,0.016568728,-0.0031318602,0.026225302,-0.13105778,4.684668e-33,0.04862894,-0.022875361,0.009987233,0.036744576,0.000526095,-0.017549772,-0.044171702,0.012209522,0.07865787,0.022369923,-0.0067916405,-0.07658748,0.01918729,0.011826541,0.08275499,0.11394526,-0.059627216,0.026806597,0.011101889,0.049055688,0.06823784,-0.10404241,0.029626193,0.0024826718,0.0115574915,0.040401027,-0.029104635,0.14005461,-0.06327892,-0.01212044,-0.03345776,0.04179248,0.027611814,0.073887475,-0.02789017,-0.01848555,0.01987872,-0.010219116,0.010388969,0.059896998,-0.024649732,-0.0044311597,0.04498749,0.03476686,0.0022812793,-0.07175198,-0.03060779,-0.07490657,-0.07444533,0.019609002,-0.07857234,0.07905466,-0.040101845,-0.015361517,-0.10816381,0.12644075,0.01860928,-0.03082828,-0.09167205,-0.003745027,0.008980196,-0.062008187,0.009039471,-0.004738605,-0.044085283,0.047683522,-6.9300054e-06,-0.02978745,0.051266436,0.004147919,-0.0417838,-0.053124156,-0.094224624,0.044461634,0.040911466,-0.04733987,0.047101773,0.042418078,0.037337765,-0.06427716,0.034275077,-0.030888936,0.022988956,-0.09612223,-0.040011823,-0.042364974,0.03363068,-0.03319922,0.051686864,-0.052707337,-0.04044526,-0.0347273,-0.059935078,-0.042877927,-0.0015791618,-4.8530273e-33,-0.019083222,0.079259254,-0.005417761,0.09263952,-0.049704384,-0.004669981,0.05478327,-0.08028497,-0.041892026,-0.02014393,0.01489849,-0.004918022,0.021971632,0.010337904,-0.017731844,-0.05880216,0.013161419,-0.03234654,-0.03752806,-0.05453312,-0.022711562,0.047520697,0.009370076,-0.054333393,-0.025106462,0.005894724,-0.118612975,0.037159245,0.03866118,-0.03948086,0.011355286,0.0114547,0.008718344,-0.03685431,0.001515926,0.052608613,0.042993955,-0.02813174,0.10183396,0.06005966,0.028180001,0.05927873,-0.11778564,0.028274281,0.0073786024,0.0016254,0.007918743,-0.04243829,0.031439427,-0.06794345,0.036673564,-0.003344047,0.05498071,0.02798673,-0.023828652,-0.045085955,-0.029908199,0.05797841,-0.035614293,0.03824433,-0.00180298,0.04357696,-0.041453976,0.015818262,-0.09317671,0.020776963,-0.025647221,0.018964836,0.09575829,0.064524144,-0.05139756,-0.035799954,0.01833704,0.006518372,0.0422872,0.04289918,-0.07043842,-0.006397204,-0.024426013,0.02377199,-0.0967261,-0.03762399,-0.06846901,-0.0072052907,0.037558604,0.02048944,0.023388622,0.070994206,-0.034238394,-0.010471442,-0.030600183,-0.01301607,-0.0935093,0.032283563,0.035083678,-6.448496e-08,-0.044550955,-0.022317622,0.08129729,-0.020554243,0.058501814,-0.009291057,0.024608873,0.04422271,-0.03956147,0.034063466,0.03677016,-0.009603797,-0.08549645,-0.018022902,0.03899224,0.049175616,0.019225268,0.056345444,-0.012869555,0.051764425,-0.039216794,-0.015399005,-0.043499388,0.0074583916,0.15766656,-0.03651148,-0.017898578,0.08112584,-0.06881345,0.021522595,0.033642404,-0.031219158,0.052621014,0.036366377,0.028544487,0.015397565,0.1310066,-0.04368349,-0.027095046,-0.0019779778,-0.018494511,0.008598775,0.024487149,0.018575853,0.02543743,0.0077243713,0.01265163,0.04842871,-0.074822605,-0.1589219,-0.00027218374,0.0041842265,-0.10659815,-0.027945008,-0.0038295905,0.07600995,-0.054435313,0.03710853,-0.13551432,-0.012244839,-0.031125508,-0.054729328,-0.004524182,-0.06527978,8,-4.610468,3.504445,5
565,"we learnt why its important to check for multi-collinearity. if there is multi collinearity among the variables then our model becomes unstable and it starts giving different answers when the model is run. next we learnt about splitting the sample data into train and test sets. usually the split is 80% and 20% but if the sample size is less we can go for 90-10 split. and there is hard rule for the splits it depends on the data size we have. to compare trained model on the test data we can use r squared. if the values are quite close then we can say the model is good but if starts diverging from each other it might be a case of overfitting on the train data and in this case we should think of some other simpler models. multiple r is the square root of r squared and kind of gives the corelation between the y and set of x. usually when the number of variables are more the r^2 value might increase because it can explain more variance ,therefore its good to have appropriate and less variables. we can use rmse to compare the results of train and test data results. p-value is used for parametrized model and not for unparametrized models like random forests and therefore its important to have performance metrics like rmse, r^2 and f stat which can be used to compare. we used sklearn library to make multilinear regression line and got to know there are not many performance metrices available therefore started with stats model and learnt some new metrices like omnibus ,jarque bera test which tell us how the residuals are distributed that is whether they are close to the normal distribution or not. and learnt about a new plot that is quantile-quantile plot from which we can visually get to know how close the residual distribution is to the normal distribution.",0.021465994,-0.10255602,-0.035167858,0.054192174,0.09587654,0.04317113,-0.038515106,0.058273513,0.06649193,0.010839977,-0.037410744,0.06493544,-0.06616236,0.09825206,0.007486319,-0.04698542,-0.013700474,-0.025781792,-0.08584207,-0.052090865,0.069872186,-0.07735972,-0.043792143,0.08252962,-0.08440029,0.026933325,-0.04011383,-0.064734966,-0.059823297,-0.008416141,0.03092845,-0.013580059,-0.029657045,-0.01759788,-0.02950703,0.005517635,0.06065964,0.029565262,0.04586238,0.062071472,-0.047457688,-0.062016442,0.06386161,0.0005164789,0.061248343,-0.0044515077,-0.06871402,-0.035993528,0.03712757,-0.018974481,-0.033664677,-0.045965843,-0.04680914,-0.025642388,0.0056269737,-0.052186504,-0.026855962,-0.006266347,0.009850983,0.0070388606,-0.028261287,0.0458671,-0.017783131,-0.011960893,0.026671289,-0.019629285,-0.084100656,-0.03210422,-0.03411006,0.007214144,0.017021839,-0.0270528,-0.051973075,-0.040944804,0.007284209,0.059476804,0.0055968477,-0.025635753,-0.02052772,-0.035956964,0.034474496,0.052576166,-0.033163603,-0.13725165,0.059380654,0.012113192,0.051536765,-0.05700477,-0.02377336,0.012385968,0.13311574,0.077001974,-0.015685773,0.07931831,0.0700144,0.11079982,-0.017514534,-0.021555737,0.022704858,0.040344678,0.056573607,0.012893829,0.037129533,0.06109441,0.019628413,-0.023963245,0.06989662,-0.06390756,0.058297362,-0.02596415,-0.000662966,0.056262627,-0.035995636,0.052456137,0.05328137,-0.0011945919,-0.09588481,0.049608294,-0.035572648,0.060491845,-0.1277002,-0.02174188,0.07302667,0.0437292,0.07652733,-0.006995904,-0.053284787,8.718194e-33,0.00214385,0.05901461,0.048253477,0.024346521,-0.052213516,-0.073016405,0.011659253,0.021995375,0.047149744,0.019888248,-0.05558121,-0.014965739,-0.013399931,0.0067802942,0.017921349,0.04823809,-0.05981504,-0.007945464,-0.020603137,0.047126967,0.072528735,-0.06366583,-0.007652073,-0.00965044,0.007783617,-0.054160725,-0.02354333,0.068442106,-0.0950107,0.015257533,-0.057959568,0.0331503,-0.047895186,0.08689386,-0.038648583,-0.055565964,-0.064282775,0.020078035,-0.025888117,-0.031169644,-0.028469473,-0.0545776,0.02669632,-0.008655588,0.04023732,0.0057404153,0.038228147,0.0045397975,-0.112982236,0.08679865,-0.013191841,0.026764259,-0.025134455,0.047294162,-0.089488424,0.08504008,0.0266688,-0.020164527,-0.020209925,0.0027867798,0.025260715,-0.042174604,0.01212987,-0.067703284,-0.038133077,0.016151922,-0.034444746,-0.035154514,0.072743624,0.056459256,-0.07376836,-0.12197003,-0.039788015,-0.039225154,0.07631757,-0.07518963,0.01835522,0.122992516,0.04576927,-0.0029264553,-0.050659787,0.0802908,0.04695054,-0.08854615,0.0014470882,-0.028298376,-0.022125803,-0.080389686,-0.016521398,0.055566095,-0.008038552,0.06365955,-0.015695011,-0.010120005,0.11041799,-8.4033586e-33,0.046669163,0.016391799,0.013040671,0.072473384,-0.08169312,-0.084687814,0.012314089,-0.010959621,-0.048924234,-0.07451286,0.049918093,-0.004217248,0.037711073,0.0072218548,0.00535251,-0.031666752,0.009281834,0.025830781,0.095865175,0.00282241,0.0020682963,0.03847352,-0.025560241,-0.07397217,-0.022953294,-0.012489712,-0.047676034,-0.09174641,0.031001851,-0.008046074,0.03904336,0.070982374,0.0006235941,0.044058457,-0.040563967,-0.018038644,-0.05360801,-0.062275056,0.03252343,0.12241972,0.039776854,0.08668696,0.015670277,0.046678286,-0.00977279,0.015508876,0.031600777,0.00066240254,0.04490359,0.06094036,0.030332418,-0.045870874,-0.015109855,-0.0034803012,0.06054434,-0.030476566,-0.04967196,0.0028013806,-0.044723745,-0.038187258,-0.017692072,0.05811383,-0.058310203,0.08198547,0.006404501,0.021995062,-0.005345794,0.008248759,0.114449345,0.04551668,-0.018007172,-0.032440167,0.05013819,0.0104949735,-0.057851803,0.017077213,-0.024008965,-0.032308653,-0.012055057,0.07443821,-0.03975106,0.0011557976,-0.07456333,0.014677505,-0.11730818,0.057309207,0.09231776,0.0011505946,-0.07843954,-0.089106224,0.025321538,0.021330558,0.021904493,-0.013689202,-0.01807801,-6.851248e-08,-0.06682077,-0.013924634,-0.007102772,-0.02431661,-0.0017395751,-0.084817976,-0.03115816,0.07312525,0.019307556,0.07914419,0.060652208,-0.041784022,-0.12715518,0.0377376,0.04006821,-0.01899047,-0.079728015,0.06454109,-0.0077194944,0.082893334,-0.003597716,-0.049672935,-0.0019310646,0.056502532,0.09945013,0.013578497,0.05675264,0.09442239,-0.019770415,-0.011462464,0.073580585,-0.018733203,0.020981522,0.041740876,0.041220747,0.10266866,-0.021856,-0.00786317,0.06309205,0.04463389,-0.010278703,0.036482174,-0.016384868,-0.02595224,0.07734034,0.018873679,0.027850801,0.04485838,-0.05565425,-0.124740854,-0.086687066,0.02709765,-0.064122766,-0.022649724,0.028343825,0.01338399,-0.010994688,0.047791734,-0.024458082,0.0065742936,-0.05939981,-0.08760569,-0.032856498,-0.039102886,8,-4.0599117,0.43656826,5
606,"in today's session we discussed briefly about closed form solutions for mlr where we got to know that these solutions exist but we don't use them because of the large size of the matrices and computing the inverses of those matrices would consume too much time and computational power and also because multicollinearity exists between two or more variables. when we want to generalise our model for the entire population but we dont have access to entire population's data so the sample we have is divided into train and test data. depending upon the size of the sample we can choose different proportions of train and test data. some parameters used for training data might not be used for the test data like the ci and other error metrics like sse and tse. we use mostly r squared value and mae or rmse to decide if our model is working good enough on the unseen(test data). an overfit model fails to generalise for unseen data and hence lacks the ability to perform good on the test data. adjusted r^2 value is used for mlr to see if our r^2 value is increasing significantly when we add a new variable and that helps us to decide how many independent variables we would want to have in our data. also the term linear regression does not always result in a straight line, it just means that the dependent variable would be a linear combination of all the independent variables. also we later jumped into python and see how to code for a lr problem using different libraries like pandas, matplotlib, sklearn etc and their functions. for a problem we have we should always try to solve the problem with different models and decide which one's better in terms of the less error values and more accurate results.",-0.052072134,-0.08109538,-0.01371331,0.07155152,0.069736026,0.024958616,-0.07598422,0.105144635,0.020502238,0.04418257,0.011132644,0.03906377,-0.08918933,-0.013267987,0.01156605,-0.06621255,0.016392041,0.033936974,-0.06595832,-0.060066313,0.025901917,0.018566329,-0.051966067,0.08624916,-0.05947134,-0.07929221,-0.04550349,-0.009835404,-0.03337402,0.038252633,0.096152924,0.042857226,-0.0020827807,-0.028526234,-0.12451682,-0.03748951,0.03882298,0.068785414,0.03146379,0.036138024,-0.015898326,-0.0014334308,0.035536643,0.019580131,0.024689035,-0.04538304,-0.041018665,-0.10550082,-0.03988132,0.076323025,-0.06291732,-0.0069407574,-0.0069011347,-0.037192177,0.06351517,-0.113335505,-0.054083545,-0.027163148,-0.004154337,-0.02558925,-0.02857692,-0.069216445,-0.058765117,-0.052696757,-0.0079106735,-0.0032237326,-0.03927251,-0.029304126,0.016350543,-0.018695593,-0.07960626,0.009215251,-0.0780125,0.030359551,-0.0021555847,0.01576953,-0.051069,0.004287109,0.02753804,0.030155752,0.07947324,0.04796811,0.019166375,-0.04970961,0.03515416,0.013948102,0.05910685,0.002578447,0.08501986,-0.008136537,0.093200386,0.01928494,-0.066519774,0.04147556,0.015923396,0.006756365,0.030143816,0.015099546,0.023304814,0.016609086,0.009095419,0.031521346,0.022158323,0.027165402,-0.022021553,-0.061397098,0.038833093,-0.025014749,0.093779825,-0.04561639,0.056557566,0.011951931,-0.04166817,0.032853063,0.036877975,-0.020923555,-0.037595883,-0.00869308,0.04204908,0.0538692,-0.028596014,0.012754726,0.06356382,0.010993925,0.056552846,0.061081145,-0.13380088,7.847952e-33,-0.0887452,0.07968301,0.042456716,0.09479859,-0.05726174,-0.02320321,-0.016458817,0.01535351,0.09441079,0.030945022,-0.00096551294,0.036728323,-0.04405682,0.010977559,0.049788684,0.056688316,-0.039221052,0.0808098,-0.037048742,0.017448092,0.10205354,-0.06399041,0.03873978,-0.02715986,0.052749384,-0.05514372,0.014534689,0.041944563,0.005859941,-0.020804074,0.015198905,-0.002254466,0.015026919,0.12392666,-0.011859542,-0.016310582,-0.011025508,-0.009257986,0.029241534,0.041370273,0.005896963,0.02138757,0.053366315,0.021540463,0.018032115,-0.06473545,0.0112220505,-0.013075733,-0.05823372,0.047368668,-0.03969356,0.042165156,-0.09786227,-0.053943522,-0.020316597,0.14678013,-0.05239147,-0.009447727,-0.0073730997,0.0590813,-0.08652359,-0.084670916,0.093023665,-0.066329345,-0.0071690097,-0.03027193,-0.0034309023,-0.06475923,0.0636864,0.009045986,-0.025192412,-0.108304344,-0.041394334,-0.0004472646,0.017904822,-0.041172314,0.05673593,0.08160752,0.090849765,-0.05443283,0.025189772,0.0738395,-0.008477791,-0.054202173,-0.010985956,-0.10599352,-0.0436315,-0.027988406,-0.020129694,-0.06049917,-0.065024905,-0.0033219245,-0.08641631,-0.052617375,-0.044318445,-7.2995444e-33,0.008372324,-0.016841494,-0.020631945,0.07222678,-0.018972753,-0.06702274,0.049071223,-0.07543357,0.038132723,-0.109358154,0.08256539,-0.06655541,0.06774088,0.039553434,-0.0021812622,-0.016672358,-0.013246247,-0.018465502,-0.012307021,-0.051139038,-0.06275112,0.037388727,0.029895777,-0.040160928,-0.033306573,0.0066331327,-0.15975307,0.016800793,0.01895904,0.006323689,-0.009501563,-0.0027045722,0.028893802,-0.024500044,-0.01635571,0.05692652,0.039104518,-0.011716911,0.031645615,0.11458605,0.026152669,0.0909885,-0.04211181,0.054185737,0.010140608,0.022756292,-0.02007829,-0.057173967,0.060663983,-0.04438306,-0.0067236144,-0.05774852,0.016048547,0.031826533,-0.023314796,-0.021340568,-0.020085284,-0.0011689102,-0.044223536,-0.0048547345,0.044966087,0.031163508,-0.026757114,0.025241781,-0.07877446,-0.006262638,0.010950254,0.019361228,0.030247381,0.018509192,-0.038285237,-0.13608968,0.06568863,0.019862758,0.014960952,-0.0074515757,0.018459277,0.00012335423,0.014823446,-0.033811208,-0.10895843,-0.016302712,-0.020952359,0.016164418,0.022484642,0.039716594,0.0680714,0.033697966,-0.03867137,-0.05776601,-0.07154752,-0.009523574,-0.0032075807,0.07034713,0.006731145,-7.176174e-08,-0.024108438,-0.00052339135,0.040719427,-0.0502107,0.031133758,-0.069715716,-0.03613662,0.08415641,-0.020922447,0.100170754,0.060774192,-0.03707616,-0.057879735,-0.024856769,0.03506926,0.06374958,-0.04974819,0.06695214,-0.01438688,0.062394656,0.007622872,0.011301334,-0.030745314,-0.024899509,0.14323378,0.029348947,-0.04751479,0.059045788,-0.031179069,0.008525081,0.056144375,-0.012180814,0.11315315,0.07002983,0.043544605,0.04035487,0.054922093,0.005115834,-0.045271803,0.057370085,-0.030155916,0.047719046,-0.06609626,0.01402583,0.112033896,0.04196922,0.008885298,0.05774971,-0.10229906,-0.060180284,-0.033548087,0.0003718897,0.03630008,0.034263425,0.043320943,-0.0065520317,-0.06399938,0.09418404,-0.03175171,0.019424805,-0.039408337,-0.046739064,-0.07915637,-0.059755154,8,-2.3658853,2.195819,5
651,"in today's class first we started by stating the fact that when y is a function of a large number of variables then multicolinearity may be possible that is there can be some kind of interdependence between these values. then we came to a conclusion that beta value can be used as a closed form in theory but it is impractical for real life scenarios. free don't take test and training to be 50 50% ratio because this does not allow our model to check the whole data and give better variance. but if we have lower number of observations then we can take the ratio of training set to be 90% otherwise ideally the training set should be around 80%. tell me talked about overfit situation which will be very very good for the training data but it won't be able to fit the testing data and our model will not be generalised. sometimes we also introduce bias to make it better which is also called bias variance tradeoff. then we talked about multiple r from the data analysis in spreadsheet. we also talked about r square and that one degree of freedom should be minus one, as the adjusted r square value will drop if we add the variables. then we started with some basic python functions on the jupyter notebook .if errors are following normal distribution then in the qq plot the point should lie in the straight line. he also learned about omnibus statistics and omnibus p value and how we use them in the first exercise as a skewness. we also learned ols and came to the fact that lower the value of aic and bic better is the model.",-0.058725722,-0.05355391,-0.063328244,0.051054936,0.07614998,0.04748808,0.013797433,0.07204461,-0.0037579935,0.024475802,-0.07668538,0.13629827,-0.042951316,-0.011670015,0.033563055,-0.02345375,0.017806428,0.015120817,-0.1339416,-0.014062391,0.0048155924,-0.050455958,0.03929513,0.09352058,-0.042648934,-0.048319794,-0.0505383,-0.03489482,-0.041484773,-0.011491966,0.057566814,0.03640079,0.012489661,-0.026967483,-0.105648786,-0.0291056,-0.012063801,0.047791205,0.04277975,0.10130525,-0.040414296,0.0015194542,-0.023913698,0.083532326,0.07110372,-0.040038027,-0.0005509834,-0.06280338,0.05465765,-0.013645012,-0.09514651,-0.03816157,-0.04783184,0.024154318,-0.007203905,-0.09233321,-0.05326249,-0.04185367,-0.039944857,-0.004019653,-0.032459553,-0.054713793,-0.04483119,-0.008990028,0.0019474956,-0.01801107,-0.09736926,0.040032685,0.019309578,0.0015019171,-0.045086075,0.082473144,-0.05728949,0.00506829,0.06444827,0.018274564,-0.0006165279,0.07059831,0.0016367701,0.04009086,0.034998816,0.018583937,0.06526988,-0.04930917,0.03273676,-0.0005172848,0.024112208,0.0429018,0.04657504,0.030772518,0.066177845,0.027574668,-0.013185129,0.029736305,0.03334229,0.061175838,-0.012435242,-0.021420848,-0.017207121,0.04110334,-0.028049255,0.033446167,0.04104286,0.0008441716,0.008344016,-0.021340681,0.04735053,-0.02833793,0.051745374,-0.07151777,-0.002214203,-0.034715805,-0.042769343,0.045379706,0.0041743363,0.03153651,-0.06890924,0.0070388904,-0.049388543,0.05697933,-0.024847327,-0.025386313,0.027843224,0.018068155,-0.023332506,0.039891746,-0.14328541,1.0017767e-32,0.0057800524,0.076163486,-0.010349657,0.060002178,-0.009564881,-0.034052975,-0.029715274,0.030599184,0.055155292,0.05143234,0.015907634,0.0017895581,0.0019840654,0.02658048,0.06730944,0.0028893298,-0.06574372,0.073861144,0.039499063,0.04845853,0.030513844,-0.050561033,-0.0054929163,-0.084315695,0.047259968,-0.025609428,0.0023789676,0.060305256,-0.03586768,0.011596833,-0.016515423,0.059480906,0.018797992,0.09069706,-0.013394154,-0.019482365,-0.068404205,0.043945137,0.03715817,0.022704732,-0.03833895,0.02544156,0.033001203,0.011705362,0.039399818,-0.062465314,0.123366766,-0.05237353,-0.11230791,-0.004734654,-0.08265214,0.022952229,-0.065441765,-0.03630715,-0.07233191,0.16984324,-0.01780435,-0.0021608267,-0.05442504,0.039823633,-0.08245253,-0.027780287,0.031626143,-0.043346237,-0.10878582,-0.044880513,-0.044716027,-0.08443457,0.040838715,-0.014970226,0.037631344,-0.0445747,-0.10257282,-0.019822473,0.017174853,0.055407695,0.09752265,0.041163303,0.03417501,-0.051161785,0.013374294,0.008986003,0.05655679,-0.08530339,-0.008451267,-0.016973073,0.053326488,0.0006560106,-0.014252503,0.014752527,-0.085741535,-0.028148074,-0.0820178,-0.0718424,0.010772028,-8.874603e-33,-0.06752442,-0.008937487,-0.10208758,0.077516794,-0.006046627,-0.0064385915,0.018990092,-0.054551434,0.0077154534,-0.021232616,-0.013687125,0.02104336,0.034333322,0.0027131378,-0.07118201,-0.09623305,-0.0019245613,-0.0033546677,-0.00049825426,-0.037279893,0.041417934,0.052013423,0.0016870557,-0.020753456,-0.03798668,0.029867528,-0.17191605,0.052408166,0.060786426,0.030786851,-0.047116596,-0.010084417,0.05002402,-0.032075394,-0.022651969,-0.0012267522,0.05162529,0.027279126,0.0878038,0.105366535,0.06367563,-0.0062432447,-0.1297836,0.017909765,0.08666482,-0.005993022,0.014346471,-0.011839102,0.08242046,-0.042621497,-0.03649446,-0.058287285,0.021412954,0.05541702,0.01872665,-0.040108513,-0.040894978,0.010297878,-0.011910341,-0.0011368097,0.055913527,0.089650035,-0.031489443,-0.00037734734,-0.010143871,0.0018111264,-0.009039578,0.036173828,0.06306598,0.026862144,-0.02687547,-0.023349874,0.008937793,0.018958034,-0.006095161,0.042050906,-0.03343532,-0.09853068,-0.012055954,0.06457189,-0.08557577,0.01909891,-0.079057395,0.008931741,0.04529009,0.08813127,0.0646642,0.09092423,-0.009396682,-0.045632973,-0.00975258,0.06523781,-0.038815353,0.03193838,-0.008074366,-7.232499e-08,0.018018693,-0.031285804,0.09432825,0.023677131,0.031318597,-0.07560728,0.028316284,0.013951716,-0.02097592,0.11723384,-0.009542984,-0.005089557,-0.05195019,0.042616982,0.008460544,0.03860542,-0.005686138,0.042930137,-0.048442204,0.08692456,0.07012108,-0.020063791,0.022263933,0.0008733471,0.12745304,-0.09635059,0.014843706,0.038638916,-0.06576306,0.005474709,0.004243763,0.017747102,0.029210905,-0.0040318817,0.050642945,0.021859847,0.068560556,0.01191769,-0.0148765035,0.0077650403,-0.06067997,-0.0054486357,0.02815672,0.013321785,0.066684924,-0.0077470248,-0.03412795,0.044273768,-0.04984225,-0.12924571,0.05486972,0.051136304,-0.05832206,0.003143732,0.0036500937,0.07623157,-0.049490705,0.02774356,-0.08132703,0.023967981,-0.059318326,-0.06767961,-0.09051802,-0.06931435,8,-3.7903874,3.5678065,5
661,"the discussion on closed form solution of mlr was revised , it was not ideal as we had to deal with matrix inversion and occurrence of multi collinearity. p-value<= .05 , feature is omitted as it doesn't contribute much. r2 multiple r and the reason behind n-1 degree of freedom in tss was discussed. training and testing data was discussed , as we want model to represent population. the issue of overfit can occur if r2 values for training and testing matrices are not close enough. ",-0.06699721,-0.059606206,-0.013430808,0.02397508,0.07537216,0.006533281,-0.07050793,-0.014064286,-0.030737432,0.046085596,0.07571976,0.014610046,-0.044560306,-0.00020344199,-0.0060800426,0.006821592,-0.039968047,-0.025778677,-0.0077177766,-0.058288205,-0.06376389,0.034026403,-0.05800028,0.061552145,-0.035087887,-0.016808653,0.040489964,-0.008355147,0.0071597467,0.020795822,0.09087336,0.09668197,0.0043921834,-0.07785587,-0.06821196,-0.032415893,-0.024734132,0.057424825,0.028908242,0.08082945,0.015470801,-0.036548086,0.038798917,-0.029730825,0.07338816,-0.004124605,-0.06348336,-0.07269352,0.0220423,0.037321083,-0.05663913,0.0031757732,-0.0567007,-0.06351732,0.006254983,-0.1688285,-0.08374082,-0.1143204,-0.051095102,-0.0006449973,-0.014611795,-0.0015525641,-0.04463686,-0.027121715,0.098595485,0.07081074,-0.114075705,-0.036030356,0.0027414314,0.028186236,-0.111850776,0.0050438596,-0.05542436,-0.022356728,-0.034829967,-6.0857124e-05,-0.0023787562,-0.028832952,0.049345084,0.062124103,0.09550197,-0.0052837688,-0.041113123,-0.04613572,0.0048470558,-0.027404653,0.0008021251,-0.046137437,0.05711695,0.010024772,0.12583354,0.01561886,0.005918291,0.035460677,-0.051601257,-0.03922098,0.0135175,0.0502653,0.057669256,0.01713174,-0.002650875,-0.0095560765,0.017449878,0.050945118,-0.13505398,-0.01891063,0.07510496,-0.0074572377,0.09000482,-0.04621263,0.03891362,0.033664383,-0.012113722,0.025136981,-0.04469907,-0.056051698,-0.03349109,0.028527893,0.067736186,0.044559654,-0.050644618,-0.024522057,0.04743064,-0.011248147,0.039443173,-0.022551378,-0.09357991,7.841837e-33,-0.084821455,0.067098446,0.056392267,0.046967544,0.025784323,0.0007748235,-0.012107466,-0.053434603,0.054268654,0.0057513327,-0.0017889686,-0.045640998,-0.014197599,-0.04771159,-0.018410454,0.02643905,0.022217961,0.08892709,-0.024973579,0.011179441,0.09613979,0.0017050215,-0.02454425,-0.09244204,0.0065855244,-0.017848331,0.02676655,-0.052821305,-0.026539927,-0.008342864,0.021492228,0.05396794,-0.037547134,0.1084151,-0.021095,0.012467455,-0.0042857877,-0.022437422,0.060299546,-0.0015570555,-0.01922163,0.039906893,0.047467314,0.0035082954,0.011131939,0.003206052,0.052766867,-0.010458344,-0.06572727,0.016810995,-0.032898217,-0.008227023,-0.13766748,-0.033223037,0.057642683,0.11492754,-0.07012573,0.014174723,-0.0068329126,0.08747669,-0.035565056,-0.015164985,0.038775794,-0.13832894,0.049931806,-0.03833836,0.00094117713,-0.08065134,0.09650055,0.07771311,0.027458193,-0.035173893,-0.009204617,-0.0477592,0.02240217,-0.016639119,0.07364313,0.063635,0.10016668,-0.018187948,-0.010512824,0.084997706,-0.00042242114,-0.014041772,0.03294102,-0.05907883,-0.028326184,-0.06959181,-0.040317394,0.018693104,-0.019104583,-0.020495098,-0.10114448,-0.019527072,0.033837263,-6.598077e-33,-0.0563051,0.021517524,0.0022559788,0.017230008,-0.049597926,-0.14597067,0.043891083,-0.016489837,0.022523535,0.0064368625,0.09798094,-0.07164667,0.03600562,0.013460908,0.045615416,0.011349199,0.0017286475,0.025597312,-0.011841347,0.0023200614,-0.017789919,0.06775477,0.014142615,0.007926752,-0.052571032,0.016700333,-0.08640357,0.03996521,0.053676732,0.0062308027,-0.03337093,-0.028798658,0.029966421,-0.0033557252,0.005916079,0.0034201331,-0.0029936444,-0.024641514,0.047626812,0.059690874,0.02909626,0.07388244,-0.063256785,-0.0056461366,0.013841902,-0.040718578,0.044703882,-0.00958482,0.10616914,-0.08918589,-0.08562598,-0.058978032,-0.0058633275,0.067899734,0.027336149,0.0407739,0.007640724,-0.011040996,0.014244901,-0.024839101,0.03428757,0.0071230084,0.0015053307,0.0162063,0.0696864,0.012679236,0.07641345,0.014525676,0.0058964794,0.046184808,0.0008300411,-0.07143613,0.11053894,0.0018881954,0.033244733,-0.041793335,-0.038047902,0.0018863962,0.010879294,-0.066030025,-0.085009396,-0.0375298,0.0043508857,0.030319167,0.011956029,-0.009796394,0.12617339,-0.06064714,-0.016604153,-0.046130795,0.008433916,0.011016662,0.06665703,0.018821647,-0.0043376624,-6.1100316e-08,-0.10882346,-0.03133803,-0.021596063,-0.027355501,0.027385512,0.012724851,-0.04431746,0.021177568,-0.008385863,0.09965336,0.031894855,-0.057716176,-0.05762801,0.021864753,-0.0021398817,0.035335578,-0.036457498,0.05627151,-0.019579304,0.0097696725,0.0353467,0.014046246,0.01413791,-0.016523257,0.05109577,0.021463722,-0.020832395,0.03770151,0.086767375,-0.023094887,-0.01580062,-0.045755908,0.07973386,0.06266373,0.0888441,0.018918177,0.006138415,-0.023521082,-0.0037630193,0.033739094,-0.038662877,0.082961544,-0.08469724,0.041602865,0.1421073,0.08795189,-0.050652422,-0.03999612,-0.11026121,-0.073166445,0.009701539,0.010082141,0.02271101,0.098873645,0.09979608,-0.0052573746,-0.016582312,0.075812556,-0.013480108,-0.0044028386,-0.0035647845,0.0042369594,-0.09814818,-0.0834268,8,-0.3820383,2.7780406,5
